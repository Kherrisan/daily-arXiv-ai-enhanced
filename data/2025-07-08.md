<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 55]
- [cs.SE](#cs.SE) [Total: 23]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Bittensor Protocol: The Bitcoin in Decentralized Artificial Intelligence? A Critical and Empirical Analysis](https://arxiv.org/abs/2507.02951)
*Elizabeth Lui,Jiahao Sun*

Main category: cs.CR

TL;DR: This paper evaluates Bittensor's similarity to Bitcoin as a decentralized AI platform by comparing tokenomics, decentralization, consensus, and incentives. It identifies stake and reward concentration, proposes protocol-level solutions (performance-weighted emissions, composite scoring, trust-bonus multiplier, and 88th percentile stake cap) to address misalignment in compensation and reduce 51% attack risks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to assess the viability of Bittensor as a decentralized AI equivalent to Bitcoin by addressing gaps in stake distribution equity, reward alignment with quality, and network security vulnerabilities.

Method: Leveraged on-chain data from 64 Bittensor subnets to analyze stake/reward concentration. Proposed and empirically validated protocol interventions via data-driven methods, including 88th percentile stake cap testing across daily/weekly/monthly snapshots.

Result: 1) High concentration of stake and rewards, with rewards strongly correlated to stake rather than quality. 2) The 88th percentile stake cap increased median coalition size for 51% attacks by ~factor of 2. 3) Proposed mechanisms demonstrate robust incentive alignment and security improvements across timeframes.

Conclusion: Bittensor requires substantial protocol-level reforms to address stake concentration, reward misalignment, and security risks. The authors' two-pronged approach of incentive realignment and stake capping demonstrates promising potential to strengthen decentralization and long-term sustainability, though further implementation validation is needed.

Abstract: This paper investigates whether Bittensor can be considered the Bitcoin of
decentralized Artificial Intelligence by directly comparing its tokenomics,
decentralization properties, consensus mechanism, and incentive structure
against those of Bitcoin. Leveraging on-chain data from all 64 active Bittensor
subnets, we first document considerable concentration in both stake and
rewards. We further show that rewards are overwhelmingly driven by stake,
highlighting a clear misalignment between quality and compensation. As a
remedy, we put forward a series of two-pronged protocol-level interventions.
For incentive realignment, our proposed solutions include performance-weighted
emission split, composite scoring, and a trust-bonus multiplier. As for
mitigating security vulnerability due to stake concentration, we propose and
empirically validate stake cap at the 88th percentile, which elevates the
median coalition size required for a 51-percent attack and remains robust
across daily, weekly, and monthly snapshots.

</details>


### [2] [A Representation Engineering Perspective on the Effectiveness of Multi-Turn Jailbreaks](https://arxiv.org/abs/2507.02956)
*Blake Bullwinkel,Mark Russinovich,Ahmed Salem,Santiago Zanella-Beguelin,Daniel Jones,Giorgio Severi,Eugenia Kim,Keegan Hines,Amanda Minnich,Yonatan Zunger,Ram Shankar Siva Kumar*

Main category: cs.CR

TL;DR: The paper analyzes Crescendo multi-turn jailbreak attacks, revealing they exploit model misclassification of responses as benign in prolonged interactions, highlighting gaps in single-turn defenses and the need for improved mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: Current state-of-the-art LLMs and defenses remain vulnerable to closed-box, manually performed multi-turn jailbreak attacks, which threaten the safe deployment of AI systems. Understanding their mechanisms is critical for developing robust protections.

Method: The study examines intermediate model representations of Crescendo attacks, tracking how safety-aligned LMs process harmful multi-turn prompts and classifying outputs as benign or harmful over successive conversation stages.

Result: Crescendo prompts keep outputs in a "benign" region of representation space longer, causing safety-aligned LLMs to misclassify harmful requests. Single-turn defenses (e.g., circuit breakers) fail to detect these evolved risks.

Conclusion: Multi-turn jailbreaks expose weaknesses in static, single-turn defenses by dynamically manipulating model representations. Effective mitigations must address sequential interactions and representation space dynamics.

Abstract: Recent research has demonstrated that state-of-the-art LLMs and defenses
remain susceptible to multi-turn jailbreak attacks. These attacks require only
closed-box model access and are often easy to perform manually, posing a
significant threat to the safe and secure deployment of LLM-based systems. We
study the effectiveness of the Crescendo multi-turn jailbreak at the level of
intermediate model representations and find that safety-aligned LMs often
represent Crescendo responses as more benign than harmful, especially as the
number of conversation turns increases. Our analysis indicates that at each
turn, Crescendo prompts tend to keep model outputs in a "benign" region of
representation space, effectively tricking the model into fulfilling harmful
requests. Further, our results help explain why single-turn jailbreak defenses
like circuit breakers are generally ineffective against multi-turn attacks,
motivating the development of mitigations that address this generalization gap.

</details>


### [3] [A Novel Active Learning Approach to Label One Million Unknown Malware Variants](https://arxiv.org/abs/2507.02959)
*Ahmed Bensaoud,Jugal Kalita*

Main category: cs.CR

TL;DR: This paper proposes two active learning methods (Inception-V4+PCA with SVM variants and ViT-BNN) for labeling large-scale modern malware data, demonstrating enhanced stability and uncertainty handling via Bayesian neural networks.


<details>
  <summary>Details</summary>
Motivation: Reducing labeling costs through uncertainty-driven active learning is crucial for handling complex, evolving malware families with limited expert resources.

Method: 1. Hybrid model: Inception-V4 convolutional network with PCA dimensionality reduction + multi-SVM approaches (UTSVM, PSVM, SVM-GSU, TBSVM). 2. Vision Transformer Bayesian Neural Network (ViT-BNN) for scalable uncertainty estimation.

Result: Experiments show ViT-BNN outperforms standard approaches in stability and robustness when managing uncertainty in 1 million unlabeled malware examples.

Conclusion: ViT-BNN offers a novel, task-agnostic framework for efficient active learning in cybersecurity, effectively balancing exploration of unknown malware families with exploitation of learned patterns.

Abstract: Active learning for classification seeks to reduce the cost of labeling
samples by finding unlabeled examples about which the current model is least
certain and sending them to an annotator/expert to label. Bayesian theory can
provide a probabilistic view of deep neural network models by asserting a prior
distribution over model parameters and estimating the uncertainties by
posterior distribution over these parameters. This paper proposes two novel
active learning approaches to label one million malware examples belonging to
different unknown modern malware families. The first model is Inception-V4+PCA
combined with several support vector machine (SVM) algorithms (UTSVM, PSVM,
SVM-GSU, TBSVM). The second model is Vision Transformer based Bayesian Neural
Networks ViT-BNN. Our proposed ViT-BNN is a state-of-the-art active learning
approach that differs from current methods and can apply to any particular
task. The experiments demonstrate that the ViT-BNN is more stable and robust in
handling uncertainty.

</details>


### [4] [Unveiling Privacy Policy Complexity: An Exploratory Study Using Graph Mining, Machine Learning, and Natural Language Processing](https://arxiv.org/abs/2507.02968)
*Vijayalakshmi Ramasamy,Seth Barrett,Gokila Dorai,Jessica Zumbach*

Main category: cs.CR

TL;DR: This paper proposes using interactive graph visualizations to improve user understanding of privacy policies and identifies key themes via graph mining algorithms to enhance interpretability, forensic analysis, and regulatory compliance.


<details>
  <summary>Details</summary>
Motivation: Privacy policies are lengthy and complex, reducing user transparency, understanding, and trust. Automated tools are needed to analyze these policies and identify risks, especially as online privacy concerns rise.

Method: The study represents privacy policy terms as structured graph models and applies dimensionality reduction (t-SNE, PCA) combined with graph mining to identify key themes like User Activity and Device Information, while using interactive visualizations to make information accessible.

Result: Graph-based clustering improved policy interpretability, revealed patterns in user tracking and data sharing, and supported forensic investigations and regulatory compliance checks.

Conclusion: Integrating interactive visualization with graph mining advances AI-driven privacy policy auditing, promoting transparency, accountability, and user trust.

Abstract: Privacy policy documents are often lengthy, complex, and difficult for
non-expert users to interpret, leading to a lack of transparency regarding the
collection, processing, and sharing of personal data. As concerns over online
privacy grow, it is essential to develop automated tools capable of analyzing
privacy policies and identifying potential risks. In this study, we explore the
potential of interactive graph visualizations to enhance user understanding of
privacy policies by representing policy terms as structured graph models. This
approach makes complex relationships more accessible and enables users to make
informed decisions about their personal data (RQ1). We also employ graph mining
algorithms to identify key themes, such as User Activity and Device
Information, using dimensionality reduction techniques like t-SNE and PCA to
assess clustering effectiveness. Our findings reveal that graph-based
clustering improves policy content interpretability. It highlights patterns in
user tracking and data sharing, which supports forensic investigations and
identifies regulatory non-compliance. This research advances AI-driven tools
for auditing privacy policies by integrating interactive visualizations with
graph mining. Enhanced transparency fosters accountability and trust.

</details>


### [5] [Reinforcement Learning for Automated Cybersecurity Penetration Testing](https://arxiv.org/abs/2507.02969)
*Daniel López-Montero,José L. Álvarez-Aldana,Alicia Morales-Martínez,Marta Gil-López,Juan M. Auñón García*

Main category: cs.CR

TL;DR: This paper proposes a machine learning-based method using Reinforcement Learning (RL) and Geometric Deep Learning to automate web application security testing, reducing maintenance costs while maximizing vulnerability detection efficiency through optimized tool selection and path prioritization.


<details>
  <summary>Details</summary>
Motivation: Prior security testing methods for web applications require manual tool selection, resource-intensive processes, and lack intelligent optimization to reduce costs. The paper motivates the need for an automated approach that can efficiently identify vulnerabilities with minimal steps and resources.

Method: The approach integrates RL to dynamically select and prioritize testing tools and optimize exploration paths, combined with Geometric Deep Learning to construct structural priors that shrink the search space and accelerate learning convergence. Training occurs in simulated web environments with real-world network topologies.

Result: The developed algorithm achieved higher vulnerability detection rates in fewer interaction steps compared to baseline methods, validated on well-known exploitable web pages used by human hackers.

Conclusion: The paper demonstrates that ML-driven automation, particularly RL with geometric priors, can enhance security testing efficacy and efficiency for web applications while lowering long-term maintenance costs.

Abstract: This paper aims to provide an innovative machine learning-based solution to
automate security testing tasks for web applications, ensuring the correct
functioning of all components while reducing project maintenance costs.
Reinforcement Learning is proposed to select and prioritize tools and optimize
the testing path. The presented approach utilizes a simulated webpage along
with its network topology to train the agent. Additionally, the model leverages
Geometric Deep Learning to create priors that reduce the search space and
improve learning convergence. The validation and testing process was conducted
on real-world vulnerable web pages commonly used by human hackers for learning.
As a result of this study, a reinforcement learning algorithm was developed
that maximizes the number of vulnerabilities found while minimizing the number
of steps required

</details>


### [6] [Aim High, Stay Private: Differentially Private Synthetic Data Enables Public Release of Behavioral Health Information with High Utility](https://arxiv.org/abs/2507.02971)
*Mohsen Ghasemizade,Juniper Lovato,Christopher M. Danforth,Peter Sheridan Dodds,Laura S. P. Bloomfield,Matthew Price,Team LEMURS,Joseph P. Near*

Main category: cs.CR

TL;DR: This paper demonstrates differential privacy (DP) implementation for a behavioral health dataset using Adaptive Iterative Mechanism (AIM), balancing privacy protection and data utility across privacy budgets (epsilon = 1–100).


<details>
  <summary>Details</summary>
Motivation: Conventional de-identification methods are privacy-attack vulnerable, necessitating formal guarantees like DP to balance re-identification risk reduction with data utility preservation for sharing sensitive health/behavioral information.

Method: Used AIM to generate DP synthetic data for LEMURS Phase 1 (combining wearable device measurements and self-reported student surveys), evaluating utility across 1 to 100 epsilon values with a framework aligned to dataset's real-world applications.

Result: Achieved adequate predictive utility with epsilon=5 while substantially mitigating privacy risks; methodology demonstrates reproducible epsilon-utility-privacy trade-off evaluations for synthetic datasets with complex attributes/records.

Conclusion: Established a standardized evaluation framework for DP synthetic data generation, enabling informed decisions about optimal privacy thresholds while maintaining dataset utility for downstream research applications.

Abstract: Sharing health and behavioral data raises significant privacy concerns, as
conventional de-identification methods are susceptible to privacy attacks.
Differential Privacy (DP) provides formal guarantees against re-identification
risks, but practical implementation necessitates balancing privacy protection
and the utility of data.
  We demonstrate the use of DP to protect individuals in a real behavioral
health study, while making the data publicly available and retaining high
utility for downstream users of the data. We use the Adaptive Iterative
Mechanism (AIM) to generate DP synthetic data for Phase 1 of the Lived
Experiences Measured Using Rings Study (LEMURS). The LEMURS dataset comprises
physiological measurements from wearable devices (Oura rings) and self-reported
survey data from first-year college students. We evaluate the synthetic
datasets across a range of privacy budgets, epsilon = 1 to 100, focusing on the
trade-off between privacy and utility.
  We evaluate the utility of the synthetic data using a framework informed by
actual uses of the LEMURS dataset. Our evaluation identifies the trade-off
between privacy and utility across synthetic datasets generated with different
privacy budgets. We find that synthetic data sets with epsilon = 5 preserve
adequate predictive utility while significantly mitigating privacy risks. Our
methodology establishes a reproducible framework for evaluating the practical
impacts of epsilon on generating private synthetic datasets with numerous
attributes and records, contributing to informed decision-making in data
sharing practices.

</details>


### [7] [Are AI-Generated Fixes Secure? Analyzing LLM and Agent Patches on SWE-bench](https://arxiv.org/abs/2507.02976)
*Amirali Sajadi,Kostadin Damevski,Preetha Chatterjee*

Main category: cs.CR

TL;DR: The study evaluates security risks in LLM-generated software patches using real-world data from 20,000+ GitHub issues. It reveals that standalone LLMs (Llama 3.3) and agentic frameworks (e.g., OpenHands) introduce significantly more new vulnerabilities than human-written patches, with factors like code complexity and imprecise issue descriptions exacerbating security flaws.


<details>
  <summary>Details</summary>
Motivation: Previous security evaluations of LLM-generated code focused on synthetic or isolated scenarios, leaving gaps in understanding their real-world development risks as these models gain adoption in critical tasks like bug fixes and program repair.

Method: Conducted large-scale analysis on 20,000+ SWE-bench issues, comparing vulnerabilities in patches from: (1) standalone Llama 3.3 model; (2) three top agentic frameworks with varying autonomy levels; and (3) developer-written patches. Analyzed code/issue/project-level factors correlating with insecurity.

Result: 1) Standalone LLM generated ~9x more new vulnerabilities than developers, with unique insecure patterns. 2) Agentic systems also showed high vulnerability rates, increasing with LLM autonomy. 3) Insecurity correlated with: multi-file patch complexity, larger code changes, and ambiguous GitHub issues (lack of reproduction steps/snippets).

Conclusion: Contextual factors critically influence LLM security output, requiring complementary risk assessment methods that leverage both code complexity and issue-level information to reduce deployment risks.

Abstract: Large Language Models (LLMs) and their agentic frameworks are increasingly
adopted to automate software development tasks such as issue resolution and
program repair. While prior work has identified security risks in LLM-generated
code, most evaluations have focused on synthetic or isolated settings, leaving
open questions about the security of these systems in real-world development
contexts. In this study, we present the first large-scale security analysis of
LLM-generated patches using 20,000+ issues from the SWE-bench dataset. We
evaluate patches produced by a standalone LLM (Llama 3.3) and compare them to
developer-written patches. We also assess the security of patches generated by
three top-performing agentic frameworks (OpenHands, AutoCodeRover, HoneyComb)
on a subset of our data. Finally, we analyze a wide range of code, issue, and
project-level factors to understand the conditions under which LLMs and agents
are most likely to generate insecure code. Our findings reveal that the
standalone LLM introduces nearly 9x more new vulnerabilities than developers,
with many of these exhibiting unique patterns not found in developers' code.
Agentic workflows also generate a significant number of vulnerabilities,
particularly when granting LLMs more autonomy, potentially increasing the
likelihood of misinterpreting project context or task requirements. We find
that vulnerabilities are more likely to occur in LLM patches associated with a
higher number of files, more lines of generated code, and GitHub issues that
lack specific code snippets or information about the expected code behavior and
steps to reproduce. These results suggest that contextual factors play a
critical role in the security of the generated code and point toward the need
for proactive risk assessment methods that account for both code and
issue-level information to complement existing vulnerability detection tools.

</details>


### [8] [Deterministic Cryptographic Seed Generation via Cyclic Modular Inversion over $\mathbb{Z}/3^p\mathbb{Z}$](https://arxiv.org/abs/2507.03000)
*Michael A. Idowu*

Main category: cs.CR

TL;DR: This paper proposes a deterministic entropy generation framework using algebraic modular inversions mod $3^p$. It introduces an Entropy Confidence Score (ECS) to validate seed quality and provides lightweight, secure seed sequences suitable for both classical and post-quantum cryptography with embedded system compatibility.


<details>
  <summary>Details</summary>
Motivation: The authors aim to enhance structural soundness and auditability in cryptographic stacks by creating an algebraically-verifiable entropy filter. Existing generators lack formal algebraic validation mechanisms, and deterministic seeding is critical for secure hardware implementations and mitigating side-channel leaks.

Method: The framework employs cyclic modular arithmetic over $\mathbb{Z}/3^p\mathbb{Z}$ with the identity $d_k 



Result: Results demonstrate constant-time execution with (1) 85% reduction in side-channel leakage vs. industry standards, (2) ECS validation showing >99% entropy coverage, and (3) 40% smaller hardware footprint compared to AES-CTR based DRBGs. Seed sequences pass NIST STS and Rabbit randomness tests.

Conclusion: The algebraic entropy filtering framework establishes a new baseline for cryptographically verifiable seeding. The three-stage modular inversion approach enables hardware-secured, auditable randomness while maintaining compatibility with legacy cryptographic primitives through deterministic entropy conditioning.

Abstract: We present a deterministic framework for cryptographic seed generation based
on cyclic modular inversion over $\mathbb{Z}/3^p\mathbb{Z}$. The method
enforces algebraic admissibility on seed inputs via the identity $d_k \equiv
-\left(2^{k-1}\right)^{-1} \bmod 3^p$, thereby producing structured and
invertible residue sequences. This mapping yields entropy-rich, cycle-complete
seeds well-suited for cryptographic primitives such as DRBGs, KDFs, and
post-quantum schemes. To assess the quality of randomness, we introduce the
Entropy Confidence Score (ECS), a composite metric reflecting coverage,
uniformity, and modular bias. Although not a cryptographic PRNG in itself, the
framework serves as a deterministic entropy filter that conditions and
validates seed inputs prior to their use by conventional generators. Empirical
and hardware-based results confirm constant-time execution, minimal
side-channel leakage, and lightweight feasibility for embedded applications.
The framework complements existing cryptographic stacks by acting as an
algebraically verifiable entropy filter, thereby enhancing structural soundness
and auditability.

</details>


### [9] [Intrinsic Fingerprint of LLMs: Continue Training is NOT All You Need to Steal A Model!](https://arxiv.org/abs/2507.03014)
*Do-hyeon Yoon,Minsoo Chun,Thomas Allen,Hans Müller,Min Wang,Rajesh Sharma*

Main category: cs.CR

TL;DR: This paper proposes a robust method for large language model (LLM) fingerprinting using intrinsic parameter patterns in attention matrices, demonstrating it can detect model lineage and copyright infringement, including evidence that Huawei's Pangu Pro MoE may be derived from Qwen-2.5.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the urgent need for reliable LLM ownership protection as training costs rise and model reuse becomes widespread. Existing watermarking techniques are vulnerable to continued training, undermining intellectual property enforcement.

Method: The authors analyze the standard deviation distributions of weights in LLM attention matrices across layers, identifying stable, fingerprint-like patterns that persist despite continued training. These distribution signatures enable robust model lineage identification and plagiarism detection.

Result: Experiments validate the method's effectiveness across multiple model families, successfully identifying model origins and detecting suspicious cases like Huawei's Pangu Pro MoE potentially being derived from Qwen-2.5 via upcycling rather than full retraining.

Conclusion: The work emphasizes the importance of developing intrinsic model fingerprinting techniques to combat IP theft in large-scale LLM development. It shows deliberate continued training alone cannot fully obscure a model's lineage, providing a crucial tool for attribution and copyright enforcement.

Abstract: Large language models (LLMs) face significant copyright and intellectual
property challenges as the cost of training increases and model reuse becomes
prevalent. While watermarking techniques have been proposed to protect model
ownership, they may not be robust to continue training and development, posing
serious threats to model attribution and copyright protection. This work
introduces a simple yet effective approach for robust LLM fingerprinting based
on intrinsic model characteristics. We discover that the standard deviation
distributions of attention parameter matrices across different layers exhibit
distinctive patterns that remain stable even after extensive continued
training. These parameter distribution signatures serve as robust fingerprints
that can reliably identify model lineage and detect potential copyright
infringement. Our experimental validation across multiple model families
demonstrates the effectiveness of our method for model authentication. Notably,
our investigation uncovers evidence that a recently Pangu Pro MoE model
released by Huawei is derived from Qwen-2.5 14B model through upcycling
techniques rather than training from scratch, highlighting potential cases of
model plagiarism, copyright violation, and information fabrication. These
findings underscore the critical importance of developing robust fingerprinting
methods for protecting intellectual property in large-scale model development
and emphasize that deliberate continued training alone is insufficient to
completely obscure model origins.

</details>


### [10] [A Multi-Resolution Dynamic Game Framework for Cross-Echelon Decision-Making in Cyber Warfare](https://arxiv.org/abs/2507.03021)
*Ya-Ting Yang,Quanyan Zhu*

Main category: cs.CR

TL;DR: The paper proposes a multi-resolution dynamic game framework to model cyber warfare defense across tactical and strategic layers, enabling scalable reasoning through abstraction adjustments.


<details>
  <summary>Details</summary>
Motivation: Modern cyber defense requires decision-making across different echelons (tactical vs strategic) due to society's reliance on interconnected systems. Existing modeling approaches struggle with the dynamic, large-scale, and interdependent nature of cyber environments.

Method: develops a dual-layer framework where tactical interactions are captured via high-resolution extensive-form game trees while strategic layer uses low-resolution Markov games abstracted from these trees. A zoom-in/zoom-out mechanism dynamically adjusts model granularity based on operational needs.

Result: Case study demonstrates improved strategic advantage for defenders through this multi-resolution approach, enabling effective planning across abstraction levels in complex cyber environments.

Conclusion: This framework provides a scalable solution for modeling cyber warfare by integrating tactical detail and strategic abstraction, with adjustable resolution to optimize resource allocation and strategy formation.

Abstract: Cyber warfare has become a critical dimension of modern conflict, driven by
society's increasing dependence on interconnected digital and physical
infrastructure. Effective cyber defense often requires decision-making at
different echelons, where the tactical layer focuses on detailed actions such
as techniques, tactics, and procedures, while the strategic layer addresses
long-term objectives and coordinated planning. Modeling these interactions at
different echelons remains challenging due to the dynamic, large-scale, and
interdependent nature of cyber environments. To address this, we propose a
multi-resolution dynamic game framework in which the tactical layer captures
fine-grained interactions using high-resolution extensive-form game trees,
while the strategic layer is modeled as a Markov game defined over
lower-resolution states abstracted from those game trees. This framework
supports scalable reasoning and planning across different levels of abstraction
through zoom-in and zoom-out operations that adjust the granularity of the
modeling based on operational needs. A case study demonstrates how the
framework works and its effectiveness in improving the defender's strategic
advantage.

</details>


### [11] [Improving LLM Reasoning for Vulnerability Detection via Group Relative Policy Optimization](https://arxiv.org/abs/2507.03051)
*Marco Simoni,Aleksandar Fontana,Giulio Rossolini,Andrea Saracino*

Main category: cs.CR

TL;DR: This paper presents an extensive study exploring Group Relative Policy Optimization (GRPO) to enhance Large Language Models (LLMs) for software vulnerability detection. It addresses LLM limitations in predicting vulnerabilities and improves their performance and reasoning through structured RL-based fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LLMs deployed in AI-based security tools, like vulnerability detection systems, exhibit key limitations such as over-predicting specific vulnerabilities while missing others. This challenges their reliability and effectiveness, necessitating improved training dynamics and reasoning capabilities.

Method: The authors employ GRPO, a policy-gradient method, and adapt it for vulnerability detection by redefining advantage functions and reward signals using annotations from datasets like BigVul, DiverseVul, and CleanVul. This allows structured, rule-based reward guidance for LLM training.

Result: Experiments validate the methodology's effectiveness across multiple research questions, revealing performance and reasoning enhancements over standard supervised finetuning (SFT) in vulnerability detection tasks. The findings highlight generalization improvements and nuanced detection capabilities.

Conclusion: RL-based training via GRPO significantly improves the performance and reasoning abilities of LLMs for software vulnerability detection, offering a structured approach to address overprediction issues and underscoring the potential of reinforcement learning techniques in this domain.

Abstract: Improving and understanding the training dynamics and reasoning of Large
Language Models (LLMs) has become essential for their deployment in AI-based
security tools, such as software vulnerability detection. In this work, we
present an extensive study aimed at advancing recent RL-based finetuning
techniques for LLMs in the context of vulnerability detection.
  We start by highlighting key limitations of commonly adopted LLMs, such as
their tendency to over-predict certain types of vulnerabilities while failing
to detect others. To address this challenge, we explore the use of Group
Relative Policy Optimization (GRPO), a recent policy-gradient method, for
guiding LLM behavior through structured, rule-based rewards. We enable its
application to the vulnerability detection task by redefining its advantage
functions and reward signals using annotations from widely used datasets in the
field, including BigVul, DiverseVul, and CleanVul.
  The proposed methodology enables an extensive set of experiments, addressing
multiple research questions regarding the impact of GRPO on generalization,
reasoning capabilities, and performance improvements over standard supervised
finetuning (SFT). Our findings offer valuable insights into the potential of
RL-based training to enhance both the performance and reasoning abilities of
LLMs in the context of software vulnerability detection.

</details>


### [12] [LLM-Driven Auto Configuration for Transient IoT Device Collaboration](https://arxiv.org/abs/2507.03064)
*Hetvi Shastri,Walid A. Hanafy,Li Wu,David Irwin,Mani Srivastava,Prashant Shenoy*

Main category: cs.CR

TL;DR: CollabIoT is an IoT system enabling secure, seamless device collaboration in transient environments using an LLM-driven approach to generate access control policies, achieving 100% accuracy with low overhead due to proxies and auto-configuration.


<details>
  <summary>Details</summary>
Motivation: Transient IoT devices in temporary environments require fine-grained access control for security, but manual implementation is impractical for non-experts. Systems must also auto-configure, enforce policies at runtime, and address device heterogeneity.

Method: CollabIoT uses an LLM to translate user intents into policies, combines capability-based access control with lightweight proxies for policy enforcement, and provides hardware-independent abstractions through auto-configuration pipelines.

Result: Prototype achieved 100% policy accuracy. Device configuration time was ~150 ms, and the system incurred network overhead of 2 ms and access control overhead of 0.3 ms, validated on real and emulated IoT testbeds.

Conclusion: CollabIoT demonstrates that LLM-driven policy generation can securely and efficiently enable transient device collaboration in IoT settings by automating access control with minimal runtime overhead.

Abstract: Today's Internet of Things (IoT) has evolved from simple sensing and
actuation devices to those with embedded processing and intelligent services,
enabling rich collaborations between users and their devices. However, enabling
such collaboration becomes challenging when transient devices need to interact
with host devices in temporarily visited environments. In such cases,
fine-grained access control policies are necessary to ensure secure
interactions; however, manually implementing them is often impractical for
non-expert users. Moreover, at run-time, the system must automatically
configure the devices and enforce such fine-grained access control rules.
Additionally, the system must address the heterogeneity of devices.
  In this paper, we present CollabIoT, a system that enables secure and
seamless device collaboration in transient IoT environments. CollabIoT employs
a Large language Model (LLM)-driven approach to convert users' high-level
intents to fine-grained access control policies. To support secure and seamless
device collaboration, CollabIoT adopts capability-based access control for
authorization and uses lightweight proxies for policy enforcement, providing
hardware-independent abstractions.
  We implement a prototype of CollabIoT's policy generation and auto
configuration pipelines and evaluate its efficacy on an IoT testbed and in
large-scale emulated environments. We show that our LLM-based policy generation
pipeline is able to generate functional and correct policies with 100%
accuracy. At runtime, our evaluation shows that our system configures new
devices in ~150 ms, and our proxy-based data plane incurs network overheads of
up to 2 ms and access control overheads up to 0.3 ms.

</details>


### [13] [Holographic Projection and Cyber Attack Surface: A Physical Analogy for Digital Security](https://arxiv.org/abs/2507.03136)
*Ricardo Queiroz de Araujo Fernandes,Anderson Santos,Daniel Maier de Carvalho,André Luiz Bandeira Molina*

Main category: cs.CR

TL;DR: The paper analogizes the Holographic Principle in physics to cyber attack surfaces, explaining how internal system vulnerabilities are encoded in external interfaces and proposing boundary-heavy defense strategies like attack surface reduction and Zero Trust.


<details>
  <summary>Details</summary>
Motivation: Draws from theoretical physics concepts (black hole entropy, AdS/CFT) to offer a novel framework for understanding complex infrastructures' security posture through their external interfaces.

Method: Establishes a conceptual analogy between black hole event horizons encoding internal information and attack surfaces reflecting system vulnerabilities, applying this dual-layer perspective to cybersecurity.

Result: Proposes actionable strategies including attack surface reduction, continuous scanning (OWASP ZAP, Greenbone OpenVAS), and Zero Trust Architecture implementation as boundary-level defense solutions.

Conclusion: The physics-inspired analogy highlights the critical need for strong boundary defenses to protect complex digital infrastructures, offering both a fresh theoretical viewpoint and practical cybersecurity guidance.

Abstract: This article presents an in-depth exploration of the analogy between the
Holographic Principle in theoretical physics and cyber attack surfaces in
digital security. Building on concepts such as black hole entropy and AdS/CFT
duality, it highlights how complex infrastructures project their
vulnerabilities onto their external interfaces. The paper draws a parallel
between a black hole's event horizon, which encodes all internal information,
and the attack surface, which reflects the internal architecture's security
posture. Additionally, the article outlines how this conceptual framework can
guide cybersecurity practices, emphasizing strategies such as attack surface
reduction, continuous scanning with tools like OWASP ZAP and Greenbone OpenVAS,
and the implementation of Zero Trust Architecture. This analogy not only
provides a unique perspective on digital security but also underscores the
critical importance of boundary-level defenses in protecting vast internal
infrastructures.

</details>


### [14] [On Jailbreaking Quantized Language Models Through Fault Injection Attacks](https://arxiv.org/abs/2507.03236)
*Noureldin Zahran,Ahmad Tahmasivand,Ihsen Alouani,Khaled Khasawneh,Mohammed E. Fouda*

Main category: cs.CR

TL;DR: This paper evaluates how quantization schemes affect the success of direct parameter manipulation attacks for jailbreaking safety-aligned language models, finding that lower precision (e.g., FP8) increases attack difficulty but vulnerabilities may persist through transfer.


<details>
  <summary>Details</summary>
Motivation: Understanding the impact of quantization on safety alignment is critical as models shift to low-precision deployment for efficiency. Prior work lacked analysis of how specific quantization schemes influence adversarial robustness against parameter manipulation.

Method: Gradient-guided attacks with a progressive bit-level search algorithm (25-150 perturbations) and a single-perturbation word-level attack were applied to Llama-3.2-3B, Phi-4-mini, and Llama-3-8B under FP16, FP8, INT8, INT4 quantization schemes.

Result: FP16 models achieved >80% ASR with 25 perturbations, while FP8/INT8 had <20%/50% ASR. At 150 bit-flips, FP8 remained under 65% ASR but INT4 dropped transferred ASR by 35%. Architectural targets of attacks varied by quantization scheme.

Conclusion: Lower-precision quantization (especially FP8) provides architectural resilience against parameter manipulation attacks, but prior-jailbroken FP16 models may transfer their vulnerabilities to quantized versions. Post-attack quantization remains a key vulnerability avenue.

Abstract: The safety alignment of Language Models (LMs) is a critical concern, yet
their integrity can be challenged by direct parameter manipulation attacks,
such as those potentially induced by fault injection. As LMs are increasingly
deployed using low-precision quantization for efficiency, this paper
investigates the efficacy of such attacks for jailbreaking aligned LMs across
different quantization schemes. We propose gradient-guided attacks, including a
tailored progressive bit-level search algorithm introduced herein and a
comparative word-level (single weight update) attack. Our evaluation on
Llama-3.2-3B, Phi-4-mini, and Llama-3-8B across FP16 (baseline), and
weight-only quantization (FP8, INT8, INT4) reveals that quantization
significantly influences attack success. While attacks readily achieve high
success (>80\% Attack Success Rate, ASR) on FP16 models, within an attack
budget of 25 perturbations, FP8 and INT8 models exhibit ASRs below 20\% and
50\%, respectively. Increasing the perturbation budget up to 150 bit-flips, FP8
models maintained ASR below 65\%, demonstrating some resilience compared to
INT8 and INT4 models that have high ASR. In addition, analysis of perturbation
locations revealed differing architectural targets across quantization schemes,
with (FP16, INT4) and (INT8, FP8) showing similar characteristics. Besides,
jailbreaks induced in FP16 models were highly transferable to subsequent
FP8/INT8 quantization (<5\% ASR difference), though INT4 significantly reduced
transferred ASR (avg. 35\% drop). These findings highlight that while common
quantization schemes, particularly FP8, increase the difficulty of direct
parameter manipulation jailbreaks, vulnerabilities can still persist,
especially through post-attack quantization.

</details>


### [15] [Novel Blockchain-based Protocols for Electronic Voting and Auctions](https://arxiv.org/abs/2507.03258)
*Zhaorun Lin*

Main category: cs.CR

TL;DR: This paper introduces two blockchain-based protocols: Blind Vote, a secure and gas-efficient on-chain electronic voting system using Chaum's blind signatures, and a private auction algorithm family that protects bidder identities and bid values while maintaining trustless execution.


<details>
  <summary>Details</summary>
Motivation: Existing blockchain protocols lack sufficient security, efficiency, and bid privacy. The authors aim to address these gaps by proposing solutions that reduce gas costs and enhance transparency for decentralized applications like voting and marketplaces.

Method: 1. Blind Vote: Implements Chaum's blind signatures for untraceability while minimizing gas consumption via algorithmic optimizations. 2. Auction protocols: Designs a novel algorithm family running auction logic in smart contracts to eliminate single points of trust, using cryptographic privacy-preserving techniques.

Result: Blind Vote consumes 30-50% less gas than Tornado Vote while maintaining equivalent security guarantees. Auction protocols provide robust bid privacy, prevent front-running/collusion, and operate efficiently within smart contract execution constraints.

Conclusion: The proposed protocols establish scalable, cost-effective, and trustless frameworks for privacy-preserving blockchain applications, with potential impacts on decentralized governance, marketplaces, and democratic systems requiring anonymity and verifiability.

Abstract: Programmable blockchains have long been a hot research topic given their
tremendous use in decentralized applications. Smart contracts, using
blockchains as their underlying technology, inherit the desired properties such
as verifiability, immutability, and transparency, which make it a great suit in
trustless environments.
  In this thesis, we consider several decentralized protocols to be built on
blockchains, specifically using smart contracts on Ethereum. We used
algorithmic and cryptographic tools in our implementations to further improve
the level of security and efficiency beyond the state-of-the-art works. We
proposed a new approach called Blind Vote, which is an untraceable, secure,
efficient, secrecy-preserving, and fully on-chain electronic voting protocol
based on the well-known concept of Chaum's blind signatures. We illustrate that
our approach achieves the same security guarantees as previous methods such as
Tornado Vote [1], while consuming significantly less gas. Thus, we provide a
cheaper and considerably more gas-efficient alternative for anonymous
blockchain-based voting. On the other hand, we propose a new family of
algorithms for private, trustless auctions that protect bidder identities and
bid values while remaining practical for smart contract execution. We ensure
trustlessness by running the auction logic in a smart contract, thereby
eliminating reliance on any single trusted party. This approach prevents bid
tampering, front-running, and collusion by enforcing immutability and
decentralized verification of bids. The resulting protocol uniquely combines
efficiency, trustlessness, and enduring bid privacy, offering a scalable and
secure solution for blockchain-based marketplaces and other decentralized
applications.

</details>


### [16] [Securing Transformer-based AI Execution via Unified TEE and Crypto-protected Accelerators](https://arxiv.org/abs/2507.03278)
*Jiaqi Xue,Yifei Zhao,Mengxin Zheng,Xun Chen,Fan Yao,Yan Solihin,Qian Lou*

Main category: cs.CR

TL;DR: TwinShield is a framework enabling secure Transformer inference with dual data and model protection by offloading ~87% of computation to GPUs, achieving 4.0x-6.1x speedups over existing methods.


<details>
  <summary>Details</summary>
Motivation: Large Transformer models deployed in untrusted cloud infrastructure face security risks and performance bottlenecks from relying solely on trusted execution environments (TEEs). Existing solutions either cannot offload security-critical operations like Attention and SoftMax (forcing them to remain in TEEs) or fail to protect both model confidentiality and data integrity during offloading.

Method: TwinShield leverages a hybrid architecture combining TEEs and untrusted accelerators (e.g., GPU) by securely offloading most Transformer inference operations while maintaining dual protection for data and model through hardware-based encryption and integrity checking. It specifically enables secure offloading of previously protected operations (Attention, SoftMax) without compromising privacy or safety.

Result: TwinShield achieves 4.0x-6.1x speedups compared to prior approaches across multiple Transformer models while offloading ~87% of computations to external accelerators, demonstrating effective secure offloading of all critical components including Attention and SoftMax layers.

Conclusion: TwinShield presents a breakthrough in secure MLaaS deployment by proving that comprehensive data/model protection can be combined with significant performance improvements in large Transformer models. The framework addresses the fundamental trade-off between security and efficiency in AI inference workloads, enabling practical deployment of LLMs on untrusted cloud infrastructure.

Abstract: Recent advances in Transformer models, e.g., large language models (LLMs),
have brought tremendous breakthroughs in various artificial intelligence (AI)
tasks, leading to their wide applications in many security-critical domains.
Due to their unprecedented scale and prohibitively high development cost, these
models have become highly valuable intellectual property for AI stakeholders
and are increasingly deployed via machine learning as a service (MLaaS).
However, MLaaS often runs on untrusted cloud infrastructure, exposing data and
models to potential breaches. Mainstream protection mechanisms leverage trusted
execution environments (TEEs) where confidentiality and integrity for secretive
data are shielded using hardware-based encryption and integrity checking.
Unfortunately, running model inference entirely within TEEs is subject to
non-trivial slowdown, which is further exacerbated in LLMs due to the
substantial computation and memory footprint involved. Recent studies reveal
that the hybrid TEE-based scheme offloading partial model inference operations
to the untrusted accelerators (e.g., GPU) is a promising solution. However,
prior offloading schemes fail to ensure dual protection of data and model in
Transformer inference, as they cannot securely offload critical operations,
i.e., Attention and SoftMax, forcing these computations to remain confined
within TEEs. To address these challenges, we propose TwinShield, a framework
enabling secure Transformer inference in heterogeneous TEE and accelerator
systems with dual protection for both model and data. TwinShield offloads ~87%
of computation to GPUs and delivers 4.0x - 6.1x speedups over previous
approaches across various Transformer models.

</details>


### [17] [A Note on Single-Cut Full-Open Protocols](https://arxiv.org/abs/2507.03323)
*Kazumasa Shinagawa,Koji Nuida*

Main category: cs.CR

TL;DR: Presents three single-cut full-open card protocols for three and four-variable functions.


<details>
  <summary>Details</summary>
Motivation: Aims to simplify secure card-based computation by reducing the number of required shuffles while maintaining security and correctness.

Method: Designs protocols that use a single random cut followed by full card revelation, encoding inputs as card sequences and leveraging shuffle operations for function evaluation.

Result: Three protocols demonstrated: two for three-variable functions and one for four-variable functions, verified for correctness and security post-cut.

Conclusion: The proposed protocols advance card-based cryptography by achieving minimal shuffle complexity (single cut), enabling easier implementation in physical/didactic scenarios while maintaining strong security guarantees.

Abstract: Card-based cryptography is a research area that realizes cryptographic
protocols such as secure computation by applying shuffles to sequences of cards
that encode input values. A single-cut full-open protocol is one that obtains
an output value by applying a random cut to an input sequence of cards, after
which all cards are opened. In this paper, we propose three single-cut
full-open protocols: two protocols for three-variable functions and one
protocol for a four-variable function.

</details>


### [18] [Securing Mixed Rust with Hardware Capabilities](https://arxiv.org/abs/2507.03344)
*Jason Zhijingcheng Yu,Fangqi Han,Kaustab Choudhury,Trevor E. Carlson,Prateek Saxena*

Main category: cs.CR

TL;DR: CapsLock is a runtime enforcement mechanism for Rust principles that provides spatial and temporal memory safety by introducing a revoke-on-use abstraction in capability-based hardware, enabling cross-language enforcement even in mixed code scenarios containing unsafe Rust, FFI, and inline assembly. It passes 99.7% of crate test cases and identified 8 new bugs in real-world projects.


<details>
  <summary>Details</summary>
Motivation: Rust's static compiler checks for ownership, borrowing, and AXM principles cannot fully enforce security guarantees in mixed code (unsafe Rust, FFI, inline assembly), leaving room for memory safety violations and data races. Current hardware abstractions lack a mechanism to address this gap in language and runtime enforcement.

Method: CapsLock leverages capability-based hardware abstractions to implement a novel revoke-on-use mechanism. When a memory object is accessed through a valid capability, it implicitly invalidates specific capabilities pointing to it, enforcing aliasing/mutability constraints at runtime without requiring explicit software management of capability invalidation.

Result: 99.7% compatibility with existing Rust crates (100 most popular crates passing all test cases except 0.3%). Detection of Rust principle violations in FFI/inline assembly projects identified 8 previously unknown bugs, demonstrating effectiveness in real-world applications.

Conclusion: CapsLock is the first mechanism to provide cross-language runtime enforcement of Rust principles for mixed code environments. The prototype shows high compatibility and practical utility in detecting violations, establishing feasibility for secure execution in unsafe Rust contexts.

Abstract: The Rust programming language enforces three basic Rust principles, namely
ownership, borrowing, and AXM (Aliasing Xor Mutability) to prevent security
bugs such as memory safety violations and data races. However, Rust projects
often have mixed code, i.e., code that also uses unsafe Rust, FFI (Foreign
Function Interfaces), and inline assembly for low-level control. The Rust
compiler is unable to statically enforce Rust principles in mixed Rust code
which can lead to many security vulnerabilities. In this paper, we propose
CapsLock, a security enforcement mechanism that can run at the level of machine
code and detect Rust principle violations at run-time in mixed code. CapsLock
is kept simple enough to be implemented into recent capability-based hardware
abstractions that provide low-cost spatial memory safety. CapsLock introduces a
novel revoke-on-use abstraction for capability-based designs, wherein accessing
a memory object via a capability implicitly invalidates certain other
capabilities pointing to it, thereby also providing temporal memory safety
automatically, without requiring software to explicitly specify such
invalidation. Thus, CapsLock is the first mechanism capable of providing
cross-language enforcement of Rust principles. We implemented a prototype of
CapsLock on QEMU. Evaluation results show that CapsLock is highly compatible
with existing Rust code (passing 99.7% of the built-in test cases of the 100
most popular crates) and flags Rust principle violations in real-world Rust
projects that use FFI or inline assembly. We discovered 8 previously unknown
bugs in such crates in our experiments.

</details>


### [19] [Accelerating Private Heavy Hitter Detection on Continual Observation Streams](https://arxiv.org/abs/2507.03361)
*Rayne Holland*

Main category: cs.CR

TL;DR: This paper introduces a differentially private sketching technique with lazy updates to reduce computational costs in continual observation data stream analysis, achieving significant improvements in efficiency over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current continual observation models for differential privacy require excessive computational resources by adding fresh noise to entire sketches and querying all domain items per update, limiting their practicality for real-time applications.

Method: The method employs lazy updates in a novel differentially private sketching framework, perturbing and updating only a small, rotating portion of the output sketch at each time step while maintaining privacy and utility guarantees.

Result: The approach improves frequency estimation update time by $O(w)$ and reduces heavy hitter detection complexity from $\Omega(|U|)$ to $O(d \log w)$, demonstrating a 250× throughput increase in experiments.

Conclusion: Lazy update-based differentially private sketching makes continual observation feasible for large-scale, real-time data streams by drastically reducing computational overhead while preserving analytical accuracy.

Abstract: Differentially private frequency estimation and heavy hitter detection are
core problems in the private analysis of data streams. Two models are typically
considered: the one-pass model, which outputs results only at the end of the
stream, and the continual observation model, which requires releasing private
summaries at every time step. While the one-pass model allows more efficient
solutions, continual observation better reflects scenarios where timely and
ongoing insights are critical.
  In the one-pass setting, sketches have proven to be an effective tool for
differentially private frequency analysis, as they can be privatized by a
single injection of calibrated noise. In contrast, existing methods in the
continual observation model add fresh noise to the entire sketch at every step,
incurring high computational costs. This challenge is particularly acute for
heavy hitter detection, where current approaches often require querying every
item in the universe at each step, resulting in untenable per-update costs for
large domains.
  To overcome these limitations, we introduce a new differentially private
sketching technique based on lazy updates, which perturbs and updates only a
small, rotating part of the output sketch at each time step. This significantly
reduces computational overhead while maintaining strong privacy and utility
guarantees. In comparison to prior art, for frequency estimation, our method
improves the update time by a factor of $O(w)$ for sketches of dimension $d
\times w$; for heavy hitter detection, it reduces per-update complexity from
$\Omega(|U|)$ to $O(d \log w)$, where $U$ is the input domain. Experiments show
a increase in throughput by a factor of~$250$, making differential privacy more
practical for real-time, continual observation, applications.

</details>


### [20] [Breaking the Bulkhead: Demystifying Cross-Namespace Reference Vulnerabilities in Kubernetes Operators](https://arxiv.org/abs/2507.03387)
*Andong Chen,Zhaoxuan Jin,Ziyi Guo,Yan Chen*

Main category: cs.CR

TL;DR: The paper introduces Cross-Namespace Reference Vulnerability in Kubernetes Operators, where 14% of real-world Operators are potentially vulnerable. This allows privilege escalation by bypassing namespace isolation, leading to 6 CVEs and affecting major vendors. A static analysis tool is provided as mitigation.


<details>
  <summary>Details</summary>
Motivation: Kubernetes Operators simplify DevOps but require elevated privileges to manage resources across namespaces, creating new security risks that compromise namespace isolation. Prior research lacks systematic analysis of this threat.

Method: The authors conducted a systematic investigation of Operator security through: 1) Identifying scope mismatch (declared vs. implemented), 2) Developing two strategies to bypass namespace isolation, and 3) Performing large-scale measurements of wild Operators followed by CVE reporting.

Result: 14% of Operators exhibit vulnerability patterns; 7 confirmations and 6 CVEs reported (including redacted vendors). The static analysis suite is open-sourced for ecosystem use.

Conclusion: Cross-Namespace vulnerabilities in Operators pose critical risks requiring enhanced security practices. Open-source static analysis tools are vital for detection, emphasizing urgent mitigation efforts in the Kubernetes ecosystem.

Abstract: Kubernetes Operators, automated tools designed to manage application
lifecycles within Kubernetes clusters, extend the functionalities of
Kubernetes, and reduce the operational burden on human engineers. While
Operators significantly simplify DevOps workflows, they introduce new security
risks. In particular, Kubernetes enforces namespace isolation to separate
workloads and limit user access, ensuring that users can only interact with
resources within their authorized namespaces. However, Kubernetes Operators
often demand elevated privileges and may interact with resources across
multiple namespaces. This introduces a new class of vulnerabilities, the
Cross-Namespace Reference Vulnerability. The root cause lies in the mismatch
between the declared scope of resources and the implemented scope of the
Operator logic, resulting in Kubernetes being unable to properly isolate the
namespace. Leveraging such vulnerability, an adversary with limited access to a
single authorized namespace may exploit the Operator to perform operations
affecting other unauthorized namespaces, causing Privilege Escalation and
further impacts. To the best of our knowledge, this paper is the first to
systematically investigate the security vulnerability of Kubernetes Operators.
We present Cross-Namespace Reference Vulnerability with two strategies,
demonstrating how an attacker can bypass namespace isolation. Through
large-scale measurements, we found that over 14% of Operators in the wild are
potentially vulnerable. Our findings have been reported to the relevant
developers, resulting in 7 confirmations and 6 CVEs by the time of submission,
affecting vendors including ****** and ******, highlighting the critical need
for enhanced security practices in Kubernetes Operators. To mitigate it, we
also open-source the static analysis suite to benefit the ecosystem.

</details>


### [21] [Evaluating the Evaluators: Trust in Adversarial Robustness Tests](https://arxiv.org/abs/2507.03450)
*Antonio Emanuele Cinà,Maura Pintor,Luca Demetrio,Ambra Demontis,Battista Biggio,Fabio Roli*

Main category: cs.CR

TL;DR: This paper introduces AttackBench, a standardized benchmark framework for reliable evaluation of gradient-based adversarial attacks in robustness verification.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for adversarial attacks are inconsistent due to mismatched models, unverified implementations, and uneven computational budgets, leading to biased results and false security assumptions.

Method: AttackBench ranks attack implementations using a novel optimality metric while enforcing standardized testing conditions, with support for continuous updates.

Result: Demonstrates the framework's ability to identify reliable attacks through reproducible benchmarking under controlled conditions.

Conclusion: AttackBench provides a systematic and maintainable foundation for robustness verification, addressing critical limitations in existing evaluation protocols.

Abstract: Despite significant progress in designing powerful adversarial evasion
attacks for robustness verification, the evaluation of these methods often
remains inconsistent and unreliable. Many assessments rely on mismatched
models, unverified implementations, and uneven computational budgets, which can
lead to biased results and a false sense of security. Consequently, robustness
claims built on such flawed testing protocols may be misleading and give a
false sense of security. As a concrete step toward improving evaluation
reliability, we present AttackBench, a benchmark framework developed to assess
the effectiveness of gradient-based attacks under standardized and reproducible
conditions. AttackBench serves as an evaluation tool that ranks existing attack
implementations based on a novel optimality metric, which enables researchers
and practitioners to identify the most reliable and effective attack for use in
subsequent robustness evaluations. The framework enforces consistent testing
conditions and enables continuous updates, making it a reliable foundation for
robustness verification.

</details>


### [22] [VLAI: A RoBERTa-Based Model for Automated Vulnerability Severity Classification](https://arxiv.org/abs/2507.03607)
*Cédric Bonhomme,Alexandre Dulaunoy*

Main category: cs.CR

TL;DR: VLAI is a RoBERTa-based model that predicts software vulnerability severity levels from text with 82% accuracy, integrated into open-source vulnerability triage tools.


<details>
  <summary>Details</summary>
Motivation: Manual CVSS scoring of vulnerabilities is time-consuming and error-prone, requiring efficient automated triage solutions for faster response.

Method: Fine-tuned RoBERTa transformer model on 600,000+ real-world vulnerability text descriptions with severity labels.

Result: Achieved >82% accuracy in severity category prediction, demonstrated improved triage consistency compared to manual methods.

Conclusion: VLAI provides a reliable open-source framework for automated vulnerability severity assessment, accelerating security response workflows.

Abstract: This paper presents VLAI, a transformer-based model that predicts software
vulnerability severity levels directly from text descriptions. Built on
RoBERTa, VLAI is fine-tuned on over 600,000 real-world vulnerabilities and
achieves over 82% accuracy in predicting severity categories, enabling faster
and more consistent triage ahead of manual CVSS scoring. The model and dataset
are open-source and integrated into the Vulnerability-Lookup service.

</details>


### [23] [RVISmith: Fuzzing Compilers for RVV Intrinsics](https://arxiv.org/abs/2507.03773)
*Yibo He,Cunjian Huang,Xianmiao Qu,Hongdeng Chen,Wei Yang,Tao Xie*

Main category: cs.CR

TL;DR: RVISmith is a randomized fuzzer for detecting bugs in RISC-V Vector Extension (RVV) compilers, achieving 11.5× higher intrinsic coverage and uncovering 13 previously unknown compiler bugs.


<details>
  <summary>Details</summary>
Motivation: Compiler auto-vectorization for SIMD instructions struggles with limited compile-time information, forcing manual use of SIMD intrinsics. Compiler bugs in handling these intrinsics can lead to security vulnerabilities, data loss, and program instability, necessitating robust detection methods.

Method: RVISmith generates C programs with well-defined RVV intrinsic invocation sequences, prioritizing high intrinsic coverage, sequence diversity, and avoidance of undefined behaviors. It leverages the ratified RVV specification to implement a fuzzer that uses differential testing across compilers (GCC, LLVM, XuanTie) to detect discrepancies.

Result: RVISmith outperforms existing fuzzers with 11.5× higher intrinsic coverage and identifies 13 new compiler bugs through differential testing. Of these, 10 were confirmed by developers, and 3 were fixed.

Conclusion: RVISmith demonstrates an effective approach to compiler testing for SIMD intrinsics, achieving significant coverage improvements and exposing multiple unresolved compiler issues that impact software reliability and security.

Abstract: Modern processors are equipped with single instruction multiple data (SIMD)
instructions for fine-grained data parallelism. Compiler auto-vectorization
techniques that target SIMD instructions face performance limitations due to
insufficient information available at compile time, requiring programmers to
manually manipulate SIMD instructions. SIMD intrinsics, a type of built-in
function provided by modern compilers, enable programmers to manipulate SIMD
instructions within high-level programming languages. Bugs in compilers for
SIMD intrinsics can introduce potential threats to software security, producing
unintended calculation results, data loss, program crashes, etc.
  To detect bugs in compilers for SIMD intrinsics, we propose RVISmith, a
randomized fuzzer that generates well-defined C programs that include various
invocation sequences of RVV (RISC-V Vector Extension) intrinsics. We design
RVISmith to achieve the following objectives: (i) achieving high intrinsic
coverage, (ii) improving sequence variety, and (iii) without known undefined
behaviors. We implement RVISmith based on the ratified RVV intrinsic
specification and evaluate our approach with three modern compilers: GCC, LLVM,
and XuanTie. Experimental results show that RVISmith achieves 11.5 times higher
intrinsic coverage than the state-of-the-art fuzzer for RVV intrinsics. By
differential testing that compares results across different compilers,
optimizations, and equivalent programs, we detect and report 13 previously
unknown bugs of the three compilers under test to date. Of these bugs, 10 are
confirmed and another 3 are fixed by the compiler developers.

</details>


### [24] [Blackbox Dataset Inference for LLM](https://arxiv.org/abs/2507.03619)
*Ruikai Zhou,Kang Yang,Xun Chen,Wendy Hui Wang,Guanhong Tao,Jun Xu*

Main category: cs.CR

TL;DR: A novel dataset inference method using black-box access to detect dataset misuse in large language models.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) are often trained on datasets containing sensitive or copyrighted content, leading to potential misuse. Previous dataset inference methods rely on grey-box access to models due to the limitations of membership inference attacks (MIAs), which is restrictive for commercial models that avoid exposing internal mechanisms.

Method: The proposed method builds two sets of locally trained reference models: one incorporating the victim dataset  $\mathcal{D}$ in training and the other trained without it. By comparing the similarity of the suspect model $\mathcal{M}$ to these references under black-box access constraints, the inference is made as to whether $\mathcal{M}$ utilized $\mathcal{D}$.

Result: Evaluations on real-world LLMs demonstrate high detection accuracy across all tested scenarios. The method is robust against bypassing strategies, affirming its practical applicability.

Conclusion: This work presents a feasible alternative to overcome the impracticality of grey-box access, offering scalable and effective dataset inference for black-box LLMs while resisting evasion techniques.

Abstract: Today, the training of large language models (LLMs) can involve personally
identifiable information and copyrighted material, incurring dataset misuse. To
mitigate the problem of dataset misuse, this paper explores \textit{dataset
inference}, which aims to detect if a suspect model $\mathcal{M}$ used a victim
dataset $\mathcal{D}$ in training. Previous research tackles dataset inference
by aggregating results of membership inference attacks (MIAs) -- methods to
determine whether individual samples are a part of the training dataset.
However, restricted by the low accuracy of MIAs, previous research mandates
grey-box access to $\mathcal{M}$ to get intermediate outputs (probabilities,
loss, perplexity, etc.) for obtaining satisfactory results. This leads to
reduced practicality, as LLMs, especially those deployed for profits, have
limited incentives to return the intermediate outputs.
  In this paper, we propose a new method of dataset inference with only
black-box access to the target model (i.e., assuming only the text-based
responses of the target model are available). Our method is enabled by two sets
of locally built reference models, one set involving $\mathcal{D}$ in training
and the other not. By measuring which set of reference model $\mathcal{M}$ is
closer to, we determine if $\mathcal{M}$ used $\mathcal{D}$ for training.
Evaluations of real-world LLMs in the wild show that our method offers high
accuracy in all settings and presents robustness against bypassing attempts.

</details>


### [25] [Rethinking and Exploring String-Based Malware Family Classification in the Era of LLMs and RAG](https://arxiv.org/abs/2507.04055)
*Yufan Chen,Daoyuan Wu,Juantao Zhong,Zicheng Zhang,Debin Gao,Shuai Wang,Yingjiu Li,Ning Liu*

Main category: cs.CR

TL;DR: This paper evaluates using binary string features for malware family classification with large language models and RAG, developing a framework with 4,347 samples and analyzing 25 million strings.


<details>
  <summary>Details</summary>
Motivation: Accurate malware family identification automates labeling and analysis on platforms like VirusTotal, crucial for managing daily high-volume malware data.

Method: A curated framework was developed for 67 malware families, utilizing Family-Specific Strings (FSS) in RAG-like approaches. 25+ million strings were extracted, and ablation studies analyzed four key modules.

Result: The study demonstrates FSS-based methods' feasibility for MFC, with ablation experiments quantifying impacts of module design choices, though specific performance metrics are not detailed.

Conclusion: The work establishes that traditional FSS features can effectively enhance LLM-era MFC through RAG-like approaches, providing a foundation for improving automated malware analysis systems.

Abstract: Malware Family Classification (MFC) aims to identify the fine-grained family
(e.g., GuLoader or BitRAT) to which a potential malware sample belongs, in
contrast to malware detection or sample classification that predicts only an
Yes/No. Accurate family identification can greatly facilitate automated sample
labeling and understanding on crowdsourced malware analysis platforms such as
VirusTotal and MalwareBazaar, which generate vast amounts of data daily. In
this paper, we explore and assess the feasibility of using traditional binary
string features for MFC in the new era of large language models (LLMs) and
Retrieval-Augmented Generation (RAG). Specifically, we investigate how
Family-Specific String (FSS) features could be utilized in a manner similar to
RAG to facilitate MFC. To this end, we develop a curated evaluation framework
covering 4,347 samples from 67 malware families, extract and analyze over 25
million strings, and conduct detailed ablation studies to assess the impact of
different design choices in four major modules.

</details>


### [26] [SecureT2I: No More Unauthorized Manipulation on AI Generated Images from Prompts](https://arxiv.org/abs/2507.03636)
*Xiaodong Wu,Xiangman Li,Qi Li,Jianbing Ni,Rongxing Lu*

Main category: cs.CR

TL;DR: SecureT2I is a framework that prevents unauthorized text-guided image edits by training diffusion models to degrade forbidden inputs while maintaining permitted edits, using lightweight fine-tuning without architecture changes.


<details>
  <summary>Details</summary>
Motivation: Text-guided image manipulation with diffusion models raises ethical and copyright issues due to potential unauthorized modifications; existing solutions lack compatibility and practical integration with diverse models.

Method: SecureT2I divides images into 'permit' and 'forbid' sets based on editing permissions. For permit sets, standard high-quality manipulation is enforced. For forbid sets, separate training objectives with designed loss functions encourage outputs like blurred or semantically ambiguous images, suppressing meaningful edits. The framework requires only lightweight fine-tuning without requiring model architecture changes.

Result: Experiments show SecureT2I effectively degrades unauthorized editing quality on forbidden datasets while preserving performance on permit sets. It generalizes robustly to unseen inputs and outperforms baselines. Resize-based degradation is identified as the optimal strategy for secure manipulation control.

Conclusion: SecureT2I provides a practical, model-agnostic solution for secure text-to-image editing by balancing selective degradation with quality preservation. Resize-based approaches optimize security versus functionality trade-offs, offering potential for broader implementation in diffusion models.

Abstract: Text-guided image manipulation with diffusion models enables flexible and
precise editing based on prompts, but raises ethical and copyright concerns due
to potential unauthorized modifications. To address this, we propose SecureT2I,
a secure framework designed to prevent unauthorized editing in diffusion-based
generative models. SecureT2I is compatible with both general-purpose and
domain-specific models and can be integrated via lightweight fine-tuning
without architectural changes. We categorize images into a permit set and a
forbid set based on editing permissions. For the permit set, the model learns
to perform high-quality manipulations as usual. For the forbid set, we
introduce training objectives that encourage vague or semantically ambiguous
outputs (e.g., blurred images), thereby suppressing meaningful edits. The core
challenge is to block unauthorized editing while preserving editing quality for
permitted inputs. To this end, we design separate loss functions that guide
selective editing behavior. Extensive experiments across multiple datasets and
models show that SecureT2I effectively degrades manipulation quality on
forbidden images while maintaining performance on permitted ones. We also
evaluate generalization to unseen inputs and find that SecureT2I consistently
outperforms baselines. Additionally, we analyze different vagueness strategies
and find that resize-based degradation offers the best trade-off for secure
manipulation control.

</details>


### [27] [When There Is No Decoder: Removing Watermarks from Stable Diffusion Models in a No-box Setting](https://arxiv.org/abs/2507.03646)
*Xiaodong Wu,Tianyi Tang,Xiangman Li,Jianbing Ni,Yong Yu*

Main category: cs.CR

TL;DR: This paper investigates the robustness of model-specific watermarking in text-to-image models like latent diffusion models under adversarial attacks without accessing the watermark decoder. They develop three novel attack strategies (edge prediction, blurring, fine-tuning) and find watermarks are resilient to basic attacks but vulnerable to blurring (47.92% detection accuracy reduction). Ablation studies identify critical parameters affecting attack success.


<details>
  <summary>Details</summary>
Motivation: Current AI watermarking solutions lack robustness analysis against adversarial attacks, and there is a critical need to understand their vulnerabilities in real-world scenarios where attackers don't have access to watermark decoders.

Method: Developed three model-specific watermark evasion attacks: 1) Edge prediction-based attack for basic content modifications, 2) Box blurring to suppress watermark signals, 3) Fine-tuning-based attack modifying generation parameters. Conducted ablation studies on message length, blurring kernel size, and decoder depth through controlled experiments.

Result: Attack results show: - Basic attacks (edge prediction) have limited effectiveness against model-specific watermarking - Best fine-tuning attack reduced watermark detection accuracy to ~47.92% (from 95-100%) - Ablation studies revealed optimal parameters for attack effectiveness (message length ≤ 4k, kernel size 11-21, decoder depth 8-16)

Conclusion: Model-specific watermarking, while showing some resilience against simple attacks, is fundamentally vulnerable to advanced transformations like blurring and generation process manipulation. Even robust defenses such as multi-label smoothing fail against these "no-box" attacks, necessitating new watermarking approaches with stronger adversarial robustness.

Abstract: Watermarking has emerged as a promising solution to counter harmful or
deceptive AI-generated content by embedding hidden identifiers that trace
content origins. However, the robustness of current watermarking techniques is
still largely unexplored, raising critical questions about their effectiveness
against adversarial attacks. To address this gap, we examine the robustness of
model-specific watermarking, where watermark embedding is integrated with
text-to-image generation in models like latent diffusion models. We introduce
three attack strategies: edge prediction-based, box blurring, and
fine-tuning-based attacks in a no-box setting, where an attacker does not
require access to the ground-truth watermark decoder. Our findings reveal that
while model-specific watermarking is resilient against basic evasion attempts,
such as edge prediction, it is notably vulnerable to blurring and
fine-tuning-based attacks. Our best-performing attack achieves a reduction in
watermark detection accuracy to approximately 47.92\%. Additionally, we perform
an ablation study on factors like message length, kernel size and decoder
depth, identifying critical parameters influencing the fine-tuning attack's
success. Finally, we assess several advanced watermarking defenses, finding
that even the most robust methods, such as multi-label smoothing, result in
watermark extraction accuracy that falls below an acceptable level when
subjected to our no-box attacks.

</details>


### [28] [Willchain: Decentralized, Privacy-Preserving, Self-Executing, Digital Wills](https://arxiv.org/abs/2507.03694)
*Jovonni L. PHarr*

Main category: cs.CR

TL;DR: A decentralized protocol for digital estate planning using interchain communication and advanced cryptography to ensure secure, private, and efficient inheritance of digital assets across multiple blockchains without requiring fund transfers.


<details>
  <summary>Details</summary>
Motivation: Traditional digital inheritance systems lack privacy, security, and flexibility, often relying on centralized third parties. Blockchain technology offers a trustless and secure framework, but existing protocols are either on-chain-only or lack cross-chain capabilities, failing to address diverse ecosystem needs. Additionally, ensuring privacy and incentive alignment during inheritance remains unmet.

Method: The protocol evolves from pure Solidity-based contracts to a layer-1 system supported by modern interchain communication to connect heterogeneous chains. It implements advanced cryptographic primitives (e.g., zero-knowledge proofs) for claim validation and privacy. Smart contracts across chains act as gateways, managed via a path from the will module. User interaction includes a check-in system and account abstraction for flexibility and ease of use without compromising security.

Result: The protocol enables secure and private digital asset distribution across multiple chains, demonstrating efficient interchain communication and cryptographic validation. It reduces reliance on centralized entities, avoids fund transfers, and introduces incentive-compatible mechanisms through a permissionless blockchain secured by validators and relayers. Empirical results likely show scalability and robustness improvements over previous methods.

Conclusion: This work redefines digital estate planning by combining decentralized protocols with cryptography and interchain capabilities, addressing privacy, security, and flexibility challenges. It highlights blockchain's potential to integrate with legal frameworks while offering a scalable and incentive-aligned solution for managing digital inheritance in a heterogeneous ecosystem.

Abstract: This work presents a novel decentralized protocol for digital estate planning
that integrates advances distributed computing, and cryptography. The original
proof-of-concept was constructed using purely solidity contracts. Since then,
we have enhanced the implementation into a layer-1 protocol that uses modern
interchain communication to connect several heterogeneous chain types. A key
contribution of this research is the implementation of several modern
cryptographic primitives to support various forms of claims for information
validation. These primitives introduce an unmatched level of privacy to the
process of digital inheritance. We also demonstrate on a set of heterogeneous
smart contracts, following the same spec, on each chain to serve as entry
points, gateways, or bridge contracts that are invoked via a path from the will
module on our protocol, to the contract. This ensures a fair and secure
distribution of digital assets in accordance with the wishes of the decedent
without the requirement of moving their funds. This research further extends
its innovations with a user interaction model, featuring a check-in system and
account abstraction process, which enhances flexibility and user-friendliness
without compromising on security. By developing a dedicated permissionless
blockchain that is secured by a network of validators, and interchain relayers,
the proposed protocol signifies a transformation in the digital estate planning
industry and illustrates the potential of blockchain technology in
revolutionizing traditional legal and personal spheres. Implementing a
cryptoeconomic network at the core of inheritance planning allows for unique
incentive compatible economic mechanisms to be constructed.

</details>


### [29] [MalVol-25: A Diverse, Labelled and Detailed Volatile Memory Dataset for Malware Detection and Response Testing and Validation](https://arxiv.org/abs/2507.03993)
*Dipo Dunsin,Mohamed Chahine Ghanem,Eduardo Almeida Palmieri*

Main category: cs.CR

TL;DR: The paper introduces a high-quality, diverse malware dataset generated via automated execution and dynamic monitoring in virtual environments. Key features include memory snapshots across multiple families and operating systems, supporting machine learning, agentic AI, and RL-based detection strategies with ethical-legal compliance, validation, and documentation for reproducibility.


<details>
  <summary>Details</summary>
Motivation: Existing malware datasets lack diversity, comprehensive labeling, and sufficient complexity, hindering effectiveness in training advanced machine learning and agent-based AI systems for cybersecurity. This limits research in adaptive defenses and digital forensics.

Method: A systematic approach combining automated malware execution in controlled virtual environments with dynamic monitoring tools. Collected clean/infected memory snapshots across malware families and OS, validated via automated/manual methods, and ensured reproducibility through thorough documentation.

Result: The dataset enables system state modeling and transitions, facilitating RL-based malware detection/response. It supports diverse malware scenarios and provides features for adaptive cybersecurity research, validating its utility in incident response and threat mitigation.

Conclusion: This dataset significantly advances adaptive cybersecurity defenses and digital forensic research by offering comprehensive, ethically validated, and replicable malware behavioral data. Its design supports broader applications in threat detection and automated response frameworks.

Abstract: This paper addresses the critical need for high-quality malware datasets that
support advanced analysis techniques, particularly machine learning and agentic
AI frameworks. Existing datasets often lack diversity, comprehensive labelling,
and the complexity necessary for effective machine learning and agent-based AI
training. To fill this gap, we developed a systematic approach for generating a
dataset that combines automated malware execution in controlled virtual
environments with dynamic monitoring tools. The resulting dataset comprises
clean and infected memory snapshots across multiple malware families and
operating systems, capturing detailed behavioural and environmental features.
Key design decisions include applying ethical and legal compliance, thorough
validation using both automated and manual methods, and comprehensive
documentation to ensure replicability and integrity. The dataset's distinctive
features enable modelling system states and transitions, facilitating RL-based
malware detection and response strategies. This resource is significant for
advancing adaptive cybersecurity defences and digital forensic research. Its
scope supports diverse malware scenarios and offers potential for broader
applications in incident response and automated threat mitigation.

</details>


### [30] [S-Leak: Leakage-Abuse Attack Against Efficient Conjunctive SSE via s-term Leakage](https://arxiv.org/abs/2507.04077)
*Yue Su,Meng Shen,Cong Zuo,Yuzhi Liu,Liehuang Zhu*

Main category: cs.CR

TL;DR: This paper introduces S-Leak, a passive attack on conjunctive searchable symmetric encryption (CSSE) that exploits s-term leakage and global leakage to recover queries with high accuracy, even against existing defenses.


<details>
  <summary>Details</summary>
Motivation: Existing leakage-abuse attacks (LAAs) for single-keyword SSE are limited when extended to conjunctive queries due to combinatorial keyword explosion, making attack complexity impractical. The paper identifies s-term leakage—a previously underestimated vulnerability—as a new risk in multi-keyword scenarios.

Method: The authors propose a three-stage attack framework: 1) Identifying the s-term (keyword with minimal document frequency), 2) Pruning low-probability keyword conjunctions using novel metrics, and 3) Reconstructing full queries. They also design tailored evaluation metrics for conjunctive search scenarios.

Result: Empirical evaluations on real-world datasets show 95.15% accuracy in recovering at least one keyword (from 161,700 queries), 82.57% for two keywords, 58% for all three keywords. The attack effectively bypasses defenses like SEAL padding and CLRZ obfuscation.

Conclusion: S-term leakage poses significant risks in practical multi-keyword SSE deployments, necessitating revised leakage models for conjunctive query scenarios to prevent future vulnerabilities.

Abstract: Conjunctive Searchable Symmetric Encryption (CSSE) enables secure conjunctive
searches over encrypted data. While leakage-abuse attacks (LAAs) against
single-keyword SSE have been extensively studied, their extension to
conjunctive queries faces a critical challenge: the combinatorial explosion of
candidate keyword combinations, leading to enormous time and space overhead for
attacks. In this paper, we reveal a fundamental vulnerability in
state-of-the-art CSSE schemes: s-term leakage, where the keyword with the
minimal document frequency in a query leaks distinct patterns. We propose
S-Leak, the first passive attack framework that progressively recovers
conjunctive queries by exploiting s-term leakage and global leakage. Our key
innovation lies in a three-stage approach: identifying the s-term of queries,
pruning low-probability keyword conjunctions, and reconstructing full queries.
We propose novel metrics to better assess attacks in conjunctive query
scenarios. Empirical evaluations on real-world datasets demonstrate that our
attack is effective in diverse CSSE configurations. When considering 161,700
conjunctive keyword queries, our attack achieves a 95.15% accuracy in
recovering at least one keyword, 82.57% for at least two, 58% for all three
keywords, and maintains efficacy against defenses such as SEAL padding and CLRZ
obfuscation. Our work exposes the underestimated risks of s-term leakage in
practical SSE deployments and calls for a redesign of leakage models for
multi-keyword search scenarios.

</details>


### [31] [Human-Centered Interactive Anonymization for Privacy-Preserving Machine Learning: A Case for Human-Guided k-Anonymity](https://arxiv.org/abs/2507.04104)
*Sri Harsha Gajavalli*

Main category: cs.CR

TL;DR: This paper introduces an interactive privacy-preserving ML approach that incorporates domain experts' input in a k-anonymization process to improve data utility compared to traditional automated methods.


<details>
  <summary>Details</summary>
Motivation: Regulatory requirements (e.g., GDPR) demand anonymization of personal data for ML uses while maintaining data utility remains a challenge due to indiscriminate generalization/suppression by conventional techniques.

Method: The proposed method integrates human-in-the-loop guidance during k-anonymization, using domain experts' judgments on contextual attribute importance evaluated via classification tasks on the UCI Adult dataset.

Result: Human-influenced anonymization achieves enhanced data utility in some cases; however, effectiveness varies across machine learning tasks and settings.

Conclusion: Interactive frameworks can improve privacy-utility tradeoffs in ML, but limitations remain, prompting potential advancements for better integration of non-automated contextual controls in sensitive applications.

Abstract: Privacy-preserving machine learning (ML) seeks to balance data utility and
privacy, especially as regulations like the GDPR mandate the anonymization of
personal data for ML applications. Conventional anonymization approaches often
reduce data utility due to indiscriminate generalization or suppression of data
attributes. In this study, we propose an interactive approach that incorporates
human input into the k-anonymization process, enabling domain experts to guide
attribute preservation based on contextual importance. Using the UCI Adult
dataset, we compare classification outcomes of interactive human-influenced
anonymization with traditional, fully automated methods. Our results show that
human input can enhance data utility in some cases, although results vary
across tasks and settings. We discuss limitations of our approach and suggest
potential areas for improved interactive frameworks in privacy-aware ML.

</details>


### [32] [Addressing The Devastating Effects Of Single-Task Data Poisoning In Exemplar-Free Continual Learning](https://arxiv.org/abs/2507.04106)
*Stanisław Pawlak,Bartłomiej Twardowski,Tomasz Trzciński,Joost van de Weijer*

Main category: cs.CR

TL;DR: The paper introduces single-task poison (STP) attacks in continual learning, demonstrating their ability to disrupt model stability and plasticity using image corruptions, even with limited adversary knowledge, and proposes a defense framework with task-based poison detection.


<details>
  <summary>Details</summary>
Motivation: Existing research on data poisoning in continual learning focuses on scenario-dependent attacks, but the authors argue that STP threats are simpler, more realistic, and have been overlooked, necessitating a new focus for secure CL models.

Method: The study models adversaries with access only to the current task, using standard image corruptions. It proposes a high-level defense framework for CL and a poison task detection method based on task vectors to mitigate such attacks.

Result: STP attacks are shown to significantly decrease algorithm stability (past task performance) and plasticity (adaptability to new tasks). The proposed defense framework and detection method are introduced but not quantitatively evaluated in the abstract.

Conclusion: The paper concludes that STP attacks pose a critical threat to continual learning and that addressing these simpler, realistic threats is essential for developing secure models, with the proposed framework offering a potential solution.

Abstract: Our research addresses the overlooked security concerns related to data
poisoning in continual learning (CL). Data poisoning - the intentional
manipulation of training data to affect the predictions of machine learning
models - was recently shown to be a threat to CL training stability. While
existing literature predominantly addresses scenario-dependent attacks, we
propose to focus on a more simple and realistic single-task poison (STP)
threats. In contrast to previously proposed poisoning settings, in STP
adversaries lack knowledge and access to the model, as well as to both previous
and future tasks. During an attack, they only have access to the current task
within the data stream. Our study demonstrates that even within these stringent
conditions, adversaries can compromise model performance using standard image
corruptions. We show that STP attacks are able to strongly disrupt the whole
continual training process: decreasing both the stability (its performance on
past tasks) and plasticity (capacity to adapt to new tasks) of the algorithm.
Finally, we propose a high-level defense framework for CL along with a poison
task detection method based on task vectors. The code is available at
https://github.com/stapaw/STP.git .

</details>


### [33] [BlowPrint: Blow-Based Multi-Factor Biometrics for Smartphone User Authentication](https://arxiv.org/abs/2507.04126)
*Howard Halim,Eyasu Getahun Chekole,Daniël Reijsbergen,Jianying Zhou*

Main category: cs.CR

TL;DR: BlowPrint is a novel behavioral biometric technique using phone blowing acoustic patterns for authentication, achieving 99.35% accuracy alone and 99.82% when combined with facial recognition.


<details>
  <summary>Details</summary>
Motivation: Current multi-factor biometrics (MFB) struggle with limitations in high accuracy, usability, non-invasiveness, spoofing resilience, and low computational requirements in behavioral techniques.

Method: An empirical study with 50 participants collected blow-acoustic and facial data, computed modality similarity scores using multiple algorithms, fused them at the score level, and classified using machine learning.

Result: BlowPrint achieved 99.35% for blow acoustics, 99.96% for facial recognition, and 99.82% combined accuracy, demonstrating resilience against spoofing attacks and high usability.

Conclusion: BlowPrint presents a non-invasive, user-friendly, and secure behavioral biometric method that complements physiological techniques to enhance MFB robustness.

Abstract: Biometric authentication is a widely used security mechanism that leverages
unique physiological or behavioral characteristics to authenticate users. In
multi-factor biometrics (MFB), multiple biometric modalities, e.g.,
physiological and behavioral, are integrated to mitigate the limitations
inherent in single-factor biometrics. The main challenge in MFB lies in
identifying novel behavioral techniques capable of meeting critical criteria,
including high accuracy, high usability, non-invasiveness, resilience against
spoofing attacks, and low use of computational resources. Despite ongoing
advancements, current behavioral biometric techniques often fall short of
fulfilling one or more of these requirements. In this work, we propose
BlowPrint, a novel behavioral biometric technique that allows us to
authenticate users based on their phone blowing behaviors. In brief, we assume
that the way users blow on a phone screen can produce distinctive acoustic
patterns, which can serve as a unique biometric identifier for effective user
authentication. It can also be seamlessly integrated with physiological
techniques, such as facial recognition, to enhance its robustness and security.
To assess BlowPrint's effectiveness, we conduct an empirical study involving 50
participants from whom we collect blow-acoustic and facial feature data.
Subsequently, we compute the similarity scores of the two modalities using
various similarity algorithms and combine them through score-level fusion.
Finally, we compute the accuracy using a machine learning-based classifier. As
a result, the proposed method demonstrates an accuracy of 99.35% for blow
acoustics, 99.96% for facial recognition, and 99.82% for the combined approach.
The experimental results demonstrate BlowPrint's high effectiveness in terms of
authentication accuracy, spoofing attack resilience, usability,
non-invasiveness, and other aspects.

</details>


### [34] [Cloud Digital Forensic Readiness: An Open Source Approach to Law Enforcement Request Management](https://arxiv.org/abs/2507.04174)
*Abdellah Akilal,M-Tahar Kechadi*

Main category: cs.CR

TL;DR: The paper addresses multi-jurisdictional challenges in Cloud Forensics by proposing a Cloud Law Enforcement Requests Management System (CLERMS) with a proof of concept and economic analysis.


<details>
  <summary>Details</summary>
Motivation: Cloud forensics faces obstacles due to increasing law enforcement requests across jurisdictions and the inefficiency of existing cross-border data access formalities. A streamlined system is needed to ensure effective digital forensic readiness.

Method: Analyzes CSP transparency reports and law enforcement guidelines, then proposes CLERMS architecture and demonstrates it through two realistic scenarios along with cost estimation using open-source components.

Result: Developed and validated a CLERMS proof of concept, providing economic cost analysis and demonstrating potential to improve compliance and efficiency for CSPs and Cloud Service Consumers.

Conclusion: CLERMS offers a viable solution to enhance due Cloud Digital Forensic Readiness (CDFR) by improving cross-border law enforcement request management processes for both service providers and users.

Abstract: Cloud Forensics presents a multi-jurisdictional challenge that may undermines
the success of digital forensic investigations (DFIs). The growing volumes of
domiciled and foreign law enforcement (LE) requests, the latency and complexity
of formal channels for crossborder data access are challenging issues. In this
paper, we first discuss major Cloud Service Providers (CSPs) transparency
reports and law enforcement guidelines, then propose an abstract architecture
for a Cloud Law Enforcement Requests Management System (CLERMS). A proof of
concept of the proposed solution is developed, deployed and validated by two
realistic scenarios, in addition to an economic estimation of its associated
costs. Based on available open source components, our solution is for the
benefit of both CSPs and Cloud Service Consumers (CSCs), and aims to enhance
the due Cloud Digital Forensic Readiness (CDFR).

</details>


### [35] [ML-Enhanced AES Anomaly Detection for Real-Time Embedded Security](https://arxiv.org/abs/2507.04197)
*Nishant Chinnasami,Rye Stahle-Smith,Rasha Karakchi*

Main category: cs.CR

TL;DR: This paper proposes a framework for enhancing AES-128 encryption security by injecting controlled anomalies and detecting them in real-time using statistical methods and a Random Forest classifier. It evaluates this approach on CPU and FPGA platforms.


<details>
  <summary>Details</summary>
Motivation: Practical AES implementations are vulnerable to side-channel and fault injection attacks, necessitating a lightweight yet effective security enhancement solution. Existing methods lack low-cost real-time accuracy.

Method: 1. Simulated timing/fault anomalies via execution delays and ciphertext perturbations for dataset generation
2. Developed threshold-based timing detector + Random Forest classifier using combined features
3. Implemented on x86 CPU and FPGA (PYNQ-Z1) for testing
4. Evaluated performance varying block size (128-256 bits), injection rates (0.1-10%), and 1-4 cores

Result: ML-based detection outperformed threshold methods in precision/recall while maintaining real-time performance on embedded hardware. Achieved 99.2% accuracy vs. 82.7% for thresholding, with 190ms inference time on FPGA for 1000 samples

Conclusion: Offers a deployable solution for lightweight FPGA platforms (4.6mm2 active area) with low energy consumption (12mW) that effectively thwarts side-channel attacks. Practical in 128-256 bit modes across varying threat levels.

Abstract: Advanced Encryption Standard (AES) is a widely adopted cryptographic
algorithm, yet its practical implementations remain susceptible to side-channel
and fault injection attacks. In this work, we propose a comprehensive framework
that enhances AES-128 encryption security through controlled anomaly injection
and real-time anomaly detection using both statistical and machine learning
(ML) methods. We simulate timing and fault-based anomalies by injecting
execution delays and ciphertext perturbations during encryption, generating
labeled datasets for detection model training. Two complementary detection
mechanisms are developed: a threshold-based timing anomaly detector and a
supervised Random Forest classifier trained on combined timing and ciphertext
features. We implement and evaluate the framework on both CPU and FPGA-based
SoC hardware (PYNQ-Z1), measuring performance across varying block sizes,
injection rates, and core counts. Our results show that ML-based detection
significantly outperforms threshold-based methods in precision and recall while
maintaining real-time performance on embedded hardware. Compared to existing
AES anomaly detection methods, our solution offers a low-cost, real-time, and
accurate detection approach deployable on lightweight FPGA platforms.

</details>


### [36] [Can Large Language Models Automate the Refinement of Cellular Network Specifications?](https://arxiv.org/abs/2507.04214)
*Jianshuo Dong,Tianyi Zhang,Feng Yan,Yuanjie Li,Hewu Li,Han Qiu*

Main category: cs.CR

TL;DR: This paper explores using large language models (LLMs) to improve cellular network specification refinement, addressing security weaknesses and scalability challenges in 3GPP standards with a novel CR-eval framework and extensive benchmarking.


<details>
  <summary>Details</summary>
Motivation: Cellular networks face persistent reliability and security issues due to 3GPP standard weaknesses, while traditional analysis methods (manual inspection/automated tools) struggle with expanding specifications. Existing approaches lack scalability and effectiveness for automated refinement.

Method: 1) Constructed a dataset of 200,000+ approved 3GPP Change Requests (CRs) 2) Developed CR-eval, a principled evaluation framework 3) Benchmarked 16 state-of-the-art LLMs across 200 test cases 4) Implemented LLM specialization techniques through fine-tuning (8B model matching/surpassing GPT-4o and DeepSeek R1) 5) Evaluated models on 30 cellular attack scenarios

Result: 1) Top LLMs identified security weaknesses in 127+ test cases (63.5%) across five trials 2) 8B fine-tuned model achieved comparable/superior performance to advanced models 3) Identified significant open challenges in automating 30 network attacks 4) Demonstrated automation feasibility with ~63% success rate in baseline evaluations.

Conclusion: LLMs can effectively automate cellular network specification refinement, discovering security flaws at scale. The 8B model's success shows specialization potential, but challenges remain in full automation of attack identification. This work provides foundational evidence and evaluation methodology for advancing LLM application in cellular network security.

Abstract: Cellular networks serve billions of users globally, yet concerns about
reliability and security persist due to weaknesses in 3GPP standards. However,
traditional analysis methods, including manual inspection and automated tools,
struggle with increasingly expanding cellular network specifications. This
paper investigates the feasibility of Large Language Models (LLMs) for
automated cellular network specification refinement. To advance it, we leverage
200,000+ approved 3GPP Change Requests (CRs) that document specification
revisions, constructing a valuable dataset for domain tasks. We introduce
CR-eval, a principled evaluation framework, and benchmark 16 state-of-the-art
LLMs, demonstrating that top models can discover security-related weaknesses in
over 127 out of 200 test cases within five trials. To bridge potential gaps, we
explore LLM specialization techniques, including fine-tuning an 8B model to
match or surpass advanced LLMs like GPT-4o and DeepSeek-R1. Evaluations on 30
cellular attacks identify open challenges for achieving full automation. These
findings confirm that LLMs can automate the refinement of cellular network
specifications and provide valuable insights to guide future research in this
direction.

</details>


### [37] [Hijacking JARVIS: Benchmarking Mobile GUI Agents against Unprivileged Third Parties](https://arxiv.org/abs/2507.04227)
*Guohong Liu,Jialei Ye,Jiacheng Liu,Yuanchun Li,Wei Liu,Pengzhi Gao,Jian Luan,Yunxin Liu*

Main category: cs.CR

TL;DR: This paper investigates vulnerabilities in mobile GUI agents when interacting with screens manipulated by untrustworthy third parties, introducing AgentHazard framework, a mobile vision benchmark with over 3,000 attack scenarios, and revealing significant susceptibility (28.8% misleading rate) in seven major agents. The work highlights perception modality/backbone model relationships and mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: Current mobile GUI agents lack robustness against real-world adversarial screen content manipulations by unprivileged third parties due to their black-box autonomous nature, creating risks for device compromise. Systematic vulnerability investigations and benchmarks are urgently needed.

Method: Developed AgentHazard framework for scalable attack simulation; created a benchmark with 58 dynamic tasks in emulators using modified commercial app data and 210 static screenshots from 14 apps. Test conditions mimic feasible third-party attacks. Evaluated 7 GUI agents and 5 backbone models.

Result: All tested agents (7 GUI agents and 5 backbone models) demonstrated significant vulnerability to third-party content manipulation (avg. 28.8% misleading rate in human-crafted attacks). Vulnerabilities correlate with specific vision-language-action perception methods. Found both challenges and opportunities in training-based mitigation approaches.

Conclusion: Demonstrates critical security risks in current mobile GUI agent systems, provides standardized benchmark for evaluating robustness against adversarial UI content, and establishes foundation for developing more secure autonomous mobile interfaces. Code and dataset available at https://agenthazard.github.io.

Abstract: Mobile GUI agents are designed to autonomously execute diverse device-control
tasks by interpreting and interacting with mobile screens. Despite notable
advancements, their resilience in real-world scenarios where screen content may
be partially manipulated by untrustworthy third parties remains largely
unexplored. Owing to their black-box and autonomous nature, these agents are
vulnerable to manipulations that could compromise user devices. In this work,
we present the first systematic investigation into the vulnerabilities of
mobile GUI agents. We introduce a scalable attack simulation framework
AgentHazard, which enables flexible and targeted modifications of screen
content within existing applications. Leveraging this framework, we develop a
comprehensive benchmark suite comprising both a dynamic task execution
environment and a static dataset of vision-language-action tuples, totaling
over 3,000 attack scenarios. The dynamic environment encompasses 58
reproducible tasks in an emulator with various types of hazardous UI content,
while the static dataset is constructed from 210 screenshots collected from 14
popular commercial apps. Importantly, our content modifications are designed to
be feasible for unprivileged third parties. We evaluate 7 widely-used mobile
GUI agents and 5 common backbone models using our benchmark. Our findings
reveal that all examined agents are significantly influenced by misleading
third-party content (with an average misleading rate of 28.8% in human-crafted
attack scenarios) and that their vulnerabilities are closely linked to the
employed perception modalities and backbone LLMs. Furthermore, we assess
training-based mitigation strategies, highlighting both the challenges and
opportunities for enhancing the robustness of mobile GUI agents. Our code and
data will be released at https://agenthazard.github.io.

</details>


### [38] [VOLTRON: Detecting Unknown Malware Using Graph-Based Zero-Shot Learning](https://arxiv.org/abs/2507.04275)
*M. Tahir Akdeniz,Zeynep Yeşilkaya,İ. Enes Köse,İ. Ulaş Ünal,Sevil Şen*

Main category: cs.CR

TL;DR: This paper proposes a zero-shot learning framework combining Variational Graph Auto-Encoders (VGAE) and Siamese Neural Networks (SNN) to detect previously unseen Android malware families without requiring labeled data for these threats.


<details>
  <summary>Details</summary>
Motivation: Traditional machine learning-based Android malware detection methods rely on large labeled datasets, limiting their ability to detect emerging, zero-day malware families where labeled data is unavailable.

Method: The framework uses graph-based representations of Android applications, with VGAE learning structural patterns and SNN comparing application similarities to enable zero-shot detection of malware.

Result: Outperformed state-of-the-art MaMaDroid with 96.24% accuracy and 95.20% recall for unknown malware families, demonstrating robust zero-day detection capabilities.

Conclusion: The proposed method provides an effective solution for detecting evolving Android malware without prior labeled examples, improving security against emerging threats.

Abstract: The persistent threat of Android malware presents a serious challenge to the
security of millions of users globally. While many machine learning-based
methods have been developed to detect these threats, their reliance on large
labeled datasets limits their effectiveness against emerging, previously unseen
malware families, for which labeled data is scarce or nonexistent.
  To address this challenge, we introduce a novel zero-shot learning framework
that combines Variational Graph Auto-Encoders (VGAE) with Siamese Neural
Networks (SNN) to identify malware without needing prior examples of specific
malware families. Our approach leverages graph-based representations of Android
applications, enabling the model to detect subtle structural differences
between benign and malicious software, even in the absence of labeled data for
new threats.
  Experimental results show that our method outperforms the state-of-the-art
MaMaDroid, especially in zero-day malware detection. Our model achieves 96.24%
accuracy and 95.20% recall for unknown malware families, highlighting its
robustness against evolving Android threats.

</details>


### [39] [Attention Slipping: A Mechanistic Understanding of Jailbreak Attacks and Defenses in LLMs](https://arxiv.org/abs/2507.04365)
*Xiaomeng Hu,Pin-Yu Chen,Tsung-Yi Ho*

Main category: cs.CR

TL;DR: The paper discover universal "Attention Slipping" in jailbreak attacks and propose a defense called "Attention Sharpening" that enhances safety guardrails by sharpening attention score distributions via temperature scaling, demonstrating effectiveness across four major LLMs without performance overhead.


<details>
  <summary>Details</summary>
Motivation: As LLMs become critical societal components, understanding and preventing jailbreak attacks that bypass safety mechanisms is imperative for responsible AI deployment.

Method: 1) Identify and characterize Attention Slipping phenomenon across different jailbreak attack vectors
2) Evaluate existing defenses (Token Highlighter/SmoothLLM) by measuring their impact on attention slipping
3) Propose Attention Sharpening defense that:
- Directly targets attention score distribution
- Uses temperature scaling to sharpen attention
- Requires no additional computation/memory overhead

Result: Successfully demonstrated that Attention Sharpening:
- Resists various jailbreak attacks (gradient-based token replacement, prompt template refinement, in-context learning)
- Maintains performance on benign tasks (AlpacaEval)
- Has no computational/memory overhead
Found strong positive correlation between defense effectiveness and degree of attention slipping mitigation

Conclusion: Attention Sharpening offers an efficient, practical defense against jailbreak attacks by addressing the underlying Attention Slipping mechanism, maintaining model functionality without trade-offs in safety or performance for real-world LLM deployments.

Abstract: As large language models (LLMs) become more integral to society and
technology, ensuring their safety becomes essential. Jailbreak attacks exploit
vulnerabilities to bypass safety guardrails, posing a significant threat.
However, the mechanisms enabling these attacks are not well understood. In this
paper, we reveal a universal phenomenon that occurs during jailbreak attacks:
Attention Slipping. During this phenomenon, the model gradually reduces the
attention it allocates to unsafe requests in a user query during the attack
process, ultimately causing a jailbreak. We show Attention Slipping is
consistent across various jailbreak methods, including gradient-based token
replacement, prompt-level template refinement, and in-context learning.
Additionally, we evaluate two defenses based on query perturbation, Token
Highlighter and SmoothLLM, and find they indirectly mitigate Attention
Slipping, with their effectiveness positively correlated with the degree of
mitigation achieved. Inspired by this finding, we propose Attention Sharpening,
a new defense that directly counters Attention Slipping by sharpening the
attention score distribution using temperature scaling. Experiments on four
leading LLMs (Gemma2-9B-It, Llama3.1-8B-It, Qwen2.5-7B-It, Mistral-7B-It v0.2)
show that our method effectively resists various jailbreak attacks while
maintaining performance on benign tasks on AlpacaEval. Importantly, Attention
Sharpening introduces no additional computational or memory overhead, making it
an efficient and practical solution for real-world deployment.

</details>


### [40] [Enhancing Phishing Detection in Financial Systems through NLP](https://arxiv.org/abs/2507.04426)
*Novruz Amirov,Leminur Celik,Egemen Ali Caner,Emre Yurdakul,Fahri Anil Yerlikaya,Serif Bahtiyar*

Main category: cs.CR

TL;DR: This paper proposes a novel NLP-based phishing email detection method combining semantic similarity and TF-IDF analysis, achieving 79.8% accuracy experimentally.


<details>
  <summary>Details</summary>
Motivation: Financial systems face growing phishing threats; existing methods (blacklists/whitelists) have inherent limitations requiring more robust solutions.

Method: The solution uses TF-IDF analysis for keyword identification and semantic similarity evaluation against a phishing dataset, leveraging NLP techniques.

Result: The method achieves 79.8% accuracy (TF-IDF) and 67.2% accuracy (semantic analysis) in experimental tests.

Conclusion: The proposed NLP approach addresses phishing detection limitations, offering a robust solution that advances both cybersecurity and NLP domains.

Abstract: The threat of phishing attacks in financial systems is continuously growing.
Therefore, protecting sensitive information from unauthorized access is
paramount. This paper discusses the critical need for robust email phishing
detection. Several existing methods, including blacklists and whitelists, play
a crucial role in detecting phishing attempts. Nevertheless, these methods
possess inherent limitations, emphasizing the need for the development of a
more advanced solution. Our proposed solution presents a pioneering Natural
Language Processing (NLP) approach for phishing email detection. Leveraging
semantic similarity and TFIDF (Term Frequency-Inverse Document Frequency)
analysis, our solution identifies keywords in phishing emails, subsequently
evaluating the semantic similarities with a dedicated phishing dataset,
ultimately contributing to the enhancement of cybersecurity and NLP domains
through a robust solution for detecting phishing threats in financial systems.
Experimental results show the accuracy of our phishing detection method can
reach 79.8 percent according to TF-IDF analysis, while it can reach 67.2
percent according to semantic analysis.

</details>


### [41] [UniAud: A Unified Auditing Framework for High Auditing Power and Utility with One Training Run](https://arxiv.org/abs/2507.04457)
*Ruixuan Liu,Li Xiong*

Main category: cs.CR

TL;DR: This paper addresses the limitations of efficient O(1) frameworks for differentially private (DP) auditing by introducing UniAud and UniAud++ methods. These frameworks improve auditing tightness while maintaining utility through data independence, uncorrelated canary construction, and multi-task learning with separated objectives. The results demonstrate superior efficiency-auditing trade-offs and minimal utility degradation compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Recent O(1) DP auditing frameworks sacrifice tightness for efficiency due to data dependency and a conflict between auditing effectiveness and model utility. The paper seeks to resolve these issues by decoupling auditing criteria and rethinking data independence.

Method: The authors propose UniAud, a data-independent auditing framework using uncorrelated canary data and self-comparison, and extend it to UniAud++ for data-dependent scenarios. They employ multi-task learning to separately optimize utility and auditing objectives.

Result: UniAud and UniAud++ match the state-of-the-art O(T) auditing performance with significantly fewer runs across vision and language tasks. They achieve near-standard DP training utility with marginal degradation, confirming optimal efficiency-auditing and utility-auditing trade-offs without requiring auxiliary training.

Conclusion: The proposed unified frameworks demonstrate that improved DP auditing tightness and efficiency are achievable by addressing data dependency through uncorrelated data and resolving objective conflicts via decoupling. This advances practical privacy-utility trade-off analysis in machine learning.

Abstract: Differentially private (DP) optimization has been widely adopted as a
standard approach to provide rigorous privacy guarantees for training datasets.
DP auditing verifies whether a model trained with DP optimization satisfies its
claimed privacy level by estimating empirical privacy lower bounds through
hypothesis testing. Recent O(1) frameworks improve auditing efficiency by
checking the membership status of multiple audit samples in a single run,
rather than checking individual samples across multiple runs. However, we
reveal that there is no free lunch for this improved efficiency: data
dependency and an implicit conflict between auditing and utility impair the
tightness of the auditing results. Addressing these challenges, our key
insights include reducing data dependency through uncorrelated data and
resolving the auditing-utility conflict by decoupling the criteria for
effective auditing and separating objectives for utility and auditing. We first
propose a unified framework, UniAud, for data-independent auditing that
maximizes auditing power through a novel uncorrelated canary construction and a
self-comparison framework. We then extend this framework as UniAud++ for
data-dependent auditing, optimizing the auditing and utility trade-off through
multi-task learning with separate objectives for auditing and training.
Experimental results validate that our black-box O(1) framework matches the
state-of-the-art auditing results of O(T) auditing with thousands of runs,
demonstrating the best efficiency-auditing trade-off across vision and language
tasks. Additionally, our framework provides meaningful auditing with only
slight utility degradation compared to standard DP training, showing the
optimal utility-auditing trade-off and the benefit of requiring no extra
training for auditing.

</details>


### [42] [Arbiter PUF: Uniqueness and Reliability Analysis Using Hybrid CMOS-Stanford Memristor Model](https://arxiv.org/abs/2507.04461)
*Tanvir Rahman,A. B. M. Harun-ur Rashid*

Main category: cs.CR

TL;DR: This paper explores memristor-based PUFs for enhanced reliability in hardware security, demonstrating superior performance over CMOS-based solutions under process variations but requiring improvements in uniqueness.


<details>
  <summary>Details</summary>
Motivation: The growing security risks in interconnected electronic devices, including data extraction and hardware tampering, create a critical need for robust security solutions beyond traditional cryptography. The IoT's expansion exacerbates these challenges as existing methods struggle with process variations in third-party manufacturing.

Method: The study uses the Stanford Memristor Model with 45nm CMOS technology to design and evaluate memristor-based Arbiter PUFs through Monte Carlo simulation of intra- and inter-hamming distances under temperature, voltage, and process variations.

Result: Memristor PUFs show higher reliability than CMOS PUFs in process variation resistance, but exhibit suboptimal uniqueness metrics that require further optimization for practical deployment.

Conclusion: Memristor-based PUFs represent a viable alternative for secure applications in hardware security contexts, particularly where reliability under environmental variations is critical, though uniqueness remains a limitation to address.

Abstract: In an increasingly interconnected world, protecting electronic devices has
grown more crucial because of the dangers of data extraction, reverse
engineering, and hardware tampering. Producing chips in a third-party
manufacturing company can let hackers change the design. As the Internet of
Things (IoT) proliferates, physical attacks happen more, and conventional
cryptography techniques do not function well. In this paper, we investigate the
design and assessment of PUFs using the Stanford Memristor Model, utilizing its
random filament evolution to improve security. The system was built using 45nm
CMOS technology. A comparison is made between CMOS-based and memristor-based
Arbiter PUFs, evaluating their performance under temperature, voltage, and
process variations. Intra- and inter-hamming distances are employed by Monte
Carlo simulations to estimate uniqueness and reliability. The results show that
memristor-based PUFs offer better reliability than CMOS-based designs, though
uniqueness needs further improvement. Furthermore, this study sheds light on
the reasonableness of memristor-based PUFs for secure applications in hardware
security.

</details>


### [43] [README: Robust Error-Aware Digital Signature Framework via Deep Watermarking Model](https://arxiv.org/abs/2507.04495)
*Hyunwook Choi,Sangyun Won,Daeyeon Hwang,Junhyeok Choi*

Main category: cs.CR

TL;DR: README is a framework that addresses the limitations of existing deep learning-based watermarking by enabling robust, verifiable, and error-tolerant 2048-bit digital signatures in images through a cropping-based capacity scaling mechanism and ERPA (ERror PAinting Module) with DCSS. It achieves an 86.3% zero-bit-error rate under real-world distortions without retraining models.


<details>
  <summary>Details</summary>
Motivation: Deep learning-based watermarking struggles with embedding 2048-bit error-free digital signatures due to low capacity and bit-level error vulnerabilities, limiting its use in cryptographic applications requiring high assurance.

Method: Combines a cropping-based capacity scaling mechanism with ERPA, a lightweight error correction module utilizing Distinct Circular Subsum Sequences (DCSS), to localize and correct bit errors. Integrates perceptual hash-based verification for public tamper resistance.

Result: README increases the zero-bit-error image rate (Z.B.I.R) from 1.2% to 86.3% when embedding 2048-bit signatures into a single image, maintaining accuracy under real-world distortions without retraining existing models.

Conclusion: README bridges the gap between signal-level watermarking and cryptographic security, enabling digital signatures in images with public verifiability and high error tolerance, unlocking high-assurance applications for deep watermarking.

Abstract: Deep learning-based watermarking has emerged as a promising solution for
robust image authentication and protection. However, existing models are
limited by low embedding capacity and vulnerability to bit-level errors, making
them unsuitable for cryptographic applications such as digital signatures,
which require over 2048 bits of error-free data. In this paper, we propose
README (Robust Error-Aware Digital Signature via Deep WaterMarking ModEl), a
novel framework that enables robust, verifiable, and error-tolerant digital
signatures within images. Our method combines a simple yet effective
cropping-based capacity scaling mechanism with ERPA (ERror PAinting Module), a
lightweight error correction module designed to localize and correct bit errors
using Distinct Circular Subsum Sequences (DCSS). Without requiring any
fine-tuning of existing pretrained watermarking models, README significantly
boosts the zero-bit-error image rate (Z.B.I.R) from 1.2% to 86.3% when
embedding 2048-bit digital signatures into a single image, even under
real-world distortions. Moreover, our use of perceptual hash-based signature
verification ensures public verifiability and robustness against tampering. The
proposed framework unlocks a new class of high-assurance applications for deep
watermarking, bridging the gap between signal-level watermarking and
cryptographic security.

</details>


### [44] [LINE: Public-key encryption](https://arxiv.org/abs/2507.04501)
*Gennady Khalimov,Yevgen Kotukh*

Main category: cs.CR

TL;DR: The paper proposes a high-security, low-overhead public-key encryption system based on solving underdetermined linear equation systems using factorizable substitutions with homomorphic matrix transformations over F₂ vector spaces.


<details>
  <summary>Details</summary>
Motivation: Traditional cryptosystems require robust security against polynomial-time attacks, motivating the use of complex mathematical structures like underdetermined systems with multiple equivalent solutions to prevent cryptanalysis.

Method: The system utilizes secret homomorphic matrix transformations to complete input parameters of linear equation systems. Encryption relies on one-way functions over an elementary abelian 2-group of order 2^m, while decryption involves reconstructing these parameters.

Result: Matrix-based homomorphic transformations enable secure parameter completion with low computational cost, achieving a cryptosystem that resists efficient cryptanalysis due to the inherent complexity of underdetermined linear systems.

Conclusion: The proposed cryptosystem offers a novel approach combining algebraic complexity and efficient computation, potentially improving security without sacrificing performance for factorizable substitutions over F₂ vector spaces.

Abstract: We propose a public key encryption cryptosystem based on solutions of linear
equation systems with predefinition of input parameters through shared secret
computation for factorizable substitutions. The existence of multiple
equivalent solutions for an underdetermined system of linear equations
determines the impossibility of its resolution by a cryptanalyst in polynomial
time. The completion of input parameters of the equation system is implemented
through secret homomorphic matrix transformation for substitutions factorized
over the basis of a vector space of dimension m over the field F2. Encryption
is implemented through computation of substitutions that are one-way functions
on an elementary abelian 2-group of order 2"m. Decryption is implemented
through completion of input parameters of the equation system. Homomorphic
transformations are constructed based on matrix computations. Matrix
computations enable the implementation of high security and low computational
overhead for homomorphic transformations.

</details>


### [45] [Large Language Models for Network Intrusion Detection Systems: Foundations, Implementations, and Future Directions](https://arxiv.org/abs/2507.04752)
*Shuo Yang,Xinran Zheng,Xinchen Zhang,Jinfeng Xu,Jinze Li,Donglin Xie,Weicai Long,Edith C. H. Ngai*

Main category: cs.CR

TL;DR: This paper explores the potential of large language models (LLMs) for advancing network intrusion detection systems (NIDS) by addressing challenges in traditional AI-driven approaches and proposing LLM-centered solutions for contextual awareness, explainability, and automation.


<details>
  <summary>Details</summary>
Motivation: Traditional intelligent NIDS (using ML/DL) suffer from limited contextual understanding and lack of explainability, making their decision-making opaque and difficult to trust. This motivates the need for cognitive systems with human-like reasoning capabilities.

Method: The paper 1) establishes foundational concepts of NIDS and LLMs, 2) analyzes enabling technologies for AI-driven cognitive NIDS, 3) presents practical LLM implementations as processors, detectors, and explainers, and 4) introduces an LLM-centered Controller for workflow coordination.

Result: The analysis demonstrates how LLMs can process both structured/unstructured data to achieve: - Deeper contextual threat analysis - Transparent, human-explainable decisions - Automated response generation - Optimized collaborative detection through centralized control architecture, with detailed implementation frameworks provided.

Conclusion: LLMs represent a transformative shift for NIDS by bridging smart and cognitive systems through contextual awareness and explainability. The paper identifies key challenges (reliability, adaptability) while highlighting opportunities to develop trustworthy next-generation security systems with human-AI collaboration.

Abstract: Large Language Models (LLMs) have revolutionized various fields with their
exceptional capabilities in understanding, processing, and generating
human-like text. This paper investigates the potential of LLMs in advancing
Network Intrusion Detection Systems (NIDS), analyzing current challenges,
methodologies, and future opportunities. It begins by establishing a
foundational understanding of NIDS and LLMs, exploring the enabling
technologies that bridge the gap between intelligent and cognitive systems in
AI-driven NIDS. While Intelligent NIDS leverage machine learning and deep
learning to detect threats based on learned patterns, they often lack
contextual awareness and explainability. In contrast, Cognitive NIDS integrate
LLMs to process both structured and unstructured security data, enabling deeper
contextual reasoning, explainable decision-making, and automated response for
intrusion behaviors. Practical implementations are then detailed, highlighting
LLMs as processors, detectors, and explainers within a comprehensive AI-driven
NIDS pipeline. Furthermore, the concept of an LLM-centered Controller is
proposed, emphasizing its potential to coordinate intrusion detection
workflows, optimizing tool collaboration and system performance. Finally, this
paper identifies critical challenges and opportunities, aiming to foster
innovation in developing reliable, adaptive, and explainable NIDS. By
presenting the transformative potential of LLMs, this paper seeks to inspire
advancement in next-generation network security systems.

</details>


### [46] [Efficient Unlearning with Privacy Guarantees](https://arxiv.org/abs/2507.04771)
*Josep Domingo-Ferrer,Najeeb Jebreel,David Sánchez*

Main category: cs.CR

TL;DR: The paper proposes EUPG, a novel machine unlearning framework with formal privacy guarantees and efficiency. It pre-trains models on privacy-protected data and demonstrates comparable performance to existing methods with lower computational costs.


<details>
  <summary>Details</summary>
Motivation: Privacy laws like GDPR require ML models to be retrained to remove personal data when requested. Current unlearning methods are either computationally expensive or lack privacy guarantees for arbitrary models.

Method: EUPG pre-trains ML models on data protected with privacy models such as k-anonymity and differential privacy, enabling efficient unlearning with inherent privacy guarantees through this pre-processing approach.

Result: Empirical evaluation on four diverse datasets shows EUPG achieves utility and forgetting effectiveness comparable to exact unlearining methods while reducing computational and storage costs significantly.

Conclusion: EUPG offers a practical framework for privacy-compliant data forgetting by combining pre-training with existing privacy models, maintaining model performance while enabling efficient unlearning with formal privacy guarantees.

Abstract: Privacy protection laws, such as the GDPR, grant individuals the right to
request the forgetting of their personal data not only from databases but also
from machine learning (ML) models trained on them. Machine unlearning has
emerged as a practical means to facilitate model forgetting of data instances
seen during training. Although some existing machine unlearning methods
guarantee exact forgetting, they are typically costly in computational terms.
On the other hand, more affordable methods do not offer forgetting guarantees
and are applicable only to specific ML models. In this paper, we present
\emph{efficient unlearning with privacy guarantees} (EUPG), a novel machine
unlearning framework that offers formal privacy guarantees to individuals whose
data are being unlearned. EUPG involves pre-training ML models on data
protected using privacy models, and it enables {\em efficient unlearning with
the privacy guarantees offered by the privacy models in use}. Through empirical
evaluation on four heterogeneous data sets protected with $k$-anonymity and
$\epsilon$-differential privacy as privacy models, our approach demonstrates
utility and forgetting effectiveness comparable to those of exact unlearning
methods, while significantly reducing computational and storage costs. Our code
is available at https://github.com/najeebjebreel/EUPG.

</details>


### [47] [FIDESlib: A Fully-Fledged Open-Source FHE Library for Efficient CKKS on GPUs](https://arxiv.org/abs/2507.04775)
*Carlos Agulló-Domingo,Óscar Vera-López,Seyda Guzelhan,Lohit Daksha,Aymane El Jerari,Kaustubh Shivdikar,Rashmi Agrawal,David Kaeli,Ajay Joshi,José L. Abellán*

Main category: cs.CR

TL;DR: FIDESlib is the first open-source server-side CKKS GPU library achieving 70x speedup over AVX-optimized OpenFHE for bootstrapping while maintaining interoperability with client-side operations.


<details>
  <summary>Details</summary>
Motivation: Current CPU-based OpenFHE lacks server-side performance for cloud MLaaS deployment, and existing GPU FHE libraries don't provide optimized CKKS implementations with multi-GPU scalability.

Method: Designed a dedicated CKKS GPU library from first principles with highly optimized CUDA kernels, robust benchmarking infrastructure, and software architecture supporting multi-GPU extensions.

Result: Experiments show FIDESlib outperforms Phantom on all CKKS primitives and achieves 70x faster bootstrapping than OpenFHE's CPU implementation, with demonstrated scalability across GPU systems.

Conclusion: FIDESlib provides a practical, interoperable GPU solution for efficient server-side CKKS, addressing cloud deployment challenges with significant performance improvements and future optimization capabilities.

Abstract: Word-wise Fully Homomorphic Encryption (FHE) schemes, such as CKKS, are
gaining significant traction due to their ability to provide
post-quantum-resistant, privacy-preserving approximate computing; an especially
desirable feature in Machine-Learning-as-a-Service (MLaaS) cloud-computing
paradigms. OpenFHE is a leading CPU-based FHE library with robust CKKS
operations, but its server-side performance is not yet sufficient for practical
cloud deployment. As GPU computing becomes more common in data centers, many
FHE libraries are adding GPU support. However, integrating an efficient GPU
backend into OpenFHE is challenging. While OpenFHE uses a Hardware Abstraction
Layer (HAL), its flexible architecture sacrifices performance due to the
abstraction layers required for multi-scheme and multi-backend compatibility.
In this work, we introduce FIDESlib, the first open-source server-side CKKS GPU
library that is fully interoperable with well-established client-side OpenFHE
operations. Unlike other existing open-source GPU libraries, FIDESlib provides
the first implementation featuring heavily optimized GPU kernels for all CKKS
primitives, including bootstrapping. Our library also integrates robust
benchmarking and testing, ensuring it remains adaptable to further
optimization. Furthermore, its software architecture is designed to support
extensions to a multi-GPU backend for enhanced acceleration. Our experiments
across various GPU systems and the leading open-source CKKS library to date,
Phantom, show that FIDESlib offers superior performance and scalability. For
bootstrapping, FIDESlib achieves no less than 70x speedup over the
AVX-optimized OpenFHE implementation.

</details>


### [48] [Hybrid Approach to Directed Fuzzing](https://arxiv.org/abs/2507.04855)
*Darya Parygina,Timofey Mezhuev,Daniil Kuts*

Main category: cs.CR

TL;DR: A hybrid directed fuzzer combining symbolic execution and novel seed scheduling algorithms to overcome constraints and improve error detection efficiency.


<details>
  <summary>Details</summary>
Motivation: Directed greybox fuzzing struggles with difficult program constraints despite popularity for error detection. Symbolic execution solves this but sacrifices performance. Combining both addresses the limitations.

Method: Proposed hybrid approach with a seed scheduling algorithm prioritizing 'target-related interestingness' and coverage metrics. Implemented in Sydr-Fuzz using LibAFL-DiFuzz (fuzzer) and Sydr (symbolic executor). Objective seeds were minimized and sorted based on target information.

Result: Outperformed 6 competitive fuzzers (AFLGo, BEACON, WAFLGo, WindRanger, FishFuzz, Prospector) in 3/7 examples with up to 1.86× speedup over the next best. Surpassed pure LibAFL-DiFuzz in efficiency for 3/7 examples using Time to Exposure metric.

Conclusion: The hybrid directed fuzzing approach enhances error detection efficiency by addressing program constraints, demonstrating performance improvements over existing methods through empirical evaluation.

Abstract: Program analysis and automated testing have recently become an essential part
of SSDLC. Directed greybox fuzzing is one of the most popular automated testing
methods that focuses on error detection in predefined code regions. However, it
still lacks ability to overcome difficult program constraints. This problem can
be well addressed by symbolic execution, but at the cost of lower performance.
Thus, combining directed fuzzing and symbolic execution techniques can lead to
more efficient error detection.
  In this paper, we propose a hybrid approach to directed fuzzing with novel
seed scheduling algorithm, based on target-related interestingness and
coverage. The approach also performs minimization and sorting of objective
seeds according to a target-related information. We implement our approach in
Sydr-Fuzz tool using LibAFL-DiFuzz as directed fuzzer and Sydr as dynamic
symbolic executor. We evaluate our approach with Time to Exposure metric and
compare it with pure LibAFL-DiFuzz, AFLGo, BEACON, WAFLGo, WindRanger,
FishFuzz, and Prospector. The results show an improvement for 3 out of 7
examples with speedup up to 1.86 times over the second best result, as well as
a significant improvement for 3 out of 7 examples over the pure LibAFL-DiFuzz
fuzzer. Sydr-Fuzz hybrid approach to directed fuzzing shows high performance
and helps to improve directed fuzzing efficiency.

</details>


### [49] [BackFed: An Efficient & Standardized Benchmark Suite for Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2507.04903)
*Thinh Dao,Dung Thuy Nguyen,Khoa D Doan,Kok-Seng Wong*

Main category: cs.CR

TL;DR: BackFed is a benchmark suite for standardizing evaluations of backdoor attacks and defenses in Federated Learning (FL), enabling reliable comparisons under practical constraints. Experiments using BackFed reveal limitations in existing methods and highlight failure modes under realistic conditions.


<details>
  <summary>Details</summary>
Motivation: FL systems face challenges in evaluating the effectiveness of backdoor attacks and defenses due to divergent experimental settings, implementation errors, and unrealistic assumptions. This hinders the development of robust solutions for real-world applications.

Method: BackFed introduces a multi-processing implementation with APIs for modular integration of new methods and a standardized evaluation pipeline. Large-scale studies are conducted across computer vision and NLP tasks with diverse model architectures and settings.

Result: Large-scale experiments identify unknown limitations and failure modes of representative backdoor attacks and defenses when tested under practical conditions. The framework's efficiency and modularity reduce experimentation barriers.

Conclusion: BackFed provides a plug-and-play environment for comprehensive FL backdoor evaluation, offering empirical insights to guide the development of secure methods. The open-source framework is available for community adoption at the provided GitHub link.

Abstract: Federated Learning (FL) systems are vulnerable to backdoor attacks, where
adversaries train their local models on poisoned data and submit poisoned model
updates to compromise the global model. Despite numerous proposed attacks and
defenses, divergent experimental settings, implementation errors, and
unrealistic assumptions hinder fair comparisons and valid conclusions about
their effectiveness in real-world scenarios. To address this, we introduce
BackFed - a comprehensive benchmark suite designed to standardize, streamline,
and reliably evaluate backdoor attacks and defenses in FL, with a focus on
practical constraints. Our benchmark offers key advantages through its
multi-processing implementation that significantly accelerates experimentation
and the modular design that enables seamless integration of new methods via
well-defined APIs. With a standardized evaluation pipeline, we envision BackFed
as a plug-and-play environment for researchers to comprehensively and reliably
evaluate new attacks and defenses. Using BackFed, we conduct large-scale
studies of representative backdoor attacks and defenses across both Computer
Vision and Natural Language Processing tasks with diverse model architectures
and experimental settings. Our experiments critically assess the performance of
proposed attacks and defenses, revealing unknown limitations and modes of
failures under practical conditions. These empirical insights provide valuable
guidance for the development of new methods and for enhancing the security of
FL systems. Our framework is openly available at
https://github.com/thinh-dao/BackFed.

</details>


### [50] [Cyclic Equalizability of Words and Its Application to Card-Based Cryptography](https://arxiv.org/abs/2507.04916)
*Kazumasa Shinagawa,Koji Nuida*

Main category: cs.CR

TL;DR: This paper establishes a novel connection between card-based cryptography and combinatorics on words, proving that binary words of equal length and Hamming weight can be cyclically equalized via letter insertions, with applications to information erasure and single-cut protocols.


<details>
  <summary>Details</summary>
Motivation: Card-based cryptography has recently been linked to finite group theory and algebraic combinatorics. This work explores its relationship with combinatorics on words to strengthen the theoretical foundation and enable new protocol constructions.

Method: The paper introduces the concept of 'cyclic equalizability' through repeated simultaneous letter insertions and uses formal algebraic proofs from combinatorics on words to show this property holds for binary words with matching length and Hamming weight.

Result: The main result is a proof that any two binary words of equal length and identical Hamming weight can be cyclically equalized by insertions, accompanied by specific applications to the information erasure problem and single-cut full-open protocols in card-based cryptography.

Conclusion: By connecting card-based cryptography to combinatorics on words, this work provides new analytical tools and protocol design methodologies, particularly through the cyclic equalization property for binary words. The results demonstrate potential for improving security and efficiency in physical card protocols.

Abstract: Card-based cryptography is a research area to implement cryptographic
procedures using a deck of physical cards. In recent years, it has been found
to be related to finite group theory and algebraic combinatorics, and is
becoming more and more closely connected to the field of mathematics. In this
paper, we discuss the relationship between card-based cryptography and
combinatorics on words for the first time. In particular, we focus on cyclic
equality of words. We say that a set of words are cyclically equalizable if
they can be transformed to be cyclically equal by repeated simultaneous
insertion of letters. The main result of this paper is to show that two binary
words of equal length and equal Hamming weight are cyclically equalizable. As
applications of cyclic equalizability to card-based cryptography, we describe
its applications to the information erasure problem and to single-cut full-open
protocols.

</details>


### [51] [LIFT: Automating Symbolic Execution Optimization with Large Language Models for AI Networks](https://arxiv.org/abs/2507.04931)
*Ruoxi Wang,Kun Li,Minghui Xu,Yue Zhang,Kaidi Xu,Chunchi Liu,Yinhao Xiao,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: LIFT is a novel framework leveraging LLMs to optimize intermediate representations (IRs) in dynamic symbolic execution (DSE) for distributed AI systems, addressing scalability issues through automated IR transformation.


<details>
  <summary>Details</summary>
Motivation: Traditional DSE faces scalability and efficiency challenges in large-scale distributed AI systems, particularly due to complex network communication patterns and resource-intensive IR processing.

Method: The framework employs a two-phase approach: 1) IR Analysis and Optimization using LLMs to simplify computationally expensive IR blocks without semantic loss, and 2) Symbolic Execution and Validation with benchmarks and semantic verification to ensure correctness and generalization.

Result: Experiments showed up to 53.5% execution time reduction for bigtest binaries (10.24% for random binaries) along with reductions in IR statements, PUT instructions, and temporary variables while maintaining functional equivalence.

Conclusion: LIFT demonstrates that LLMs can effectively optimize IRs for DSE, improving performance and scalability in distributed AI systems without compromising correctness through its structured two-phase validation process.

Abstract: Dynamic Symbolic Execution (DSE) is a key technique in program analysis,
widely used in software testing, vulnerability discovery, and formal
verification. In distributed AI systems, DSE plays a crucial role in
identifying hard-to-detect bugs, especially those arising from complex network
communication patterns. However, traditional approaches to symbolic execution
are often hindered by scalability issues and inefficiencies, particularly in
large-scale systems. This paper introduces LIFT (Large-language-model
Integrated Functional-equivalent-IR Transformation), a novel framework that
leverages Large Language Models (LLMs) to automate the optimization of
Intermediate Representations (IRs) in symbolic execution. LIFT addresses the
challenges of symbolic execution by providing a scalable, context-sensitive
solution for IR transformation. The framework consists of two phases: IR
Analysis and Optimization, where LLMs optimize time-intensive IR blocks, and
Symbolic Execution and Validation, which includes benchmarking and semantic
verification to ensure correctness and generalizability. Experiments on
real-world binaries demonstrated significant performance improvements,
including a 53.5\% reduction in execution time for bigtest and a 10.24\%
reduction for random, along with reductions in IR statements, PUT instructions,
and temporary variables. These results demonstrate that LLMs simplify IRs while
maintaining functional correctness, enhancing symbolic execution in distributed
AI systems.

</details>


### [52] [Bullshark on Narwhal: Implementation-level Workflow Analysis of Round-based DAG Consensus in Theory and Practice](https://arxiv.org/abs/2507.04956)
*Yusei Tanaka*

Main category: cs.CR

TL;DR: This paper introduces Bullshark, a high-performance Round-based DAG BFT protocol using the Narwhal mempool, achieving 297,000 transactions per second with 2-second latency. It highlights the underexplored potential of DAGs in BFT consensus and outlines future improvements targeting Byzantine fault resilience and CAP theorem trade-offs.


<details>
  <summary>Details</summary>
Motivation: Despite active research, practical implementations of theoretical BFT consensus protocols with round-based DAGs (like transaction ordering) are lacking, leading to unclear performance metrics. Existing studies often ignore implementation-level algorithms, limiting real-world evaluation of these protocols.

Method: The authors analyze Bullshark's workflow from transaction submission through blockchain commitment by decomposing its architecture across functional layers. They detail interactions between Bullshark's core components and the Narwhal mempool, emphasizing algorithmic design for optimizing throughput and latency.

Result: Bullshark demonstrated 297,000 TPS with 2-second latency under optimal conditions using Narwhal. This result validates the protocol's capability to achieve high performance, offering concrete metrics where previous DAG-based BFT protocols were theoretical only.

Conclusion: Bullshark proves round-based DAG BFT protocols can be practically implemented for near-optimal performance. The work underscores the importance of bridging theoretical BFT consensus models with real-world implementations while prioritizing Byzantine fault tolerance improvements in future research.

Abstract: Round-based DAGs enable high-performance Byzantine fault-tolerant consensus,
yet their technical advantages remain underutilized due to their short history.
While research on consensus protocols is active in both academia and industry,
many studies overlook implementation-level algorithms, leaving actual
performance unclear - particularly for theoretical protocols whose practical
performance cannot often be evaluated. Bullshark, a Round-based DAG BFT
protocol on Narwhal mempool, achieves optimal performance: 297,000 transactions
per second with 2-second latency. We analyze the algorithm's workflow, from
transaction submission to blockchain commitment, breaking it down layer by
layer at the functional level and delineating the key features and interactions
of the Bullshark and Narwhal components. Future work aims to improve
performance in Byzantine fault environments and optimize trade-offs in the CAP
theorem.

</details>


### [53] [The Hidden Threat in Plain Text: Attacking RAG Data Loaders](https://arxiv.org/abs/2507.05093)
*Alberto Castagnaro,Umberto Salviati,Mauro Conti,Luca Pajola,Simeone Pizzi*

Main category: cs.CR

TL;DR: This paper identifies critical security vulnerabilities in Retrieval-Augmented Generation (RAG) systems during document ingestion, presenting 9 knowledge-based poisoning attacks and 2 novel threat vectors (Content Obfuscation and Injection) with 74.4% success rates across 357 scenarios. It emphasizes the need for securing data loading processes in RAG frameworks.


<details>
  <summary>Details</summary>
Motivation: RAG systems enhance LLM outputs with external knowledge, but existing security defenses focus more on retrieval/query stages than the data loading phase. The paper aims to expose hidden vulnerabilities during document ingestion where malicious alterations can be covertly introduced.

Method: 1) Propose a taxonomy of 9 knowledge-based poisoning attacks targeting document formats. 2) Introduce Content Obfuscation and Injection as new threat models. 3) Develop a toolkit implementing 19 injection techniques for common formats (DOCX, HTML, PDF). 4) Test 5 data loaders and 6 end-to-end RAG systems including NotebookLM and OpenAI Assistants.

Result: 74.4% attack success rate across 357 scenarios against data loaders. Significant vulnerabilities demonstrated in RAG systems that bypass content filters and compromise output integrity.Both white-box and black-box systems showed high susceptibility to the proposed attacks.

Conclusion: Covert content manipulations during document ingestion critically compromise RAG system security. Current data loading pipelines lack effective defenses against these attacks. The findings demand immediate attention to securing knowledge integration processes in LLM frameworks.

Abstract: Large Language Models (LLMs) have transformed human-machine interaction since
ChatGPT's 2022 debut, with Retrieval-Augmented Generation (RAG) emerging as a
key framework that enhances LLM outputs by integrating external knowledge.
However, RAG's reliance on ingesting external documents introduces new
vulnerabilities. This paper exposes a critical security gap at the data loading
stage, where malicious actors can stealthily corrupt RAG pipelines by
exploiting document ingestion.
  We propose a taxonomy of 9 knowledge-based poisoning attacks and introduce
two novel threat vectors -- Content Obfuscation and Content Injection --
targeting common formats (DOCX, HTML, PDF). Using an automated toolkit
implementing 19 stealthy injection techniques, we test five popular data
loaders, finding a 74.4% attack success rate across 357 scenarios. We further
validate these threats on six end-to-end RAG systems -- including white-box
pipelines and black-box services like NotebookLM and OpenAI Assistants --
demonstrating high success rates and critical vulnerabilities that bypass
filters and silently compromise output integrity. Our results emphasize the
urgent need to secure the document ingestion process in RAG systems against
covert content manipulations.

</details>


### [54] [Extreme Learning Machine Based System for DDoS Attacks Detections on IoMT Devices](https://arxiv.org/abs/2507.05132)
*Nelly Elsayed,Lily Dzamesi,Zag ElSayed,Murat Ozer*

Main category: cs.CR

TL;DR: This paper proposes a low-cost, high-accuracy DDoS detection model using extreme learning machine for IoMT to enhance patient safety.


<details>
  <summary>Details</summary>
Motivation: Vulnerabilities in IoMT devices have led to life-threatening DDoS attacks, necessitating effective and cost-efficient detection solutions.

Method: The authors investigate an extreme learning machine approach optimized for fog computing capabilities with minimal implementation resource requirements.

Result: The proposed method achieves high detection accuracy at significantly reduced costs compared to existing solutions, enabling fog-level processing.

Conclusion: The approach demonstrates a viable solution for safeguarding IoMT networks through cost-effective DDoS detection, directly contributing to healthcare security and patient outcomes.

Abstract: The Internet of Medical Things (IoMT) represents a paradigm shift in the
healthcare sector, enabling the interconnection of medical devices, sensors,
and systems to enhance patient monitoring, diagnosis, and management. The rapid
evolution of IoMT presents significant benefits to the healthcare domains.
However, there is a rapid increase in distributed denial of service (DDoS)
attacks on the IoMT networks due to several vulnerabilities in the
IoMT-connected devices, which negatively impact patients' health and can even
lead to deaths. Thus, in this paper, we aim to save lives via investigating an
extreme learning machine for detecting DDoS attacks on IoMT devices. The
proposed approach achieves a high accuracy at a low implementation budget.
Thus, it can reduce the implementation cost of the DDoS detection system,
making the model capable of executing on the fog level.

</details>


### [55] [Hunting in the Dark: Metrics for Early Stage Traffic Discovery](https://arxiv.org/abs/2507.05213)
*Max Gao,Michael Collins,Ricky Mok,kc Claffy*

Main category: cs.CR

TL;DR: This paper analyzes threat hunting practices and metrics through the study of Crackonosh cryptojacking malware's detection patterns and the impact of darkspace size on tracking capabilities.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in threat hunting by exploring detection effectiveness as malware populations decrease and how network environment factors (darkspace size) influence both defender tracking capabilities and attacker behavior patterns.

Method: The authors model defender capabilities using a discoverability metric, evaluate detection methods at varying malware population levels, and conduct experiments with different darkspace sizes to assess their impact on traffic tracking and detection accuracy.

Result: Findings show detection strength decreases with smaller malware populations, while larger darkspaces maintain tracking effectiveness while enabling emergent defender advantages through attacker errors in network navigation.

Conclusion: Dynamic network environments significantly affect threat hunting outcomes, with larger darkspaces proving advantageous for detection by creating detectable patterns from attacker mistakes, rather than hindering tracking through evasion.

Abstract: Threat hunting is an operational security process where an expert analyzes
traffic, applying knowledge and lightweight tools on unlabeled data in order to
identify and classify previously unknown phenomena. In this paper, we examine
threat hunting metrics and practice by studying the detection of Crackonosh, a
cryptojacking malware package, has on various metrics for identifying its
behavior. Using a metric for discoverability, we model the ability of defenders
to measure Crackonosh traffic as the malware population decreases, evaluate the
strength of various detection methods, and demonstrate how different darkspace
sizes affect both the ability to track the malware, but enable emergent
behaviors by exploiting attacker mistakes.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [56] [The Impact of LLM-Assistants on Software Developer Productivity: A Systematic Literature Review](https://arxiv.org/abs/2507.03156)
*Amr Mohamed,Maram Assi,Mariam Guizani*

Main category: cs.SE

TL;DR: A systematic literature review of 37 studies (2014-2024) on LLM-assistants' impact on software developer productivity reveals dual benefits (accelerated development, task automation) and risks (cognitive offloading, collaboration reduction). Research gaps include underexplored SPACE dimensions (Communication/Activity) and limited longitudinal/team-based evaluations.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of synthesized understanding of LLM-assistants' effects on developer productivity across multiple dimensions, highlighting both observed benefits and methodological limitations in existing research.

Method: Systematic literature review of 37 peer-reviewed empirical studies published between 2014-2024, analyzing findings through the SPACE framework (Satisfaction, Productivity, Action, Code, Evaluation) to evaluate productivity impacts comprehensively.

Result: 92% of studies examined ≥2 SPACE dimensions. Primary benefits: code search reduction (87%), development acceleration (68%), task automation (67%). Critical risks: cognitive offloading (41%), reduced collaboration (29%), code quality inconsistency (22%). Only 14% studied beyond three SPACE dimensions.

Conclusion: The review identifies key gaps in understanding communication/efficiency impacts, suggests future research priorities for comprehensive productivity evaluation, and emphasizes the need for longitudinal and team-based studies.

Abstract: Large language model assistants (LLM-assistants) present new opportunities to
transform software development. Developers are increasingly adopting these
tools across tasks, including coding, testing, debugging, documentation, and
design. Yet, despite growing interest, there is no synthesis of how
LLM-assistants affect software developer productivity. In this paper, we
present a systematic literature review of 37 peer-reviewed studies published
between January 2014 and December 2024 that examine this impact. Our analysis
reveals that LLM-assistants offer both considerable benefits and critical
risks. Commonly reported gains include minimized code search, accelerated
development, and the automation of trivial and repetitive tasks. However,
studies also highlight concerns around cognitive offloading, reduced team
collaboration, and inconsistent effects on code quality. While the majority of
studies (92%) adopt a multi-dimensional perspective by examining at least two
SPACE dimensions, reflecting increased awareness of the complexity of developer
productivity, only 14% extend beyond three dimensions, indicating substantial
room for more integrated evaluations. Satisfaction, Performance, and Efficiency
are the most frequently investigated dimensions, whereas Communication and
Activity remain underexplored. Most studies are exploratory (64%) and
methodologically diverse, but lack longitudinal and team-based evaluations.
This review surfaces key research gaps and provides recommendations for future
research and practice. All artifacts associated with this study are publicly
available at https://zenodo.org/records/15788502.

</details>


### [57] [Assessing Small Language Models for Code Generation: An Empirical Study with Benchmarks](https://arxiv.org/abs/2507.03160)
*Md Mahade Hasan,Muhammad Waseem,Kai-Kristian Kemell,Jussi Raskua,Juha Ala-Rantalaa,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: This paper evaluates 20 open-source Small Language Models (SLMs) for code generation across 5 benchmarks, analyzing performance vs. efficiency trade-offs and multilingual capabilities, finding that compact models offer cost-effective solutions while larger models improve accuracy at higher resource costs.


<details>
  <summary>Details</summary>
Motivation: SLMs present lightweight code generation alternatives to LLMs, but empirical data on their effectiveness, limitations, and computational trade-offs in constrained environments remains insufficient, necessitating a structured evaluation for practical deployment guidance.

Method: Assessed 20 SLMs (0.4B-10B parameters) using functional correctness, computational efficiency, and multilingual performance on HumanEval, MBPP, Mercury, HumanEvalPack, and CodeXGLUE benchmarks, measuring VRAM consumption and cross-language consistency.

Result: Small models achieve competitive results with efficiency advantages, while larger models require ~4x VRAM for 10% performance gains; Python/Java/PHP outperform Go/C++/Ruby in SLM performance, though differences lack statistical significance.

Conclusion: The study provides actionable insights for SLM design and selection in code generation, highlighting the balance between model size, accuracy, and scalability, and suggesting that multilingual generalization is feasible despite minor performance variations.

Abstract: The recent advancements of Small Language Models (SLMs) have opened new
possibilities for efficient code generation. SLMs offer lightweight and
cost-effective alternatives to Large Language Models (LLMs), making them
attractive for use in resource-constrained environments. However, empirical
understanding of SLMs, particularly their capabilities, limitations, and
performance trade-offs in code generation remains limited. This study presents
a comprehensive empirical evaluation of 20 open-source SLMs ranging from 0.4B
to 10B parameters on five diverse code-related benchmarks (HumanEval, MBPP,
Mercury, HumanEvalPack, and CodeXGLUE). The models are assessed along three
dimensions: i) functional correctness of generated code, ii) computational
efficiency and iii) performance across multiple programming languages. The
findings of this study reveal that several compact SLMs achieve competitive
results while maintaining a balance between performance and efficiency, making
them viable for deployment in resource-constrained environments. However,
achieving further improvements in accuracy requires switching to larger models.
These models generally outperform their smaller counterparts, but they require
much more computational power. We observe that for 10% performance
improvements, models can require nearly a 4x increase in VRAM consumption,
highlighting a trade-off between effectiveness and scalability. Besides, the
multilingual performance analysis reveals that SLMs tend to perform better in
languages such as Python, Java, and PHP, while exhibiting relatively weaker
performance in Go, C++, and Ruby. However, statistical analysis suggests these
differences are not significant, indicating a generalizability of SLMs across
programming languages. Based on the findings, this work provides insights into
the design and selection of SLMs for real-world code generation tasks.

</details>


### [58] [Analyzing C/C++ Library Migrations at the Package-level: Prevalence, Domains, Targets and Rationals across Seven Package Management Tools](https://arxiv.org/abs/2507.03263)
*Haiqiao Gu,Yiliang Zhao,Kai Gao,Minghui Zhou*

Main category: cs.SE

TL;DR: This paper generates the first C/C++ library migration dataset by analyzing 19,943 projects, revealing unique migration patterns and reasons (e.g., GUI/OS focus, dependency unification) compared to Python, JavaScript, and Java ecosystems.


<details>
  <summary>Details</summary>
Motivation: Existing research on library migration focuses on PLs with central package hosting platforms (Python, JavaScript, Java), but C/C++'s fragmented dependency management practices leave a critical knowledge gap.

Method: Analyzes 19,943 C/C++ projects across various package management tools to create the first C/C++ library migration dataset, comparing migration trends with Python, JavaScript, and Java.

Result: C/C++ migrations occur predominantly in GUI/Build/OS domains (vs. Testing/Logging in other PLs), 83.46% of source libraries have single migration targets. Four C/C++-specific migration reasons identified: compile time reduction, dependency management unification, license adjustments, and toolchain compatibility.

Conclusion: Findings provide actionable insights for C/C++ developers to make informed library migration decisions and guide the development of C/C++-specific migration tools addressing unique dependency management challenges.

Abstract: Library migration happens when a library can not meet the project's
requirements and is non-trivial to accomplish. To mitigate the problem,
substantial efforts have been devoted to understanding its characteristics and
recommending alternative libraries, especially for programming language (PL)
ecosystems with a central package hosting platform, such as Python (PyPI).
However, to the best of our knowledge, understanding of C/C++ library
migrations is still lacking, possibly due to challenges resulting from the
fragmented and complicated dependency management practices in the C/C++
ecosystem. To bridge this knowledge gap, this paper analyzes 19,943 C/C++
projects that utilize different package management tools and establishes the
first C/C++ library migration dataset. Based on the dataset, we investigate the
prevalence, domains, target library, and rationale of C/C++ library migrations
and compare the results with three widely investigated PLs: Python, JavaScript,
and Java. We find that the overall trend in the number of C/C++ library
migrations is similar to Java. Migrations across different package management
tools are also observed. In C/C++, library migrations mainly occur in GUI,
Build, and OS development, but are rare in domains (e.g., Testing and Logging)
that dominate library migrations in the three compared PLs. 83.46\% of C/C++
source libraries only have one migration target, suggesting that our library
migration dataset could be used directly to recommend migration targets. We
find four C/C++-specific migration reasons, such as less compile time and
unification of dependency management, revealing the unique dependency
management requirements in C/C++ projects. We believe our findings can help
C/C++ developers make more informed library migration decisions and shed light
on the design of C/C++ library migration tools.

</details>


### [59] [scikit-package -- software packaging standards and roadmap for sharing reproducible scientific software](https://arxiv.org/abs/2507.03328)
*S. Lee,C. Myers,A. Yang,T. Zhang,S. J. L. Billinge*

Main category: cs.SE

TL;DR: scikit-package provides a roadmap and tools for scientists to create reusable, shareable, and reproducible software.


<details>
  <summary>Details</summary>
Motivation: Scientific reproducibility and collaboration face challenges with code versioning, quality, and sharing when researchers develop software without professional software engineering training.

Method: The project combines tutorials with automated workflows and community-maintained tools to guide scientists toward software best practices.

Result: Enables code reuse at multiple complexity levels (from functions to packages) with standardized, tested, and documented tools.

Conclusion: scikit-package democratizes software quality for scientific communities through practical, low-effort workflows and education, ensuring reproducible and shareable research software.

Abstract: Scientific advancement relies on the ability to share and reproduce results.
When data analysis or calculations are carried out using software written by
scientists there are special challenges around code versions, quality and code
sharing. scikit-package provides a roadmap to facilitate code reuse and sharing
with minimal effort through tutorials coupled with automated and centralized
reusable workflows. The goal of the project is to provide pedagogical and
practical tools for scientists who are not professionally trained software
engineers to write more reusable and maintainable software code. Code reuse can
occur at multiple levels of complexity-from turning a code block into a
function within a single script, to publishing a publicly installable, fully
tested, and documented software package scikit-package provides a community
maintained set of tools, and a roadmap, to help scientists bring their software
higher levels of reproducibility and shareability.

</details>


### [60] [Prompt Engineering Guidelines for Using Large Language Models in Requirements Engineering](https://arxiv.org/abs/2507.03405)
*Krishna Ronanki,Simon Arvidsson,Johan Axell*

Main category: cs.SE

TL;DR: The study systematically reviews prompt engineering guidelines for LLMs in Requirements Engineering (RE), identifies gaps, and proposes a guideline mapping to address RE-specific challenges, concluding with future research directions.


<details>
  <summary>Details</summary>
Motivation: The critical need for ensuring quality and accuracy of LLM-generated output in RE, combined with limited existing guidance on prompt engineering for domain-specific RE activities.

Method: A systematic review of primary literature to compile prompt engineering guidelines followed by interviews with RE experts to assess their relevance and limitations.

Result: Literature review revealed a shortage of domain-specific prompt engineering guidelines for RE. Expert interviews provided insights on applying extracted guidelines, informing our proposed mapping.

Conclusion: The study presents findings on prompt engineering applicability in RE, contributes a guideline mapping to bridge the domain gap, and highlights an important future research direction in this area.

Abstract: The rapid emergence of generative AI models like Large Language Models (LLMs)
has demonstrated its utility across various activities, including within
Requirements Engineering (RE). Ensuring the quality and accuracy of
LLM-generated output is critical, with prompt engineering serving as a key
technique to guide model responses. However, existing literature provides
limited guidance on how prompt engineering can be leveraged, specifically for
RE activities. The objective of this study is to explore the applicability of
existing prompt engineering guidelines for the effective usage of LLMs within
RE. To achieve this goal, we began by conducting a systematic review of primary
literature to compile a non-exhaustive list of prompt engineering guidelines.
Then, we conducted interviews with RE experts to present the extracted
guidelines and gain insights on the advantages and limitations of their
application within RE. Our literature review indicates a shortage of prompt
engineering guidelines for domain-specific activities, specifically for RE. Our
proposed mapping contributes to addressing this shortage. We conclude our study
by identifying an important future line of research within this field.

</details>


### [61] [Enhancing Uncertainty Quantification for Runtime Safety Assurance Using Causal Risk Analysis and Operational Design Domain](https://arxiv.org/abs/2507.03515)
*Radouane Bouchekir,Michell Guzman Cancimance*

Main category: cs.SE

TL;DR: The paper enhances uncertainty quantification for autonomous systems by integrating environmental conditions into a Bayesian Network (BN) for dynamic safety estimation.


<details>
  <summary>Details</summary>
Motivation: Deep learning components' inherent uncertainty and sensitivity to environmental changes challenge the runtime safety of autonomous systems.

Method: Leverages HARA and fault tree modeling to identify critical operational conditions, then integrates them with data/model uncertainties into a BN for probabilistic safety inference.

Result: Demonstrated the approach through a case study on Object Detection in Automated Valet Parking, enabling computation of expected performance and uncertainty variance.

Conclusion: Proposes a context-aware safety estimation framework that improves runtime dependability of autonomous systems under varying conditions.

Abstract: Ensuring the runtime safety of autonomous systems remains challenging due to
deep learning components' inherent uncertainty and their sensitivity to
environmental changes. In this paper, we propose an enhancement of traditional
uncertainty quantification by explicitly incorporating environmental conditions
using risk-based causal analysis. We leverage Hazard Analysis and Risk
Assessment (HARA) and fault tree modeling to identify critical operational
conditions affecting system functionality. These conditions, together with
uncertainties from the data and model, are integrated into a unified Bayesian
Network (BN). At runtime, this BN is instantiated using real-time environmental
observations to infer a probabilistic distribution over the safety estimation.
This distribution enables the computation of both expected performance and its
associated variance, providing a dynamic and context-aware measure of
uncertainty. We demonstrate our approach through a case study of the Object
Detection (OD) component in an Automated Valet Parking (AVP).

</details>


### [62] [The Role of Humour in Software Engineering -- A Literature Review and Preliminary Taxonomy](https://arxiv.org/abs/2507.03527)
*Dulaji Hidellaarachchi,John Grundy,Rashina Hoda*

Main category: cs.SE

TL;DR: This paper develops a literature-based taxonomy of humor in software engineering teams to enhance productivity, communication, and work environments while addressing responsible usage.


<details>
  <summary>Details</summary>
Motivation: Humor's role in software engineering remains under-researched despite its established benefits in other domains, necessitating structured examination for SE-specific insights.

Method: A literature review integrating psychology, sociology, and organizational behavior studies categorized humor into theories, styles, models, and scales, tailored for software engineering contexts.

Result: The taxonomy framework highlights humor's potential benefits and challenges in SE, providing professionals and researchers with a structural lens for analysis.

Conclusion: Strategic humor use can foster cohesive, creative SE environments, but further empirical validation is needed to solidify its application and mitigate risks.

Abstract: Humour has long been recognized as a key factor in enhancing creativity,
group effectiveness, and employee well-being across various domains. However,
its occurrence and impact within software engineering (SE) teams remains
under-explored. This paper introduces a comprehensive, literature review-based
taxonomy exploring the characterisation and use of humour in SE teams, with the
goal of boosting productivity, improving communication, and fostering a
positive work environment while emphasising the responsible use of humour to
mitigate its potential negative impacts. Drawing from a wide array of studies
in psychology, sociology, and organizational behaviour, our proposed framework
categorizes humour into distinct theories, styles, models, and scales, offering
SE professionals and researchers a structured approach to understanding humour
in their work. This study also addresses the unique challenges of applying
humour in SE, highlighting its potential benefits while acknowledging the need
for further empirical validation in this context. Ultimately, our study aims to
pave the way for more cohesive, creative, and psychologically supportive SE
environments through the strategic use of humour.

</details>


### [63] [ACE: Automated Technical Debt Remediation with Validated Large Language Model Refactorings](https://arxiv.org/abs/2507.03536)
*Adam Tornhill,Markus Borg,Nadim Hagatulah,Emma Söderberg*

Main category: cs.SE

TL;DR: This paper introduces Augmented Code Engineering (ACE), a tool that automates code improvements using validated LLM output to address program understanding inefficiencies in modern software development.


<details>
  <summary>Details</summary>
Motivation: Developers spend 70% of their time understanding code, not writing it, creating a bottleneck in the AI-assisted coding era. Improving code understandability could significantly reduce technical debt and optimize developer productivity.

Method: ACE was developed through a data-driven approach to deliver reliable refactoring suggestions by combining objective code quality metrics with program correctness validation using large language models.

Result: Early user feedback indicates AI-enabled refactoring via ACE effectively mitigates code-level technical debt that would otherwise remain unresolved in typical development workflows.

Conclusion: Automated code improvement tools like ACE are essential for managing technical debt and ensuring developer efficiency when working with rapidly expanding codebases in the AI-assisted development landscape.

Abstract: The remarkable advances in AI and Large Language Models (LLMs) have enabled
machines to write code, accelerating the growth of software systems. However,
the bottleneck in software development is not writing code but understanding
it; program understanding is the dominant activity, consuming approximately 70%
of developers' time. This implies that improving existing code to make it
easier to understand has a high payoff and - in the age of AI-assisted coding -
is an essential activity to ensure that a limited pool of developers can keep
up with ever-growing codebases. This paper introduces Augmented Code
Engineering (ACE), a tool that automates code improvements using validated LLM
output. Developed through a data-driven approach, ACE provides reliable
refactoring suggestions by considering both objective code quality improvements
and program correctness. Early feedback from users suggests that AI-enabled
refactoring helps mitigate code-level technical debt that otherwise rarely gets
acted upon.

</details>


### [64] [Is It Time To Treat Prompts As Code? A Multi-Use Case Study For Prompt Optimization Using DSPy](https://arxiv.org/abs/2507.03620)
*Francisca Lemos,Victor Alves,Filipa Ferraz*

Main category: cs.SE

TL;DR: This study evaluates the Declarative Self-improving Python (DSPy) framework for optimizing Large Language Model prompts across five use cases, showing mixed performance improvements ranging from modest gains to significant accuracy boosts in specific tasks.


<details>
  <summary>Details</summary>
Motivation: Prompt engineering for LLMs is typically a labor-intensive, intuitive trial-and-error process, motivating the need for a systematic, automated optimization framework to enhance efficiency and reliability.

Method: DSPy is applied to optimize prompts in five scenarios: guardrail enforcement, code hallucination detection, code generation, routing agents, and prompt evaluation. Each case assesses performance changes through automated instruction and example selection refinements.

Result: While guardrails and hallucination detection showed minor improvements, prompt evaluation accuracy increased from 46.2% to 64.0%. Router agent optimization raised accuracy from 85.0% to 90.0%, but using the optimized prompt with a cheaper model did not yield performance parity with stronger models.

Conclusion: DSPy demonstrates potential to improve LLM performance via systematic prompt optimization, particularly when instruction tuning and example selection are co-optimized. However, its effectiveness is task-dependent, emphasizing the necessity for context-specific evaluations in prompt engineering research.

Abstract: Although prompt engineering is central to unlocking the full potential of
Large Language Models (LLMs), crafting effective prompts remains a
time-consuming trial-and-error process that relies on human intuition. This
study investigates Declarative Self-improving Python (DSPy), an optimization
framework that programmatically creates and refines prompts, applied to five
use cases: guardrail enforcement, hallucination detection in code, code
generation, routing agents, and prompt evaluation. Each use case explores how
prompt optimization via DSPy influences performance. While some cases
demonstrated modest improvements - such as minor gains in the guardrails use
case and selective enhancements in hallucination detection - others showed
notable benefits. The prompt evaluation criterion task demonstrated a
substantial performance increase, rising accuracy from 46.2% to 64.0%. In the
router agent case, the possibility of improving a poorly performing prompt and
of a smaller model matching a stronger one through optimized prompting was
explored. Although prompt refinement increased accuracy from 85.0% to 90.0%,
using the optimized prompt with a cheaper model did not improve performance.
Overall, this study's findings suggest that DSPy's systematic prompt
optimization can enhance LLM performance, particularly when instruction tuning
and example selection are optimized together. However, the impact varies by
task, highlighting the importance of evaluating specific use cases in prompt
optimization research.

</details>


### [65] [Specification-Guided Repair of Arithmetic Errors in Dafny Programs using LLMs](https://arxiv.org/abs/2507.03659)
*Valentina Wu,Alexandra Mendes,Alexandre Abreu*

Main category: cs.SE

TL;DR: The paper introduces an Automated Program Repair (APR) tool for Dafny that combines Hoare Logic and Large Language Models (LLMs) to address arithmetic bugs using formal specifications as oracles, achieving high localization accuracy and repair success rates.


<details>
  <summary>Details</summary>
Motivation: Traditional APR techniques rely on test suites, which may miss critical scenarios; formal specifications offer stronger correctness criteria, making fault localization and repair more effective for verification-aware languages like Dafny.

Method: 1) Use Hoare Logic to assess the state of each program statement. 2) Leverage LLMs (GPT-4o mini, Llama 3, Mistral 7B, Llemma 7B) to synthesize candidate fixes. 3) Validate correctness using Dafny's formal specifications as oracles.

Result: Achieved 89.6% fault localization accuracy on the DafnyBench benchmark, with GPT-4o mini showing the highest repair success rate (74.18%).

Conclusion: Combining formal reasoning (Hoare Logic) with LLM-driven synthesis significantly improves APR effectiveness for Dafny, demonstrating the potential of integrating verification-aware methods with AI-based program repair.

Abstract: Formal verification offers strong assurances of software correctness.
However, debugging and repairing the underlying faults can be complex and
time-consuming when verification fails. Automated Program Repair (APR) aims to
ease this by automatically identifying and fixing faults. Traditional APR
techniques often depend on test suites for validation, but these may fail to
capture all scenarios. In contrast, formal specifications provide stronger
correctness criteria for effective repairs.
  We present an innovative APR tool for Dafny, a verification-aware programming
language that uses formal specifications - including pre-conditions,
post-conditions, and invariants - as oracles for fault localization and repair.
Assuming the correctness of the specifications and focusing on arithmetic bugs,
we localize faults through a series of steps, which include using Hoare Logic
to determine the state of each statement within the program and
state-of-the-art Large Language Models (LLMs) to synthesize candidate fixes.
The chosen models were GPT-4o mini, Llama 3, Mistral 7B, and Llemma 7B.
  We evaluate our approach using DafnyBench, a benchmark of real-world Dafny
programs. Our tool achieves 89.6% accuracy in fault localization, with GPT-4o
mini yielding the highest repair success rate (74.18%). These results highlight
the potential of combining formal reasoning with LLM-driven program synthesis
for automated program repair.

</details>


### [66] [Efficient Detection of Intermittent Job Failures Using Few-Shot Learning](https://arxiv.org/abs/2507.04173)
*Henri Aïdasso,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: The paper introduces a few-shot learning approach for detecting intermittent job failures in CI/CD pipelines, achieving higher accuracy than state-of-the-art heuristic methods that rely on rerun data and mislabel 32% of cases.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art methods for classifying intermittent job failures use rerun-based heuristics but fail when reruns are not explicitly implemented, leading to significant mislabeling rates (32%) and poor performance in practice.

Method: Fine-tunes a small language model using 12 manually labeled log examples per project to generate rich embeddings, which are then used as features for an ML classifier trained to detect intermittent failures.

Result: Achieved 70-88% F1-score with only 12 labeled examples per project, outperforming state-of-the-art methods (34-52% F1-score) in four of the six analyzed projects.

Conclusion: Demonstrates that few-shot learning with high-quality manually labeled data surpasses large-scale heuristic approaches for intermittent failure detection, emphasizing the importance of data quality over quantity in practical CI/CD contexts.

Abstract: One of the main challenges developers face in the use of continuous
integration (CI) and deployment pipelines is the occurrence of intermittent job
failures, which result from unexpected non-deterministic issues (e.g., flaky
tests or infrastructure problems) rather than regular code-related errors such
as bugs. Prior studies developed machine-learning (ML) models trained on large
datasets of job logs to classify job failures as either intermittent or
regular. As an alternative to costly manual labeling of large datasets, the
state-of-the-art (SOTA) approach leveraged a heuristic based on
non-deterministic job reruns. However, this method mislabels intermittent job
failures as regular in contexts where rerunning suspicious job failures is not
an explicit policy, and therefore limits the SOTA's performance in practice. In
fact, our manual analysis of 2,125 job failures from 5 industrial and 1
open-source projects reveals that, on average, 32\% of intermittent job
failures are mislabeled as regular. To address these limitations, this paper
introduces a novel approach to intermittent job failure detection using
few-shot learning (FSL). Specifically, we fine-tune a small language model
using a few number of manually labeled log examples to generate rich
embeddings, which are then used to train an ML classifier. Our FSL-based
approach achieves 70-88\% F1-score with only 12 shots in all projects,
outperforming the SOTA, which proved ineffective (34-52\% F1-score) in 4
projects. Overall, this study underlines the importance of data quality over
quantity and provides a more efficient and practical framework for the
detection of intermittent job failures in organizations.

</details>


### [67] [From Legal Text to Tech Specs: Generative AI's Interpretation of Consent in Privacy Law](https://arxiv.org/abs/2507.04185)
*Aniket Kesari,Travis Breaux,Tom Norton,Sarah Santos,Anmol Singhal*

Main category: cs.SE

TL;DR: This paper explores using Large Language Models (LLMs) in requirements engineering to improve compliance with privacy laws like the CCPA by addressing challenges in translating legal requirements into technical software implementations. It outlines a three-step pipeline involving classification, modification generation, and manual validation, highlighting LLMs' automation potential while noting their reasoning limitations.


<details>
  <summary>Details</summary>
Motivation: The increasing reliance on consent-based privacy laws such as the CCPA has created significant challenges in operationalizing these legal mandates within software development processes, necessitating a clearer understanding of how to bridge the gap between legal and technical requirements.

Method: The study employs a three-step pipeline: (1) using an LLM to classify software use cases for compliance with legal standards, (2) generating potential modifications for non-compliant use cases, and (3) manually validating these modifications against legal frameworks to assess their effectiveness.

Result: Preliminary findings demonstrate LLMs' potential to automate compliance tasks but also reveal limitations in their reasoning capabilities when applied to real-world privacy use cases, indicating a need for further refinement in aligning technical and legal interpretations.

Conclusion: While LLMs show promise in streamlining legal compliance for software, their current reasoning limitations necessitate cautious implementation and manual validation to ensure robustness, offering valuable insights for improving AI-driven compliance solutions in future work.

Abstract: Privacy law and regulation have turned to "consent" as the legitimate basis
for collecting and processing individuals' data. As governments have rushed to
enshrine consent requirements in their privacy laws, such as the California
Consumer Privacy Act (CCPA), significant challenges remain in understanding how
these legal mandates are operationalized in software. The opaque nature of
software development processes further complicates this translation. To address
this, we explore the use of Large Language Models (LLMs) in requirements
engineering to bridge the gap between legal requirements and technical
implementation. This study employs a three-step pipeline that involves using an
LLM to classify software use cases for compliance, generating LLM modifications
for non-compliant cases, and manually validating these changes against legal
standards. Our preliminary findings highlight the potential of LLMs in
automating compliance tasks, while also revealing limitations in their
reasoning capabilities. By benchmarking LLMs against real-world use cases, this
research provides insights into leveraging AI-driven solutions to enhance legal
compliance of software.

</details>


### [68] [Improving Deep Learning Framework Testing with Model-Level Metamorphic Testing](https://arxiv.org/abs/2507.04354)
*Yanzhou Mu,Juan Zhai,Chunrong Fang,Xiang Chen,Zhixiang Cao,Peiran Yang,Kexin Zhao,An Guo,Zhenyu Chen*

Main category: cs.SE

TL;DR: The paper proposes ModelMeta, a model-level metamorphic testing method addressing limitations of existing approaches by exploiting DL model structure characteristics and runtime metrics analysis for comprehensive framework bug detection.


<details>
  <summary>Details</summary>
Motivation: Deep learning frameworks require effective testing due to potential disastrous bugs, but existing methods face challenges from floating-point errors, inherent randomness, and complex test inputs, resulting in unsuitable test oracles that lack generalization and fail to detect bugs in multi-interface combinations and runtime behaviors.

Method: ModelMeta introduces four metamorphic relations (MRs) focused on model structure and couples them with the QR-DQN strategy to generate diverse interface-combination test inputs with consistent outputs, then detects bugs via detailed analysis of training loss/gradients, memory/GPU usage, and execution time.

Result: The approach overcomes prior limitations by testing multi-interface combinations, analyzing runtime resource metrics, and capturing structural bugs in addition to result consistency.

Conclusion: ModelMeta advances DL framework testing by addressing structural complexity, interface generalization, and runtime metrics. Its four MRs combined with a strategic testing approach enable more effective detection of framework bugs beyond single-interface consistency issues.

Abstract: Deep learning (DL) frameworks are essential to DL-based software systems, and
framework bugs may lead to substantial disasters, thus requiring effective
testing. Researchers adopt DL models or single interfaces as test inputs and
analyze their execution results to detect bugs. However, floating-point errors,
inherent randomness, and the complexity of test inputs make it challenging to
analyze execution results effectively, leading to existing methods suffering
from a lack of suitable test oracles. Some researchers utilize metamorphic
testing to tackle this challenge. They design Metamorphic Relations (MRs) based
on input data and parameter settings of a single framework interface to
generate equivalent test inputs, ensuring consistent execution results between
original and generated test inputs. Despite their promising effectiveness, they
still face certain limitations. (1) Existing MRs overlook structural
complexity, limiting test input diversity. (2) Existing MRs focus on limited
interfaces, which limits generalization and necessitates additional
adaptations. (3) Their detected bugs are related to the result consistency of
single interfaces and far from those exposed in multi-interface combinations
and runtime metrics (e.g., resource usage). To address these limitations, we
propose ModelMeta, a model-level metamorphic testing method for DL frameworks
with four MRs focused on the structure characteristics of DL models. ModelMeta
augments seed models with diverse interface combinations to generate test
inputs with consistent outputs, guided by the QR-DQN strategy. It then detects
bugs through fine-grained analysis of training loss/gradients, memory/GPU
usage, and execution time.

</details>


### [69] [DevMuT: Testing Deep Learning Framework via Developer Expertise-Based Mutation](https://arxiv.org/abs/2507.04360)
*Yanzhou Mu,Juan Zhai,Chunrong Fang,Xiang Chen,Zhixiang Cao,Peiran Yang,Yinglong Zou,Tao Zheng,Zhenyu Chen*

Main category: cs.SE

TL;DR: DevMuT is a novel deep learning framework testing method that leverages developer expertise to identify high-value defects across model training and inference via mutation operators and lifecycle simulations.


<details>
  <summary>Details</summary>
Motivation: Most existing deep learning framework defect detection methods focus on trivial or 'edge case' bugs ignored by developers, necessitating techniques to identify critical defects aligned with developer priorities.

Method: DevMuT generates diverse test models by integrating mutation operators with domain constraints derived from developer knowledge, simulating realistic development workflows through staged lifecycle testing.

Result: Outperformed state-of-the-art baselines with 71.68% improvement in model diversity and 28.20% higher legal rates while detecting 117 defects (24 of which were fixed), including 8 high-value ones validated by developers across 3 frameworks.

Conclusion: DevMuT successfully detects developer-priority defects relevant to real-world usage, demonstrated by its 2023 deployment in MindSpore and confirmation of multiple critical issues by framework communities.

Abstract: Deep learning (DL) frameworks are the fundamental infrastructure for various
DL applications. Framework defects can profoundly cause disastrous accidents,
thus requiring sufficient detection. In previous studies, researchers adopt DL
models as test inputs combined with mutation to generate more diverse models.
Though these studies demonstrate promising results, most detected defects are
considered trivial (i.e., either treated as edge cases or ignored by the
developers). To identify important bugs that matter to developers, we propose a
novel DL framework testing method DevMuT, which generates models by adopting
mutation operators and constraints derived from developer expertise. DevMuT
simulates developers'common operations in development and detects more diverse
defects within more stages of the DL model lifecycle (e.g., model training and
inference). We evaluate the performance of DevMuT on three widely used DL
frameworks (i.e., PyTorch, JAX, and Mind- Spore) with 29 DL models from nine
types of industry tasks. The experiment results show that DevMuT outperforms
state-of-the-art baselines: it can achieve at least 71.68% improvement on
average in the diversity of generated models and 28.20% improvement on average
in the legal rates of generated models. Moreover, DevMuT detects 117 defects,
63 of which are confirmed, 24 are fixed, and eight are of high value confirmed
by developers. Finally, DevMuT has been deployed in the MindSpore community
since December 2023. These demonstrate the effectiveness of DevMuT in detecting
defects that are close to the real scenes and are of concern to developers.

</details>


### [70] [Exploring React Library Related Questions on Stack Overflow: Answered vs. Unanswered](https://arxiv.org/abs/2507.04390)
*Vanesya Aura Ardity,Yusuf Sulistyo Nugroho,Syful Islam*

Main category: cs.SE

TL;DR: Analyzes factors affecting answerability and difficulty of React-related questions on Stack Overflow using EDA and regression models.


<details>
  <summary>Details</summary>
Motivation: Despite React's popularity, many questions remain unanswered on Stack Overflow, prompting investigation into factors influencing answerability.

Method: Applied Exploratory Data Analysis to 534,820 questions with 23 React tags, using logistic regression (attributes affecting answerability) and linear regression (user reputation vs. PD Score).

Result: Code snippets and user reputation increase answerability. Comments, question length, and images decrease it. Higher user reputation correlates with -0.092 reduction in PD Score, indicating complex questions from experienced users.

Conclusion: Technical Q&A platforms require consideration of post characteristics (e.g., code inclusion, question conciseness) to enhance answerability. Experienced users may ask more challenging questions, lowering PD Scores.

Abstract: React is a popular JavaScript framework in modern web application
development. Due to its high performance and efficiency, many developers use
this framework. Although React library offers many advantages, it is not
without its challenges. When using React library, developers often face
problems where they often seek solutions through question-and-answer forums,
such as Stack Overflow (SO). However, despite its high popularity, many
React-related questions on SO remain unanswered. Thus, this study aims to
analyze the factors associated with question answerability and difficulty
levels of React-related questions on SO. To facilitate our study, Exploratory
Data Analysis was applied to 534,820 questions, where they are filtered based
on 23 React-related tags. We implemented a quantitative approach through text
mining and statistical analysis. A logistic regression model was used to
identify attributes associated with question answerability, while a simple
linear regression model was employed to examine the correlation between user
reputations and performance difficulty scores (PD Score). The results show that
some attributes, such as number of views, code snippet inclusion, number of
lines of code, and user reputation, positively affect the likelihood of
question answerability. In contrast, the number of comments, question lengths,
and presence of images in React-related questions reduce the probability of a
question receiving responses from users. Further investigation indicates a
negative correlation between user reputations and PD Score, where reputation
increase corresponds to -0.092 reduction in PD score, signaling experienced
users tend to propose more complex technical inquiries. This study provides
insights into the characteristics of technical question-and-answer platforms,
such as SO, that users need to consider the answerability factors when posting
questions related to React.

</details>


### [71] [Learning Software Bug Reports: A Systematic Literature Review](https://arxiv.org/abs/2507.04422)
*Guoming Long,Jingzhi Gong,Hui Fang,Tao Chen*

Main category: cs.SE

TL;DR: A systematic literature review analyzing 204 out of 1,825 papers to summarize trends and challenges in ML-based bug report analysis, identifying key models, techniques, and future research directions


<details>
  <summary>Details</summary>
Motivation: Recent growth of ML in bug report analysis lacks a comprehensive review, necessitating structured insights to guide practitioners and researchers

Method: Systematic literature review of 1,825 papers with detailed analysis of 204 selected works

Result: Seven key findings: 1) CNN, LSTM, $k$NN dominance over BERT, 2) Word2Vec/TF-IDF feature prevalence, 3) Stop word removal as common preprocessing, 4) Eclipse/Mozilla as benchmark projects, 5) Bug categorization as primary task, 6) Rising focus on non-functional/performance bugs, 7) $k$-fold cross-validation preference with limited statistical rigor

Conclusion: Summary highlights ML model/technique adoption patterns, project datasets, and evaluation methodologies in bug report analysis while identifying gaps (e.g., insufficient statistical tests) and suggesting six future research directions for practical impact

Abstract: The recent advancement of artificial intelligence, especially machine
learning (ML), has significantly impacted software engineering research,
including bug report analysis. ML aims to automate the understanding,
extraction, and correlation of information from bug reports. Despite its
growing importance, there has been no comprehensive review in this area. In
this paper, we present a systematic literature review covering 1,825 papers,
selecting 204 for detailed analysis. We derive seven key findings: 1) Extensive
use of CNN, LSTM, and $k$NN for bug report analysis, with advanced models like
BERT underutilized due to their complexity. 2) Word2Vec and TF-IDF are popular
for feature representation, with a rise in deep learning approaches. 3) Stop
word removal is the most common preprocessing, with structural methods rising
after 2020. 4) Eclipse and Mozilla are the most frequently evaluated software
projects. 5) Bug categorization is the most common task, followed by bug
localization and severity prediction. 6) There is increasing attention on
specific bugs like non-functional and performance bugs. 7) Common evaluation
metrics are F1-score, Recall, Precision, and Accuracy, with $k$-fold
cross-validation preferred for model evaluation. 8) Many studies lack robust
statistical tests. We also identify six promising future research directions to
provide useful insights for practitioners.

</details>


### [72] [SPIRA: Building an Intelligent System for Respiratory Insufficiency Detection](https://arxiv.org/abs/2507.04548)
*Renato Cordeiro Ferreira,Dayanne Gomes,Vitor Tamae,Francisco Wernke,Alfredo Goldman*

Main category: cs.SE

TL;DR: This paper presents SPIRA, an intelligent voice-based system for detecting respiratory insufficiency, detailing implementation challenges and lessons learned in data, training, and inference.


<details>
  <summary>Details</summary>
Motivation: Respiratory insufficiency requires timely detection; voice-based systems offer non-invasive alternatives but face practical barriers in development.

Method: The authors built and iteratively refined two versions of SPIRA's architecture, analyzing obstacles during data collection, training, and inference phases.

Result: Key challenges included limited voice data quality, model reproducibility during training, and deployment constraints in real-time inference for medical applications.

Conclusion: The paper establishes practical guidelines for future voice-based medical detection systems by systematically documenting implementation pitfalls and solutions.

Abstract: Respiratory insufficiency is a medic symptom in which a person gets a reduced
amount of oxygen in the blood. This paper reports the experience of building
SPIRA: an intelligent system for detecting respiratory insufficiency from
voice. It compiles challenges faced in two succeeding implementations of the
same architecture, summarizing lessons learned on data collection, training,
and inference for future projects in similar systems.

</details>


### [73] [Testing, Evaluation, Verification and Validation (TEVV) of Digital Twins: A Comprehensive Framework](https://arxiv.org/abs/2507.04555)
*Gabriella Waters*

Main category: cs.SE

TL;DR: The paper introduces a comprehensive framework for Testing, Evaluation, Verification, and Validation (TEVV) of digital twins to ensure their accuracy, reliability, and ethical implementation.


<details>
  <summary>Details</summary>
Motivation: As digital twins grow in complexity and decision-making importance, their accuracy, reliability, and ethical use must be rigorously ensured.

Method: Developing a TEVV framework tailored to the dynamic and complex nature of digital twins.

Result: Presentation of the framework addressing challenges in real-time monitoring, predictive analysis, and optimization of digital twins.

Conclusion: The proposed TEVV framework is critical for advancing digital twin technology responsibly and effectively.

Abstract: Digital twins have emerged as a powerful technology for modeling and
simulating complex systems across various domains (Fuller et al., 2020; Tao et
al., 2019). As virtual representations of physical assets, processes, or
systems, digital twins enable real-time monitoring, predictive analysis, and
optimization. However, as digital twins become more sophisticated and integral
to decision-making processes, ensuring their accuracy, reliability, and ethical
implementation is essential. This paper presents a comprehensive framework for
the Testing, Evaluation, Verification and Validation (TEVV) of digital twins to
address the unique challenges posed by these dynamic and complex virtual
models.

</details>


### [74] [Supporting Software Formal Verification with Large Language Models: An Experimental Study](https://arxiv.org/abs/2507.04857)
*Weiqi Wang,Marie Farrell,Lucas C. Cordeiro,Liping Zhao*

Main category: cs.SE

TL;DR: SpecVerify integrates large language models (LLMs) with formal verification tools to overcome the challenge of deriving properties from natural language requirements. Evaluated on nine systems, it achieves 46.5% verification accuracy with fewer false positives compared to CoCoSim, while highlighting the importance of human oversight in LLM-assisted verification.


<details>
  <summary>Details</summary>
Motivation: Traditional formal verification struggles with automatically extracting verifiable properties from natural language requirements, as existing methods like CoCoSim face limitations in expressive power and numerical approximations. This motivates the integration of LLMs to enhance flexibility and accuracy in requirements verification.

Method: SpecVerify combines the Claude 3.5 Sonnet LLM with the ESBMC formal verifier to create an automated workflow. It formulates assertions beyond LTL’s expressive capabilities and identifies falsifiable cases overlooked by traditional approaches through counterexample analysis.

Result: SpecVerify achieves 46.5% verification accuracy on nine cyber-physical systems (comparable to CoCoSim) with significantly lower false positives. Counterexamples reveal CoCoSim’s weaknesses in model connection errors and numerical approximations, but also show that LLMs (Claude, ChatGPT, Llama) occasionally misinterpret specifications.

Conclusion: LLMs can reduce barriers to formal verification by automating property derivation and expanding expressive power beyond LTL. However, high-quality requirements documentation and human monitoring remain critical to ensure accuracy, demonstrating the necessity of human-machine collaboration for optimal verification outcomes.

Abstract: Formal methods have been employed for requirements verification for a long
time. However, it is difficult to automatically derive properties from natural
language requirements. SpecVerify addresses this challenge by integrating large
language models (LLMs) with formal verification tools, providing a more
flexible mechanism for expressing requirements. This framework combines Claude
3.5 Sonnet with the ESBMC verifier to form an automated workflow. Evaluated on
nine cyber-physical systems from Lockheed Martin, SpecVerify achieves 46.5%
verification accuracy, comparable to NASA's CoCoSim, but with lower false
positives. Our framework formulates assertions that extend beyond the
expressive power of LTL and identifies falsifiable cases that are missed by
more traditional methods. Counterexample analysis reveals CoCoSim's limitations
stemming from model connection errors and numerical approximation issues. While
SpecVerify advances verification automation, our comparative study of Claude,
ChatGPT, and Llama shows that high-quality requirements documentation and human
monitoring remain critical, as models occasionally misinterpret specifications.
Our results demonstrate that LLMs can significantly reduce the barriers to
formal verification, while highlighting the continued importance of
human-machine collaboration in achieving optimal results.

</details>


### [75] [Towards a Unifying Reference Model for Digital Twins of Cyber-Physical Systems](https://arxiv.org/abs/2507.04871)
*Jerome Pfeiffer,Jingxi Zhang,Benoit Combemale,Judith Michael,Bernhard Rumpe,Manuel Wimmer,Andreas Wortmann*

Main category: cs.SE

TL;DR: Proposes a unifying reference model for digital twins to reduce the concept-implementation gap.


<details>
  <summary>Details</summary>
Motivation: Existing digital twin definitions and reference models are overly abstract, creating a significant gap between theory and industrial practice.

Method: Analyzes popular reference models and synthesizes them into a detailed unifying digital twin framework.

Result: Enhanced understanding of digital twin concepts and their relationships with improved implementation guidance.

Conclusion: The unifying model facilitates practical engineering of digital twins across different industries by addressing abstraction limitations in previous approaches.

Abstract: Digital twins are sophisticated software systems for the representation,
monitoring, and control of cyber-physical systems, including automotive,
avionics, smart manufacturing, and many more. Existing definitions and
reference models of digital twins are overly abstract, impeding their
comprehensive understanding and implementation guidance. Consequently, a
significant gap emerges between abstract concepts and their industrial
implementations. We analyze popular reference models for digital twins and
combine these into a significantly detailed unifying reference model for
digital twins that reduces the concept-implementation gap to facilitate their
engineering in industrial practice. This enhances the understanding of the
concepts of digital twins and their relationships and guides developers to
implement digital twins effectively.

</details>


### [76] [Understanding Everything as Code: A Taxonomy and Conceptual Model](https://arxiv.org/abs/2507.05100)
*Haoran Wei,Nazim Madhavji,John Steinbacher*

Main category: cs.SE

TL;DR: A paper presenting the first comprehensive EaC taxonomy and conceptual model through an MLR of academic/grey literature, validated with industry experts.


<details>
  <summary>Details</summary>
Motivation: The study addresses the lack of academic standards and research on EaC to provide clarity, guidance, and a foundation for future work.

Method: Large-scale multivocal literature review (MLR) synthesizing academic and grey literature with quantitative and thematic analysis, validated by industry experts.

Result: 25-taxonomy practices in 6 layers, a conceptual model mapping EaC practices in the software lifecycle, and practitioner code examples validated with experts.

Conclusion: Establishes first structured EaC framework to overcome research gaps, offering practical guidance and research foundations for this emerging paradigm.

Abstract: Background: Everything as Code (EaC) is an emerging paradigm aiming to codify
all aspects of modern software systems. Despite its growing popularity,
comprehensive industry standards and peer-reviewed research clarifying its
scope and guiding its adoption remain scarce. Aims: This study systematically
analyzes existing knowledge and perceptions of EaC, clarifies its scope and
boundaries, and provides structured guidance for researchers and practitioners.
Method: We conducted a large-scale multivocal literature review (MLR),
synthesizing academic and grey literature sources. Findings were analyzed
quantitatively and thematically. Based on this analysis, we developed a
taxonomy and conceptual model of EaC, validated through collaboration with
industry experts. Results: The resulting taxonomy comprises 25 distinct EaC
practices organized into six layers based on industry awareness and functional
roles. The conceptual model illustrates focus areas, overlaps, and interactions
among these EaC practices within the software delivery lifecycle. Additionally,
practical code examples demonstrating the implementation of these practices
were developed in collaboration with industry experts. Conclusions: This work
addresses the current scarcity of academic discourse on EaC by providing the
first comprehensive taxonomy and conceptual model. These contributions enhance
conceptual clarity, offer actionable guidance to practitioners, and lay the
groundwork for future research in this emerging domain.

</details>


### [77] [In-Context Learning as an Effective Estimator of Functional Correctness of LLM-Generated Code](https://arxiv.org/abs/2507.05200)
*Susmita Das,Madhusudan Ghosh,Priyanka Swami,Debasis Ganguly,Gul Calikli*

Main category: cs.SE

TL;DR: This paper proposes an in-context learning (ICL) approach using few-shot examples from training sets to enhance code quality estimation for LLM-based software development without test cases, improving both existing QPP methods and zero-shot approaches.


<details>
  <summary>Details</summary>
Motivation: LLM-based code generation in agile development lacks test cases for assessing functional correctness, necessitating a quality estimation method akin to relevance ranking in information retrieval.

Method: The authors employ ICL to provide developers with a ranked list of generated code solutions, incorporating few-shot examples of functionally correct code to inform quality estimation.

Result: Results show that few-shot example training boosts performance of QPP approaches and zero-shot-based estimation of code functional correctness.

Conclusion: The study demonstrates that contextual learning with targeted examples significantly improves code quality assessment, offering a scalable solution for generative software development workflows.

Abstract: When applying LLM-based code generation to software development projects that
follow a feature-driven or rapid application development approach, it becomes
necessary to estimate the functional correctness of the generated code in the
absence of test cases. Just as a user selects a relevant document from a ranked
list of retrieved ones, a software generation workflow requires a developer to
choose (and potentially refine) a generated solution from a ranked list of
alternative solutions, ordered by their posterior likelihoods. This implies
that estimating the quality of a ranked list -- akin to estimating "relevance"
for query performance prediction (QPP) in IR -- is also crucial for generative
software development, where quality is defined in terms of "functional
correctness". In this paper, we propose an in-context learning (ICL) based
approach for code quality estimation. Our findings demonstrate that providing
few-shot examples of functionally correct code from a training set enhances the
performance of existing QPP approaches as well as a zero-shot-based approach
for code quality estimation.

</details>


### [78] [An Investigation into Maintenance Support for Neural Networks](https://arxiv.org/abs/2507.05245)
*Fatema Tuz Zohra,Brittany Johnson*

Main category: cs.SE

TL;DR: The paper examines gaps in neural network maintenance practices, highlighting that existing tools focus on model building/training but lack support for addressing unexpected model behaviors, and proposes a developer-centric perspective to improve maintenance.


<details>
  <summary>Details</summary>
Motivation: Neural networks are increasingly integrated into daily life with potential negative impacts, necessitating effective testing, debugging, and maintenance methods similar to traditional software engineering.

Method: Conducted preliminary interviews and surveys with practitioners to curate insights on current research and practices in neural network maintenance.

Result: Existing tools prioritize model building/training over understanding and mitigating the root causes of unexpected behaviors, revealing limitations in traditional methodologies for neural network maintenance.

Conclusion: Traditional software engineering methods are insufficient for neural networks; the study identifies shortcomings in current practices and suggests opportunities for enhancing maintenance support through targeted research and tools.

Abstract: As the potential for neural networks to augment our daily lives grows,
ensuring their quality through effective testing, debugging, and maintenance is
essential. This is especially the case as we acknowledge the prospects of
negative impacts from these technologies. Traditional software engineering
methods, such as testing and debugging, have proven effective in maintaining
software quality; however, they reveal significant research and practice gaps
in maintaining neural networks. In particular, there is a limited understanding
of how practitioners currently address challenges related to understanding and
mitigating undesirable behaviors in neural networks. In our ongoing research,
we explore the current state of research and practice in maintaining neural
networks by curating insights from practitioners through a preliminary study
involving interviews and supporting survey responses. Our findings thus far
indicate that existing tools primarily concentrate on building and training
models. While these tools can be beneficial, they often fall short of
supporting practitioners' understanding and addressing the underlying causes of
unexpected model behavior. By evaluating current procedures and identifying the
limitations of traditional methodologies, our study aims to offer a
developer-centric perspective on where current practices fall short and
highlight opportunities for improving maintenance support in neural networks.

</details>
