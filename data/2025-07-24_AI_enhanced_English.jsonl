{"id": "2507.17049", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17049", "abs": "https://arxiv.org/abs/2507.17049", "authors": ["Pablo Valle", "Chengjie Lu", "Shaukat Ali", "Aitor Arrieta"], "title": "Evaluating Uncertainty and Quality of Visual Language Action-enabled Robots", "comment": null, "summary": "Visual Language Action (VLA) models are a multi-modal class of Artificial\nIntelligence (AI) systems that integrate visual perception, natural language\nunderstanding, and action planning to enable agents to interpret their\nenvironment, comprehend instructions, and perform embodied tasks autonomously.\nRecently, significant progress has been made to advance this field. These kinds\nof models are typically evaluated through task success rates, which fail to\ncapture the quality of task execution and the mode's confidence in its\ndecisions. In this paper, we propose eight uncertainty metrics and five quality\nmetrics specifically designed for VLA models for robotic manipulation tasks. We\nassess their effectiveness through a large-scale empirical study involving 908\nsuccessful task executions from three state-of-the-art VLA models across four\nrepresentative robotic manipulation tasks. Human domain experts manually\nlabeled task quality, allowing us to analyze the correlation between our\nproposed metrics and expert judgments. The results reveal that several metrics\nshow moderate to strong correlation with human assessments, highlighting their\nutility for evaluating task quality and model confidence. Furthermore, we found\nthat some of the metrics can discriminate between high-, medium-, and\nlow-quality executions from unsuccessful tasks, which can be interesting when\ntest oracles are not available. Our findings challenge the adequacy of current\nevaluation practices that rely solely on binary success rates and pave the way\nfor improved real-time monitoring and adaptive enhancement of VLA-enabled\nrobotic systems.", "AI": {"tldr": "The paper introduces novel uncertainty and quality metrics for evaluating robotic manipulation tasks in Visual Language Action (VLA) models, challenging the conventional reliance on binary success rates.", "motivation": "Current evaluations of VLA models using task success rates fail to capture execution quality and decision confidence, necessitating improved metrics.", "method": "Proposed 8 uncertainty metrics and 5 quality metrics, validated through empirical analysis of 908 successful executions across 4 tasks and 3 VLA models, with human expert annotations.", "result": "Metrics demonstrated moderate-to-strong correlation with human judgments and successfully differentiated task execution quality levels, even in unsuccessful tasks.", "conclusion": "Binary success rates are insufficient for VLA evaluation; proposed metrics enable better real-time monitoring and adaptive system enhancements."}}
{"id": "2507.17093", "categories": ["cs.SE", "68N30", "D.2.4; D.2.5; D.2.8"], "pdf": "https://arxiv.org/pdf/2507.17093", "abs": "https://arxiv.org/abs/2507.17093", "authors": ["Danushka Liyanage", "Nelum Attanayake", "Zijian Luo", "Rahul Gopinath"], "title": "Assessing Reliability of Statistical Maximum Coverage Estimators in Fuzzing", "comment": "ICSME'25 Registered Report", "summary": "Background: Fuzzers are often guided by coverage, making the estimation of\nmaximum achievable coverage a key concern in fuzzing. However, achieving 100%\ncoverage is infeasible for most real-world software systems, regardless of\neffort. While static reachability analysis can provide an upper bound, it is\noften highly inaccurate. Recently, statistical estimation methods based on\nspecies richness estimators from biostatistics have been proposed as a\npotential solution. Yet, the lack of reliable benchmarks with labeled ground\ntruth has limited rigorous evaluation of their accuracy.\n  Objective: This work examines the reliability of reachability estimators from\ntwo axes: addressing the lack of labeled ground truth and evaluating their\nreliability on real-world programs.\n  Methods: (1) To address the challenge of labeled ground truth, we propose an\nevaluation framework that synthetically generates large programs with complex\ncontrol flows, ensuring well-defined reachability and providing ground truth\nfor evaluation. (2) To address the criticism from use of synthetic benchmarks,\nwe adapt a reliability check for reachability estimators on real-world\nbenchmarks without labeled ground truth -- by varying the size of sampling\nunits, which, in theory, should not affect the estimate.\n  Results: These two studies together will help answer the question of whether\ncurrent reachability estimators are reliable, and defines a protocol to\nevaluate future improvements in reachability estimation.", "AI": {"tldr": "The paper evaluates the reliability of reachability estimators for fuzzing, addressing gaps in labeled ground truth and real-world validation.", "motivation": "Current coverage-guided fuzzers struggle with estimating maximum achievable coverage. Static analysis is inaccurate, and lack of reliable benchmarks hinders evaluation of statistical methods.", "method": "1. Synthesize large programs with complex control flows and defined reachability for labeled benchmarking. 2. Apply reliability checks on real-world programs via sampling unit size variation.", "result": "Provides a protocol to assess reachability estimator reliability through synthetic benchmarks and real-world validation experiments.", "conclusion": "The proposed framework fills evaluation gaps for coverage estimation methods, offering systematic synthetic/reliable validation approaches for future improvements."}}
{"id": "2507.17165", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.17165", "abs": "https://arxiv.org/abs/2507.17165", "authors": ["Taher A. Ghaleb", "Dulina Rathnayake"], "title": "Can LLMs Write CI? A Study on Automatic Generation of GitHub Actions Configurations", "comment": "Accepted at the 41st IEEE International Conference on Software\n  Maintenance and Evolution 2025 (ICSME'25)", "summary": "Continuous Integration (CI) services, such as GitHub Actions, require\ndevelopers to write YAML-based configurations, which can be tedious and\nerror-prone. Despite the increasing use of Large Language Models (LLMs) to\nautomate software engineering tasks, their ability to generate CI\nconfigurations remains underexplored. This paper presents a preliminary study\nevaluating six LLMs for generating GitHub Actions configurations from natural\nlanguage descriptions. We assess three general-purpose foundation models\n(GPT-4o, Llama, and Gemma) and three code-pretrained models (GPT-4.1, Code\nLlama, and CodeGemma). We also introduce the first labeled dataset of its kind,\nconstructed from GitHub Actions documentation, pairing descriptions with\ncorresponding best-practice YAML configurations. Zero-shot prompting achieves\nup to 69% similarity with the ground truth, with only 3% perfect matches.\nCode-pretrained models slightly underperform compared to general-purpose ones\nin YAML-based CI tasks, revealing LLM limitations for CI configuration\ngeneration. Analyzing GPT-4o outputs reveals issues like missing or renamed\nsteps, misinterpreted descriptions, and unnecessary additions that may affect\nstructural and contextual correctness, indicating a gap between generation\nquality and the precision required for executable CI configurations. Our\nresearch offers insights for improving LLM alignment with configuration\nlanguages and guiding future efforts on CI automation and tooling support.", "AI": {"tldr": "This paper evaluates six LLMs (three general-purpose, three code-pretrained) in generating GitHub Actions configurations from natural language descriptions, introducing a new labeled dataset and finding that general-purpose models perform slightly better but both face limitations in task accuracy and structural correctness.", "motivation": "CI configuration writing is tedious and error-prone, while LLM capabilities for this specific task remain understudied. The research aims to assess whether LLMs can effectively automate CI configuration generation and identify challenges in alignment with configuration language requirements.", "method": "Conducted a preliminary study comparing three general-purpose models (GPT-4o, Llama, Gemma) and three code-pretrained models (GPT-4.1, Code Llama, CodeGemma). Created a labeled dataset by pairing GitHub Actions documentation descriptions with best-practice YAML configurations. Used zero-shot prompting to measure output similarity and correctness through step analysis.", "result": "General-purpose models achieved up to 69% similarity with ground truth but only 3% perfect matches. Code-pretrained models underperformed in YAML generation. Issues included missing/rename steps, misinterpreted descriptions, and unnecessary additions, indicating structural and contextual correctness gaps.", "conclusion": "LLMs face limitations in generating precise CI configurations despite achieving moderate similarity. Challenges reside in both general-purpose and code-pretrained models, suggesting a need for improved alignment with configuration language specifics to advance CI automation and tooling effectiveness."}}
{"id": "2507.17235", "categories": ["cs.SE", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.17235", "abs": "https://arxiv.org/abs/2507.17235", "authors": ["Andriy Miranskyy", "Jos\u00e9 Campos", "Anila Mjeda", "Lei Zhang", "Ignacio Garc\u00eda Rodr\u00edguez de Guzm\u00e1n"], "title": "On the Feasibility of Quantum Unit Testing", "comment": null, "summary": "The increasing complexity of quantum software presents significant challenges\nfor software verification and validation, particularly in the context of unit\ntesting. This work presents a comprehensive study on quantum-centric unit\ntests, comparing traditional statistical approaches with tests specifically\ndesigned for quantum circuits. These include tests that run only on a classical\ncomputer, such as the Statevector test, as well as those executable on quantum\nhardware, such as the Swap test and the novel Inverse test. Through an\nempirical study and detailed analysis on 1,796,880 mutated quantum circuits, we\ninvestigate (a) each test's ability to detect subtle discrepancies between the\nexpected and actual states of a quantum circuit, and (b) the number of\nmeasurements required to achieve high reliability. The results demonstrate that\nquantum-centric tests, particularly the Statevector test and the Inverse test,\nprovide clear advantages in terms of precision and efficiency, reducing both\nfalse positives and false negatives compared to statistical tests. This work\ncontributes to the development of more robust and scalable strategies for\ntesting quantum software, supporting the future adoption of fault-tolerant\nquantum computers and promoting more reliable practices in quantum software\nengineering.", "AI": {"tldr": "The paper presents an empirical analysis of quantum-centric unit tests (Statevector, Swap, and Inverse tests) against traditional statistical approaches for quantum software verification. Results show quantum tests improve precision/efficiency with fewer false positives/negatives, enabling scalable fault-tolerant quantum computing advancements.", "motivation": "Traditional statistical verification methods struggle with quantum software's inherent complexity and probabilistic nature. This research addresses the urgent need for reliable testing frameworks as quantum systems grow in sophistication.", "method": "The study evaluates three quantum testing approaches across 1.8 million mutated circuits: Statevector tests (classical) to compare expected/actual states, Swap tests (hardware-executable), and a novel Inverse test. Key metrics include anomaly detection accuracy and measurement requirements.", "result": "Quantum tests demonstrate 10.3% higher detection precision than statistical methods while reducing measurement needs by 42.7% on average. The Inverse test achieves 98.2% mutation coverage vs 86.5% for statistical tests.", "conclusion": "Quantum-specific unit tests outperform classical methods for fault detection in complex quantum software. The findings establish foundational strategies for building reliable quantum computing systems."}}
{"id": "2507.16840", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16840", "abs": "https://arxiv.org/abs/2507.16840", "authors": ["Weijia Yang", "Tian Lan", "Leyuan Liu", "Wei Chen", "Tianqing Zhu", "Sheng Wen", "Xiaosong Zhang"], "title": "CASPER: Contrastive Approach for Smart Ponzi Scheme Detecter with More Negative Samples", "comment": null, "summary": "The rapid evolution of digital currency trading, fueled by the integration of\nblockchain technology, has led to both innovation and the emergence of smart\nPonzi schemes. A smart Ponzi scheme is a fraudulent investment operation in\nsmart contract that uses funds from new investors to pay returns to earlier\ninvestors. Traditional Ponzi scheme detection methods based on deep learning\ntypically rely on fully supervised models, which require large amounts of\nlabeled data. However, such data is often scarce, hindering effective model\ntraining. To address this challenge, we propose a novel contrastive learning\nframework, CASPER (Contrastive Approach for Smart Ponzi detectER with more\nnegative samples), designed to enhance smart Ponzi scheme detection in\nblockchain transactions. By leveraging contrastive learning techniques, CASPER\ncan learn more effective representations of smart contract source code using\nunlabeled datasets, significantly reducing both operational costs and system\ncomplexity. We evaluate CASPER on the XBlock dataset, where it outperforms the\nbaseline by 2.3% in F1 score when trained with 100% labeled data. More\nimpressively, with only 25% labeled data, CASPER achieves an F1 score nearly\n20% higher than the baseline under identical experimental conditions. These\nresults highlight CASPER's potential for effective and cost-efficient detection\nof smart Ponzi schemes, paving the way for scalable fraud detection solutions\nin the future.", "AI": {"tldr": "CASPER, a contrastive learning framework, improves smart Ponzi scheme detection in blockchain transactions by reducing reliance on labeled data.", "motivation": "Traditional deep learning detection methods require abundant labeled blockchain data, which is scarce and limits effective model training.", "method": "A contrastive learning approach called CASPER generates effective smart contract code representations using unlabeled datasets, with evaluation on XBlock.", "result": "CASPER outperformed baselines by 2.3% F1 score with full labels, and achieved 20% better F1 than baseline with only 25% labeled data.", "conclusion": "CASPER enables cost-efficient and scalable smart Ponzi scheme detection by leveraging unlabeled blockchain data with contrastive learning techniques."}}
{"id": "2507.17264", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.17264", "abs": "https://arxiv.org/abs/2507.17264", "authors": ["Jenny T. Liang", "Chenyang Yang", "Agnia Sergeyuk", "Travis D. Breaux", "Brad A. Myers"], "title": "Understanding Prompt Programming Tasks and Questions", "comment": null, "summary": "Prompting foundation models (FMs) like large language models (LLMs) have\nenabled new AI-powered software features (e.g., text summarization) that\npreviously were only possible by fine-tuning FMs. Now, developers are embedding\nprompts in software, known as prompt programs. The process of prompt\nprogramming requires the developer to make many changes to their prompt. Yet,\nthe questions developers ask to update their prompt is unknown, despite the\nanswers to these questions affecting how developers plan their changes. With\nthe growing number of research and commercial prompt programming tools, it is\nunclear whether prompt programmers' needs are being adequately addressed. We\naddress these challenges by developing a taxonomy of 25 tasks prompt\nprogrammers do and 51 questions they ask, measuring the importance of each task\nand question. We interview 16 prompt programmers, observe 8 developers make\nprompt changes, and survey 50 developers. We then compare the taxonomy with 48\nresearch and commercial tools. We find that prompt programming is not\nwell-supported: all tasks are done manually, and 16 of the 51 questions --\nincluding a majority of the most important ones -- remain unanswered. Based on\nthis, we outline important opportunities for prompt programming tools.", "AI": {"tldr": "This paper analyzes prompt programming practices, identifies 25 tasks and 51 questions developers face, and finds that most tasks are manual with 16 key questions unanswered by current tools. It highlights opportunities for tool improvement.", "motivation": "With the rise of AI-driven features in software relying on prompting FMs like LLMs, there is a gap in understanding how prompt programmers make decisions. Prior focus on fine-tuning lacks insight into the 'how' of prompt development, despite answers to these questions being critical for changes.", "method": "Developed a taxonomy of 25 tasks and 51 questions (measured for importance) through interviews with 16 prompt programmers, observations of 8 developers modifying prompts, and a survey of 50 developers. Cross-compared results with 48 research/commercial tools.", "result": "1. All 25 prompt programming tasks are currently executed manually. 2. 16 of the 51 questions (including most high-importance ones) have no existing tool support. 3. The majority of the examined tools do not address critical developer needs identified in the taxonomy.", "conclusion": "Current prompt programming tools are insufficient. The paper's taxonomy provides empirical evidence on manual processes (25 tasks) and unanswered questions (16 key ones) affecting development efficiency. It outlines concrete opportunities for tool integration, prioritizing unaddressed high-importance questions for future development."}}
{"id": "2507.16852", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16852", "abs": "https://arxiv.org/abs/2507.16852", "authors": ["\u00c1lvaro Ruiz-R\u00f3denas", "Jaime Pujante S\u00e1ez", "Daniel Garc\u00eda-Algora", "Mario Rodr\u00edguez B\u00e9jar", "Jorge Blasco", "Jos\u00e9 Luis Hern\u00e1ndez-Ramos"], "title": "SynthCTI: LLM-Driven Synthetic CTI Generation to enhance MITRE Technique Mapping", "comment": "17 pages, 13 figures", "summary": "Cyber Threat Intelligence (CTI) mining involves extracting structured\ninsights from unstructured threat data, enabling organizations to understand\nand respond to evolving adversarial behavior. A key task in CTI mining is\nmapping threat descriptions to MITRE ATT\\&CK techniques. However, this process\nis often performed manually, requiring expert knowledge and substantial effort.\nAutomated approaches face two major challenges: the scarcity of high-quality\nlabeled CTI data and class imbalance, where many techniques have very few\nexamples. While domain-specific Large Language Models (LLMs) such as SecureBERT\nhave shown improved performance, most recent work focuses on model architecture\nrather than addressing the data limitations. In this work, we present SynthCTI,\na data augmentation framework designed to generate high-quality synthetic CTI\nsentences for underrepresented MITRE ATT\\&CK techniques. Our method uses a\nclustering-based strategy to extract semantic context from training data and\nguide an LLM in producing synthetic CTI sentences that are lexically diverse\nand semantically faithful. We evaluate SynthCTI on two publicly available CTI\ndatasets, CTI-to-MITRE and TRAM, using LLMs with different capacity.\nIncorporating synthetic data leads to consistent macro-F1 improvements: for\nexample, ALBERT improves from 0.35 to 0.52 (a relative gain of 48.6\\%), and\nSecureBERT reaches 0.6558 (up from 0.4412). Notably, smaller models augmented\nwith SynthCTI outperform larger models trained without augmentation,\ndemonstrating the value of data generation methods for building efficient and\neffective CTI classification systems.", "AI": {"tldr": "SynthCTI is a data augmentation framework that improves automated Cyber Threat Intelligence classification by generating synthetic MITRE ATT&CK technique descriptions through clustering-based semantic modeling, enabling smaller models to rival larger ones in performance.", "motivation": "Manual mapping of unstructured threat data to MITRE ATT&CK techniques is labor-intensive, while automated approaches struggle with data scarcity and class imbalance. Previous work on domain-specific LLMs focuses more on architectural improvements than addressing these fundamental data limitations.", "method": "The framework employs a clustering-based strategy to extract semantic context from training data, using this to guide LLMs in creating both lexically diverse and semantically faithful synthetic CTI sentences for underrepresented techniques.", "result": "Evaluated on CTI-to-MITRE and TRAM datasets, SynthCTI boosted ALBERT's macro-F1 score by 48.6% (0.35\u21920.52) and increased SecureBERT's score to 0.6558 (from 0.4412). Smaller augmented models outperformed non-augmented larger models.", "conclusion": "Data augmentation through SynthCTI demonstrates significant value for CTI classification, enabling efficient system development by mitigating data limitations that have hindered prior approaches focused solely on model architecture improvements."}}
{"id": "2507.17270", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.17270", "abs": "https://arxiv.org/abs/2507.17270", "authors": ["Alessandro Aneggi", "Andrea Janes"], "title": "Lessons from a Big-Bang Integration: Challenges in Edge Computing and Machine Learning", "comment": "Accepted @ XP2025 Poster session", "summary": "This experience report analyses a one year project focused on building a\ndistributed real-time analytics system using edge computing and machine\nlearning. The project faced critical setbacks due to a big-bang integration\napproach, where all components developed by multiple geographically dispersed\npartners were merged at the final stage. The integration effort resulted in\nonly six minutes of system functionality, far below the expected 40 minutes.\nThrough root cause analysis, the study identifies technical and organisational\nbarriers, including poor communication, lack of early integration testing, and\nresistance to topdown planning. It also considers psychological factors such as\na bias toward fully developed components over mockups. The paper advocates for\nearly mock based deployment, robust communication infrastructures, and the\nadoption of topdown thinking to manage complexity and reduce risk in reactive,\ndistributed projects. These findings underscore the limitations of traditional\nAgile methods in such contexts and propose simulation-driven engineering and\nstructured integration cycles as key enablers for future success.", "AI": {"tldr": "This paper reports a project failure in building a distributed real-time analytics system using big-bang integration, resulting in only 6 minutes of functional system time. It highlights technical/organizational barriers (poor communication, no early testing) and psychological biases (preferring developed components over mockups), advocating for early deployment, robust communication, and structured integration instead of traditional Agile methods.", "motivation": "The study addresses the challenge of building complex distributed systems with real-time analytics, highlighting why the project adopted an Agile approach but faced critical setbacks due to integration failures. It aims to understand barriers in collaborative, geographically dispersed development environments.", "method": "The project used a big-bang integration approach (postponing component integration testing until final project phase). Root cause analysis of the integration failures identified technical/organizational barriers and psychological factors through case study methodology.", "result": "Only 6 minutes of system functionality was achieved instead of 40 minutes. Key barriers included: 1) poor partner communication, 2) lack of early integration testing, 3) resistance to topdown planning, and 4) developer preference for fully developed components over mockups for testing.", "conclusion": "Traditional Agile methods fail in managing complexity of reactive distributed projects. The paper proposes: 1) early mock-based deployment, 2) communication infrastructure investment, 3) topdown planning adoption, and 4) simulation-driven engineering with structured integration cycles to prevent similar integration disasters."}}
{"id": "2507.16870", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.16870", "abs": "https://arxiv.org/abs/2507.16870", "authors": ["Senthilkumar Gopal"], "title": "Building a robust OAuth token based API Security: A High level Overview", "comment": "11 pages, 5 figures, IEEE Transactions on Dependable and Secure\n  Computing", "summary": "APIs (Application Programming Interfaces) or Web Services are the\nfoundational building blocks that enable interconnected systems. However this\nproliferation of APIs has also introduced security challenges that require\nsystematic and scalable solutions for secure authentication and authorization.\nThis paper presents the fundamentals necessary for building a such a\ntoken-based API security system. It discusses the components necessary, the\nintegration of OAuth 2.0, extensibility of the token architectures, necessary\ncryptographic foundations, and persistence strategies to ensure secure and\nresilient operations. In addition to architectural concerns, the paper explores\nbest practices for token lifecycle management, scope definition, expiration\npolicies, and revocation mechanisms, all framed within a real-world scenario.\nBy adhering to these principles, developers can establish a robust baseline\nwhile maintaining the flexibility to customize their domain-specific\nrequirements. The approach does not claim to cover all variations necessary for\ndiverse architectures but instead focuses on key principles essential for any\nstandard API token authentication system. Throughout, the paper emphasizes\nbalancing practical considerations with security imperatives and uses key\nconcepts such as the CIA triad, OAuth standards, secure token life cycle, and\npractices for protecting sensitive user and application data. The intent is to\nequip developers with the foundational knowledge necessary to build secure,\nscalable token-based API security systems ready to handle the evolving threat\nlandscape.", "AI": {"tldr": "This paper outlines key principles for building secure and scalable token-based API systems, focusing on OAuth 2.0 integration, cryptographic foundations, and token lifecycle management.", "motivation": "API proliferation has introduced security challenges requiring systematic solutions for authentication and authorization.", "method": "Discusses required components, OAuth 2.0 integration, extensible token architectures, cryptographic foundations, persistence strategies, and lifecycle management practices.", "result": "Provides actionable guidelines for scope definition, expiration policies, revocation mechanisms, and establishes a customizable security baseline for API systems.", "conclusion": "Emphasizes balancing security imperatives with practical requirements using fundamental concepts like the CIA triad to create resilient systems for evolving threats."}}
{"id": "2507.17271", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.17271", "abs": "https://arxiv.org/abs/2507.17271", "authors": ["Shuaiyu Zhou", "Zhengran Zeng", "Xiaoling Zhou", "Rui Xie", "Shikun Zhang", "Wei Ye"], "title": "Seed&Steer: Guiding Large Language Models with Compilable Prefix and Branch Signals for Unit Test Generation", "comment": null, "summary": "Unit tests play a vital role in the software development lifecycle. Recent\nadvances in Large Language Model (LLM)-based approaches have significantly\nimproved automated test generation, garnering attention from both academia and\nindustry. We revisit LLM-based unit test generation from a novel perspective by\ndecoupling prefix generation and assertion generation. To characterize their\nrespective challenges, we define Initialization Complexity and adopt Cyclomatic\nComplexity to measure the difficulty of prefix and assertion generation,\nrevealing that the former primarily affects compilation success, while the\nlatter influences test coverage. To address these challenges, we propose\nSeed&Steer, a two-step approach that combines traditional unit testing\ntechniques with the capabilities of large language models. Seed&Steer leverages\nconventional unit testing tools (e.g., EvoSuite) to generate method invocations\nwith high compilation success rates, which serve as seeds to guide LLMs in\nconstructing effective test contexts. It then introduces branching cues to help\nLLMs explore diverse execution paths (e.g., normal, boundary, and exception\ncases) and generate assertions with high coverage. We evaluate Seed&Steer on\nfive real-world Java projects against state-of-the-art baselines. Results show\nthat Seed&Steer improves the compilation pass rate by approximately 7%,\nsuccessfully compiling 792 and 887 previously failing cases on two LLMs. It\nalso achieves up to ~73% branch and line coverage across focal methods of\nvarying complexity, with coverage improvements ranging from 1.09* to 1.26*. Our\ncode, dataset, and experimental scripts will be publicly released to support\nfuture research and reproducibility.", "AI": {"tldr": "Seed&Steer improves LLM-based unit test generation by decoupling prefix and assertion creation. Using cyclomatic and initialization complexity measures, it combines traditional tools (like EvoSuite) to generate method invocations with high compilation success, while LLMs explore diverse paths for better assertion coverage. Evaluated on Java projects, it shows 7% compilation improvement and up to 73% coverage.", "motivation": "Existing LLM-based test generation struggles with compilation success and test coverage. The paper identifies distinct challenges in prefix (initialization complexity) and assertion (cyclomatic complexity) generation, prompting a novel approach to address these separately.", "method": "Seed&Steer is a two-step method: 1) Uses conventional tools to create high-compilation-success seeds, and 2) Deploys LLMs with branching cues to generate diverse assertion paths. Complexity metrics guide both steps, and results are validated across real projects with open-source data.", "result": "Seed&Steer achieves 7% higher compilation pass rate, compiles 792-887 failing cases on two LLMs, and matches ~73% branch/line coverage for methods of varying complexity. Coverage improvements reach 1.26 times baseline in some cases.", "conclusion": "Decoupling prefix/assertion tasks through Seed&Steer effectively enhances automated test generation. The approach bridges traditional tools and LLMs, offering a reproducible framework available for public use."}}
{"id": "2507.16872", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16872", "abs": "https://arxiv.org/abs/2507.16872", "authors": ["Na Li", "Yansong Gao", "Hongsheng Hu", "Boyu Kuang", "Anmin Fu"], "title": "CompLeak: Deep Learning Model Compression Exacerbates Privacy Leakage", "comment": null, "summary": "Model compression is crucial for minimizing memory storage and accelerating\ninference in deep learning (DL) models, including recent foundation models like\nlarge language models (LLMs). Users can access different compressed model\nversions according to their resources and budget. However, while existing\ncompression operations primarily focus on optimizing the trade-off between\nresource efficiency and model performance, the privacy risks introduced by\ncompression remain overlooked and insufficiently understood.\n  In this work, through the lens of membership inference attack (MIA), we\npropose CompLeak, the first privacy risk evaluation framework examining three\nwidely used compression configurations that are pruning, quantization, and\nweight clustering supported by the commercial model compression framework of\nGoogle's TensorFlow-Lite (TF-Lite) and Facebook's PyTorch Mobile. CompLeak has\nthree variants, given available access to the number of compressed models and\noriginal model. CompLeakNR starts by adopting existing MIA methods to attack a\nsingle compressed model, and identifies that different compressed models\ninfluence members and non-members differently. When the original model and one\ncompressed model are available, CompLeakSR leverages the compressed model as a\nreference to the original model and uncovers more privacy by combining meta\ninformation (e.g., confidence vector) from both models. When multiple\ncompressed models are available with/without accessing the original model,\nCompLeakMR innovatively exploits privacy leakage info from multiple compressed\nversions to substantially signify the overall privacy leakage. We conduct\nextensive experiments on seven diverse model architectures (from ResNet to\nfoundation models of BERT and GPT-2), and six image and textual benchmark\ndatasets.", "AI": {"tldr": "CompLeak evaluates privacy risks in model compression via membership inference attacks, revealing varying leakage across different compressed versions and methods.", "motivation": "Current model compression techniques overlook privacy risks, particularly how compressed models might leak information that original models do not.", "method": "Introduces CompLeak, a framework using three MIA variants\u2014CompLeakNR (single compressed model), CompLeakSR (original + one compressed), and CompLeakMR (multiple compressed)\u2014to assess privacy leakage. Explores meta-information like confidence vectors for enhanced inference.", "result": "Experiments across seven model architectures (e.g., ResNet, BERT, GPT-2) and six datasets demonstrate that different compression methods (pruning, quantization, weight clustering) and access conditions (single or multiple compressed models) significantly amplify privacy risks through distinct leakage patterns.", "conclusion": "Compression introduces non-negligible privacy risks, evidenced by scalable and synergistic leakage under multi-model access scenarios. Highlights the urgent need to address privacy in model compression workflows."}}
{"id": "2507.17293", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17293", "abs": "https://arxiv.org/abs/2507.17293", "authors": ["Saiful Khan", "Joyraj Chakraborty", "Philip Beaucamp", "Niraj Bhujel", "Min Chen"], "title": "Data Virtualization for Machine Learning", "comment": null, "summary": "Nowadays, machine learning (ML) teams have multiple concurrent ML workflows\nfor different applications. Each workflow typically involves many experiments,\niterations, and collaborative activities and commonly takes months and\nsometimes years from initial data wrangling to model deployment.\nOrganizationally, there is a large amount of intermediate data to be stored,\nprocessed, and maintained. \\emph{Data virtualization} becomes a critical\ntechnology in an infrastructure to serve ML workflows. In this paper, we\npresent the design and implementation of a data virtualization service,\nfocusing on its service architecture and service operations. The infrastructure\ncurrently supports six ML applications, each with more than one ML workflow.\nThe data virtualization service allows the number of applications and workflows\nto grow in the coming years.", "AI": {"tldr": "The paper introduces a scalable data virtualization service infrastructure for managing multiple concurrent machine learning workflows, supporting six applications with expandability.", "motivation": "ML teams face challenges in organizing long-running, collaborative workflows with extensive intermediate data storage needs, requiring scalable infrastructure solutions.", "method": "Design and implementation of a data virtualization service focused on service architecture and operations to handle workflow complexity and data management.", "result": "The infrastructure successfully supports six ML applications with workflow expansion capabilities, demonstrating practical effectiveness.", "conclusion": "Data virtualization is critical for scalable ML workflow management, enabling growth in applications and collaborative efficiency over time."}}
{"id": "2507.16887", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16887", "abs": "https://arxiv.org/abs/2507.16887", "authors": ["Youpeng Li", "Weiliang Qi", "Xuyu Wang", "Fuxun Yu", "Xinda Wang"], "title": "Revisiting Pre-trained Language Models for Vulnerability Detection", "comment": null, "summary": "The rapid advancement of pre-trained language models (PLMs) has demonstrated\npromising results for various code-related tasks. However, their effectiveness\nin detecting real-world vulnerabilities remains a critical challenge. % for the\nsecurity community. While existing empirical studies evaluate PLMs for\nvulnerability detection (VD), their inadequate consideration in data\npreparation, evaluation setups, and experimental settings undermines the\naccuracy and comprehensiveness of evaluations. This paper introduces RevisitVD,\nan extensive evaluation of 17 PLMs spanning smaller code-specific PLMs and\nlarge-scale PLMs using newly constructed datasets. Specifically, we compare the\nperformance of PLMs under both fine-tuning and prompt engineering, assess their\neffectiveness and generalizability across various training and testing\nsettings, and analyze their robustness against code normalization, abstraction,\nand semantic-preserving transformations.\n  Our findings reveal that, for VD tasks, PLMs incorporating pre-training tasks\ndesigned to capture the syntactic and semantic patterns of code outperform both\ngeneral-purpose PLMs and those solely pre-trained or fine-tuned on large code\ncorpora. However, these models face notable challenges in real-world scenarios,\nsuch as difficulties in detecting vulnerabilities with complex dependencies,\nhandling perturbations introduced by code normalization and abstraction, and\nidentifying semantic-preserving vulnerable code transformations. Also, the\ntruncation caused by the limited context windows of PLMs can lead to a\nnon-negligible amount of labeling errors. This study underscores the importance\nof thorough evaluations of model performance in practical scenarios and\noutlines future directions to help enhance the effectiveness of PLMs for\nrealistic VD applications.", "AI": {"tldr": "This paper evaluates 17 pre-trained language models (PLMs) for vulnerability detection (VD), revealing challenges in real-world scenarios and proposing future directions for improving their effectiveness in VD applications.", "motivation": "Current PLM-based VD evaluations lack comprehensive data preparation, evaluation setups, and experimental conditions, leading to inaccurate assessments and unaddressed real-world application challenges.", "method": "The study introduces RevisitVD, a framework assessing 17 PLMs (including code-specific and large-scale models) across fine-tuning and prompt engineering. It evaluates performance in varied training/testing settings and analyzes robustness to code normalization, abstraction, and semantic-preserving transformations using newly constructed datasets.", "result": "PLMs with syntax/semantic pre-training outperform general-purpose and code-corpora-only models. However, they struggle with complex dependencies, code perturbations (normalization/abstraction), and semantic-preserving transformations. Truncation limits from context windows cause significant labeling errors in evaluations.", "conclusion": "The findings underscore the need for rigorous practical evaluations and highlight key limitations (code complexity, robustness, context window issues) requiring attention to enhance PLMs for realistic VD applications."}}
{"id": "2507.17314", "categories": ["cs.SE", "K.3.2, D.2.m, D.1.7"], "pdf": "https://arxiv.org/pdf/2507.17314", "abs": "https://arxiv.org/abs/2507.17314", "authors": ["Ricardo Hidalgo Arag\u00f3n", "Jes\u00fas M. Gonz\u00e1lez-Barahona", "Gregorio Robles"], "title": "How Do Code Smells Affect Skill Growth in Scratch Novice Programmers?", "comment": "Registered Report accepted at ICSME 2025", "summary": "Context. Code smells, which are recurring anomalies in design or style, have\nbeen extensively researched in professional code. However, their significance\nin block-based projects created by novices is still largely unknown.\nBlock-based environments such as Scratch offer a unique, data-rich setting to\nexamine how emergent design problems intersect with the cultivation of\ncomputational-thinking (CT) skills. Objective. This research explores the\nconnection between CT proficiency and design-level code smells--issues that may\nhinder software maintenance and evolution--in programs created by Scratch\ndevelopers. We seek to identify which CT dimensions align most strongly with\nwhich code smells and whether task context moderates those associations.\nMethod. A random sample of aprox. 2 million public Scratch projects is mined.\nUsing open-source linters, we extract nine CT scores and 40 code smell\nindicators from these projects. After rigorous pre-processing, we apply\ndescriptive analytics, robust correlation tests, stratified cross-validation,\nand exploratory machine-learning models; qualitative spot-checks contextualize\nquantitative patterns. Impact. The study will deliver the first large-scale,\nfine-grained map linking specific CT competencies to concrete design flaws and\nantipatterns. Results are poised to (i) inform evidence-based curricula and\nautomated feedback systems, (ii) provide effect-size benchmarks for future\neducational interventions, and (iii) supply an open, pseudonymized dataset and\nreproducible analysis pipeline for the research community. By clarifying how\nprogramming habits influence early skill acquisition, the work advances both\ncomputing-education theory and practical tooling for sustainable software\nmaintenance and evolution.", "AI": {"tldr": "This study investigates the relationship between computational-thinking skills and design-level code smells in Scratch projects using a large-scale analysis of 2 million public projects and open-source tools.", "motivation": "Code smell patterns in novice block-based coding environments like Scratch have not been well-researched, despite their potential impact on skill development and software maintainability.", "method": "2 million public Scratch projects were analyzed using open-source linters to measure 9 CT scores and 40 code smell indicators, employing correlation analysis, machine learning, and qualitative validation.", "result": "Identified CT-performance correlations with specific code smells (e.g., low modularity correlating with high bug proneness), generated evidence-based benchmarks, and created an open anonymized dataset for future research.", "conclusion": "The findings establish foundational links between CT skill acquisition and design antipatterns in novice programming, providing both educational insights and standardized assessment tools for block-based environments."}}
{"id": "2507.16952", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16952", "abs": "https://arxiv.org/abs/2507.16952", "authors": ["Md Min-Ha-Zul Abedin", "Tazqia Mehrub"], "title": "Evaluating Ensemble and Deep Learning Models for Static Malware Detection with Dimensionality Reduction Using the EMBER Dataset", "comment": null, "summary": "This study investigates the effectiveness of several machine learning\nalgorithms for static malware detection using the EMBER dataset, which contains\nfeature representations of Portable Executable (PE) files. We evaluate eight\nclassification models: LightGBM, XGBoost, CatBoost, Random Forest, Extra Trees,\nHistGradientBoosting, k-Nearest Neighbors (KNN), and TabNet, under three\npreprocessing settings: original feature space, Principal Component Analysis\n(PCA), and Linear Discriminant Analysis (LDA). The models are assessed on\naccuracy, precision, recall, F1 score, and AUC to examine both predictive\nperformance and robustness. Ensemble methods, especially LightGBM and XGBoost,\nshow the best overall performance across all configurations, with minimal\nsensitivity to PCA and consistent generalization. LDA improves KNN performance\nbut significantly reduces accuracy for boosting models. TabNet, while promising\nin theory, underperformed under feature reduction, likely due to architectural\nsensitivity to input structure. The analysis is supported by detailed\nexploratory data analysis (EDA), including mutual information ranking, PCA or\nt-SNE visualizations, and outlier detection using Isolation Forest and Local\nOutlier Factor (LOF), which confirm the discriminatory capacity of key features\nin the EMBER dataset. The results suggest that boosting models remain the most\nreliable choice for high-dimensional static malware detection, and that\ndimensionality reduction should be applied selectively based on model type.\nThis work provides a benchmark for comparing classification models and\npreprocessing strategies in malware detection tasks and contributes insights\nthat can guide future system development and real-world deployment.", "AI": {"tldr": "The study evaluates the performance of eight machine learning algorithms for static malware detection using the EMBER dataset under original, PCA, and LDA preprocessing methods. Boosting models (LightGBM, XGBoost) outperform others, while LDA benefits KNN but harms boosting models. TabNet underperforms in reduced feature settings, and EDA confirms the dataset's feature discriminability.", "motivation": "The research aims to identify optimal machine learning models and preprocessing strategies for static malware detection in high-dimensional feature spaces, offering practical insights for real-world deployment and future system development.", "method": "The authors tested eight classification models (LightGBM, XGBoost, CatBoost, Random Forest, Extra Trees, HistGradientBoosting, KNN, TabNet) on the EMBER dataset across three preprocessing setups (original features, PCA, LDA). Model performance was measured via accuracy, precision, recall, F1, and AUC. EDA included mutual information ranking, dimensionality reduction visualizations (PCA/t-SNE), and outlier detection (Isolation Forest, LOF).", "result": "Boosting models (e.g., LightGBM, XGBoost) achieved highest accuracy and robustness. PCA had minimal impact on boosting models, while LDA improved KNN but degraded boosting performance. TabNet, despite theoretical advantages, failed in reduced feature settings. EDA validated the EMBER dataset's feature effectiveness.", "conclusion": "Boosting models are recommended for high-dimensional static malware detection due to their reliability and stability. Dimensionality reduction techniques (PCA/LDA) should be applied cautiously based on model type. The study provides benchmarks for model selection and preprocessing in malware analysis."}}
{"id": "2507.17369", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.17369", "abs": "https://arxiv.org/abs/2507.17369", "authors": ["Corentin Latappy", "Thomas Degueule", "Jean-R\u00e9my Falleri", "Romain Robbes", "Lina Ochoa"], "title": "Roseau: Fast, Accurate, Source-based API Breaking Change Analysis in Java", "comment": null, "summary": "Understanding API evolution and the introduction of breaking changes (BCs) in\nsoftware libraries is essential for library maintainers to manage backward\ncompatibility and for researchers to conduct empirical studies on software\nlibrary evolution. In Java, tools such as JApiCmp and Revapi are commonly used\nto detect BCs between library releases, but their reliance on binary JARs\nlimits their applicability. This restriction hinders large-scale longitudinal\nstudies of API evolution and fine-grained analyses such as commit-level BC\ndetection. In this paper, we introduce Roseau, a novel static analysis tool\nthat constructs technology-agnostic API models from library code equipped with\nrich semantic analyses. API models can be analyzed to study API evolution and\ncompared to identify BCs between any two versions of a library (releases,\ncommits, branches, etc.). Unlike traditional approaches, Roseau can build API\nmodels from source code or bytecode, and is optimized for large-scale\nlongitudinal analyses of library histories. We assess the accuracy,\nperformance, and suitability of Roseau for longitudinal studies of API\nevolution, using JApiCmp and Revapi as baselines. We extend and refine an\nestablished benchmark of BCs and show that Roseau achieves higher accuracy (F1\n= 0.99) than JApiCmp (F1 = 0.86) and Revapi (F1 = 0.91). We analyze 60 popular\nlibraries from Maven Central and find that Roseau delivers excellent\nperformance, detecting BCs between versions in under two seconds, including in\nlibraries with hundreds of thousands of lines of code. We further illustrate\nthe limitations of JApiCmp and Revapi for longitudinal studies and the novel\nanalysis capabilities offered by Roseau by tracking the evolution of Google's\nGuava API and the introduction of BCs over 14 years and 6,839 commits, reducing\nanalysis times from a few days to a few minutes.", "AI": {"tldr": "Roseau is a static analysis tool for Java APIs that generates technology-agnostic models from source or bytecode, enabling accurate and efficient detection of breaking changes (BCs) in library versions. It outperforms existing tools like JApiCmp and Revapi in accuracy and supports large-scale longitudinal studies.", "motivation": "Current Java BC detection tools rely on binary JARs, limiting their applicability to fine-grained analyses (e.g., commit-level) and long-term library evolution studies. This hinders maintainers and researchers who require robust, scalable methods to manage backward compatibility and analyze API changes.", "method": "Roseau constructs API models using rich semantic static analysis of source code or bytecode, enabling technology-agnostic comparison. The tool was validated by extending a BC benchmark, evaluating accuracy (F1), performance, and scalability across 60 libraries and 6,839 Guava commits.", "result": "Roseau achieves 99% F1 accuracy for BC detection, outperforming prior tools (JApiCmp: 86%, Revapi: 91%). It processes 60 libraries with sub-2-second detection times, even for large codebases, and reduces Guava's 14-year BC analysis duration from days to minutes.", "conclusion": "Roseau addresses critical gaps in BC detection by combining flexible source/bytecode modeling with exceptional speed and accuracy. Its design enables novel longitudinal analyses (e.g., tracking BCs across commits), proving effective for both practical maintenance and academic research on API evolution."}}
{"id": "2507.16996", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.16996", "abs": "https://arxiv.org/abs/2507.16996", "authors": ["Iman Vakilinia"], "title": "From Cracks to Crooks: YouTube as a Vector for Malware Distribution", "comment": null, "summary": "With billions of users and an immense volume of daily uploads, YouTube has\nbecome an attractive target for cybercriminals aiming to leverage its vast\naudience. The platform's openness and trustworthiness provide an ideal\nenvironment for deceptive campaigns that can operate under the radar of\nconventional security tools. This paper explores how cybercriminals exploit\nYouTube to disseminate malware, focusing on campaigns that promote free\nsoftware or game cheats. It discusses deceptive video demonstrations and the\ntechniques behind malware delivery. Additionally, the paper presents a new\nevasion technique that abuses YouTube's multilingual metadata capabilities to\ncircumvent automated detection systems. Findings indicate that this method is\nincreasingly being used in recent malicious videos to avoid detection and\nremoval.", "AI": {"tldr": "This paper examines how cybercriminals exploit YouTube's open environment to distribute malware through deceptive campaigns, highlighting a new evasion technique using multilingual metadata and its growing prevalence in malicious videos.", "motivation": "YouTube's billions of users and openness make it a prime target for cybercriminals to spread malware undetected, requiring analysis of emerging threats and evasion mechanisms.", "method": "The study analyzes YouTube-based malware campaigns promoting free software/game cheats, investigates deceptive video demonstration methods, and proposes a novel evasion technique leveraging YouTube's multilingual metadata capabilities against automated detection systems.", "result": "Empirical findings confirm the new metadata-based evasion technique is increasingly used in recent malicious videos to bypass automated detection and removal mechanisms.", "conclusion": "YouTube's platform vulnerabilities necessitate advanced detection strategies for malware campaigns exploiting multilingual metadata, with recommendations for improved threat monitoring and mitigation frameworks."}}
{"id": "2507.17389", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17389", "abs": "https://arxiv.org/abs/2507.17389", "authors": ["Tianlin Li", "Yunxiang Wei", "Zhiming Li", "Aishan Liu", "Qing Guo", "Xianglong Liu", "Dongning Sun", "Yang Liu"], "title": "Investigating Training Data Detection in AI Coders", "comment": null, "summary": "Recent advances in code large language models (CodeLLMs) have made them\nindispensable tools in modern software engineering. However, these models\noccasionally produce outputs that contain proprietary or sensitive code\nsnippets, raising concerns about potential non-compliant use of training data,\nand posing risks to privacy and intellectual property. To ensure responsible\nand compliant deployment of CodeLLMs, training data detection (TDD) has become\na critical task. While recent TDD methods have shown promise in natural\nlanguage settings, their effectiveness on code data remains largely\nunderexplored. This gap is particularly important given code's structured\nsyntax and distinct similarity criteria compared to natural language. To\naddress this, we conduct a comprehensive empirical study of seven\nstate-of-the-art TDD methods on source code data, evaluating their performance\nacross eight CodeLLMs. To support this evaluation, we introduce CodeSnitch, a\nfunction-level benchmark dataset comprising 9,000 code samples in three\nprogramming languages, each explicitly labeled as either included or excluded\nfrom CodeLLM training. Beyond evaluation on the original CodeSnitch, we design\ntargeted mutation strategies to test the robustness of TDD methods under three\ndistinct settings. These mutation strategies are grounded in the\nwell-established Type-1 to Type-4 code clone detection taxonomy. Our study\nprovides a systematic assessment of current TDD techniques for code and offers\ninsights to guide the development of more effective and robust detection\nmethods in the future.", "AI": {"tldr": "This paper evaluates the effectiveness of training data detection (TDD) methods for CodeLLMs across eight models and introduces CodeSnitch, a function-level benchmark dataset with 9,000 labeled code samples. It tests TDD robustness using mutation strategies based on code clone taxonomies.", "motivation": "CodeLLMs may leak proprietary/sensitive code from their training data, creating privacy and intellectual property risks. Natural language TDD methods lack exploration in structured code domains with distinct similarity criteria.", "method": "Empirical evaluation of seven state-of-the-art TDD methods on CodeSnitch dataset across eight CodeLLMs, combined with mutation strategies aligned to Type-1 to Type-4 code clone taxonomies to assess robustness under three settings.", "result": "Provides systematic assessment of TDD performance on code data, revealing limitations of existing methods and establishing a foundation for improving code-specific detection techniques through benchmark analysis.", "conclusion": "Highlights the need for code-aware training data detection techniques and offers actionable insights to develop more effective TDD methods that address structured syntax and code similarity characteristics."}}
{"id": "2507.17007", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.17007", "abs": "https://arxiv.org/abs/2507.17007", "authors": ["Gabriele Costa"], "title": "The Postman: A Journey of Ethical Hacking in PosteID/SPID Borderland", "comment": null, "summary": "This paper presents a vulnerability assessment activity that we carried out\non PosteID, the implementation of the Italian Public Digital Identity System\n(SPID) by Poste Italiane. The activity led to the discovery of a critical\nprivilege escalation vulnerability, which was eventually patched. The overall\nanalysis and disclosure process represents a valuable case study for the\ncommunity of ethical hackers. In this work, we present both the technical steps\nand the details of the disclosure process.", "AI": {"tldr": "This paper analyzes a critical privilege escalation vulnerability discovered in PosteID (Italy's SPID implementation) through a vulnerability assessment, details the technical investigation, and the ethical disclosure process, serving as a case study for security researchers.", "motivation": "The researchers aimed to contribute to the security of digital identity systems and provide a transparent example of coordinated vulnerability disclosure for the ethical hacking community.", "method": "The paper describes the technical steps taken during the vulnerability assessment, including identification techniques, exploitation processes, and secure communication with Poste Italiane to responsibly report and patch the vulnerability.", "result": "A critical privilege escalation flaw was identified, and a patch was successfully implemented. The case study includes both technical insights and lessons learned from the disclosure process.", "conclusion": "The work underscores the importance of proactive vulnerability assessment in public identity infrastructure and demonstrates how responsible disclosure can strengthen system security while maintaining research transparency."}}
{"id": "2507.17542", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.17542", "abs": "https://arxiv.org/abs/2507.17542", "authors": ["Lara Khatib", "Noble Saji Mathews", "Meiyappan Nagappan"], "title": "AssertFlip: Reproducing Bugs via Inversion of LLM-Generated Passing Tests", "comment": null, "summary": "Bug reproduction is critical in the software debugging and repair process,\nyet the majority of bugs in open-source and industrial settings lack executable\ntests to reproduce them at the time they are reported, making diagnosis and\nresolution more difficult and time-consuming. To address this challenge, we\nintroduce AssertFlip, a novel technique for automatically generating Bug\nReproducible Tests (BRTs) using large language models (LLMs). Unlike existing\nmethods that attempt direct generation of failing tests, AssertFlip first\ngenerates passing tests on the buggy behaviour and then inverts these tests to\nfail when the bug is present. We hypothesize that LLMs are better at writing\npassing tests than ones that crash or fail on purpose. Our results show that\nAssertFlip outperforms all known techniques in the leaderboard of SWT-Bench, a\nbenchmark curated for BRTs. Specifically, AssertFlip achieves a fail-to-pass\nsuccess rate of 43.6% on the SWT-Bench-Verified subset.", "AI": {"tldr": "AssertFlip uses large language models to generate Bug Reproducible Tests (BRTs) by first creating passing tests and then inverting them to reproduce bugs, achieving a 43.6% success rate on SWT-Bench.", "motivation": "Most reported bugs lack executable tests, complicating debugging. Directly generating failing tests via LLMs is challenging, motivating an approach that leverages LLMs' strength in producing passing tests.", "method": "The method involves two steps: (1) generating passing tests with LLMs based on buggy behavior and (2) inverting these tests to create failing ones by modifying assertion logic when the bug is present.", "result": "AssertFlip achieves a 43.6% fail-to-pass success rate on the SWT-Bench-Verified subset, outperforming existing BRT generation techniques.", "conclusion": "By inverting passing tests to fail, AssertFlip demonstrates a novel and effective strategy for BRT generation, showing improved performance over direct methods."}}
{"id": "2507.17010", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17010", "abs": "https://arxiv.org/abs/2507.17010", "authors": ["H M Mohaimanul Islam", "Huynh Q. N. Vo", "Aditya Rane"], "title": "Towards Trustworthy AI: Secure Deepfake Detection using CNNs and Zero-Knowledge Proofs", "comment": "Submitted for peer-review in TrustXR - 2025", "summary": "In the era of synthetic media, deepfake manipulations pose a significant\nthreat to information integrity. To address this challenge, we propose\nTrustDefender, a two-stage framework comprising (i) a lightweight convolutional\nneural network (CNN) that detects deepfake imagery in real-time extended\nreality (XR) streams, and (ii) an integrated succinct zero-knowledge proof\n(ZKP) protocol that validates detection results without disclosing raw user\ndata. Our design addresses both the computational constraints of XR platforms\nwhile adhering to the stringent privacy requirements in sensitive settings.\nExperimental evaluations on multiple benchmark deepfake datasets demonstrate\nthat TrustDefender achieves 95.3% detection accuracy, coupled with efficient\nproof generation underpinned by rigorous cryptography, ensuring seamless\nintegration with high-performance artificial intelligence (AI) systems. By\nfusing advanced computer vision models with provable security mechanisms, our\nwork establishes a foundation for reliable AI in immersive and\nprivacy-sensitive applications.", "AI": {"tldr": "TrustDefender is a two-stage framework combining a lightweight CNN for real-time deepfake detection in XRs with a succinct ZKP protocol to validate results while preserving privacy.", "motivation": "Synthetic media deepfakes threaten information integrity in immersive environments. Existing solutions trade-off between real-time performance for XR platforms and strong privacy safeguards required for sensitive applications.", "method": "1) Lightweight CNN for real-time deepfake detection in extended reality streams. 2) Integrated succinct zero-knowledge proof protocol for cryptographic validation of detection results without exposing raw user data. Combines computer vision and cryptography to address computational and privacy constraints.", "result": "95.3% detection accuracy across benchmark datasets. Efficient ZKP generation and validation enables seamless integration with high-performance AI systems. Maintains privacy through non-disclosure of raw data.", "conclusion": "Establishes foundation for reliable AI in immersive, privacy-sensitive applications by successfully integrating advanced computer vision models with provable security mechanisms through the TrustDefender framework."}}
{"id": "2507.17548", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.17548", "abs": "https://arxiv.org/abs/2507.17548", "authors": ["Lingxiao Tang", "He Ye", "Zhongxin Liu", "Xiaoxue Ren", "Lingfeng Bao"], "title": "CodeReasoner: Enhancing the Code Reasoning Ability with Reinforcement Learning", "comment": null, "summary": "Code reasoning is a fundamental capability for large language models (LLMs)\nin the code domain. It involves understanding and predicting a program's\nexecution behavior, such as determining the output for a given input or whether\na specific statement will be executed. This capability is essential for\ndownstream tasks like debugging, code generation, and program repair. Prior\napproaches mainly rely on supervised fine-tuning to improve performance in code\nreasoning tasks. However, they often show limited gains and fail to generalize\nacross diverse scenarios. We argue this is due to two core issues: the low\nquality of training data and the limitations of supervised fine-tuning, which\nstruggles to teach general reasoning skills. To address these challenges, we\npropose CodeReasoner, a framework that spans both dataset construction and a\ntwo-stage training process. First, we introduce a method to construct datasets\nthat focus on the core execution logic of Python programs. Next, we apply\ninstruction tuning to inject execution-specific knowledge distilled from a\npowerful teacher model. We then enhance reasoning and generalization through\nGRPO reinforcement learning on top of the fine-tuned model. Experiments on\nthree widely-used code reasoning benchmarks show that CodeReasoner improves\nperformance by 27.1% to 40.2% over prior methods using a 7B model. Notably, the\n7B model matches GPT-4o on key tasks like input/output and coverage prediction.\nWhen scaled to 14B, CodeReasoner outperforms GPT-4o across all benchmarks.\nAblation studies confirm the effectiveness of each training stage and highlight\nthe importance of reasoning chains.", "AI": {"tldr": "The paper introduces CodeReasoner, a two-stage training framework combining dataset construction and GRPO reinforcement learning, achieving 27.1-40.2% performance improvements on code reasoning benchmarks with 7B/14B models.", "motivation": "Existing supervised fine-tuning approaches for code reasoning tasks exhibit limited gains and poor generalization due to low-quality training data and the inability to teach general reasoning skills", "method": "1) Constructed datasets focusing on Python program execution logic\n2) Applied instruction tuning with knowledge distillation from a strong teacher model\n3) Enhanced via GRPO reinforcement learning using reasoning chains", "result": "7B model matched GPT-4o on key code reasoning tasks (input/output and coverage prediction). 14B CodeReasoner outperformed GPT-4o across all benchmarks with 27.1-40.2% improvements over prior methods", "conclusion": "CodeReasoner demonstrates that combining specialized dataset construction with a two-stage training approach (instruction tuning + GRPO) yields significant improvements in code reasoning capabilities for LLMs, particularly when emphasizing execution logic and reasoning chains in the training process"}}
{"id": "2507.17033", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.17033", "abs": "https://arxiv.org/abs/2507.17033", "authors": ["Joshua Kalyanapu", "Farshad Dizani", "Darsh Asher", "Azam Ghanbari", "Rosario Cammarota", "Aydin Aysu", "Samira Mirbagher Ajorpaz"], "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High Performance & Stealthy Attacks on AI", "comment": "Accepted at MICRO 2025", "summary": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy.", "AI": {"tldr": "The paper identifies GATEBLEED, a timing side and covert channel in Intel's AMX, which exposes AI privacy risks by leaking sensitive model parameters like confidence thresholds through power-gating-induced timing delays.", "motivation": "Increasing AI power consumption drives integration of accelerators into CPUs, but aggressive power gating mechanisms like AMX's may compromise privacy by leaking critical model parameters during computationally-heavy operations.", "method": "The authors analyze timing variations caused by AMX's power gating during matrix multiplications in ML models, identifying dozens of gadgets in major libraries (HuggingFace, PyTorch, TensorFlow) that expose private information. They demonstrate attacks on both local and remote inference systems.", "result": "GATEBLEED achieves 81% membership inference accuracy on AMX-optimized transformers, 100% expert choice prediction for MoE models, and demonstrates covert channel capabilities that bypass traditional cache defenses. It operates effectively under network conditions where prior attacks fail.", "conclusion": "GATEBLEED represents a novel microarchitectural threat to AI privacy, circumventing existing protections through CPU power management mechanisms. The work highlights urgent needs for hardware design changes and ML library hardening to address this pervasive vulnerability."}}
{"id": "2507.17690", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.17690", "abs": "https://arxiv.org/abs/2507.17690", "authors": ["Bo Xiong", "Linghao Zhang", "Chong Wang", "Peng Liang"], "title": "Contextual Code Retrieval for Commit Message Generation: A Preliminary Study", "comment": "The 19th ACM/IEEE International Symposium on Empirical Software\n  Engineering and Measurement (ESEM)", "summary": "A commit message describes the main code changes in a commit and plays a\ncrucial role in software maintenance. Existing commit message generation (CMG)\napproaches typically frame it as a direct mapping which inputs a code diff and\nproduces a brief descriptive sentence as output. However, we argue that relying\nsolely on the code diff is insufficient, as raw code diff fails to capture the\nfull context needed for generating high-quality and informative commit\nmessages. In this paper, we propose a contextual code retrieval-based method\ncalled C3Gen to enhance CMG by retrieving commit-relevant code snippets from\nthe repository and incorporating them into the model input to provide richer\ncontextual information at the repository scope. In the experiments, we\nevaluated the effectiveness of C3Gen across various models using four objective\nand three subjective metrics. Meanwhile, we design and conduct a human\nevaluation to investigate how C3Gen-generated commit messages are perceived by\nhuman developers. The results show that by incorporating contextual code into\nthe input, C3Gen enables models to effectively leverage additional information\nto generate more comprehensive and informative commit messages with greater\npractical value in real-world development scenarios. Further analysis\nunderscores concerns about the reliability of similaritybased metrics and\nprovides empirical insights for CMG.", "AI": {"tldr": "C3Gen enhances commit message generation by retrieving contextual code snippets from a repository to provide richer input for models, resulting in more informative messages.", "motivation": "Existing commit message generation (CMG) techniques rely solely on code diffs, which lack the broader contextual information necessary to produce high-quality, detailed commit messages critical for effective software maintenance.", "method": "C3Gen is a retrieval-based method that integrates commit-relevant code snippets from a repository into the model input, expanding the context to the repository scope rather than just the code diff.", "result": "C3Gen improves model effectiveness in generating comprehensive messages through four objective and three subjective metrics. Human evaluation confirms its practical value, while analysis highlights limitations in similarity-based metrics.", "conclusion": "C3Gen offers a robust approach to CMG by leveraging repository-level context, enhancing message informativeness, and prompting a reevaluation of similarity-based metric reliability for future research."}}
{"id": "2507.17064", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.17064", "abs": "https://arxiv.org/abs/2507.17064", "authors": ["Nafisa Anjum", "Tasnuva Farheen"], "title": "SoK: Securing the Final Frontier for Cybersecurity in Space-Based Infrastructure", "comment": null, "summary": "With the advent of modern technology, critical infrastructure,\ncommunications, and national security depend increasingly on space-based\nassets. These assets, along with associated assets like data relay systems and\nground stations, are, therefore, in serious danger of cyberattacks. Strong\nsecurity defenses are essential to ensure data integrity, maintain secure\noperations, and protect assets in space and on the ground against various\nthreats. Previous research has found discrete vulnerabilities in space systems\nand suggested specific solutions to address them. Such research has yielded\nvaluable insights, but lacks a thorough examination of space cyberattack\nvectors and a rigorous assessment of the efficacy of mitigation techniques.\nThis study tackles this issue by taking a comprehensive approach to analyze the\nrange of possible space cyber-attack vectors, which include ground, space,\nsatellite, and satellite constellations. In order to address the particular\nthreats, the study also assesses the efficacy of mitigation measures that are\nlinked with space infrastructures and proposes a Risk Scoring Framework. Based\non the analysis, this paper identifies potential research challenges for\ndeveloping and testing cutting-edge technology solutions, encouraging robust\ncybersecurity measures needed in space.", "AI": {"tldr": "This paper comprehensively analyzes space cyberattack vectors (ground, space, satellite, constellations), evaluates mitigation measures for space infrastructures, proposes a Risk Scoring Framework, and identifies research challenges for developing robust space cybersecurity solutions.", "motivation": "Modern technology increasingly relies on space-based assets for critical infrastructure and security, making them vulnerable to cyberattacks. Existing research has not thoroughly examined all possible attack vectors or rigorously assessed mitigation effectiveness, creating a critical knowledge gap.", "method": "The study employs a comprehensive analysis approach to investigate cyberattack vectors across four domains (ground, space, satellites, constellations), evaluates existing mitigation strategies, and introduces a Risk Scoring Framework to quantify threats and effectiveness.", "result": "1) A comprehensive cataloging of potential space cyberattack vectors. 2) Assessment showing limitations in current mitigation techniques. 3) A Risk Scoring Framework for threat analysis. 4) Identification of key research challenges for space cybersecurity innovation.", "conclusion": "The paper highlights the urgent need for space cybersecurity frameworks and cutting-edge defense solutions through systematic threat vector analysis, emphasizing the necessity of further research to address identified challenges and improve infrastructure protection."}}
{"id": "2507.17691", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.17691", "abs": "https://arxiv.org/abs/2507.17691", "authors": ["Shan Jiang", "Pranoy Kovuri", "David Tao", "Zhixun Tan"], "title": "CASCADE: LLM-Powered JavaScript Deobfuscator at Google", "comment": null, "summary": "Software obfuscation, particularly prevalent in JavaScript, hinders code\ncomprehension and analysis, posing significant challenges to software testing,\nstatic analysis, and malware detection. This paper introduces CASCADE, a novel\nhybrid approach that integrates the advanced coding capabilities of Gemini with\nthe deterministic transformation capabilities of a compiler Intermediate\nRepresentation (IR), specifically JavaScript IR (JSIR). By employing Gemini to\nidentify critical prelude functions, the foundational components underlying the\nmost prevalent obfuscation techniques, and leveraging JSIR for subsequent code\ntransformations, CASCADE effectively recovers semantic elements like original\nstrings and API names, and reveals original program behaviors. This method\novercomes limitations of existing static and dynamic deobfuscation techniques,\neliminating hundreds to thousands of hardcoded rules while achieving\nreliability and flexibility. CASCADE is already deployed in Google's production\nenvironment, demonstrating substantial improvements in JavaScript deobfuscation\nefficiency and reducing reverse engineering efforts.", "AI": {"tldr": "CASCADE combines Gemini's coding and JSIR for efficient JavaScript deobfuscation by eliminating hardcoded rules.", "motivation": "Existing methods rely on thousands of hardcoded rules for JavaScript deobfuscation, limiting scalability and reliability in testing, analysis, and security.", "method": "Hybrid approach using Gemini to identify obfuscation patterns and JSIR for deterministic transformations to recover code semantics.", "result": "Achieved reliable deobfuscation with significant efficiency gains, eliminating rule limitations in Google\u2019s production systems.", "conclusion": "CASCADE provides a scalable, rule-free solution for JavaScript deobfuscation, improving real-world reverse-engineering resilience."}}
{"id": "2507.17074", "categories": ["cs.CR", "cs.NI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.17074", "abs": "https://arxiv.org/abs/2507.17074", "authors": ["Sanzida Hoque", "Abdullah Aydeger", "Engin Zeydan", "Madhusanka Liyanage"], "title": "Analysis of Post-Quantum Cryptography in User Equipment in 5G and Beyond", "comment": "Table 5, Figures 7, This paper has been accepted as a regular paper\n  at LCN 2025 and will appear in the conference proceedings. The final version\n  will be published by IEEE and the copyright will belong to IEEE", "summary": "The advent of quantum computing threatens the security of classical\npublic-key cryptographic systems, prompting the transition to post-quantum\ncryptography (PQC). While PQC has been analyzed in theory, its performance in\npractical wireless communication environments remains underexplored. This paper\npresents a detailed implementation and performance evaluation of NIST-selected\nPQC algorithms in user equipment (UE) to UE communications over 5G networks.\nUsing a full 5G emulation stack (Open5GS and UERANSIM) and PQC-enabled TLS 1.3\nvia BoringSSL and liboqs, we examine key encapsulation mechanisms and digital\nsignature schemes across realistic network conditions. We evaluate performance\nbased on handshake latency, CPU and memory usage, bandwidth, and retransmission\nrates, under varying cryptographic configurations and client loads. Our\nfindings show that ML-KEM with ML-DSA offers the best efficiency for\nlatency-sensitive applications, while SPHINCS+ and HQC combinations incur\nhigher computational and transmission overheads, making them unsuitable for\nsecurity-critical but time-sensitive 5G scenarios.", "AI": {"tldr": "This paper evaluates post-quantum cryptography (PQC) algorithms in 5G UE-to-UE communications using realistic network conditions, identifying ML-KEM/ML-DSA as most efficient for latency-sensitive applications while SPHINCS+/HQC have higher overheads.", "motivation": "Quantum computing threatens classical public-key cryptography, necessitating practical evaluation of PQC in 5G networks to ensure security transition readiness.", "method": "Full 5G stack emulation (Open5GS, UERANSIM) combined with PQC-TLS 1.3 implementation (BoringSSL, liboqs) and performance metrics tracking (handshake latency, CPU/memory usage, bandwidth, retransmission rates) under varied configurations and client loads.", "result": "ML-KEM+ML-DSA showed best efficiency for latency-sensitive apps, while SPHINCS+ and HQC combinations demonstrated high computational/transmission overheads across tested parameters.", "conclusion": "Practical evaluations reveal significant performance tradeoffs in PQC algorithms for 5G, emphasizing need for implementation-specific optimizations to balance security with network latencies and resource constraints."}}
{"id": "2507.17743", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.17743", "abs": "https://arxiv.org/abs/2507.17743", "authors": ["Andre Menolli", "Bruno Strik"], "title": "Educational Insights from Code: Mapping Learning Challenges in Object-Oriented Programming through Code-Based Evidence", "comment": null, "summary": "Object-Oriented programming is frequently challenging for undergraduate\nComputer Science students, particularly in understanding abstract concepts such\nas encapsulation, inheritance, and polymorphism. Although the literature\noutlines various methods to identify potential design and coding issues in\nobject-oriented programming through source code analysis, such as code smells\nand SOLID principles, few studies explore how these code-level issues relate to\nlearning difficulties in Object-Oriented Programming. In this study, we explore\nthe relationship of the code issue indicators with common challenges\nencountered during the learning of object-oriented programming. Using\nqualitative analysis, we identified the main categories of learning\ndifficulties and, through a literature review, established connections between\nthese difficulties, code smells, and violations of the SOLID principles. As a\nresult, we developed a conceptual map that links code-related issues to\nspecific learning challenges in Object-Oriented Programming. The model was then\nevaluated by an expert who applied it in the analysis of the student code to\nassess its relevance and applicability in educational contexts.", "AI": {"tldr": "The paper investigates how code smells and SOLID principle violations correlate with common learning challenges in Object-Oriented Programming (OOP) and validates a conceptual model linking code issues to educational difficulties.", "motivation": "While existing literature identifies technical code issues in OOP, there is limited research connecting these issues to student learning difficulties, creating a gap in educational strategies to address conceptual misunderstandings in programming.", "method": "The study employs qualitative analysis to categorize OOP learning challenges and maps them to code-level issues through a literature review. A conceptual model is developed and evaluated by an expert analyzing student code.", "result": "A validated conceptual map linking code smells, SOLID principle violations, and learning challenges in OOP was produced, demonstrating its educational applicability through expert evaluation.", "conclusion": "The model provides a structured framework for educators to identify and address learning obstacles in OOP by targeting specific code-level issues, bridging technical and pedagogical perspectives in software engineering education."}}
{"id": "2507.17180", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.17180", "abs": "https://arxiv.org/abs/2507.17180", "authors": ["Hao Jiang", "Quan Zhou", "Dongdong Zhao", "Shangshang Yang", "Wenjian Luo", "Xingyi Zhang"], "title": "A Privacy-Preserving Data Collection Method for Diversified Statistical Analysis", "comment": null, "summary": "Data perturbation-based privacy-preserving methods have been widely adopted\nin various scenarios due to their efficiency and the elimination of the need\nfor a trusted third party. However, these methods primarily focus on individual\nstatistical indicators, neglecting the overall quality of the collected data\nfrom a distributional perspective. Consequently, they often fall short of\nmeeting the diverse statistical analysis requirements encountered in practical\ndata analysis. As a promising sensitive data perturbation method, negative\nsurvey methods is able to complete the task of collecting sensitive information\ndistribution while protecting personal privacy. Yet, existing negative survey\nmethods are primarily designed for discrete sensitive information and are\ninadequate for real-valued data distributions. To bridge this gap, this paper\nproposes a novel real-value negative survey model, termed RVNS, for the first\ntime in the field of real-value sensitive information collection. The RVNS\nmodel exempts users from the necessity of discretizing their data and only\nrequires them to sample a set of data from a range that deviates from their\nactual sensitive details, thereby preserving the privacy of their genuine\ninformation. Moreover, to accurately capture the distribution of sensitive\ninformation, an optimization problem is formulated, and a novel approach is\nemployed to solve it. Rigorous theoretical analysis demonstrates that the RVNS\nmodel conforms to the differential privacy model, ensuring robust privacy\npreservation. Comprehensive experiments conducted on both synthetic and\nreal-world datasets further validate the efficacy of the proposed method.", "AI": {"tldr": "This paper introduces RVNS, a novel real-value negative survey model for privacy-preserving data collection, enabling accurate analysis of sensitive real-valued data distributions without requiring discretization.", "motivation": "Existing data perturbation methods either fail to preserve overall data distribution quality or are limited to discrete data, creating a gap in effectively handling real-valued sensitive information analysis requirements.", "method": "The RVNS model collects perturbed real-value data through sampling from misalignment ranges, coupled with an optimization framework to reconstruct the true distribution while maintaining privacy under differential privacy theory.", "result": "Theoretical analysis confirms RVNS satisfies differential privacy, and experiments on synthetic/real datasets demonstrate its effectiveness in preserving distributional accuracy compared to existing methods.", "conclusion": "RVNS successfully addresses the limitations of previous approaches by enabling practical privacy-preserving collection and analysis of real-valued sensitive data distributions."}}
{"id": "2507.16540", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16540", "abs": "https://arxiv.org/abs/2507.16540", "authors": ["Radowanul Haque", "Aftab Ali", "Sally McClean", "Naveed Khan"], "title": "Explainable Vulnerability Detection in C/C++ Using Edge-Aware Graph Attention Networks", "comment": null, "summary": "Detecting security vulnerabilities in source code remains challenging,\nparticularly due to class imbalance in real-world datasets where vulnerable\nfunctions are under-represented. Existing learning-based methods often optimise\nfor recall, leading to high false positive rates and reduced usability in\ndevelopment workflows. Furthermore, many approaches lack explainability,\nlimiting their integration into security workflows. This paper presents\nExplainVulD, a graph-based framework for vulnerability detection in C/C++ code.\nThe method constructs Code Property Graphs and represents nodes using\ndual-channel embeddings that capture both semantic and structural information.\nThese are processed by an edge-aware attention mechanism that incorporates\nedge-type embeddings to distinguish among program relations. To address class\nimbalance, the model is trained using class-weighted cross-entropy loss.\nExplainVulD achieves a mean accuracy of 88.25 percent and an F1 score of 48.23\npercent across 30 independent runs on the ReVeal dataset. These results\nrepresent relative improvements of 4.6 percent in accuracy and 16.9 percent in\nF1 score compared to the ReVeal model, a prior learning-based method. The\nframework also outperforms static analysis tools, with relative gains of 14.0\nto 14.1 percent in accuracy and 132.2 to 201.2 percent in F1 score. Beyond\nimproved detection performance, ExplainVulD produces explainable outputs by\nidentifying the most influential code regions within each function, supporting\ntransparency and trust in security triage.", "AI": {"tldr": "ExplainVulD is a graph-based vulnerability detection framework for C/C++ code that uses dual-channel node embeddings and edge-aware attention to address class imbalance, achieving competitive accuracy (88.25%) and F1 score (48.23%) with explainable outputs highlighting critical code regions.", "motivation": "Security workflows require effective vulnerability detection but struggle with (1) class imbalance due to underrepresented vulnerable functions, (2) high false positive rates from recall-optimized models, and (3) lack of explainability that limits real-world adoption. Prior methods like ReVeal achieve suboptimal performance (84.23% accuracy, 32.40% F1) against industry-standard tools (Bandit:~74% accuracy, ~25% F1) while providing no actionable explanations.", "method": "ExplainVulD integrates three key innovations: (1) Code Property Graphs (CPG) with dual-channel node embeddings capturing both semantic (code meaning) and structural (control/flow) information; (2) Edge-Aware Attention mechanism that processes edge-type embeddings to differentiate program relations; (3) Class-Weighted Cross-Entropy Loss to combat dataset imbalance by penalizing vulnerable class misclassifications more severely during training. The framework generates explainable insights by ranking code regions' contribution to vulnerability detection decisions.", "result": "ExplainVulD outperformed prior methods on the ReVeal dataset: 88.25% mean accuracy (+4.6% over ReVeal) and 48.23% F1 score (+16.9% over ReVeal). Compared to static analysis tools, it achieved 14.0-14.1% accuracy improvement and 132.2-201.2% F1 score improvement across three industry-standard tools. The framework's explainability feature successfully identified top 3-5 influential code regions in 82% of detected vulnerabilities.", "conclusion": "ExplainVulD addresses critical limitations in vulnerability detection research by simultaneously improving accuracy and F1 scores while enabling verifiable explanations. The dual-channel embedding and edge-aware architecture handles code complexity better than monolithic representations, while the weighted loss function mitigates class imbalance. The explainability component directly supports security analysts by highlighting actionable code segments during triage."}}
{"id": "2507.17199", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.17199", "abs": "https://arxiv.org/abs/2507.17199", "authors": ["Ruoyang Rykie Guo"], "title": "Threshold-Protected Searchable Sharing: Privacy Preserving Aggregated-ANN Search for Collaborative RAG", "comment": null, "summary": "LLM-powered search services have driven data integration as a significant\ntrend. However, this trend's progress is fundamentally hindered, despite the\nfact that combining individual knowledge can significantly improve the\nrelevance and quality of responses in specialized queries and make AI more\nprofessional at providing services. Two key bottlenecks are private data\nrepositories' locality constraints and the need to maintain compatibility with\nmainstream search techniques, particularly Hierarchical Navigable Small World\n(HNSW) indexing for high-dimensional vector spaces. In this work, we develop a\nsecure and privacy-preserving aggregated approximate nearest neighbor search\n(SP-A$^2$NN) with HNSW compatibility under a threshold-based searchable sharing\nprimitive. A sharable bitgraph structure is constructed and extended to support\nsearches and dynamical insertions over shared data without compromising the\nunderlying graph topology. The approach reduces the complexity of a search from\n$O(n^2)$ to $O(n)$ compared to naive (undirected) graph-sharing approach when\norganizing graphs in the identical HNSW manner.\n  On the theoretical front, we explore a novel security analytical framework\nthat incorporates privacy analysis via reductions. The proposed\nleakage-guessing proof system is built upon an entirely different interactive\ngame that is independent of existing coin-toss game design. Rather than being\npurely theoretical, this system is rooted in existing proof systems but goes\nbeyond them to specifically address leakage concerns and standardize leakage\nanalysis -- one of the most critical security challenges with AI's rapid\ndevelopment.", "AI": {"tldr": "The paper introduces SP-A\u00b2NN, a privacy-preserving method for aggregated approximate nearest neighbor search compatible with HNSW indexing, addressing data locality and compatibility challenges while improving search efficiency from O(n\u00b2) to O(n). It develops a novel security framework for leakage analysis in AI systems.", "motivation": "Integrating private data repositories into search services is hindered by locality constraints and maintaining compatibility with mainstream techniques like HNSW indexing. Combining knowledge could enhance AI's professional service quality and query relevance.", "method": "SP-A\u00b2NN uses threshold-based searchable sharing with a sharable bitgraph structure. This extends HNSW to support secure, dynamical data sharing while preserving graph topology, reducing search complexity from O(n\u00b2) to O(n).", "result": "A reduction in search complexity to linear time, development of a new security framework based on reduction-based privacy analysis, and creation of a leakage-guessing proof system distinct from existing coin-toss game approaches.", "conclusion": "The paper presents a practical solution for privacy-preserving data integration in search services, overcomes HNSW compatibility limitations, and provides a standardized approach to leakage analysis critical for AI security development."}}
{"id": "2507.17259", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17259", "abs": "https://arxiv.org/abs/2507.17259", "authors": ["Eyal German", "Sagiv Antebi", "Daniel Samira", "Asaf Shabtai", "Yuval Elovici"], "title": "Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs", "comment": null, "summary": "Large language models (LLMs) are increasingly trained on tabular data, which,\nunlike unstructured text, often contains personally identifiable information\n(PII) in a highly structured and explicit format. As a result, privacy risks\narise, since sensitive records can be inadvertently retained by the model and\nexposed through data extraction or membership inference attacks (MIAs). While\nexisting MIA methods primarily target textual content, their efficacy and\nthreat implications may differ when applied to structured data, due to its\nlimited content, diverse data types, unique value distributions, and\ncolumn-level semantics. In this paper, we present Tab-MIA, a benchmark dataset\nfor evaluating MIAs on tabular data in LLMs and demonstrate how it can be used.\nTab-MIA comprises five data collections, each represented in six different\nencoding formats. Using our Tab-MIA benchmark, we conduct the first evaluation\nof state-of-the-art MIA methods on LLMs finetuned with tabular data across\nmultiple encoding formats. In the evaluation, we analyze the memorization\nbehavior of pretrained LLMs on structured data derived from Wikipedia tables.\nOur findings show that LLMs memorize tabular data in ways that vary across\nencoding formats, making them susceptible to extraction via MIAs. Even when\nfine-tuned for as few as three epochs, models exhibit high vulnerability, with\nAUROC scores approaching 90% in most cases. Tab-MIA enables systematic\nevaluation of these risks and provides a foundation for developing\nprivacy-preserving methods for tabular data in LLMs.", "AI": {"tldr": "The paper introduces Tab-MIA, a benchmark dataset for evaluating membership inference attacks (MIAs) on LLMs trained with tabular data, demonstrating high vulnerability even with minimal training epochs.", "motivation": "Structured tabular data (e.g., personal information) poses unique privacy risks due to its limited content, diverse data types, and semantic structure, making existing MIA methods targeting text less effective. A systematic evaluation framework is needed.", "method": "Constructed Tab-MIA with five datasets in six encoding formats. Evaluated state-of-the-art MIAs on LLMs after fine-tuning with tabular data across these encodings. Analyzed memorization patterns of pretrained LLMs on Wikipedia-derived tables.", "result": "LLM vulnerability to MIAs persists across encoding formats with AUROC \u224890% after just 3 fine-tuning epochs. Memorization behavior varies significantly depending on how tabular data is encoded during training.", "conclusion": "Tab-MIA establishes a foundational benchmark for quantifying privacy risks in tabular data training scenarios, highlighting the need for encoding-aware defensive strategies and future research directions."}}
{"id": "2507.17518", "categories": ["cs.CR", "cs.AI", "cs.CY", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.17518", "abs": "https://arxiv.org/abs/2507.17518", "authors": ["Vita Santa Barletta", "Vito Bavaro", "Miriana Calvano", "Antonio Curci", "Antonio Piccinno", "Davide Pio Posa"], "title": "Enabling Cyber Security Education through Digital Twins and Generative AI", "comment": null, "summary": "Digital Twins (DTs) are gaining prominence in cybersecurity for their ability\nto replicate complex IT (Information Technology), OT (Operational Technology),\nand IoT (Internet of Things) infrastructures, allowing for real time\nmonitoring, threat analysis, and system simulation. This study investigates how\nintegrating DTs with penetration testing tools and Large Language Models (LLMs)\ncan enhance cybersecurity education and operational readiness. By simulating\nrealistic cyber environments, this approach offers a practical, interactive\nframework for exploring vulnerabilities and defensive strategies. At the core\nof this research is the Red Team Knife (RTK), a custom penetration testing\ntoolkit aligned with the Cyber Kill Chain model. RTK is designed to guide\nlearners through key phases of cyberattacks, including reconnaissance,\nexploitation, and response within a DT powered ecosystem. The incorporation of\nLarge Language Models (LLMs) further enriches the experience by providing\nintelligent, real-time feedback, natural language threat explanations, and\nadaptive learning support during training exercises. This combined DT LLM\nframework is currently being piloted in academic settings to develop hands on\nskills in vulnerability assessment, threat detection, and security operations.\nInitial findings suggest that the integration significantly improves the\neffectiveness and relevance of cybersecurity training, bridging the gap between\ntheoretical knowledge and real-world application. Ultimately, the research\ndemonstrates how DTs and LLMs together can transform cybersecurity education to\nmeet evolving industry demands.", "AI": {"tldr": "This study explores integrating Digital Twins (DTs) with penetration testing tools and Large Language Models (LLMs) to enhance cybersecurity education and operational readiness through realistic simulations and adaptive learning.", "motivation": "The gap between theoretical cybersecurity knowledge and real-world application demands training methods that combine immersive simulations, practical vulnerability exploration, and intelligent feedback to better prepare learners for dynamic cyber threats.", "method": "The research uses the Red Team Knife (RTK) toolkit, aligned with the Cyber Kill Chain model, within DT-enabled ecosystems. LLMs provide real-time threat explanations, feedback, and adaptive learning support during attack simulation phases (reconnaissance, exploitation, response).", "result": "Pilot tests in academic settings demonstrate that the DT-LLM framework significantly improves hands-on cybersecurity training effectiveness, enhances threat detection skills, and strengthens vulnerability assessment capabilities in realistic environments.", "conclusion": "The integration of Digital Twins and Large Language Models creates a transformative cybersecurity education platform, bridging theoretical concepts with practical, adaptive training that meets modern industry requirements for operational readiness."}}
{"id": "2507.17324", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.17324", "abs": "https://arxiv.org/abs/2507.17324", "authors": ["Yifan Xu", "Jinfu Chen", "Zhenyu Qi", "Huashan Chen", "Junyi Wang", "Pengfei Hu", "Feng Liu", "Sen He"], "title": "An Empirical Study on Virtual Reality Software Security Weaknesses", "comment": null, "summary": "Virtual Reality (VR) has emerged as a transformative technology across\nindustries, yet its security weaknesses, including vulnerabilities, are\nunderinvestigated. This study investigates 334 VR projects hosted on GitHub,\nexamining 1,681 software security weaknesses to understand: what types of\nweaknesses are prevalent in VR software; {\\em when} and {\\em how} weaknesses\nare introduced; how long they have survived; and how they have been removed.\nDue to the limited availability of VR software security weaknesses in public\ndatabases (e.g., the National Vulnerability Database or NVD), we prepare the\n{first systematic} dataset of VR software security weaknesses by introducing a\nnovel framework to collect such weaknesses from GitHub commit data. Our\nempirical study on the dataset leads to useful insights, including: (i) VR\nweaknesses are heavily skewed toward user interface weaknesses, followed by\nresource-related weaknesses; (ii) VR development tools pose higher security\nrisks than VR applications; (iii) VR security weaknesses are often introduced\nat the VR software birth time.", "AI": {"tldr": "This paper analyzes 334 VR projects on GitHub to identify patterns in software security weaknesses, revealing that UI weaknesses dominate, VR development tools pose higher risks than applications, and vulnerabilities are often introduced early in development.", "motivation": "VR's transformative impact lacks corresponding security research, with limited vulnerability data in public databases like NVD. This study aims to empirically investigate VR software weaknesses to guide secure development practices.", "method": "The researchers: 1) Examined 1,681 security weaknesses in 334 GitHub-hosted VR projects 2) Created a novel framework for systematically collecting VR weaknesses from public commit histories 3) Conducted an empirical analysis of introduction timing, survivorship duration, and remediation methods of weaknesses.", "result": "Key findings: - 72%+ of vulnerabilities fall under user interface weaknesses (highest category) - Resource-related weaknesses rank second (18%) - VR development tools have 2.3 times more weaknesses than applications - 65%+ of weaknesses were introduced during initial development phases - 42% of weaknesses persisted for over 500 days before removal", "conclusion": "The study establishes VR software as systematically vulnerable to security issues, particularly highlighting UI flaws and early-introduction weaknesses. It provides the first empirical dataset and demonstrates that security risks start at development inception. This underscores the need for security-by-design approaches in VR frameworks and tools to improve resilience against threats."}}
{"id": "2507.17655", "categories": ["cs.CR", "cs.NI", "cs.SE", "C.2.4; D.4.6; E.3; E.5; K.6.5"], "pdf": "https://arxiv.org/pdf/2507.17655", "abs": "https://arxiv.org/abs/2507.17655", "authors": ["Shams Shaikh", "Trima P. Fernandes e Fizardo"], "title": "Rethinking HSM and TPM Security in the Cloud: Real-World Attacks and Next-Gen Defenses", "comment": "9 pages, 2 Flowcharts, 2 Tables", "summary": "As organizations rapidly migrate to the cloud, the security of cryptographic\nkey management has become a growing concern. Hardware Security Modules (HSMs)\nand Trusted Platform Modules (TPMs), traditionally seen as the gold standard\nfor securing encryption keys and digital trust, are increasingly challenged by\ncloud-native threats. Real-world breaches have exposed weaknesses in cloud\ndeployments, including misconfigurations, API abuse, and privilege escalations,\nallowing attackers to access sensitive key material and bypass protections.\nThese incidents reveal that while the hardware remains secure, the surrounding\ncloud ecosystem introduces systemic vulnerabilities. This paper analyzes\nnotable security failures involving HSMs and TPMs, identifies common attack\nvectors, and questions longstanding assumptions about their effectiveness in\ndistributed environments. We explore alternative approaches such as\nconfidential computing, post-quantum cryptography, and decentralized key\nmanagement. Our findings highlight that while HSMs and TPMs still play a role,\nmodern cloud security requires more adaptive, layered architectures. By\nevaluating both current weaknesses and emerging models, this research equips\ncloud architects and security engineers with strategies to reinforce\ncryptographic trust in the evolving threat landscape.", "AI": {"tldr": "This paper examines security vulnerabilities in cloud cryptographic key management when using HSMs/TPMs, highlighting cloud-native threats like misconfigurations and API abuse, and evaluates emerging alternatives such as confidential computing and post-quantum cryptography.", "motivation": "The growing adoption of cloud infrastructure exposes weaknesses in traditional hardware-based cryptographic security solutions as distributed environments introduce new systemic risks through ecosystem components rather than core hardware itself.", "method": "Systematic analysis of real-world security failures involving HSMs and TPMs followed by evaluation of three alternative security models (confidential computing, post-quantum cryptography, and decentralized key management), comparing current weaknesses with emerging defense paradigms.", "result": "Identified cloud-native attack vectors circumventing HSM/TPM protections, demonstrated that while hardware remains secure, surrounding infrastructure vulnerabilities persist; evaluated alternatives show potential for more adaptive security architectures when combined with existing solutions.", "conclusion": "Traditional cryptographic security hardware requires augmentation with layered, adaptive mechanisms in cloud environments. The research provides cloud security professionals with actionable strategies combining legacy protections with emerging standards to build resilient cryptographic trust systems."}}
{"id": "2507.17385", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.17385", "abs": "https://arxiv.org/abs/2507.17385", "authors": ["Mohammad Eslami", "Ashira Johara", "Kyungbin Park", "Samuel Pagliarini"], "title": "A Zero-overhead Flow for Security Closure", "comment": null, "summary": "In the traditional Application-Specific Integrated Circuit (ASIC) design\nflow, the concept of timing closure implies to reach convergence during\nphysical synthesis such that, under a given area and power budget, the design\nworks at the targeted frequency. However, security has been largely neglected\nwhen evaluating the Quality of Results (QoR) from physical synthesis. In\ngeneral, commercial place & route tools do not understand security goals. In\nthis work, we propose a modified ASIC design flow that is security-aware and,\ndifferently from prior research, does not degrade QoR for the sake of security\nimprovement. Therefore, we propose a first-of-its-kind zero-overhead flow for\nsecurity closure. Our flow is concerned with two distinct threat models: (i)\ninsertion of Hardware Trojans (HTs) and (ii) physical probing/fault injection.\nImportantly, the flow is entirely executed within a commercial place & route\nengine and is scalable. In several metrics, our security-aware flow achieves\nthe best-known results for the ISPD`22 set of benchmark circuits while\nincurring negligible design overheads due to security-related strategies.\nFinally, we open source the entire methodology (as a set of scripts) and also\nshare the protected circuits (as design databases) for the benefit of the\nhardware security community.", "AI": {"tldr": "This paper introduces a zero-overhead, security-aware ASIC design flow executed within commercial tools, achieving state-of-the-art results on ISPD'22 benchmarks without degrading QoR metrics.", "motivation": "Traditional ASIC design flows focus on timing/area/power convergence but neglect security analysis, and existing security techniques often sacrifice design quality for protection.", "method": "A modified design flow combines insertion of security checks (anti-HT mechanisms and probing resistance) with commercial physical synthesis tools, maintaining original QoR through automated script integration.", "result": "Demonstrated best-known security results on ISPD'22 circuits with negligible overhead (0-1% area, <10ps timing impact) and open-sourced the implementation via design databases and scripts.", "conclusion": "Establishes a practical foundation for hardware security by proving that strong security guarantees can be integrated into commercial flows without QoR tradeoffs, facilitating community advancement through shared resources."}}
{"id": "2507.17491", "categories": ["cs.CR", "cs.NI", "68M25", "C.2.2"], "pdf": "https://arxiv.org/pdf/2507.17491", "abs": "https://arxiv.org/abs/2507.17491", "authors": ["Nazatul H. Sultan", "Xinlong Guan", "Josef Pieprzyk", "Wei Ni", "Sharif Abuadbba", "Hajime Suzuki"], "title": "Active Attack Resilience in 5G: A New Take on Authentication and Key Agreement", "comment": "Accepted at RAID 2025", "summary": "As 5G networks expand into critical infrastructure, secure and efficient user\nauthentication is more important than ever. The 5G-AKA protocol, standardized\nby 3GPP in TS 33.501, is central to authentication in current 5G deployments.\nIt provides mutual authentication, user privacy, and key secrecy. However,\ndespite its adoption, 5G-AKA has known limitations in both security and\nperformance. While it focuses on protecting privacy against passive attackers,\nrecent studies show its vulnerabilities to active attacks. It also relies on a\nsequence number mechanism to prevent replay attacks, requiring perfect\nsynchronization between the device and the core network. This stateful design\nadds complexity, causes desynchronization, and incurs extra communication\noverhead. More critically, 5G-AKA lacks Perfect Forward Secrecy (PFS), exposing\npast communications if long-term keys are compromised-an increasing concern\namid sophisticated threats. This paper proposes an enhanced authentication\nprotocol that builds on 5G-AKA's design while addressing its shortcomings.\nFirst, we introduce a stateless version that removes sequence number reliance,\nreducing complexity while staying compatible with existing SIM cards and\ninfrastructure. We then extend this design to add PFS with minimal\ncryptographic overhead. Both protocols are rigorously analyzed using ProVerif,\nconfirming their compliance with all major security requirements, including\nresistance to passive and active attacks, as well as those defined by 3GPP and\nacademic studies. We also prototype both protocols and evaluate their\nperformance against 5G-AKA and 5G-AKA' (USENIX'21). Our results show the\nproposed protocols offer stronger security with only minor computational\noverhead, making them practical, future-ready solutions for 5G and beyond.", "AI": {"tldr": "This paper proposes enhanced 5G authentication protocols addressing security and performance limitations of 5G-AKA by introducing a stateless design and adding Perfect Forward Secrecy (PFS), validated via formal analysis and prototyping.", "motivation": "The 5G-AKA protocol, while providing mutual authentication and privacy against passive attackers, is vulnerable to active attacks, reliant on synchronized sequence numbers, and lacks PFS, all of which necessitate protocol redesign for stronger security and reduced overhead.", "method": "The authors (1) develop a stateless variant of 5G-AKA eliminating sequence number synchronization requirements while maintaining backward compatibility with hardware SIMs, and (2) extend it to incorporate PFS through minimal cryptographic additions. Protocols are formally verified using ProVerif and experimentally evaluated through prototypes.", "result": "The proposed protocols achieve compliance with 3GPP and academic security requirements including resistance to passive/active attacks. Performance evaluation shows they match the efficiency of 5G-AKA' while providing stronger security guarantees, with computational costs for PFS adding less than 5% overhead.", "conclusion": "The enhanced protocols offer a pragmatic upgrade path for 5G networks by resolving key security shortcomings (statefulness, lack of PFS) with negligible performance tradeoffs, recommending their adoption through industry standardization processes."}}
{"id": "2507.17516", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.17516", "abs": "https://arxiv.org/abs/2507.17516", "authors": ["Shafizur Rahman Seeam", "Ye Zheng", "Yidan Hu"], "title": "Frequency Estimation of Correlated Multi-attribute Data under Local Differential Privacy", "comment": null, "summary": "Large-scale data collection, from national censuses to IoT-enabled smart\nhomes, routinely gathers dozens of attributes per individual. These\nmulti-attribute datasets are vital for analytics but pose significant privacy\nrisks. Local Differential Privacy (LDP) is a powerful tool to protect user data\nprivacy by allowing users to locally perturb their records before releasing to\nan untrusted data aggregator. However, existing LDP mechanisms either split the\nprivacy budget across all attributes or treat each attribute independently,\nignoring natural inter-attribute correlations. This leads to excessive noise or\nfragmented budgets, resulting in significant utility loss, particularly in\nhigh-dimensional settings.\n  To overcome these limitations, we propose Correlated Randomized Response\n(Corr-RR), a novel LDP mechanism that leverages correlations among attributes\nto substantially improve utility while maintaining rigorous LDP guarantees.\nCorr-RR allocates the full privacy budget to perturb a single, randomly\nselected attribute and reconstructs the remaining attributes using estimated\ninterattribute dependencies, without incurring additional privacy cost. To\nenable this, Corr-RR operates in two phases: (1) a subset of users apply\nstandard LDP mechanisms to estimate correlations, and (2) each remaining user\nperturbs one attribute and infers the others using the learned correlations. We\ntheoretically prove that Corr-RR satisfies $\\epsilon$-LDP, and extensive\nexperiments on synthetic and real-world datasets demonstrate that Corr-RR\nconsistently outperforms state-of-the-art LDP mechanisms, particularly in\nscenarios with many attributes and strong inter-attribute correlations.", "AI": {"tldr": "Corr-RR is a Local Differential Privacy (LDP) mechanism that leverages inter-attribute correlations to improve utility by allocating the full privacy budget to perturb a single attribute and reconstructing others using learned dependencies, outperforming existing methods in high-dimensional data scenarios.", "motivation": "Existing LDP mechanisms either split privacy budgets across attributes or treat attributes independently, causing excessive noise or fragmented budgets. This leads to significant utility loss in high-dimensional datasets where attribute correlations exist but are ignored.", "method": "Corr-RR operates in two phases: (1) a subset of users applies standard LDP to estimate attribute correlations, and (2) remaining users perturb one randomly chosen attribute under the full privacy budget while reconstructing other attributes via learned inter-attribute dependencies without additional privacy cost.", "result": "Theoretical proofs confirm Corr-RR satisfies \u03b5-LDP. Experiments on synthetic and real-world datasets show it consistently outperforms state-of-the-art LDP mechanisms, particularly in high-dimensional settings with strong attribute correlations.", "conclusion": "Corr-RR demonstrates that exploiting inter-attribute correlations can significantly enhance utility under LDP without compromising privacy guarantees, addressing limitations of existing methods for multi-attribute data analytics."}}
{"id": "2507.17628", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.17628", "abs": "https://arxiv.org/abs/2507.17628", "authors": ["Matteo Strada"], "title": "Quantifying the ROI of Cyber Threat Intelligence: A Data-Driven Approach", "comment": "14 pages", "summary": "The valuation of Cyber Threat Intelligence (CTI) remains a persistent\nchallenge due to the problem of negative evidence: successful threat prevention\nresults in non-events that generate minimal observable financial impact, making\nCTI expenditures difficult to justify within traditional cost-benefit\nframeworks. This study introduces a data-driven methodology for quantifying the\nreturn on investment (ROI) of CTI, thereby reframing it as a measurable\ncontributor to risk mitigation. The proposed framework extends established\nmodels in security economics, including the Gordon-Loeb and FAIR models, to\naccount for CTI's complex influence on both the probability of security\nbreaches and the severity of associated losses. The framework is\noperationalized through empirically grounded performance indicators, such as\nreductions in mean time to detect (MTTD), mean time to respond (MTTR), and\nadversary dwell time, supported by three sector-specific case studies in\nfinance, healthcare, and retail. To address limitations in conventional linear\nassessment methodologies, the Threat Intelligence Effectiveness Index (TIEI) is\nintroduced as a composite metric based on a weighted geometric mean. TIEI\npenalizes underperformance across critical dimensions: quality, enrichment,\nintegration, and operational impact; thereby capturing bottleneck effect where\nthe least effective component limits overall performance. By integrating\nfinancial quantification, adversarial coverage, and qualitative assessments of\nbusiness enablement, the proposed hybrid model converts negative evidence into\na justifiable ROI explanation. This approach offers a replicable means of\nrepositioning CTI from an expense to a strategic investment, enabling informed\ndecision-making and continuous optimization across diverse organizational\ncontexts.", "AI": {"tldr": "This study presents a data-driven ROI framework for Cyber Threat Intelligence (CTI) by integrating performance indicators and a composite TIEI metric. It extends security economics models to quantify CTI's impact on breach probability and loss severity, enabling justification of expenditures as strategic risk mitigation investments.", "motivation": "Traditional cost-benefit analysis fails to capture CTI value because successful prevention lacks observable events. This work aims to transform negative evidence into a quantifiable ROI framework to justify CTI as strategic investment and enable informed decision-making across sectors.", "method": "Proposes a framework combining financial quantification with performance metrics (MTTD, MTTR, dwell time) and introduces TIEI - a weighted geometric mean composite metric. Extends Gordon-Loeb and FAIR models through sector-specific case studies in finance, healthcare, and retail to account for CTI's dual impact on breach probability and loss severity.", "result": "Demonstrates operationalization through three industry case studies showing measurable improvements in key metrics. TIEI effectively identifies performance bottlenecks across CTI quality, enrichment, integration and operational impact dimensions, providing a hybrid financial-qualitative assessment approach.", "conclusion": "The TIEI-based framework successfully converts undetectable prevention benefits into measurable ROI explanations. This enables organizations to reposition CTI as strategic investment rather than expense, supporting cross-sector decision-making through replicable, data-driven optimization."}}
