<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 14]
- [cs.SE](#cs.SE) [Total: 12]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [PLA: Prompt Learning Attack against Text-to-Image Generative Models](https://arxiv.org/abs/2508.03696)
*Xinqi Lyu,Yihao Liu,Yanjie Li,Bin Xiao*

Main category: cs.CR

TL;DR: This paper investigates black-box adversarial attacks on Text-to-Image models to bypass safety mechanisms and proposes a novel prompt learning attack framework (PLA) that leverages multimodal similarities for effective gradient-based training, achieving high success rates over existing methods.


<details>
  <summary>Details</summary>
Motivation: The growing adoption of T2I models with risks of misuse for NSFW content generation necessitates stronger safety mechanisms, but previous suboptimal attack methods (limited by word substitution without gradient optimization) highlight weaknesses in current defenses.

Method: PLA: a black-box adversarial attack framework that designs gradient-driven training using multimodal similarities to learn adversarial prompts without model architecture/parameter access.

Result: PLA successfully bypasses prompt filters and post-hoc safety checkers in black-box T2I models with higher success rates than state-of-the-art methods.

Conclusion: The proposed PLA framework demonstrates significant effectiveness in attacking T2I safety mechanisms under black-box scenarios, emphasizing the need for stronger defense strategies against gradient-driven adversarial prompts.

Abstract: Text-to-Image (T2I) models have gained widespread adoption across various
applications. Despite the success, the potential misuse of T2I models poses
significant risks of generating Not-Safe-For-Work (NSFW) content. To
investigate the vulnerability of T2I models, this paper delves into adversarial
attacks to bypass the safety mechanisms under black-box settings. Most previous
methods rely on word substitution to search adversarial prompts. Due to limited
search space, this leads to suboptimal performance compared to gradient-based
training. However, black-box settings present unique challenges to training
gradient-driven attack methods, since there is no access to the internal
architecture and parameters of T2I models. To facilitate the learning of
adversarial prompts in black-box settings, we propose a novel prompt learning
attack framework (PLA), where insightful gradient-based training tailored to
black-box T2I models is designed by utilizing multimodal similarities.
Experiments show that our new method can effectively attack the safety
mechanisms of black-box T2I models including prompt filters and post-hoc safety
checkers with a high success rate compared to state-of-the-art methods.
Warning: This paper may contain offensive model-generated content.

</details>


### [2] [RX-INT: A Kernel Engine for Real-Time Detection and Analysis of In-Memory Threats](https://arxiv.org/abs/2508.03879)
*Arjun Juneja*

Main category: cs.CR

TL;DR: RX-INT improves fileless threat detection by combining real-time thread creation monitoring, stateful VAD scanning, and heuristics to address limitations in existing tools like TOCTOU race conditions and PE structure dependencies.


<details>
  <summary>Details</summary>
Motivation: Fileless execution techniques (e.g., manual mapping, module stomping) evade traditional security tools by operating within legitimate processes and exploiting detection gaps like PE structure assumptions and TOCTOU vulnerabilities.

Method: RX-INT utilizes a kernel-assisted detection engine with real-time thread creation tracking, stateful VAD scanning, and memory hashing to capture illicit modifications. It implements heuristics to enhance detection accuracy.

Result: RX-INT outperformed PE-sieve in benchmarks, detecting a manually mapped region overlooked by PE-sieve while avoiding TOCTOU susceptibility. The architecture achieved higher detection rates for fileless threats.

Conclusion: RX-INT's design offers a significant advancement in fileless threat detection, demonstrating practical effectiveness for anti-cheat systems and memory security through improved resilience and real-time analysis capabilities.

Abstract: Malware and cheat developers use fileless execution techniques to evade
traditional, signature-based security products. These methods include various
types of manual mapping, module stomping, and threadless injection which work
entirely within the address space of a legitimate process, presenting a
challenge for detection due to ambiguity between what is legitimate and what
isn't. Existing tools often have weaknesses, such as a dependency on Portable
Executable (PE) structures or a vulnerability to time-of-check-to-time-of-use
(TOCTOU) race conditions where an adversary cleans up before a periodic scan
has the chance to occur. To address this gap, we present RX-INT, a
kernel-assisted system featuring an architecture that provides resilience
against TOCTOU attacks. RX-INT introduces a detection engine that combines a
real-time thread creation monitor with a stateful Virtual Address Descriptor
(VAD) scanner alongside various heuristics within. This engine snapshots both
private and image-backed memory regions, using real-time memory hashing to
detect illicit modifications like module stomping. Critically, we demonstrate a
higher detection rate in certain benchmarks of this approach through a direct
comparison with PE-sieve, a commonly used and powerful memory forensics tool.
In our evaluation, RX-INT successfully detected a manually mapped region that
was not identified by PE-sieve. We then conclude that our architecture
represents a tangible difference in the detection of fileless threats, with
direct applications in the fields of anti-cheat and memory security.

</details>


### [3] [Simulating Cyberattacks through a Breach Attack Simulation (BAS) Platform empowered by Security Chaos Engineering (SCE)](https://arxiv.org/abs/2508.03882)
*Arturo Sánchez-Matas,Pablo Escribano Ruiz,Daniel Díaz-López,Angel Luis Perales Gómez,Pantaleone Nespoli,Gregorio Martínez Pérez*

Main category: cs.CR

TL;DR: This paper proposes integrating Security Chaos Engineering with Breach Attack Simulation platforms using a three-layer architecture (SCE Orchestrator, Connector, and BAS layers) to improve attack simulation effectiveness through automated sequences and threat intelligence, demonstrating enhanced vulnerability identification beyond traditional methods.


<details>
  <summary>Details</summary>
Motivation: Organizations must adapt to evolving cyber threats by discovering elusive attack vectors. Traditional simulations fall short, necessitating innovative methods like combining Security Chaos Engineering (SCE) with Breach Attack Simulation (BAS) for proactive defense testing.

Method: A structured three-layer architecture (SCE Orchestrator, Connector, BAS layers) was developed, leveraging MITRE Caldera in the BAS layer to execute automated attack sequences inferred from adversary profiles and threat intelligence databases.

Result: Evaluation results show the proposed integration enhances attack simulation effectiveness beyond traditional BAS scenarios, enabling dynamic testing and more realistic identification of vulnerabilities.

Conclusion: Integrating SCE and BAS with adversary-profile-driven automation offers a scalable, proactive strategy for cyber defense, improving simulation realism and organizational readiness against complex threats.

Abstract: In today digital landscape, organizations face constantly evolving cyber
threats, making it essential to discover slippery attack vectors through novel
techniques like Security Chaos Engineering (SCE), which allows teams to test
defenses and identify vulnerabilities effectively. This paper proposes to
integrate SCE into Breach Attack Simulation (BAS) platforms, leveraging
adversary profiles and abilities from existing threat intelligence databases.
This innovative proposal for cyberattack simulation employs a structured
architecture composed of three layers: SCE Orchestrator, Connector, and BAS
layers. Utilizing MITRE Caldera in the BAS layer, our proposal executes
automated attack sequences, creating inferred attack trees from adversary
profiles. Our proposal evaluation illustrates how integrating SCE with BAS can
enhance the effectiveness of attack simulations beyond traditional scenarios,
and be a useful component of a cyber defense strategy.

</details>


### [4] [ASTRA: Autonomous Spatial-Temporal Red-teaming for AI Software Assistants](https://arxiv.org/abs/2508.03936)
*Xiangzhe Xu,Guangyu Shen,Zian Su,Siyuan Cheng,Hanxi Guo,Lu Yan,Xuan Chen,Jiasheng Jiang,Xiaolong Jin,Chengpeng Wang,Zhuo Zhang,Xiangyu Zhang*

Main category: cs.CR

TL;DR: ASTRA is an automated system for uncovering safety flaws in AI coding assistants, particularly in cybersecurity. It uses knowledge graphs to explore vulnerabilities in both input space and reasoning processes, finding 11-66% more issues and improving alignment training by 17%.


<details>
  <summary>Details</summary>
Motivation: Current red-teaming tools for AI coding assistants use fixed benchmarks or unrealistic prompts, missing many real-world vulnerabilities in critical domains like cybersecurity. There is a need for systematic methods to test safety in these systems.

Method: ASTRA operates in three stages: 1) Constructing structured domain-specific knowledge graphs of software tasks and weaknesses; 2) Online exploration targeting input space (spatial) and reasoning processes (temporal) using adaptive probing guided by these graphs; 3) Generating violation-inducing test cases to enhance model alignment. It combines offline domain modeling with online knowledge graph adaptation.

Result: ASTRA found 11-66% more vulnerabilities than existing techniques in two major domains and 17% more effective alignment training with generated test cases. It demonstrates practical value for safer AI systems.

Conclusion: ASTRA provides a novel approach to red-teaming AI coding assistants by focusing on realistic developer inputs and using dynamic domain knowledge adaptation. It offers significant improvements over previous tools in uncovering safety issues and training safer models.

Abstract: AI coding assistants like GitHub Copilot are rapidly transforming software
development, but their safety remains deeply uncertain-especially in
high-stakes domains like cybersecurity. Current red-teaming tools often rely on
fixed benchmarks or unrealistic prompts, missing many real-world
vulnerabilities. We present ASTRA, an automated agent system designed to
systematically uncover safety flaws in AI-driven code generation and security
guidance systems. ASTRA works in three stages: (1) it builds structured
domain-specific knowledge graphs that model complex software tasks and known
weaknesses; (2) it performs online vulnerability exploration of each target
model by adaptively probing both its input space, i.e., the spatial
exploration, and its reasoning processes, i.e., the temporal exploration,
guided by the knowledge graphs; and (3) it generates high-quality
violation-inducing cases to improve model alignment. Unlike prior methods,
ASTRA focuses on realistic inputs-requests that developers might actually
ask-and uses both offline abstraction guided domain modeling and online domain
knowledge graph adaptation to surface corner-case vulnerabilities. Across two
major evaluation domains, ASTRA finds 11-66% more issues than existing
techniques and produces test cases that lead to 17% more effective alignment
training, showing its practical value for building safer AI systems.

</details>


### [5] [Isolate Trigger: Detecting and Eradicating Evade-Adaptive Backdoors](https://arxiv.org/abs/2508.04094)
*Chengrui Sun,Hua Zhang,Haoran Gao,Zian Tian,Jianjin Zhao,qi Li,Hongliang Zhu,Zongliang Shen,Shang Wang,Anmin Fu*

Main category: cs.CR

TL;DR: The paper introduces IsTr, a universal detection and defense framework for evasive backdoor attacks (EABs) that break existing detection methods by using small/overlapping triggers. The framework improves distance/gradient analysis to identify and repair hidden backdoor triggers across tasks like MNIST, facial recognition, and traffic sign systems.


<details>
  <summary>Details</summary>
Motivation: Existing Non-Essential Features (NEF) detection methods fail against EAB attacks which use small, non-overlapping triggers and efficient evasion techniques, creating urgent need for better universal detection solutions for deep learning security.

Method: Develops Isolate Trigger (IsTr) framework through: 1) Backdoor triggering analysis, 2) Introducing Steps and Differential-Middle-Slice components to enhance distance/gradient theories, 3) Dual-purpose attack detection and model repair capabilities via trigger isolation.

Result: IsTr successfully detects six EAB attack variants (Badnets, Sin-Wave, Multi-trigger, SSBAs, CASSOCK, HCB) with high precision across diverse tasks including driving safety applications. Experiments validate exceptional robustness even against combined attacks and overlapping source/trigger features.

Conclusion: IsTr establishes a new baseline for backdoor defense by achieving universal detection of evasive attacks that defeat legacy methods, demonstrating both high efficiency and dual functionality for model repair in production environments.

Abstract: All current detection of backdoor attacks on deep learning models fall under
the category of a non essential features(NEF), which focus on fighting against
simple and efficient vertical class backdoor -- trigger is small, few and not
overlapping with the source. Evade-adaptive backdoor (EAB) attacks have evaded
NEF detection and improved training efficiency. We introduces a precise,
efficient and universal detection and defense framework coined as Isolate
Trigger (IsTr). IsTr aims to find the hidden trigger by breaking the barrier of
the source features. Therefore, it investigates the essence of backdoor
triggering, and uses Steps and Differential-Middle-Slice as components to
update past theories of distance and gradient. IsTr also plays a positive role
in the model, whether the backdoor exists. For example, accurately find and
repair the wrong identification caused by deliberate or unintentional training
in automatic driving. Extensive experiments on robustness scross various tasks,
including MNIST, facial recognition, and traffic sign recognition, confirm the
high efficiency, generality and precision of the IsTr. We rigorously evaluated
the effectiveness of the IsTr against a series of six EAB attacks, including
Badnets, Sin-Wave, Multi-trigger, SSBAs, CASSOCK, HCB. None of these
countermeasures evade, even when attacks are combined and the trigger and
source overlap.

</details>


### [6] [SenseCrypt: Sensitivity-guided Selective Homomorphic Encryption for Joint Federated Learning in Cross-Device Scenarios](https://arxiv.org/abs/2508.04100)
*Borui Li,Li Yan,Junhao Han,Jianmin Liu,Lei Yu*

Main category: cs.CR

TL;DR: SenseCrypt is a sensitivity-guided selective HE framework that balances privacy and efficiency in cross-device FL by clustering clients, optimizing encryption ratios, and reducing training time by 58.4%-88.7% over traditional HE methods.


<details>
  <summary>Details</summary>
Motivation: Traditional selective HE in FL causes client straggling and fails to reduce overhead effectively in cross-device settings with heterogeneous data and system capabilities.

Method: 1. Cluster clients using model parameter sensitivity to identify similar data distributions. 2. Develop a scoring mechanism to determine client-specific encryption ratios without straggling. 3. Formulate and solve a multi-objective optimization problem for parameter selection balancing HE overhead and security.

Result: SenseCrypt achieves security against state-of-the-art inversion attacks, maintains model accuracy comparable to IID data, and reduces training time by 58.4%-88.7% relative to traditional HE approaches.

Conclusion: SenseCrypt demonstrates robust privacy-efficiency tradeoffs for cross-device FL by adaptively encrypting model parameters based on sensitivity and client heterogeneity.

Abstract: Homomorphic Encryption (HE) prevails in securing Federated Learning (FL), but
suffers from high overhead and adaptation cost. Selective HE methods, which
partially encrypt model parameters by a global mask, are expected to protect
privacy with reduced overhead and easy adaptation. However, in cross-device
scenarios with heterogeneous data and system capabilities, traditional
Selective HE methods deteriorate client straggling, and suffer from degraded HE
overhead reduction performance. Accordingly, we propose SenseCrypt, a
Sensitivity-guided selective Homomorphic EnCryption framework, to adaptively
balance security and HE overhead per cross-device FL client. Given the
observation that model parameter sensitivity is effective for measuring
clients' data distribution similarity, we first design a privacy-preserving
method to respectively cluster the clients with similar data distributions.
Then, we develop a scoring mechanism to deduce the straggler-free ratio of
model parameters that can be encrypted by each client per cluster. Finally, for
each client, we formulate and solve a multi-objective model parameter selection
optimization problem, which minimizes HE overhead while maximizing model
security without causing straggling. Experiments demonstrate that SenseCrypt
ensures security against the state-of-the-art inversion attacks, while
achieving normal model accuracy as on IID data, and reducing training time by
58.4%-88.7% as compared to traditional HE methods.

</details>


### [7] [Evaluating Selective Encryption Against Gradient Inversion Attacks](https://arxiv.org/abs/2508.04155)
*Jiajun Gu,Yuhang Yao,Shuaiqi Wang,Carlee Joe-Wong*

Main category: cs.CR

TL;DR: The paper proposes a distance-based significance analysis framework for selective encryption in federated learning, demonstrating that gradient magnitude is an effective metric against optimization-based gradient inversion attacks, though no universal strategy exists across all scenarios.


<details>
  <summary>Details</summary>
Motivation: Gradient inversion attacks threaten privacy in distributed learning frameworks like federated learning. Traditional encryption is computationally expensive, necessitating lightweight alternatives like selective encryption, which requires practical significance metrics for deciding which gradient components to encrypt.

Method: The authors systematically evaluate selective encryption methods using diverse significance metrics against state-of-the-art gradient inversion attacks. They develop a distance-based significance analysis framework to theoretically identify critical gradient elements and empirically test its effectiveness through experiments across LeNet, CNN, BERT, and GPT-2 architectures with varied attack types.

Result: The framework reduces computational overhead while maintaining attack resilience. Gradient magnitude consistently offers robust protection against optimization-based gradient inversions, but the performance of selective encryption strategies varies significantly across different attack scenarios and model architectures.

Conclusion: Selective encryption is a viable approach for balancing computational efficiency and privacy in federated learning. The study establishes gradient magnitude as a reliable metric for certain attacks, but emphasizes the need for scenario-specific strategies. The proposed distance-based framework provides a theoretical guideline for practical implementation of privacy-preserving gradient communications.

Abstract: Gradient inversion attacks pose significant privacy threats to distributed
training frameworks such as federated learning, enabling malicious parties to
reconstruct sensitive local training data from gradient communications between
clients and an aggregation server during the aggregation process. While
traditional encryption-based defenses, such as homomorphic encryption, offer
strong privacy guarantees without compromising model utility, they often incur
prohibitive computational overheads. To mitigate this, selective encryption has
emerged as a promising approach, encrypting only a subset of gradient data
based on the data's significance under a certain metric. However, there have
been few systematic studies on how to specify this metric in practice. This
paper systematically evaluates selective encryption methods with different
significance metrics against state-of-the-art attacks. Our findings demonstrate
the feasibility of selective encryption in reducing computational overhead
while maintaining resilience against attacks. We propose a distance-based
significance analysis framework that provides theoretical foundations for
selecting critical gradient elements for encryption. Through extensive
experiments on different model architectures (LeNet, CNN, BERT, GPT-2) and
attack types, we identify gradient magnitude as a generally effective metric
for protection against optimization-based gradient inversions. However, we also
observe that no single selective encryption strategy is universally optimal
across all attack scenarios, and we provide guidelines for choosing appropriate
strategies for different model architectures and privacy requirements.

</details>


### [8] [Secure Development of a Hooking-Based Deception Framework Against Keylogging Techniques](https://arxiv.org/abs/2508.04178)
*Md Sajidul Islam Sajid,Shihab Ahmed,Ryan Sosnoski*

Main category: cs.CR

TL;DR: This paper introduces a deception framework using API hooking with a hardened hooking layer to counter advanced keyloggers incorporating anti-hooking techniques. The system injects realistic decoy keystrokes to mislead attackers, demonstrating resilience against bypass attempts with negligible performance overhead.


<details>
  <summary>Details</summary>
Motivation: Traditional keylogger defenses focus on detection and removal, which prevent malicious activity but lack engagement or countermeasures against evasion tactics used by sophisticated keyloggers. A new approach is needed to actively mislead adversaries and counter advanced malware techniques.

Method: The framework employs API hooking to intercept input-related malware API calls at runtime and inject decoy keystrokes. A hardened hooking layer is introduced to detect and rapidly reinstate hooks disrupted by anti-hooking strategies, enhancing deception resilience.

Result: Experimental evaluation against a custom 'super keylogger' with evasion strategies and 50 samples from ten keylogger families showed the system resists bypass attempts, maintains operational stealth, and successfully deceives attackers without significant performance degradation.

Conclusion: The hardened hooking deception framework effectively mitigates advanced keyloggers by maintaining real-time, resilient deception. The results demonstrate that runtime keystroke deception is a practical and robust defense mechanism, offering alternatives to detection-focused approaches and mitigating anti-hooking evasion techniques.

Abstract: Keyloggers remain a serious threat in modern cybersecurity, silently
capturing user keystrokes to steal credentials and sensitive information.
Traditional defenses focus mainly on detection and removal, which can halt
malicious activity but do little to engage or mislead adversaries. In this
paper, we present a deception framework that leverages API hooking to intercept
input-related API calls invoked by keyloggers at runtime and inject realistic
decoy keystrokes. A core challenge, however, lies in the increasing adoption of
anti-hooking techniques by advanced keyloggers. Anti-hooking strategies allow
malware to bypass or detect instrumentation. To counter this, we introduce a
hardened hooking layer that detects tampering and rapidly reinstates disrupted
hooks, ensuring continuity of deception. We evaluate our framework against a
custom-built "super keylogger" incorporating multiple evasion strategies, as
well as 50 real-world malware samples spanning ten prominent keylogger
families. Experimental results demonstrate that our system successfully resists
sophisticated bypass attempts, maintains operational stealth, and reliably
deceives attackers by feeding them decoys. The system operates with negligible
performance overhead and no observable impact on user experience. Our findings
show that resilient, runtime deception can play a practical and robust role in
confronting advanced threats.

</details>


### [9] [BadTime: An Effective Backdoor Attack on Multivariate Long-Term Time Series Forecasting](https://arxiv.org/abs/2508.04189)
*Kunlan Xiang,Haomiao Yang,Meng Hao,Haoxin Wang,Shaofeng Li,Wenbo Jiang*

Main category: cs.CR

TL;DR: This paper proposes BadTime, the first effective backdoor attack method for Multivariate Long-Term Time Series Forecasting (MLTSF) models, improving predictive performance through contrast-guided data poisoning and puzzle-like trigger structures while maintaining high stealthiness.


<details>
  <summary>Details</summary>
Motivation: MLTSF models' robustness against malicious backdoor attacks is unexplored despite their use in critical domains, threatening their reliability and trustworthiness required for deployment.

Method: 1) Contrast-guided sample selection for data poisoning 
2) Graph attention network to identify influential variables 
3) Lag analysis-based trigger localization with puzzle-like distributed triggers 
4) Alternating optimization of model and triggers via tailored objectives

Result: BadTime outperforms SOTA attacks in: 
- Reducing MAE on target variables by >50% 
- Achieving 3× higher stealthiness in experiments 
- Demonstrating robust performance across multiple MLTSF benchmarks

Conclusion: This work establishes the first practical backdoor attack framework for time series forecasting, revealing vulnerabilities through an attacker's perspective while providing insights for developing more secure MLTSF systems.

Abstract: Multivariate Long-Term Time Series Forecasting (MLTSF) models are
increasingly deployed in critical domains such as climate, finance, and
transportation. Although a variety of powerful MLTSF models have been proposed
to improve predictive performance, the robustness of MLTSF models against
malicious backdoor attacks remains entirely unexplored, which is crucial to
ensuring their reliable and trustworthy deployment. To address this gap, we
conduct an in-depth study on backdoor attacks against MLTSF models and propose
the first effective attack method named BadTime. BadTime executes a backdoor
attack by poisoning training data and customizing the backdoor training
process. During data poisoning, BadTime proposes a contrast-guided strategy to
select the most suitable training samples for poisoning, then employs a graph
attention network to identify influential variables for trigger injection.
Subsequently, BadTime further localizes optimal positions for trigger injection
based on lag analysis and proposes a puzzle-like trigger structure that
distributes the trigger across multiple poisoned variables to jointly steer the
prediction of the target variable. During backdoor training, BadTime
alternately optimizes the model and triggers via proposed tailored optimization
objectives. Extensive experiments show that BadTime significantly outperforms
state-of-the-art (SOTA) backdoor attacks on time series forecasting by reducing
MAE by over 50% on target variables and boosting stealthiness by more than 3
times.

</details>


### [10] [DP-DocLDM: Differentially Private Document Image Generation using Latent Diffusion Models](https://arxiv.org/abs/2508.04208)
*Saifullah Saifullah,Stefan Agne,Andreas Dengel,Sheraz Ahmed*

Main category: cs.CR

TL;DR: This paper proposes using conditional latent diffusion models with differential privacy to generate synthetic document images for classification, avoiding performance losses from standard DP training methods.


<details>
  <summary>Details</summary>
Motivation: Current DP techniques degrade model performance and limit standard training procedures when addressing privacy risks in document image classification systems. Direct real data leakage mitigation is challenging/costly for downstream tasks.

Method: Combines LDMs and DP to create class-specific synthetic document images under strict privacy constraints. Tests three pretraining approaches (unconditional/class/layout-conditional) with DPDM/DP-Promise algorithms for private fine-tuning.

Result: Synthetic documents across RVL-CDIP and Tobacco3482 datasets maintained realism while achieving substantial performance improvements over DP-Adam in downstream classification, particularly on small-scale datasets with epsilon 1-10 privacy levels.

Conclusion: Synthetic data generation provides a practical solution for maintaining privacy without performance penalties in document image classification, outperforming direct DP training methods when implementing strict privacy constraints.

Abstract: As deep learning-based, data-driven information extraction systems become
increasingly integrated into modern document processing workflows, one primary
concern is the risk of malicious leakage of sensitive private data from these
systems. While some recent works have explored Differential Privacy (DP) to
mitigate these privacy risks, DP-based training is known to cause significant
performance degradation and impose several limitations on standard training
procedures, making its direct application to downstream tasks both difficult
and costly. In this work, we aim to address the above challenges within the
context of document image classification by substituting real private data with
a synthetic counterpart. In particular, we propose to use conditional latent
diffusion models (LDMs) in combination with differential privacy (DP) to
generate class-specific synthetic document images under strict privacy
constraints, which can then be utilized to train a downstream classifier
following standard training procedures. We investigate our approach under
various pretraining setups, including unconditional, class-conditional, and
layout-conditional pretraining, in combination with multiple private training
strategies such as class-conditional and per-label private fine-tuning with
DPDM and DP-Promise algorithms. Additionally, we evaluate it on two well-known
document benchmark datasets, RVL-CDIP and Tobacco3482, and show that it can
generate useful and realistic document samples across various document types
and privacy levels ($\varepsilon \in \{1, 5, 10\}$). Lastly, we show that our
approach achieves substantial performance improvements in downstream
evaluations on small-scale datasets, compared to the direct application of
DP-Adam.

</details>


### [11] [Per-element Secure Aggregation against Data Reconstruction Attacks in Federated Learning](https://arxiv.org/abs/2508.04285)
*Takumi Suimon,Yuki Koizumi,Junji Takemasa,Toru Hasegawa*

Main category: cs.CR

TL;DR: This paper proposes an enhancement to secure aggregation in federated learning that prevents data leakage by masks aggregated values at indices with fewer than $t$ non-zero contributions, ensuring modularity and compatibility with existing protocols while maintaining practical overhead.


<details>
  <summary>Details</summary>
Motivation: Federated learning's secure aggregation (SecAgg) is vulnerable to data reconstruction attacks when sparse model updates reveal individual contributions. Existing SecAgg methods inadequately protect non-zero values in sparse vectors, creating a critical privacy risk.

Method: The authors introduce a per-element masking strategy to conceal values in the aggregate sum at indices with less than $t$ non-zero contributions. This leverages existing cryptographic primitives in SecAgg protocols like Flamingo, ensuring compatibility without requiring significant architectural changes.

Result: Experimental analysis demonstrates the mechanism effectively mitigates reconstruction attacks while introducing minimal computational and communication overhead (within acceptable practical limits).

Conclusion: The proposed method provides a robust, low-overhead solution to a critical vulnerability in SecAgg for sparse federated learning updates, enhancing privacy without compromising system efficiency or compatibility.

Abstract: Federated learning (FL) enables collaborative model training without sharing
raw data, but individual model updates may still leak sensitive information.
Secure aggregation (SecAgg) mitigates this risk by allowing the server to
access only the sum of client updates, thereby concealing individual
contributions. However, a significant vulnerability has recently attracted
increasing attention: when model updates are sparse vectors, a non-zero value
contributed by a single client at a given index can be directly revealed in the
aggregate, enabling precise data reconstruction attacks. In this paper, we
propose a novel enhancement to SecAgg that reveals aggregated values only at
indices with at least $t$ non-zero contributions. Our mechanism introduces a
per-element masking strategy to prevent the exposure of under-contributed
elements, while maintaining modularity and compatibility with many existing
SecAgg implementations by relying solely on cryptographic primitives already
employed in a typical setup. We integrate this mechanism into Flamingo, a
low-round SecAgg protocol, to provide a robust defense against such attacks.
Our analysis and experimental results indicate that the additional
computational and communication overhead introduced by our mechanism remains
within an acceptable range, supporting the practicality of our approach.

</details>


### [12] [Attack Pattern Mining to Discover Hidden Threats to Industrial Control Systems](https://arxiv.org/abs/2508.04561)
*Muhammad Azmi Umer,Chuadhry Mujeeb Ahmed,Aditya Mathur,Muhammad Taha Jilani*

Main category: cs.CR

TL;DR: The paper presents a data-driven method for generating attack patterns for Industrial Control Systems (ICS) using data from a water treatment plant, validated through a case study with over 100,000 patterns.


<details>
  <summary>Details</summary>
Motivation: A comprehensive ICS security assessment requires diverse and numerous attack patterns, which traditional methods may not efficiently generate.

Method: A data-driven technique was developed to analyze operational data from an ICS (specifically a water treatment plant) to algorithmically generate attack patterns.

Result: Over 100,000 attack patterns were created, and their validity was evaluated through a detailed case study focusing on the ICS context.

Conclusion: The technique demonstrates effectiveness in generating large-scale validated attack patterns for ICS security, enabling more robust threat modeling and detection strategies.

Abstract: This work focuses on validation of attack pattern mining in the context of
Industrial Control System (ICS) security. A comprehensive security assessment
of an ICS requires generating a large and variety of attack patterns. For this
purpose we have proposed a data driven technique to generate attack patterns
for an ICS. The proposed technique has been used to generate over 100,000
attack patterns from data gathered from an operational water treatment plant.
In this work we present a detailed case study to validate the attack patterns.

</details>


### [13] [Measuring the Carbon Footprint of Cryptographic Privacy-Enhancing Technologies](https://arxiv.org/abs/2508.04583)
*Marc Damie,Mihai Pop,Merijn Posthuma*

Main category: cs.CR

TL;DR: This paper establishes a standardized methodology to evaluate the carbon footprint of cryptographic privacy-enhancing technologies (PETs) and quantifies their environmental impact across key applications like encrypted web browsing, machine learning, and email.


<details>
  <summary>Details</summary>
Motivation: PETs are critical for meeting privacy regulations but lack environmental analysis. The ICT sector needs to reduce carbon emissions, yet the trade-off between privacy and carbon cost remains unaddressed, making it hard to prioritize sustainable implementations.

Method: The authors propose a systematic evaluation framework to measure carbon footprint increases of cryptographic PETs. They apply this methodology to five use cases (HTTPS, encrypted ML inference/training, encrypted databases, encrypted email) by comparing their energy consumption against non-private counterparts.

Result: Significant carbon footprint variations were observed, with HTTPS showing a 2× increase and encrypted ML training reaching an alarming 100,000× increase, highlighting the need for tailored assessments of privacy-technology combinations.

Conclusion: The study bridges the gap in PET sustainability analysis and provides decision-making data for privacy-carbon trade-offs. It also identifies urgent research paths to develop PETs that simultaneously achieve strong privacy and low environmental impact.

Abstract: Privacy-enhancing technologies (PETs) have attracted significant attention in
response to privacy regulations, driving the development of applications that
prioritize user data protection. At the same time, the information and
communication technology (ICT) sector faces growing pressure to reduce its
environmental footprint, particularly its carbon emissions. While numerous
studies have assessed the energy footprint of various ICT applications, the
environmental footprint of cryptographic PETs remains largely unexplored.
  Our work addresses this gap by proposing a standardized methodology for
evaluating the carbon footprint of PETs. To demonstrate this methodology, we
focus on PETs supporting client-server applications as they are the simplest to
deploy. In particular, we measure the energy consumption and carbon footprint
increase induced by five cryptographic PETs (compared to their non-private
equivalent): HTTPS web browsing, encrypted machine learning (ML) inference,
encrypted ML training, encrypted databases, and encrypted emails. Our findings
reveal significant variability in carbon footprint increases, ranging from a
twofold increase in HTTPS web browsing to a 100,000-fold increase in encrypted
ML.
  Our study provides essential data to help decision-makers assess
privacy-carbon trade-offs in such applications. Finally, we outline key
research directions for developing PETs that balance strong privacy protection
with environmental sustainability.

</details>


### [14] [4-Swap: Achieving Grief-Free and Bribery-Safe Atomic Swaps Using Four Transactions](https://arxiv.org/abs/2508.04641)
*Kirti Singh,Vinay J. Ribeiro,Susmita Mandal*

Main category: cs.CR

TL;DR: 4-Swap is a cross-chain asset exchange protocol that completes grief-free and bribery-safe swaps in four transactions by consolidating components into single-chain operations, improving upon prior methods (6-5 transactions) with faster execution and Bitcoin compatibility.


<details>
  <summary>Details</summary>
Motivation: Traditional cross-chain exchanges face asset security risks with third parties, while decentralized atomic swaps are vulnerable to griefing attacks. Existing grief-free alternatives like Hedged Atomic Swaps and GF Swap require 6 and 5 transactions respectively, lacking efficiency.

Method: 4-Swap reduces transaction count to four by merging griefing premium and principal assets into one transaction per chain, leveraging a single on-chain deposit model and timelock mechanisms. Game-theoretic analysis confirms protocol security against rational attackers.

Result: Achieved first cross-chain atomic swap protocol with guaranteed four transactions, eliminating griefing and bribe incentives. Demonstrated compatibility with Bitcoin’s current opcode set, enabling faster execution through reduced on-chain activity.

Conclusion: 4-Swap establishes a mathematically optimal cross-chain exchange solution, combining robust security with maximum efficiency. Its minimal transaction count and resistance to incentive-driven attacks represent significant advancements in blockchain interoperability.

Abstract: Cross-chain asset exchange is crucial for blockchain interoperability.
Existing solutions rely on trusted third parties and risk asset loss, or use
decentralized alternatives like atomic swaps, which suffer from grief attacks.
Griefing occurs when a party prematurely exits, locking the counterparty's
assets until a timelock expires. Hedged Atomic Swaps mitigate griefing by
introducing a penalty premium; however, they increase the number of
transactions from four (as in Tier Nolan's swap) to six, which in turn
introduces new griefing risks. Grief-Free (GF) Swap reduces this to five
transactions by consolidating assets and premiums on a single chain. However,
no existing protocol achieves grief-free asset exchange in just four
transactions.
  This paper presents 4-Swap, the first cross-chain atomic swap protocol that
is both grief-free and bribery-safe, while completing asset exchange in just
four transactions. By combining the griefing premium and principal into a
single transaction per chain, 4-Swap reduces on-chain transactions, leading to
faster execution compared to previous grief-free solutions. It is fully
compatible with Bitcoin and operates without the need for any new opcodes. A
game-theoretic analysis shows that rational participants have no incentive to
deviate from the protocol, ensuring robust compliance and security.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [15] [Empathy Guidelines for Improving Practitioner Well-being & Software Engineering Practices](https://arxiv.org/abs/2508.03846)
*Hashini Gunatilake,John Grundy,Rashina Hoda,Ingo Mueller*

Main category: cs.SE

TL;DR: This paper presents 17 empathy guidelines and a visual prioritization framework for software engineering, offering practical strategies to integrate empathy into daily work and overcome real-world implementation challenges.


<details>
  <summary>Details</summary>
Motivation: Empathy enhances teamwork, communication, and decision-making in software engineering but is rarely systematically applied.

Method: Derived from prior practitioner strategies, the authors created actionable guidelines through analysis of real-world applications, challenges, and adoption barriers shared by software engineers.

Result: A visual framework categorizing 17 empathy guidelines by importance, implementation ease, and willingness to adopt, providing flexible integration paths for teams and organizations.

Conclusion: The guidelines and prioritization framework enable software engineering teams to translate empathy principles into sustained practices for improved collaboration and outcomes.

Abstract: Empathy is a powerful yet often overlooked element in software engineering
(SE), supporting better teamwork, smoother communication, and effective
decision-making. In our previous study, we identified a range of practitioner
strategies for fostering empathy in SE contexts. Building on these insights,
this paper introduces 17 actionable empathy guidelines designed to support
practitioners, teams, and organisations. We also explore how these guidelines
can be implemented in practice by examining real-world applications,
challenges, and strategies to overcome them shared by software practitioners.
To support adoption, we present a visual prioritisation framework that
categorises the guidelines based on perceived importance, ease of
implementation, and willingness to adopt. The findings offer practical and
flexible suggestions for integrating empathy into everyday SE work, helping
teams move from principles to sustainable action.

</details>


### [16] [Evaluating Software Supply Chain Security in Research Software](https://arxiv.org/abs/2508.03856)
*Richard Hegewald,Rebecca Beyer*

Main category: cs.SE

TL;DR: This study highlights the weak security practices in 3,248 research software repositories (avg. score 3.5/10) using OpenSSF Scorecard and proposes low-effort security improvements. Key vulnerable practices include lack of signed releases and branch protection.


<details>
  <summary>Details</summary>
Motivation: Research software security is critical for scientific integrity but overlooked, with vulnerabilities from open-source dependencies and distributed development workflows potentially compromising results.

Method: Analysis of 3,248 peer-reviewed research repositories via OpenSSF Scorecard metrics to evaluate security practices implementation rates and threat exposure.

Result: Average security score of 3.5/10 across 11 security categories, with only 3% maintaining signed releases and similar low adoption of branch protection and dependency management.

Conclusion: Weak security posture exposes scientific research to supply chain attacks; implementable recommendations include enabling branch protection, signing releases, and monitoring dependencies through existing tools.

Abstract: The security of research software is essential for ensuring the integrity and
reproducibility of scientific results. However, research software security is
still largely unexplored. Due to its dependence on open source components and
distributed development practices, research software is particularly vulnerable
to supply chain attacks. This study analyses 3,248 high-quality, largely
peer-reviewed research software repositories using the OpenSSF Scorecard. We
find a generally weak security posture with an average score of 3.5/10.
Important practices, such as signed releases and branch protection, are rarely
implemented. Finally, we present actionable, low-effort recommendations that
can help research teams improve software security and mitigate potential
threats to scientific integrity.

</details>


### [17] [From App Features to Explanation Needs: Analyzing Correlations and Predictive Potential](https://arxiv.org/abs/2508.03881)
*Martin Obaidi,Kushtrim Qengaj,Jakob Droste,Hannah Deters,Marc Herrmann,Jil Klünder,Elisa Schmid,Kurt Schneider*

Main category: cs.SE

TL;DR: The study found that predicting explanation needs from app metadata is largely unreliable, emphasizing the necessity of direct user feedback for designing explainable software systems.


<details>
  <summary>Details</summary>
Motivation: Modern software systems require users to understand interactions and system behaviors, prompting the need to identify explanation requirements early in development through app properties.

Method: Analysis of a dataset with 4,495 app reviews and metadata (e.g., app version, ratings, in-app purchases) using correlation analysis and linear regression models, validated on a manually labeled subset of 495 reviews.

Result: Most app properties showed weak correlations with explanation needs; models had limited predictive power, and manual validation confirmed these limitations. Categories like Security & Privacy had higher predictive potential than Interaction and User Interface.

Conclusion: Explanation needs are context-dependent and cannot be reliably inferred from metadata alone; developers should combine metadata analysis with direct user feedback to create user-centered explainable systems.

Abstract: In today's digitized world, software systems must support users in
understanding both how to interact with a system and why certain behaviors
occur. This study investigates whether explanation needs, classified from user
reviews, can be predicted based on app properties, enabling early consideration
during development and large-scale requirements mining. We analyzed a gold
standard dataset of 4,495 app reviews enriched with metadata (e.g., app
version, ratings, age restriction, in-app purchases). Correlation analyses
identified mostly weak associations between app properties and explanation
needs, with moderate correlations only for specific features such as app
version, number of reviews, and star ratings. Linear regression models showed
limited predictive power, with no reliable forecasts across configurations.
Validation on a manually labeled dataset of 495 reviews confirmed these
findings. Categories such as Security & Privacy and System Behavior showed
slightly higher predictive potential, while Interaction and User Interface
remained most difficult to predict. Overall, our results highlight that
explanation needs are highly context-dependent and cannot be precisely inferred
from app metadata alone. Developers and requirements engineers should therefore
supplement metadata analysis with direct user feedback to effectively design
explainable and user-centered software systems.

</details>


### [18] [A Human Centric Requirements Engineering Framework for Assessing Github Copilot Output](https://arxiv.org/abs/2508.03922)
*Soroush Heydari*

Main category: cs.SE

TL;DR: The paper introduces a human-centered evaluation framework for GitHub Copilot, analyzing its adaptability to user expertise and effectiveness in collaborative programming through chat-based interactions.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation frameworks for AI programming assistants focus on technical metrics (code correctness, efficiency) while neglecting human factors critical to successful tool integration, such as adaptability to expertise levels and collaborative effectiveness.

Method: The study analyzed GitHub Copilot's chat interface interactions, measured its ability to adapt explanations and code generation to varying user expertise levels, and assessed its collaborative programming capabilities using a human-centered requirements framework with defined metrics.

Result: Test results were discussed alongside their implications for evaluating human requirements in automated programming, demonstrating where Copilot succeeds/fails in adapting to users and supporting collaboration.

Conclusion: The work establishes a foundational framework for human-centered evaluation of AI assistants, highlighting the need to integrate user expertise and collaboration factors in future development and analysis of programming automation tools.

Abstract: The rapid adoption of Artificial Intelligence(AI) programming assistants such
as GitHub Copilot introduces new challenges in how these software tools address
human needs. Many existing evaluation frameworks address technical aspects such
as code correctness and efficiency, but often overlook crucial human factors
that affect the successful integration of AI assistants in software development
workflows. In this study, I analyzed GitHub Copilot's interaction with users
through its chat interface, measured Copilot's ability to adapt explanations
and code generation to user expertise levels, and assessed its effectiveness in
facilitating collaborative programming experiences. I established a
human-centered requirements framework with clear metrics to evaluate these
qualities in GitHub Copilot chat. I discussed the test results and their
implications for future analysis of human requirements in automated
programming.

</details>


### [19] [Analyzing Prominent LLMs: An Empirical Study of Performance and Complexity in Solving LeetCode Problems](https://arxiv.org/abs/2508.03931)
*Everton Guimaraes,Nathalia Nascimento,Chandan Shivalingaiah,Asish Nelapati*

Main category: cs.SE

TL;DR: This paper systematically benchmarks four large language models (ChatGPT, Copilot, Gemini, DeepSeek) on 150 LeetCode problems in Java and Python, revealing substantial performance disparities across different levels of algorithmic complexity and offering guidance for real-world software engineering applications.


<details>
  <summary>Details</summary>
Motivation: LLMs are revolutionizing software engineering through automated coding, testing, and debugging. A comparative analysis is necessary to help developers optimize model selection for specific tasks and understand the tradeoffs between performance and complexity.

Method: The study evaluated model solutions on 150 LeetCode problems (easy/medium/hard) using two languages. Solutions were assessed via three metrics: execution speed, memory efficiency, and algorithmic complexity analysis.

Result: 1) ChatGPT showed consistent efficiency. 2) Copilot and DeepSeek exhibited increased variability with domain complexity. 3) Gemini required exponentially more trials for effective solutions on hard problems. 4) Language choice (Java/Python) affected performance profiles across models.

Conclusion: This benchmark analysis provides empirical insights into LLM strengths/limitations for developers, highlighting how model architecture, training data, and prompt engineering influence real-world coding performance with domain-specific implications.

Abstract: Large Language Models (LLMs) like ChatGPT, Copilot, Gemini, and DeepSeek are
transforming software engineering by automating key tasks, including code
generation, testing, and debugging. As these models become integral to
development workflows, a systematic comparison of their performance is
essential for optimizing their use in real world applications. This study
benchmarks these four prominent LLMs on one hundred and fifty LeetCode problems
across easy, medium, and hard difficulties, generating solutions in Java and
Python. We evaluate each model based on execution time, memory usage, and
algorithmic complexity, revealing significant performance differences. ChatGPT
demonstrates consistent efficiency in execution time and memory usage, while
Copilot and DeepSeek show variability as task complexity increases. Gemini,
although effective on simpler tasks, requires more attempts as problem
difficulty rises. Our findings provide actionable insights into each model's
strengths and limitations, offering guidance for developers selecting LLMs for
specific coding tasks and providing insights on the performance and complexity
of GPT-like generated solutions.

</details>


### [20] [Model Compression vs. Adversarial Robustness: An Empirical Study on Language Models for Code](https://arxiv.org/abs/2508.03949)
*Md. Abdul Awal,Mrigank Rochan,Chanchal K. Roy*

Main category: cs.SE

TL;DR: The paper evaluates the adversarial robustness of compressed code language models, revealing a trade-off between computational efficiency and security, and highlights the need for balanced compression strategies.


<details>
  <summary>Details</summary>
Motivation: Adopting compressed code language models is hindered by unknown impacts on adversarial robustness, crucial for safe deployment in security-sensitive applications. This study addresses this gap to ensure reliable model compression.

Method: Assessed three compressed code language models across three software analytics tasks using six evaluation metrics and four classical adversarial attacks to quantify robustness degradation.

Result: Compressed models retain similar performance under normal conditions but demonstrate significantly reduced robustness under adversarial attacks, confirming the trade-off between size reduction and security.

Conclusion: Model compression for code introduces adversarial vulnerabilities, emphasizing the necessity of developing compression techniques that preserve both efficiency and robustness for practical use.

Abstract: Transformer-based language models for code have shown remarkable performance
in various software analytics tasks, but their adoption is hindered by high
computational costs, slow inference speeds, and substantial environmental
impact. Model compression techniques such as pruning, quantization, and
knowledge distillation have gained traction in addressing these challenges.
However, the impact of these strategies on the robustness of compressed
language models for code in adversarial scenarios remains poorly understood.
Understanding how these compressed models behave under adversarial attacks is
essential for their safe and effective deployment in real-world applications.
To bridge this knowledge gap, we conduct a comprehensive evaluation of how
common compression strategies affect the adversarial robustness of compressed
models. We assess the robustness of compressed versions of three widely used
language models for code across three software analytics tasks, using six
evaluation metrics and four commonly used classical adversarial attacks. Our
findings indicate that compressed models generally maintain comparable
performance to their uncompressed counterparts. However, when subjected to
adversarial attacks, compressed models exhibit significantly reduced
robustness. These results reveal a trade-off between model size reduction and
adversarial robustness, underscoring the need for careful consideration when
deploying compressed models in security-critical software applications. Our
study highlights the need for further research into compression strategies that
strike a balance between computational efficiency and adversarial robustness,
which is essential for deploying reliable language models for code in
real-world software applications.

</details>


### [21] [Experimental Analysis of Productive Interaction Strategy with ChatGPT: User Study on Function and Project-level Code Generation Tasks](https://arxiv.org/abs/2508.04125)
*Sangwon Hyun,Hyunjun Kim,Jinhyuk Jang,Hyojin Choi,M. Ali Babar*

Main category: cs.SE

TL;DR: This paper investigates Human-LLM Interaction (HLI) features in code generation beyond function-level, identifying three productivity-impact features, five productivity-enhancing guidelines, and a 29-error taxonomy from project-level experiments with 36 participants.


<details>
  <summary>Details</summary>
Motivation: Current studies on LLM prompting techniques in Software Engineering (SE) focus narrowly on function-level practices and known patterns, neglecting real-world workflow complexities involving multi-class dependencies and varying HLI features that affect code generation productivity.

Method: The researchers designed a user study with 36 diverse participants solving two project-level benchmark tasks using GPT assistants. They analyzed code generation productivity through HLI feature evaluation, along with user experience data from screen recordings and GPT chat logs.

Result: (1) 3/15 HLI features significantly impact code generation productivity; (2) 5 actionable guidelines for improving HLI processes; (3) Taxonomy of 29 runtime/logic errors during HLI code generation with mitigation strategies.

Conclusion: The study expands SE research on LLM prompting by analyzing project-level complexities and HLI dynamics, providing practical guidelines and an error taxonomy to enhance productivity in code generation workflows involving class dependencies and other real-world challenges.

Abstract: The application of Large Language Models (LLMs) is growing in the productive
completion of Software Engineering tasks. Yet, studies investigating the
productive prompting techniques often employed a limited problem space,
primarily focusing on well-known prompting patterns and mainly targeting
function-level SE practices. We identify significant gaps in real-world
workflows that involve complexities beyond class-level (e.g., multi-class
dependencies) and different features that can impact Human-LLM Interactions
(HLIs) processes in code generation. To address these issues, we designed an
experiment that comprehensively analyzed the HLI features regarding the code
generation productivity. Our study presents two project-level benchmark tasks,
extending beyond function-level evaluations. We conducted a user study with 36
participants from diverse backgrounds, asking them to solve the assigned tasks
by interacting with the GPT assistant using specific prompting patterns. We
also examined the participants' experience and their behavioral features during
interactions by analyzing screen recordings and GPT chat logs. Our statistical
and empirical investigation revealed (1) that three out of 15 HLI features
significantly impacted the productivity in code generation; (2) five primary
guidelines for enhancing productivity for HLI processes; and (3) a taxonomy of
29 runtime and logic errors that can occur during HLI processes, along with
suggested mitigation plans.

</details>


### [22] [EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation](https://arxiv.org/abs/2508.04295)
*Chaofan Wang,Tingrui Yu,Jie Wang,Dong Chen,Wenrui Zhang,Yuling Shi,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: EvoC2Rust is a new automated framework that addresses the challenges of converting entire C projects to Rust by combining a skeleton-guided translation strategy with feature-mapping LLMs and static analysis, achieving better syntax, semantic accuracy, and code safety than previous methods.


<details>
  <summary>Details</summary>
Motivation: Legacy C codebases need translation to Rust for safety-critical systems, but existing rule-based and LLM-based approaches struggle with large-scale projects and safety/semantic equivalence issues.

Method: EvoC2Rust uses a three-stage evolutionary process: decomposing C projects into modules, generating a compilable Rust skeleton via a feature-mapping-enhanced LLM, incrementally translating functions, and repairing errors through LLM integration and static analysis.

Result: On average, EvoC2Rust improved syntax and semantic accuracy by 17.24% and 14.32% over LLM-based methods and achieved 96.79% higher code safety rates than rule-based tools; module-level test and compilation success rates exceeded 89% on industrial projects.

Conclusion: EvoC2Rust effectively bridges the limitations of rule-based and LLM-based translation methods through evolutionary augmentation, demonstrating superior performance for large-scale, safety-critical C-to-Rust translation.

Abstract: Rust's compile-time safety guarantees make it ideal for safety-critical
systems, creating demand for translating legacy C codebases to Rust. While
various approaches have emerged for this task, they face inherent trade-offs:
rule-based solutions face challenges in meeting code safety and idiomaticity
requirements, while LLM-based solutions often fail to generate semantically
equivalent Rust code, due to the heavy dependencies of modules across the
entire codebase. Recent studies have revealed that both solutions are limited
to small-scale programs. In this paper, we propose EvoC2Rust, an automated
framework for converting entire C projects to equivalent Rust ones. EvoC2Rust
employs a skeleton-guided translation strategy for project-level translation.
The pipeline consists of three evolutionary stages: 1) it first decomposes the
C project into functional modules, employs a feature-mapping-enhanced LLM to
transform definitions and macros and generates type-checked function stubs,
which form a compilable Rust skeleton; 2) it then incrementally translates the
function, replacing the corresponding stub placeholder; 3) finally, it repairs
compilation errors by integrating LLM and static analysis. Through evolutionary
augmentation, EvoC2Rust combines the advantages of both rule-based and
LLM-based solutions. Our evaluation on open-source benchmarks and six
industrial projects demonstrates EvoC2Rust's superior performance in
project-level C-to-Rust translation. On average, it achieves 17.24% and 14.32%
improvements in syntax and semantic accuracy over the LLM-based approaches,
along with a 96.79% higher code safety rate than the rule-based tools. At the
module level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates
on industrial projects, even for complex codebases and long functions.

</details>


### [23] [Vanilla-Converter: A Tool for Converting Camunda 7 BPMN Models into Camunda 8 Models](https://arxiv.org/abs/2508.04352)
*Dragana Sunaric,Charlotte Verbruggen,Dominik Bork*

Main category: cs.SE

TL;DR: Vanilla-Converter is a command-line tool for migrating BPMN models from Camunda 7 to 8, automating transformation and providing detailed logs for remaining manual tasks.


<details>
  <summary>Details</summary>
Motivation: Organizations face complex manual migration processes due to fundamental differences between Camunda 7 and 8 platforms.

Method: The tool automates BPMN model transformation, supports various elements, and generates logs to track automatic changes and manual conversion needs.

Result: Three case studies with real Camunda 7 models were successfully converted to valid, executable Camunda 8 models, confirming the tool's effectiveness.

Conclusion: Vanilla-Converter effectively addresses Camunda 7-to-8 migration challenges through automation and detailed logging, reducing manual effort for complex transitions.

Abstract: As organizations prepare for the end-of-life of Camunda 7, manual migration
remains complex due to fundamental differences between the two platforms. We
present Vanilla-Converter, a command-line tool that facilitates the migration
of BPMN models from Camunda 7 to Camunda 8. Vanilla-Converter automates the
transformation process, supports a wide range of BPMN elements, and produces a
transformed model and a detailed transformation log indicating automatic
changes and remaining manual conversion tasks. The tool's effectiveness is
demonstrated through three case studies with real industrially used Camunda 7
models, confirming its ability to convert these models into valid and
executable Camunda 8 models.

</details>


### [24] [Breaking New Ground in Software Defect Prediction: Introducing Practical and Actionable Metrics with Superior Predictive Power for Enhanced Decision-Making](https://arxiv.org/abs/2508.04408)
*Carlos Andrés Ramírez Cataño,Makoto Itoh*

Main category: cs.SE

TL;DR: This paper proposes a human error-based framework for method-level software defect prediction using developers' coding habits, demonstrating improved performance, importance distribution, and actionability over existing code and commit history metrics through analysis of 21 critical infrastructure open-source projects.


<details>
  <summary>Details</summary>
Motivation: Existing research on software defect prediction focuses heavily on algorithmic code metrics, while human factors (linked to defect root causes) remain underexplored. This limits actionable insights for practitioners addressing preventable human errors in software development.

Method: We developed a novel human factors framework:1) Defined defect-predictive coding habit metrics 2) Conducted empirical comparison against state-of-the-art code/commit-based metrics 3) Perfomed systematic prediction importance analysis. Evaluated using 21 large-scale critical infrastructure open-source projects.

Result: 1) Human habit framework produced better average prediction performance (30% higher than code/history baselines) 2) All novel metrics outperformed code/history metrics in importance rankings 3) Enhanced model explainability and practical recommendations showed: a) Specific coding patterns with defect risk b) Human error prevention strategies for different scenarios c) Real-time correction possibilities through habit monitoring

Conclusion: This work establishes that human factors-based metrics systematically enhance method-level defect prediction. Practitioners can now detect, understand, and mitigate defects at source by addressing developers' coding habits, marking a shift from purely algorithmic approaches to human-centric defect prevention strategies in software engineering.

Abstract: Software defect prediction using code metrics has been extensively researched
over the past five decades. However, prediction harnessing non-software metrics
is under-researched. Considering that the root cause of software defects is
often attributed to human error, human factors theory might offer key
forecasting metrics for actionable insights. This paper explores automated
software defect prediction at the method level based on the developers' coding
habits. First, we propose a framework for deciding the metrics to conduct
predictions. Next, we compare the performance of our metrics to that of the
code and commit history metrics shown by research to achieve the highest
performance to date. Finally, we analyze the prediction importance of each
metric. As a result of our analyses of twenty-one critical infrastructure
large-scale open-source software projects, we have presented: (1) a human
error-based framework with metrics useful for defect prediction at method
level; (2) models using our proposed metrics achieve better average prediction
performance than the state-of-the-art code metrics and history measures; (3)
the prediction importance of all metrics distributes differently with each of
the novel metrics having better average importance than code and history
metrics; (4) the novel metrics dramatically enhance the explainability,
practicality, and actionability of software defect prediction models,
significantly advancing the field. We present a systematic approach to
forecasting defect-prone software methods via a human error framework. This
work empowers practitioners to act on predictions, empirically demonstrating
how developer coding habits contribute to defects in software systems.

</details>


### [25] [Large Language Models Versus Static Code Analysis Tools: A Systematic Benchmark for Vulnerability Detection](https://arxiv.org/abs/2508.04448)
*Damian Gnieciak,Tomasz Szandala*

Main category: cs.SE

TL;DR: This paper evaluates six code security tools (SonarQube, CodeQL, Snyk Code, and three LLMs) on 10 C# projects with 63 vulnerabilities. LLMs outperform static analyzers in detection accuracy but face challenges with false positives and precise localization. A hybrid toolchain combining both approaches is recommended for safety-critical applications.


<details>
  <summary>Details</summary>
Motivation: Modern software security needs robust tools. Traditional rule-based static analyzers may lack contextual understanding, while LLMs show potential for broader code reasoning. This study addresses the need to quantify their complementary strengths and weaknesses for practical security workflows.

Method: 10 real-world C# projects encoding 63 vulnerabilities (SQL injection, hard-coded secrets, outdated dependencies) were analyzed using SonarQube, CodeQL, Snyk Code (static analysis) and GPT-4.1, Mistral Large, DeepSeek V3 (LLMs). Metrics included F-1 scores (0.750-0.797 vs 0.260-0.546), analysis latency, false-positive rates, and developer effort for validation.

Result: Language-based scanners achieved higher F-1 scores (0.750-0.797) than static analyzers (0.260-0.546), driven by superior recall but lower precision. DeepSeek V3 showed 37% false-positives. All LLMs struggled with line/column-level issue localization due to tokenization artifacts. The study demonstrates LLMs' competition with static tools but highlights practical deployment barriers.

Conclusion: LLMs match static analyzers in vulnerability detection while offering contextual insights, making them suitable for early-stage triage. Rule-based tools maintain advantages for high-assurance verification. The proposed hybrid pipeline addresses tool limitations through complementary strengths. The open benchmark and result harness enable future reproducible research in next-generation code security. (Key limitation: LLM mislocalization due to tokenization artifacts.)

Abstract: Modern software relies on a multitude of automated testing and quality
assurance tools to prevent errors, bugs and potential vulnerabilities. This
study sets out to provide a head-to-head, quantitative and qualitative
evaluation of six automated approaches: three industry-standard rule-based
static code-analysis tools (SonarQube, CodeQL and Snyk Code) and three
state-of-the-art large language models hosted on the GitHub Models platform
(GPT-4.1, Mistral Large and DeepSeek V3). Using a curated suite of ten
real-world C# projects that embed 63 vulnerabilities across common categories
such as SQL injection, hard-coded secrets and outdated dependencies, we measure
classical detection accuracy (precision, recall, F-score), analysis latency,
and the developer effort required to vet true positives. The language-based
scanners achieve higher mean F-1 scores,0.797, 0.753 and 0.750, than their
static counterparts, which score 0.260, 0.386 and 0.546, respectively. LLMs'
advantage originates from superior recall, confirming an ability to reason
across broader code contexts. However, this benefit comes with substantial
trade-offs: DeepSeek V3 exhibits the highest false-positive ratio, all language
models mislocate issues at line-or-column granularity due to tokenisation
artefacts. Overall, language models successfully rival traditional static
analysers in finding real vulnerabilities. Still, their noisier output and
imprecise localisation limit their standalone use in safety-critical audits. We
therefore recommend a hybrid pipeline: employ language models early in
development for broad, context-aware triage, while reserving deterministic
rule-based scanners for high-assurance verification. The open benchmark and
JSON-based result harness released with this paper lay a foundation for
reproducible, practitioner-centric research into next-generation automated code
security.

</details>


### [26] [Manifestations of Empathy in Software Engineering: How, Why, and When It Matters](https://arxiv.org/abs/2508.04479)
*Hashini Gunatilake,John Grundy,Rashina Hoda,Ingo Mueller*

Main category: cs.SE

TL;DR: This study investigates empathy in software engineering through 22 interviews and a survey of 116 practitioners, revealing how empathy is expressed, what motivates it, its perceived usefulness in SE activities, and influencing factors, with practical implications for researchers and practitioners.


<details>
  <summary>Details</summary>
Motivation: Prior research emphasizes the importance of empathy in software engineering but lacks understanding of its practical manifestations, motivators, and influencing factors, necessitating this exploration to bridge theoretical and applied perspectives.

Method: The study employs a mixed-methods approach, combining 22 qualitative interviews with a quantitative survey involving 116 software practitioners, to gather insights on empathetic practices in SE contexts.

Result: Findings include insights into empathy expression in SE, key motivators (e.g., collaboration, communication), specific activities where empathy is valued or challenged, and organizational and project-level factors (e.g., tools, culture) that influence empathetic behavior.

Conclusion: The research provides actionable recommendations for integrating empathy into SE processes, highlighting its role in enhancing team dynamics and decision-making while addressing barriers like tool limitations and cultural norms.

Abstract: Empathy plays a crucial role in software engineering (SE), influencing
collaboration, communication, and decision-making. While prior research has
highlighted the importance of empathy in SE, there is limited understanding of
how empathy manifests in SE practice, what motivates SE practitioners to
demonstrate empathy, and the factors that influence empathy in SE work. Our
study explores these aspects through 22 interviews and a large scale survey
with 116 software practitioners. Our findings provide insights into the
expression of empathy in SE, the drivers behind empathetic practices, SE
activities where empathy is perceived as useful or not, and the other factors
that influence empathy. In addition, we offer practical implications for SE
practitioners and researchers, offering a deeper understanding of how to
effectively integrate empathy into SE processes.

</details>
