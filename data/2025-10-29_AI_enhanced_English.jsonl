{"id": "2510.23619", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23619", "abs": "https://arxiv.org/abs/2510.23619", "authors": ["Yuyang Miao", "Huijun Xing", "Danilo P. Mandic", "Tony G. Constantinides"], "title": "Short Ticketing Detection Framework Analysis Report", "comment": null, "summary": "This report presents a comprehensive analysis of an unsupervised multi-expert\nmachine learning framework for detecting short ticketing fraud in railway\nsystems. The study introduces an A/B/C/D station classification system that\nsuccessfully identifies suspicious patterns across 30 high-risk stations. The\nframework employs four complementary algorithms: Isolation Forest, Local\nOutlier Factor, One-Class SVM, and Mahalanobis Distance. Key findings include\nthe identification of five distinct short ticketing patterns and potential for\nshort ticketing recovery in transportation systems.", "AI": {"tldr": "A paper analyzing an unsupervised multi-expert ML framework for detecting railway short ticketing fraud using four algorithms and a new station classification system.", "motivation": "Detecting short ticketing fraud is a significant challenge in railway systems, affecting revenue and operational efficiency. This paper addresses the need for effective and unsupervised methods to identify such fraud patterns, which can lead to revenue recovery and better security in the transportation sector.", "method": "The approach is based on an A/B/C/D station classification system to tag high-risk stations and combines four unsupervised algorithms\u2014Isolation Forest, LOF, One-Class SVM, and Mahalanobis Distance\u2014to create a multi-expert framework for anomaly detection.", "result": "The framework successfully identified five short ticketing fraud patterns in the dataset and showed recovery potential across 30 high-risk railway stations.", "conclusion": "The integration of multiple unsupervised machine learning techniques offers a robust solution for detecting short ticketing fraud, which could help modern transportation systems recover lost revenue and improve passenger behavior through targeted interventions."}}
{"id": "2510.23643", "categories": ["cs.CR", "cs.AI", "cs.LG", "I.2.6; D.4.6"], "pdf": "https://arxiv.org/pdf/2510.23643", "abs": "https://arxiv.org/abs/2510.23643", "authors": ["Zhixin Pan", "Ziyu Shu", "Linh Nguyen", "Amberbir Alemayoh"], "title": "SAND: A Self-supervised and Adaptive NAS-Driven Framework for Hardware Trojan Detection", "comment": null, "summary": "The globalized semiconductor supply chain has made Hardware Trojans (HT) a\nsignificant security threat to embedded systems, necessitating the design of\nefficient and adaptable detection mechanisms. Despite promising machine\nlearning-based HT detection techniques in the literature, they suffer from ad\nhoc feature selection and the lack of adaptivity, all of which hinder their\neffectiveness across diverse HT attacks. In this paper, we propose SAND, a\nselfsupervised and adaptive NAS-driven framework for efficient HT detection.\nSpecifically, this paper makes three key contributions. (1) We leverage\nself-supervised learning (SSL) to enable automated feature extraction,\neliminating the dependency on manually engineered features. (2) SAND integrates\nneural architecture search (NAS) to dynamically optimize the downstream\nclassifier, allowing for seamless adaptation to unseen benchmarks with minimal\nfine-tuning. (3) Experimental results show that SAND achieves a significant\nimprovement in detection accuracy (up to 18.3%) over state-of-the-art methods,\nexhibits high resilience against evasive Trojans, and demonstrates strong\ngeneralization.", "AI": {"tldr": "SAND is a self-supervised and adaptive NAS-driven framework for efficient Hardware Trojan detection, showing improved accuracy and adaptability over existing methods.", "motivation": "The globalized semiconductor supply chain has made Hardware Trojans a security threat, and current ML-based detection methods face issues with ad hoc feature selection and lack of adaptability.", "method": "SAND combines self-supervised learning for automated feature extraction and neural architecture search to dynamically optimize the classifier, allowing adaptation to new benchmarks with minimal tuning.", "result": "SAND outperforms state-of-the-art methods by up to 18.3% in detection accuracy, shows high resilience against evasive Trojans, and exhibits strong generalization.", "conclusion": "SAND provides an efficient, adaptive solution to HT detection by leveraging SSL and NAS, addressing current limitations in feature selection and adaptability."}}
{"id": "2510.23673", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23673", "abs": "https://arxiv.org/abs/2510.23673", "authors": ["Bin Wang", "Zexin Liu", "Hao Yu", "Ao Yang", "Yenan Huang", "Jing Guo", "Huangsheng Cheng", "Hui Li", "Huiyu Wu"], "title": "MCPGuard : Automatically Detecting Vulnerabilities in MCP Servers", "comment": null, "summary": "The Model Context Protocol (MCP) has emerged as a standardized interface\nenabling seamless integration between Large Language Models (LLMs) and external\ndata sources and tools. While MCP significantly reduces development complexity\nand enhances agent capabilities, its openness and extensibility introduce\ncritical security vulnerabilities that threaten system trustworthiness and user\ndata protection. This paper systematically analyzes the security landscape of\nMCP-based systems, identifying three principal threat categories: (1) agent\nhijacking attacks stemming from protocol design deficiencies; (2) traditional\nweb vulnerabilities in MCP servers; and (3) supply chain security. To address\nthese challenges, we comprehensively survey existing defense strategies,\nexamining both proactive server-side scanning approaches, ranging from layered\ndetection pipelines and agentic auditing frameworks to zero-trust registry\nsystems, and runtime interaction monitoring solutions that provide continuous\noversight and policy enforcement. Our analysis reveals that MCP security\nfundamentally represents a paradigm shift where the attack surface extends from\ntraditional code execution to semantic interpretation of natural language\nmetadata, necessitating novel defense mechanisms tailored to this unique threat\nmodel.", "AI": {"tldr": "TLDR", "motivation": "Motivation", "method": "Method", "result": "Result", "conclusion": "Conclusion"}}
{"id": "2510.23675", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23675", "abs": "https://arxiv.org/abs/2510.23675", "authors": ["Yuchong Xie", "Zesen Liu", "Mingyu Luo", "Zhixiang Zhang", "Kaikai Zhang", "Zongjie Li", "Ping Chen", "Shuai Wang", "Dongdong She"], "title": "QueryIPI: Query-agnostic Indirect Prompt Injection on Coding Agents", "comment": null, "summary": "Modern coding agents integrated into IDEs combine powerful tools and\nsystem-level actions, exposing a high-stakes attack surface. Existing Indirect\nPrompt Injection (IPI) studies focus mainly on query-specific behaviors,\nleading to unstable attacks with lower success rates. We identify a more\nsevere, query-agnostic threat that remains effective across diverse user\ninputs. This challenge can be overcome by exploiting a common vulnerability:\nleakage of the agent's internal prompt, which turns the attack into a\nconstrained white-box optimization problem. We present QueryIPI, the first\nquery-agnostic IPI method for coding agents. QueryIPI refines malicious tool\ndescriptions through an iterative, prompt-based process informed by the leaked\ninternal prompt. Experiments on five simulated agents show that QueryIPI\nachieves up to 87 percent success, outperforming baselines, and the generated\nmalicious descriptions also transfer to real-world systems, highlighting a\npractical security risk to modern LLM-based coding agents.", "AI": {"tldr": "QueryIPI: Query-Agnostic Indirect Prompt Injection for Improved Attack Success Rate in Coding Agents", "motivation": "Existing IPI methods for coding agents are query-specific, leading to low and unstable attack success rates. This paper addresses more severe, query-agnostic threats that are resilient to input variations.", "method": "Through exploiting the leakage of the agent's internal prompt, the paper develops QueryIPI, which iteratively refines malicious tool descriptions using an internal-prompt-guided approach to transform attacks into white-box optimization problems.", "result": "QueryIPI achieves up to 87% attack success rates across five simulated agents and demonstrates practical transferability of the malicious tool descriptions to real-world systems.", "conclusion": "The study reveals a substantial security vulnerability in modern coding agents related to internal prompt leakage and shows that adversarial tool descriptions crafted through this method affect real-world systems, pointing to the urgency for improved security measures."}}
{"id": "2510.23627", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23627", "abs": "https://arxiv.org/abs/2510.23627", "authors": ["Fred Zimmerman"], "title": "AI-Driven Development of a Publishing Imprint: Xynapse Traces", "comment": null, "summary": "Xynapse Traces is an experimental publishing imprint created via a fusion of\nhuman and algorithmic methods using a configuration-driven architecture and a\nmulti-model AI integration framework. The system achieved a remarkable 90%\nreduction in time-to-market (from a typical 6-12 months to just 2-4 weeks),\nwith 80% cost reduction compared to traditional imprint development, while\npublishing 52 books in its first year and maintaining exceptional quality\nmetrics, including 99% citation accuracy and 100% validation success after\ninitial corrections. Key technical innovations include a continuous ideation\npipeline with tournament-style evaluation, a novel codex design for\ntranscriptive meditation practice, comprehensive automation spanning from\nideation through production and distribution, and publisher personas that\ndefine and guide the imprint's mission. The system also integrates automated\nverification with human oversight, ensuring that gains in speed do not\ncompromise publishing standards. This effort has significant implications for\nthe future of book publishing, suggesting new paradigms for human-AI\ncollaboration that democratize access to sophisticated publishing capabilities\nand make previously unviable niche markets accessible.", "AI": {"tldr": "Xynapse Traces is an experimental publishing imprint that uses a fusion of human and algorithmic methods to significantly reduce time-to-market and costs, while maintaining high quality metrics. It published 52 books in its first year with 99% citation accuracy and 100% validation success after initial corrections.", "motivation": "The motivation behind Xynapse Traces is to transform the book publishing industry by leveraging human-AI collaboration to reduce time-to-market and costs, while maintaining high quality standards. This is intended to democratize access to sophisticated publishing capabilities and make previously unviable niche markets accessible.", "method": "The method involves the creation of Xynapse Traces via a fusion of human and algorithmic methods using a configuration-driven architecture and a multi-model AI integration framework. Key technical innovations include a continuous ideation pipeline with tournament-style evaluation, a novel codex design for transcriptive meditation practice, comprehensive automation spanning from ideation through production and distribution, and publisher personas that define and guide the imprint's mission. This is integrated with automated verification and human oversight to ensure standards are not compromised.", "result": "The system achieved a remarkable 90% reduction in time-to-market (from a typical 6-12 months to just 2-4 weeks), with 80% cost reduction compared to traditional imprint development, and published 52 books in its first year. The imprint maintained exceptional quality metrics, including 99% citation accuracy and 100% validation success after initial corrections.", "conclusion": "The creation of Xynapse Traces has significant implications for the future of book publishing, suggesting new paradigms for human-AI collaboration that democratize access to sophisticated publishing capabilities and make previously unviable niche markets accessible."}}
{"id": "2510.23847", "categories": ["cs.CR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23847", "abs": "https://arxiv.org/abs/2510.23847", "authors": ["Joel Poncha Lemayian", "Ghyslain Gagnon", "Kaiwen Zhang", "Pascal Giard"], "title": "EthVault: A Secure and Resource-Conscious FPGA-Based Ethereum Cold Wallet", "comment": "Under review for publication", "summary": "Cryptocurrency blockchain networks safeguard digital assets using\ncryptographic keys, with wallets playing a critical role in generating,\nstoring, and managing these keys. Wallets, typically categorized as hot and\ncold, offer varying degrees of security and convenience. However, they are\ngenerally software-based applications running on microcontrollers.\nConsequently, they are vulnerable to malware and side-channel attacks, allowing\nperpetrators to extract private keys by targeting critical algorithms, such as\nECC, which processes private keys to generate public keys and authorize\ntransactions. To address these issues, this work presents EthVault, the first\nhardware architecture for an Ethereum hierarchically deterministic cold wallet,\nfeaturing hardware implementations of key algorithms for secure key generation.\nAlso, an ECC architecture resilient to side-channel and timing attacks is\nproposed. Moreover, an architecture of the child key derivation function, a\nfundamental component of cryptocurrency wallets, is proposed. The design\nminimizes resource usage, meeting market demand for small, portable\ncryptocurrency wallets. FPGA implementation results validate the feasibility of\nthe proposed approach. The ECC architecture exhibits uniform execution behavior\nacross varying inputs, while the complete design utilizes only 27%, 7%, and 6%\nof LUTs, registers, and RAM blocks, respectively, on a Xilinx Zynq UltraScale+\nFPGA.", "AI": {"tldr": "This paper introduces EthVault, a hardware-based Ethereum cold wallet architecture to enhance security against software-based threats.", "motivation": "Software-based cryptocurrency wallets, whether hot or cold, are vulnerable to malware and side-channel attacks, which can compromise private keys used for generating public keys and authorizing transactions.", "method": "The paper proposes EthVault, a hardware architecture for Ethereum hierarchically deterministic cold wallets, with hardware implementations of key algorithms like ECC and child key derivation. This design is optimized for low resource usage on an FPGA.", "result": "FPGA implementation results showed that the ECC architecture displays uniform execution behavior across varying inputs. The complete design used only 27% of LUTs, 7% of registers, and 6% of RAM blocks on a Xilinx Zynq UltraScale+ FPGA, confirming its feasibility for compact, secure cold wallets.", "conclusion": "EthVault offers a secure, hardware-based solution to the vulnerabilities of software-only cryptocurrency wallets, minimizing resource consumption while meeting the demand for small, portable devices."}}
{"id": "2510.23642", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.23642", "abs": "https://arxiv.org/abs/2510.23642", "authors": ["Yuansheng Ni", "Songcheng Cai", "Xiangchao Chen", "Jiarong Liang", "Zhiheng Lyu", "Jiaqi Deng", "Kai Zou", "Ping Nie", "Fei Yuan", "Xiang Yue", "Wenhu Chen"], "title": "VisCoder2: Building Multi-Language Visualization Coding Agents", "comment": null, "summary": "Large language models (LLMs) have recently enabled coding agents capable of\ngenerating, executing, and revising visualization code. However, existing\nmodels often fail in practical workflows due to limited language coverage,\nunreliable execution, and lack of iterative correction mechanisms. Progress has\nbeen constrained by narrow datasets and benchmarks that emphasize single-round\ngeneration and single-language tasks. To address these challenges, we introduce\nthree complementary resources for advancing visualization coding agents.\nVisCode-Multi-679K is a large-scale, supervised dataset containing 679K\nvalidated and executable visualization samples with multi-turn correction\ndialogues across 12 programming languages. VisPlotBench is a benchmark for\nsystematic evaluation, featuring executable tasks, rendered outputs, and\nprotocols for both initial generation and multi-round self-debug. Finally, we\npresent VisCoder2, a family of multi-language visualization models trained on\nVisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms\nstrong open-source baselines and approaches the performance of proprietary\nmodels like GPT-4.1, with further gains from iterative self-debug, reaching\n82.4% overall execution pass rate at the 32B scale, particularly in symbolic or\ncompiler-dependent languages.", "AI": {"tldr": "This paper presents three resources\u2014VisCode-Multi-679K, VisPlotBench, and VisCoder2\u2014to improve visualization coding agents with multi-language support and iterative correction, outperforming open-source models and approaching GPT-4", "motivation": "Existing visualization coding agents have limitations due to narrow datasets and benchmarks, leading to issues like limited language coverage, unreliable execution, and lack of iterative correction mechanisms.", "method": "The authors introduce three resources: (1) VisCode-Multi-679K, a large-scale multi-language dataset with validated code samples and correction dialogues, (2) VisPlotBench, a comprehensive benchmark for evaluating execution and correction, and (3) VisCoder2, a series of multi-language LLMs trained on the dataset.", "result": "VisCoder2 outperforms open-source models and approaches the performance of GPT-4, achieving up to 82.4% execution pass rate in large models (32B scale), especially in symbolic/compiler-dependent languages.", "conclusion": "The proposed resources support multi-language and iterative correction, showing VisCoder2 significantly improves visualization coding agents\u2019 reliability and capability relative to prior methods."}}
{"id": "2510.23891", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23891", "abs": "https://arxiv.org/abs/2510.23891", "authors": ["Jiaqi Xue", "Yifei Zhao", "Mansour Al Ghanim", "Shangqian Gao", "Ruimin Sun", "Qian Lou", "Mengxin Zheng"], "title": "PRO: Enabling Precise and Robust Text Watermark for Open-Source LLMs", "comment": null, "summary": "Text watermarking for large language models (LLMs) enables model owners to\nverify text origin and protect intellectual property. While watermarking\nmethods for closed-source LLMs are relatively mature, extending them to\nopen-source models remains challenging, as developers cannot control the\ndecoding process. Consequently, owners of open-source LLMs lack practical means\nto verify whether text was generated by their models. A core difficulty lies in\nembedding watermarks directly into model weights without hurting detectability.\nA promising idea is to distill watermarks from a closed-source model into an\nopen one, but this suffers from (i) poor detectability due to mismatch between\nlearned and predefined patterns, and (ii) fragility to downstream modifications\nsuch as fine-tuning or model merging. To overcome these limitations, we propose\nPRO, a Precise and Robust text watermarking method for open-source LLMs. PRO\njointly trains a watermark policy model with the LLM, producing patterns that\nare easier for the model to learn and more consistent with detection criteria.\nA regularization term further simulates downstream perturbations and penalizes\ndegradation in watermark detectability, ensuring robustness under model edits.\nExperiments on open-source LLMs (e.g., LLaMA-3.2, LLaMA-3, Phi-2) show that PRO\nsubstantially improves both watermark detectability and resilience to model\nmodifications.", "AI": {"tldr": "PRO is a new text watermarking method for open-source LLMs that improves detectability and resilience through joint training and regularization.", "motivation": "Open-source LLMs lack practical methods to verify text origin, and existing watermarking methods face challenges with detectability and fragility to modifications.", "method": "PRO jointly trains a watermark policy with the LLM and uses a regularization term to simulate downstream perturbations, enhancing both learning and robustness.", "result": "PRO significantly improves watermark detectability and resilience to model modifications in open-source LLMs like LLaMA-3.2, LLaMA-3, and Phi-2.", "conclusion": "PRO provides an effective solution for watermarking open-source LLMs with enhanced detectability and robustness to modifications."}}
{"id": "2510.23664", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23664", "abs": "https://arxiv.org/abs/2510.23664", "authors": ["Eranga Bandara", "Ross Gore", "Xueping Liang", "Sachini Rajapakse", "Isurunima Kularathne", "Pramoda Karunarathna", "Peter Foytik", "Sachin Shetty", "Ravi Mukkamala", "Abdul Rahman", "Amin Hass", "Ng Wee Keong", "Kasun De Zoysa", "Aruna Withanage", "Nilaan Loganathan"], "title": "Agentsway -- Software Development Methodology for AI Agents-based Teams", "comment": null, "summary": "The emergence of Agentic AI is fundamentally transforming how software is\ndesigned, developed, and maintained. Traditional software development\nmethodologies such as Agile, Kanban, ShapeUp, etc, were originally designed for\nhuman-centric teams and are increasingly inadequate in environments where\nautonomous AI agents contribute to planning, coding, testing, and continuous\nlearning. To address this methodological gap, we present \"Agentsway\" a novel\nsoftware development framework designed for ecosystems where AI agents operate\nas first-class collaborators. Agentsway introduces a structured lifecycle\ncentered on human orchestration, and privacy-preserving collaboration among\nspecialized AI agents. The framework defines distinct roles for planning,\nprompting, coding, testing, and fine-tuning agents, each contributing to\niterative improvement and adaptive learning throughout the development process.\nBy integrating fine-tuned LLMs that leverage outputs and feedback from\ndifferent agents throughout the development cycle as part of a retrospective\nlearning process, Agentsway enhances domain-specific reasoning, and explainable\ndecision-making across the entire software development lifecycle. Responsible\nAI principles are further embedded across the agents through the coordinated\nuse of multiple fine-tuned LLMs and advanced reasoning models, ensuring\nbalanced, transparent, and accountable decision-making. This work advances\nsoftware engineering by formalizing agent-centric collaboration, integrating\nprivacy-by-design principles, and defining measurable metrics for productivity\nand trust. Agentsway represents a foundational step toward the next generation\nof AI-native, self-improving software development methodologies. To the best of\nour knowledge, this is the first research effort to introduce a dedicated\nmethodology explicitly designed for AI agent-based software engineering teams.", "AI": {"tldr": "This paper introduces 'Agentsway', a novel software development framework tailored for AI agent-based teams, addressing the inadequacy of traditional methods in such environments. It emphasizes human orchestration, privacy-preserving collaboration, and integrates fine-tuned LLMs for adaptive learning and explainable decision-making.", "motivation": "Traditional software development methodologies are ill-suited for environments where AI agents autonomously contribute to software development tasks. This creates a critical need for methodologies tailored to AI agent-based development workflows, which is currently lacking.", "method": "The paper proposes 'Agentsway', a structured lifecycle framework for AI agent-based software development. Key features include: 1) Specialized agent roles (planning, prompting, coding, testing, fine-tuning) 2) Human orchestration of AI agents 3) Privacy-preserving inter-agent collaboration 4) Integration of fine-tuned LLMs for retrospective learning 5) Embedding of responsible AI principles through coordinated agent interactions", "result": "Agentsway provides a framework that enables AI agent-based software development through structured collaboration, iterative improvement, and adaptive learning. The methodology addresses gaps in traditional approaches by formalizing agent-centric workflows, integrating privacy-by-design, and establishing metrics for productivity and trust in AI-driven development.", "conclusion": "Agentsway represents a foundational advancement in AI-native software development methodologies. By formalizing agent-centric collaboration, integrating privacy principles, and enabling adaptive learning, the framework establishes a new paradigm for software engineering. This work fills a critical methodological gap, providing the first dedicated approach for AI agent-based development teams."}}
{"id": "2510.23927", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23927", "abs": "https://arxiv.org/abs/2510.23927", "authors": ["Daniel Spokoyny", "Nikolai Vogler", "Xin Gao", "Tianyi Zheng", "Yufei Weng", "Jonghyun Park", "Jiajun Jiao", "Geoffrey M. Voelker", "Stefan Savage", "Taylor Berg-Kirkpatrick"], "title": "Victim as a Service: Designing a System for Engaging with Interactive Scammers", "comment": null, "summary": "Pig butchering, and similar interactive online scams, lower their victims'\ndefenses by building trust over extended periods of conversation - sometimes\nweeks or months. They have become increasingly public losses (at least $75B by\none recent study). However, because of their long-term conversational nature,\nthey are extremely challenging to investigate at scale. In this paper, we\ndescribe the motivation, design, implementation, and experience with\nCHATTERBOX, an LLM-based system that automates long-term engagement with online\nscammers, making large-scale investigations of their tactics possible. We\ndescribe the techniques we have developed to attract scam attempts, the system\nand LLM-engineering required to convincingly engage with scammers, and the\nnecessary capabilities required to satisfy or evade \"milestones\" in scammers'\nworkflow.", "AI": {"tldr": "The paper introduces CHATTERBOX, an LLM-based system for automated long-term engagement with online scammers to facilitate large-scale investigations.", "motivation": "Pig butchering scams build trust over time and are hard to investigate at scale due to their conversational nature, leading to significant financial losses.", "method": "Developing CH chatterbox, an LLM-based system automating long-term engagement with scammers by attracting attempts, convincingly interacting, and completing workflow milestones.", "result": "CHATTERBOX enables automated, large-scale investigation into tactics used by online scammers over extended periods.", "conclusion": "LLM-based automation can effectively support investigations into long-term conversational online scams, reducing human effort and enabling scalable analysis."}}
{"id": "2510.23674", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23674", "abs": "https://arxiv.org/abs/2510.23674", "authors": ["Bin Wang", "Hui Li", "AoFan Liu", "BoTao Yang", "Ao Yang", "YiLu Zhong", "Weixiang Huang", "Yanping Zhang", "Runhuai Huang", "Weimin Zeng"], "title": "RefleXGen:The unexamined code is not worth using", "comment": null, "summary": "Security in code generation remains a pivotal challenge when applying large\nlanguage models (LLMs). This paper introduces RefleXGen, an innovative method\nthat significantly enhances code security by integrating Retrieval-Augmented\nGeneration (RAG) techniques with guided self-reflection mechanisms inherent in\nLLMs. Unlike traditional approaches that rely on fine-tuning LLMs or developing\nspecialized secure code datasets - processes that can be resource-intensive -\nRefleXGen iteratively optimizes the code generation process through\nself-assessment and reflection without the need for extensive resources. Within\nthis framework, the model continuously accumulates and refines its knowledge\nbase, thereby progressively improving the security of the generated code.\nExperimental results demonstrate that RefleXGen substantially enhances code\nsecurity across multiple models, achieving a 13.6% improvement with GPT-3.5\nTurbo, a 6.7% improvement with GPT-4o, a 4.5% improvement with CodeQwen, and a\n5.8% improvement with Gemini. Our findings highlight that improving the quality\nof model self-reflection constitutes an effective and practical strategy for\nstrengthening the security of AI-generated code.", "AI": {"tldr": "RefleXGen uses RAG and self-reflection to enhance code security in LLM applications without relying on fine-tuning or specialized datasets. Results show security improvements of 4.5% to 13.6% across several models.", "motivation": "Security is a critical issue in code generation using large language models (LLMs). Traditional methods depend on costly and resource-heavy processes such as model fine-tuning or training with specialized secure code datasets.", "method": "RefleXGen integrates Retrieval-Augmented Generation (RAG) techniques with guided self-reflection in LLMs. It iteratively optimizes code generation through self-assessment and reflection, enriching the model's knowledge base over time without requiring extensive resources.", "result": "RefleXGen achieved 13.6% improvement with GPT-3.5 Turbo, 6.7% with GPT-4o, 4.5% with CodeQwen, and 5.8% with Gemini, showing consistent security enhancements across multiple models.", "conclusion": "RefleXGen provides an effective and practical strategy for improving code security in AI systems by enhancing the self-reflection capabilities of LLMs, avoiding traditional, resource-intensive methods."}}
{"id": "2510.23938", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23938", "abs": "https://arxiv.org/abs/2510.23938", "authors": ["Marcin Spoczynski", "Marcela S. Melara"], "title": "Scalable GPU-Based Integrity Verification for Large Machine Learning Models", "comment": null, "summary": "We present a security framework that strengthens distributed machine learning\nby standardizing integrity protections across CPU and GPU platforms and\nsignificantly reducing verification overheads. Our approach co-locates\nintegrity verification directly with large ML model execution on GPU\naccelerators, resolving the fundamental mismatch between how large ML workloads\ntypically run (primarily on GPUs) and how security verifications traditionally\noperate (on separate CPU-based processes), delivering both immediate\nperformance benefits and long-term architectural consistency. By performing\ncryptographic operations natively on GPUs using dedicated compute units (e.g.,\nIntel Arc's XMX units, NVIDIA's Tensor Cores), our solution eliminates the\npotential architectural bottlenecks that could plague traditional CPU-based\nverification systems when dealing with large models. This approach leverages\nthe same GPU-based high-memory bandwidth and parallel processing primitives\nthat power ML workloads ensuring integrity checks keep pace with model\nexecution even for massive models exceeding 100GB. This framework establishes a\ncommon integrity verification mechanism that works consistently across\ndifferent GPU vendors and hardware configurations. By anticipating future\ncapabilities for creating secure channels between trusted execution\nenvironments and GPU accelerators, we provide a hardware-agnostic foundation\nthat enterprise teams can deploy regardless of their underlying CPU and GPU\ninfrastructures.", "AI": {"tldr": "A security framework enhances distributed machine learning by aligning integrity verification with GPU execution and reducing overhead.", "motivation": "Traditional security verification methods for ML do not align with the GPU-centric execution of large models, causing inefficiencies and architectural mismatches.", "method": "The authors integrate integrity verification directly into GPU operations using native hardware components, thereby leveraging the same high-memory bandwidth and parallel processing as ML workloads.", "result": "The framework reduces verification overhead and enables consistent integrity checks across different GPU platforms and large models exceeding 100GB.", "conclusion": "This GPU-native security framework provides performance improvements and scalability for large-scale ML systems while supporting future secure communication with TEEs."}}
{"id": "2510.23761", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.23761", "abs": "https://arxiv.org/abs/2510.23761", "authors": ["Kevin Han", "Siddharth Maddikayala", "Tim Knappe", "Om Patel", "Austen Liao", "Amir Barati Farimani"], "title": "TDFlow: Agentic Workflows for Test Driven Software Engineering", "comment": null, "summary": "We introduce TDFlow, a novel test-driven agentic workflow that frames\nrepository-scale software engineering as a test-resolution task, specifically\ndesigned to solve human-written tests. Given a set of tests, TDFlow repeatedly\nproposes, revises, and debugs repository-scale patches using precisely\nengineered sub-agents and tightly constrained tools. The workflow decomposes\nsoftware engineering program repair into four components governed by respective\nsub-agents. This simple, forced decoupling of patch proposing, debugging, patch\nrevision, and optional test generation (1) reduces long-context burden on any\nindividual sub-agent, (2) focuses each sub-agent on specific, pre-defined\nsub-tasks, and (3) allows for specialized performance improvement on specific\nsub-tasks. When provided human-written tests, TDFlow attains 88.8% pass rate on\nSWE-Bench Lite (an absolute improvement of 27.8% over the next best system) and\n94.3% on SWE-Bench Verified. Manual inspection of the 800 TDFlow runs within\nSWE-Bench Lite and Verified uncover only 7 instances of test hacking, which\nwere subsequently counted as failures. Furthermore, we show that the primary\nobstacle to human-level software engineering performance lies within writing\nsuccessful reproduction tests. We envision a human-LLM interactive system\npowered by TDFlow where human developers write tests solved by LLM systems.\nTogether, these results indicate that modern LLMs, when embedded in a narrowly\nengineered, test-driven workflow, already achieve human-level test resolution\n-- with the final frontier for fully autonomous repository repair being the\naccurate generation of valid reproduction tests.", "AI": {"tldr": "TDFlow is a test-driven agentic workflow achieving human-level test resolution in repository-scale software engineering with 88.8%-94.3% pass rates, demonstrating LLMs can excel in structured workflows but highlight the challenge of autonomous test generation.", "motivation": "Existing systems struggle with software engineering tasks due to long-context burdens and task-agnostic agent roles. The paper addresses this by decomposing program repair into specialized sub-agents.", "method": "TDFlow structures program repair into four components (patch proposing, debugging, revision, test generation), using sub-agents with focused tasks and constrained tools to reduce context demands and improve task-specific performance.", "result": "88.8% pass rate on SWE-Bench Lite (27.8PP\u2191), 94.3_% on SWE-Bench Verified with only 7/800 test-hacking instances. Identifies reproduction test generation as the primary bottleneck to human-level performance.", "conclusion": "Modern LLMs achieve human-level test resolution in structured test-driven workflows, but autonomous test generation remains the final barrier to full repository-scale repair autonomy. Human-LLM collaboration is proposed as a path forward."}}
{"id": "2510.24072", "categories": ["cs.CR", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.24072", "abs": "https://arxiv.org/abs/2510.24072", "authors": ["Austin Shouli", "Yulia Bobkova", "Ajay Kumar Shrestha"], "title": "Covert Surveillance in Smart Devices: A SCOUR Framework Analysis of Youth Privacy Implications", "comment": "To appear in the IEEE UEMCON 2025 proceedings", "summary": "This paper investigates how smart devices covertly capture private\nconversations and discusses in more in-depth the implications of this for youth\nprivacy. Using a structured review guided by the PRISMA methodology, the\nanalysis focuses on privacy concerns, data capture methods, data storage and\nsharing practices, and proposed technical mitigations. To structure and\nsynthesize findings, we introduce the SCOUR framework, encompassing\nSurveillance mechanisms, Consent and awareness, Operational data flow, Usage\nand exploitation, and Regulatory and technical safeguards. Findings reveal that\nsmart devices have been covertly capturing personal data, especially with smart\ntoys and voice-activated smart gadgets built for youth. These issues are\nworsened by unclear data collection practices and insufficient transparency in\nsmart device applications. Balancing privacy and utility in smart devices is\ncrucial, as youth are becoming more aware of privacy breaches and value their\npersonal data more. Strategies to improve regulatory and technical safeguards\nare also provided. The review identifies research gaps and suggests future\ndirections. The limitations of this literature review are also explained. The\nfindings have significant implications for policy development and the\ntransparency of data collection for smart devices.", "AI": {"tldr": "The paper explores how smart devices secretly collect private data, especially affecting youth privacy. Using PRISMA methodology, it introduces the SCOUR framework for analyzing privacy issues and proposes strategies for improving safeguards. Key findings highlight risks from smart toys and gadgets, emphasizing the need for better transparency and policies.", "motivation": "Smart devices often covertly capture private data, creating risks for youth privacy. As children become more aware of data breaches, balancing privacy and utility is essential to protect them. This paper aims to address these concerns and guide future policy and technical measures.", "method": "The study conducts a systematic literature review following the PRISMA methodology. It synthesizes findings using the SCOUR framework, which includes aspects like surveillance mechanisms, operational data flow, and technical safeguards. The analysis covers data capture methods, storage, sharing practices, and mitigation strategies.", "result": "Findings indicate that smart devices, particularly smart toys and voice-activated gadgets, are capturing private conversations without clear transparency. Unethical data collection and storage practices are prevalent, reinforcing the need for improved regulatory and technical safeguards. The review also highlights existing research gaps and suggests future directions.", "conclusion": "The research underscores the importance of developing stronger policies and more transparent practices in smart device data collection, specifically for youth. It provides actionable insights into mitigating privacy risks and encourages further exploration of the subject, ensuring both technological utility and ethical responsibility."}}
{"id": "2510.23893", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23893", "abs": "https://arxiv.org/abs/2510.23893", "authors": ["Rodrigo Falc\u00e3o", "Stefan Schweitzer", "Julien Siebert", "Emily Calvet", "Frank Elberzhager"], "title": "Evaluating the effectiveness of LLM-based interoperability", "comment": null, "summary": "Background: Systems of systems are becoming increasingly dynamic and\nheterogeneous, and this adds pressure on the long-standing challenge of\ninteroperability. Besides its technical aspect, interoperability has also an\neconomic side, as development time efforts are required to build the\ninteroperability artifacts. Objectives: With the recent advances in the field\nof large language models (LLMs), we aim at analyzing the effectiveness of\nLLM-based strategies to make systems interoperate autonomously, at runtime,\nwithout human intervention. Method: We selected 13 open source LLMs and curated\nfour versions of a dataset in the agricultural interoperability use case. We\nperformed three runs of each model with each version of the dataset, using two\ndifferent strategies. Then we compared the effectiveness of the models and the\nconsistency of their results across multiple runs. Results: qwen2.5-coder:32b\nwas the most effective model using both strategies DIRECT (average pass@1 >=\n0.99) and CODEGEN (average pass@1 >= 0.89) in three out of four dataset\nversions. In the fourth dataset version, which included an unit conversion, all\nmodels using the strategy DIRECT failed, whereas using CODEGEN\nqwen2.5-coder:32b succeeded with an average pass@1 = 0.75. Conclusion: Some\nLLMs can make systems interoperate autonomously. Further evaluation in\ndifferent domains is recommended, and further research on reliability\nstrategies should be conducted.", "AI": {"tldr": "This paper evaluates open-source large language models (LLMs) for autonomous system interoperability in agriculture, identifying qwen2.5-coder:32b as the most effective model across multiple strategies and dataset versions.", "motivation": "Dynamic, heterogeneous systems require interoperability solutions that minimize economic development costs. LLMs offer potential for automated runtime interoperability without human intervention.", "method": "13 open-source LLMs were tested on four agricultural dataset versions using two strategies (DIRECT and CODEGEN) with three runs each, measuring effectiveness and result consistency.", "result": "qwen2.5-coder:32b achieved avg pass@1 \u22650.99 (DIRECT) and \u22650.89 (CODEGEN) across three dataset versions. CODEGEN outperformed DIRECT in unit conversion tasks (avg pass@1=0.75).", "conclusion": "LLMs enable autonomous system interoperability, but further domain testing and reliability studies are needed for broader application."}}
{"id": "2510.24101", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.24101", "abs": "https://arxiv.org/abs/2510.24101", "authors": ["Nam Tran", "Khoa Nguyen", "Dongxi Liu", "Josef Pieprzyk", "Willy Susilo"], "title": "Traceable Signatures from Lattices", "comment": "45 pages", "summary": "Traceable signatures (Kiayas et al., EUROCRYPT 2004) is an anonymous digital\nsignature system that extends the tracing power of the opening authority in\ngroup signatures. There are many known constructions of traceable signatures,\nbut all are based on number-theoretic/pairing assumptions. For such reason,\nthey may not be secure in the presence of quantum computers. This work revisits\nthe notion of traceable signatures and presents a lattice-based construction\nprovably secure in the quantum random oracle model (QROM).", "AI": {"tldr": "This paper introduces a quantum-resistant traceable signature system based on lattices.", "motivation": "Pri... (truncated due to length)", "method": "Th... (truncated due to length)", "result": "Th... (truncated due to length)", "conclusion": "Th... (truncated due to length)"}}
{"id": "2510.23970", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23970", "abs": "https://arxiv.org/abs/2510.23970", "authors": ["Maria C. Borges", "Julian Legler", "Lucca Di Benedetto"], "title": "Validating Alerts in Cloud-Native Observability", "comment": "16th Symposium on Software Performance (SSP'25)", "summary": "Observability and alerting form the backbone of modern reliability\nengineering. Alerts help teams catch faults early before they turn into\nproduction outages and serve as first clues for troubleshooting. However,\ndesigning effective alerts is challenging. They need to strike a fine balance\nbetween catching issues early and minimizing false alarms. On top of this,\nalerts often cover uncommon faults, so the code is rarely executed and\ntherefore rarely checked. To address these challenges, several industry\npractitioners advocate for testing alerting code with the same rigor as\napplication code. Still, there's a lack of tools that support such systematic\ndesign and validation of alerts.\n  This paper introduces a new alerting extension for the observability\nexperimentation tool OXN. It lets engineers experiment with alerts early during\ndevelopment. With OXN, engineers can now tune rules at design time and\nroutinely validate the firing behavior of their alerts, avoiding future\nproblems at runtime.", "AI": {"tldr": "This paper introduces a new alerting extension for the observability tool OXN, addressing the challenges of designing effective alerts through systematic design and validation during development.", "motivation": "Designing effective alerts is challenging due to the fine balance between catching issues early and minimizing false alarms, compounded by rare alert code execution and a lack of systematic validation tools.", "method": "The authors introduce an extension to OXN, an observability experimentation tool, enabling engineers to experiment with alerts during development and validate their behavior at design time.", "result": "The extension allows engineers to tune alert rules during development and systematically validate their behavior, reducing the likelihood of issues at runtime.", "conclusion": "By integrating alert validation into the development process via OXN's new extension, the approach enables more reliable and systematic alert design, improving overall system reliability."}}
{"id": "2510.24141", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.24141", "abs": "https://arxiv.org/abs/2510.24141", "authors": ["Miao Zhang", "Shenao Wang", "Guilin Zheng", "Yanjie Zhao", "Haoyu Wang"], "title": "Demystifying Cookie Sharing Risks in WebView-based Mobile App-in-app Ecosystems", "comment": "To appear in the 40th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE'25)", "summary": "Mini-programs, an emerging mobile application paradigm within super-apps,\noffer a seamless and installation-free experience. However, the adoption of the\nweb-view component has disrupted their isolation mechanisms, exposing new\nattack surfaces and vulnerabilities. In this paper, we introduce a novel\nvulnerability called Cross Mini-program Cookie Sharing (CMCS), which arises\nfrom the shared web-view environment across mini-programs. This vulnerability\nallows unauthorized data exchange across mini-programs by enabling one\nmini-program to access cookies set by another within the same web-view context,\nviolating isolation principles. As a preliminary step, we analyzed the web-view\nmechanisms of four major platforms, including WeChat, AliPay, TikTok, and\nBaidu, and found that all of them are affected by CMCS vulnerabilities.\nFurthermore, we demonstrate the collusion attack enabled by CMCS, where\nprivileged mini-programs exfiltrate sensitive user data via cookies accessible\nto unprivileged mini-programs. To measure the impact of collusion attacks\nenabled by CMCS vulnerabilities in the wild, we developed MiCoScan, a static\nanalysis tool that detects mini-programs affected by CMCS vulnerabilities.\nMiCoScan employs web-view context modeling to identify clusters of\nmini-programs sharing the same web-view domain and cross-webview data flow\nanalysis to detect sensitive data transmissions to/from web-views. Using\nMiCoScan, we conducted a large-scale analysis of 351,483 mini-programs,\nidentifying 45,448 clusters sharing web-view domains, 7,965 instances of\nprivileged data transmission, and 9,877 mini-programs vulnerable to collusion\nattacks. Our findings highlight the widespread prevalence and significant\nsecurity risks posed by CMCS vulnerabilities, underscoring the urgent need for\nimproved isolation mechanisms in mini-program ecosystems.", "AI": {"tldr": "This paper presents a novel vulnerability, Cross Mini-program Cookie Sharing (CMCS), in mini-programs of popular super-apps due to shared web-view environments, allowing unauthorized data exchange and enabling collusion attacks. The authors developed MiCoScan, a static analysis tool, to detect these vulnerabilities, revealing significant security risks through large-scale analysis.", "motivation": "Mini-programs, while convenient, have introduced new security risks because of their shared web-view component. The paper aims to address the vulnerabilities that arise from this design, which can lead to serious data exchange issues among mini-programs.", "method": "The researchers conducted an initial analysis of web-view mechanisms across four major platforms and discovered the CMCS vulnerability. They then developed MiCoScan, utilizing web-view context modeling and cross-web-view data flow analysis to detect instances of CMCS vulnerabilities in a large-scale study of mini-programs.", "result": "The analysis revealed 45,448 clusters of mini-programs sharing web-view domains, 7,965 cases of privileged data transmission, and 9,877 mini-programs vulnerable to collusion attacks. This demonstrates the extensive presence and risk of CMCS vulnerabilities across the platforms studied.", "conclusion": "The study concludes that CMCS vulnerabilities pose significant security threats to the mini-program ecosystem and emphasizes the need for stronger isolation mechanisms to mitigate these risks."}}
{"id": "2510.24019", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24019", "abs": "https://arxiv.org/abs/2510.24019", "authors": ["Xing Xing", "Wei Wang", "Lipeng Ma", "Weidong Yang", "Junjie Zheng"], "title": "Lifecycle-Aware code generation: Leveraging Software Engineering Phases in LLMs", "comment": null, "summary": "Recent progress in large language models (LLMs) has advanced automatic code\ngeneration, yet most approaches rely on direct, single-step translation from\nproblem descriptions to code, disregarding structured software engineering\npractices. We introduce a lifecycle-aware framework that systematically\nincorporates intermediate artifacts such as requirements analysis, state\nmachine modeling, and pseudocode into both the training and inference stages.\nThis design aligns code generation with standard software development phases\nand enables more structured reasoning. Experiments show that lifecycle-level\nfine-tuning improves code correctness by up to 75% over the same model before\nfine-tuning, with performance gains compounding across intermediate stages.\nMulti-step inference consistently surpasses single-step generation,\ndemonstrating the effectiveness of intermediate scaffolding. Notably,\nopen-source LLMs, once fine-tuned under our framework, match or slightly\noutperform models pretrained on code. When applied to DeepSeek-Coder-1.3B, our\nframework yields relative CodeBLEU improvements of 34.3%, 20.0%, 11.2%, and\n22.3% over ChatGPT-3.5, ChatGPT-4o-mini, DeepSeek-R1, and LLaMA-8B,\nrespectively. Our pipeline also proves robust with up to 80\\% less training\ndata, confirming its resilience. Ablation studies further reveal that each\nintermediate artifact contributes distinctly to final code quality, with state\nmachine modeling yielding the most substantial impact. Our source code and\ndetailed experimental data are available at\nhttps://anonymous.4open.science/r/Lifecycle-Aware-3CCB.", "AI": {"tldr": "A lifecycle-aware framework for code generation in large language models, incorporating intermediate artifacts during training and inference, leading to significant improvements in code correctness and performance.", "motivation": "The need for code generation that aligns with structured software engineering practices, rather than relying on a direct translation approach from problem descriptions to code.", "method": "The proposed method introduces a framework that incorporates intermediate artifacts such as requirements analysis, state machine modeling, and pseudocode into both the training and inference stages of code generation. This aligns code generation with standard software development phases. Training involves lifecycle-level fine-tuning with these intermediate stages, while inference uses multi-step generation leveraging these artifacts for scaffolding.", "result": "Lifecycle-level fine-tuning improved code correctness by up to 75% over the same model before fine-tuning. Multi-step inference outperformed single-step generation. Open-source LLMs, after fine-tuning, matched or slightly outperformed code-pretrained models. The framework achieved CodeBLEU improvements of 34.3%, 20.0%, 11.2%, and 22.3% over ChatGPT-3.5, ChatGPT-4o-mini, DeepSeek-R1, and LLaMA-8B, respectively.", "conclusion": "The proposed lifecycle-aware framework for code generation effectively enhances code correctness and performance by integrating intermediate artifacts and aligning with software development lifecycles. The robustness of the pipeline with less training data and the distinct contributions of each intermediate artifact confirm the framework's effectiveness."}}
{"id": "2510.24317", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.24317", "abs": "https://arxiv.org/abs/2510.24317", "authors": ["Mar\u00eda Sanz-G\u00f3mez", "V\u00edctor Mayoral-Vilches", "Francesco Balassone", "Luis Javier Navarrete-Lozano", "Crist\u00f3bal R. J. Veas Chavez", "Maite del Mundo de Torres"], "title": "Cybersecurity AI Benchmark (CAIBench): A Meta-Benchmark for Evaluating Cybersecurity AI Agents", "comment": null, "summary": "Cybersecurity spans multiple interconnected domains, complicating the\ndevelopment of meaningful, labor-relevant benchmarks. Existing benchmarks\nassess isolated skills rather than integrated performance. We find that\npre-trained knowledge of cybersecurity in LLMs does not imply attack and\ndefense abilities, revealing a gap between knowledge and capability. To address\nthis limitation, we present the Cybersecurity AI Benchmark (CAIBench), a\nmodular meta-benchmark framework that allows evaluating LLM models and agents\nacross offensive and defensive cybersecurity domains, taking a step towards\nmeaningfully measuring their labor-relevance. CAIBench integrates five\nevaluation categories, covering over 10,000 instances: Jeopardy-style CTFs,\nAttack and Defense CTFs, Cyber Range exercises, knowledge benchmarks, and\nprivacy assessments. Key novel contributions include systematic simultaneous\noffensive-defensive evaluation, robotics-focused cybersecurity challenges\n(RCTF2), and privacy-preserving performance assessment (CyberPII-Bench).\nEvaluation of state-of-the-art AI models reveals saturation on security\nknowledge metrics (~70\\% success) but substantial degradation in multi-step\nadversarial (A\\&D) scenarios (20-40\\% success), or worse in robotic targets\n(22\\% success). The combination of framework scaffolding and LLM model choice\nsignificantly impacts performance; we find that proper matches improve up to\n2.6$\\times$ variance in Attack and Defense CTFs. These results demonstrate a\npronounced gap between conceptual knowledge and adaptive capability,\nemphasizing the need for a meta-benchmark.", "AI": {"tldr": "The paper introduces CAIBench, a benchmark framework for evaluating AI models in cybersecurity, highlighting the gap between knowledge and applied skills in LLMs.", "motivation": "Existing benchmarks focus on isolated skills, neglecting integrated performance and real-world labor relevance in cybersecurity domains.", "method": "CAIBench is a modular meta-benchmark framework encompassing five evaluation categories, including CTFs, knowledge tests, and privacy assessments. It introduces novel components like RCTF2 and CyberPII-Bench.", "result": "Leading AI models achieve 70% on knowledge metrics but show significant drop to 20-40% in A&D scenarios and 22% in robotic targets. Proper framework and model alignment boosts performance by 2.6x in A&D CTFs.", "conclusion": "CAIBench reveals a critical knowledge-capability divide in AI for cybersecurity. The framework is a step toward assessing labor-relevance, with results showing high variance in complex adversarial settings."}}
{"id": "2510.24142", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24142", "abs": "https://arxiv.org/abs/2510.24142", "authors": ["Joran Leest", "Ilias Gerostathopoulos", "Patricia Lago", "Claudia Raibulet"], "title": "Monitoring and Observability of Machine Learning Systems: Current Practices and Gaps", "comment": null, "summary": "Production machine learning (ML) systems fail silently -- not with crashes,\nbut through wrong decisions. While observability is recognized as critical for\nML operations, there is a lack empirical evidence of what practitioners\nactually capture. This study presents empirical results on ML observability in\npractice through seven focus group sessions in several domains. We catalog the\ninformation practitioners systematically capture across ML systems and their\nenvironment and map how they use it to validate models, detect and diagnose\nfaults, and explain observed degradations. Finally, we identify gaps in current\npractice and outline implications for tooling design and research to establish\nML observability practices.", "AI": {"tldr": "This paper studies ML observability in production through focus groups, cataloging captured data and identifying gaps for future tooling and research.", "motivation": "Production ML systems fail silently through wrong decisions rather than crashes, highlighting the need for observability in ML operations, which currently lacks empirical understanding.", "method": "The paper employs seven focus group sessions across various domains to analyze ML observability practices empirically.", "result": "The study catalogs practitioner-captured data for ML validation and fault detection/diagnosis, with insights into how data explains model degradation.", "conclusion": "The paper identifies current gaps in ML observability, offering implications for tooling design and research to improve future practices."}}
{"id": "2510.24393", "categories": ["cs.CR", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.24393", "abs": "https://arxiv.org/abs/2510.24393", "authors": ["Yan Meng", "Jiachun Li", "Matthew Pillari", "Arjun Deopujari", "Liam Brennan", "Hafsah Shamsie", "Haojin Zhu", "Yuan Tian"], "title": "Your Microphone Array Retains Your Identity: A Robust Voice Liveness Detection System for Smart Speakers", "comment": "This is a paper accepted by USENIX Security 2022. See:\n  https://www.usenix.org/conference/usenixsecurity22/presentation/meng", "summary": "Though playing an essential role in smart home systems, smart speakers are\nvulnerable to voice spoofing attacks. Passive liveness detection, which\nutilizes only the collected audio rather than the deployed sensors to\ndistinguish between live-human and replayed voices, has drawn increasing\nattention. However, it faces the challenge of performance degradation under the\ndifferent environmental factors as well as the strict requirement of the fixed\nuser gestures.\n  In this study, we propose a novel liveness feature, array fingerprint, which\nutilizes the microphone array inherently adopted by the smart speaker to\ndetermine the identity of collected audios. Our theoretical analysis\ndemonstrates that by leveraging the circular layout of microphones, compared\nwith existing schemes, array fingerprint achieves a more robust performance\nunder the environmental change and user's movement. Then, to leverage such a\nfingerprint, we propose ARRAYID, a lightweight passive detection scheme, and\nelaborate a series of features working together with array fingerprint. Our\nevaluation on the dataset containing 32,780 audio samples and 14 spoofing\ndevices shows that ARRAYID achieves an accuracy of 99.84%, which is superior to\nexisting passive liveness detection schemes.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.24188", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24188", "abs": "https://arxiv.org/abs/2510.24188", "authors": ["C\u00e9sar Santos", "Ermeson Andrade", "Roberto Natella"], "title": "Investigating Software Aging in LLM-Generated Software Systems", "comment": "Presented at the 17th International Workshop on Software Aging and\n  Rejuvenation (WoSAR), 2025", "summary": "Automatically generated software, especially code produced by Large Language\nModels (LLMs), is increasingly adopted to accelerate development and reduce\nmanual effort. However, little is known about the long-term reliability of such\nsystems under sustained execution. In this paper, we experimentally investigate\nthe phenomenon of software aging in applications generated by LLM-based tools.\nUsing the Bolt platform and standardized prompts from Baxbench, we generated\nfour service-oriented applications and subjected them to 50-hour load tests.\nResource usage, response time, and throughput were continuously monitored to\ndetect degradation patterns. The results reveal significant evidence of\nsoftware aging, including progressive memory growth, increased response time,\nand performance instability across all applications. Statistical analyzes\nconfirm these trends and highlight variability in the severity of aging\naccording to the type of application. Our findings show the need to consider\naging in automatically generated software and provide a foundation for future\nstudies on mitigation strategies and long-term reliability evaluation.", "AI": {"tldr": "This paper investigates software aging in LLM-generated applications by stress-testing four apps for 50 hours, revealing memory leaks, performance instability, and degradation patterns, emphasizing the need for long-term reliability strategies in automatically generated software.", "motivation": "LLM-generated code is expanding in software development, yet its long-term reliability under sustained use remains unexplored, risking unexpected failures in critical systems. This study fills this gap by empirically analyzing software aging in LLM-derived applications.", "method": "The authors used the Bolt platform and Baxbench prompts to generate four service-oriented applications, which were subjected to continuous 50-hour load tests. Resource consumption, response time, and throughput were monitored to identify aging patterns, with statistical analysis validating degradation trends.", "result": "All tested applications exhibited severe software aging symptoms: progressive memory growth, exponentially increasing response times, and declining throughput. Statistical analysis confirmed these trends and showed variability in aging severity across different application types.", "conclusion": "LLM-generated software is vulnerable to aging effects over time, challenging assumptions about its reliability. This work establishes the first empirical evidence of such issues and paves the way for developing mitigation techniques and long-term stability evaluation frameworks."}}
{"id": "2510.24408", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.24408", "abs": "https://arxiv.org/abs/2510.24408", "authors": ["Yifan Wu", "Xuewei Feng", "Yuxiang Yang", "Ke Xu"], "title": "Uncovering Gaps Between RFC Updates and TCP/IP Implementations: LLM-Facilitated Differential Checks on Intermediate Representations", "comment": "15 pages, 7 figures", "summary": "As the core of the Internet infrastructure, the TCP/IP protocol stack\nundertakes the task of network data transmission. However, due to the\ncomplexity of the protocol and the uncertainty of cross-layer interaction,\nthere are often inconsistencies between the implementation of the protocol\nstack code and the RFC standard. This inconsistency may not only lead to\ndifferences in protocol functions but also cause serious security\nvulnerabilities. At present, with the continuous expansion of protocol stack\nfunctions and the rapid iteration of RFC documents, it is increasingly\nimportant to detect and fix these inconsistencies. With the rise of large\nlanguage models, researchers have begun to explore how to extract protocol\nspecifications from RFC documents through these models, including protocol\nstack modeling, state machine extraction, text ambiguity analysis, and other\nrelated content. However, existing methods rely on predefined patterns or\nrule-based approaches that fail to generalize across different protocol\nspecifications. Automated and scalable detection of these inconsistencies\nremains a significant challenge. In this study, we propose an automated\nanalysis framework based on LLM and differential models. By modeling the\niterative relationship of the protocol and based on the iterative update\nrelationship of the RFC standard, we perform incremental code function analysis\non different versions of kernel code implementations to automatically perform\ncode detection and vulnerability analysis. We conduct extensive evaluations to\nvalidate the effectiveness of our framework, demonstrating its effectiveness in\nidentifying potential vulnerabilities caused by RFC code inconsistencies.", "AI": {"tldr": "The paper presents an automated analysis framework using LLM and differential models to detect inconsistencies between TCP/IP protocol stack code and RFC standards, addressing security vulnerabilities.", "motivation": "Current manual and rule-based methods for detecting inconsistencies in protocol stack implementations are insufficient as RFC documents evolve rapidly and protocol stack complexity increases, leading to potential security issues.", "method": "The proposed framework leverages large language models (LLM) and differential models to analyze the iterative relationship of protocols and RFC updates, enabling incremental code analysis across different kernel versions for automated detection and vulnerability assessment.", "result": "The evaluation demonstrates the effectiveness of the framework in identifying vulnerabilities caused by RFC code inconsistencies, with practical validation and analysis.", "conclusion": "An automated, scalable solution is essential for managing inconsistencies between evolving RFC standards and protocol implementations, and the presented framework offers a robust approach to address this challenge."}}
{"id": "2510.24241", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24241", "abs": "https://arxiv.org/abs/2510.24241", "authors": ["Zixian Zhang", "Takfarinas Saber"], "title": "MAGNET: A Multi-Graph Attentional Network for Code Clone Detection", "comment": null, "summary": "Code clone detection is a fundamental task in software engineering that\nunderpins refactoring, debugging, plagiarism detection, and vulnerability\nanalysis. Existing methods often rely on singular representations such as\nabstract syntax trees (ASTs), control flow graphs (CFGs), and data flow graphs\n(DFGs), which capture only partial aspects of code semantics. Hybrid approaches\nhave emerged, but their fusion strategies are typically handcrafted and\nineffective. In this study, we propose MAGNET, a multi-graph attentional\nframework that jointly leverages AST, CFG, and DFG representations to capture\nsyntactic and semantic features of source code. MAGNET integrates residual\ngraph neural networks with node-level self-attention to learn both local and\nlong-range dependencies, introduces a gated cross-attention mechanism for\nfine-grained inter-graph interactions, and employs Set2Set pooling to fuse\nmulti-graph embeddings into unified program-level representations. Extensive\nexperiments on BigCloneBench and Google Code Jam demonstrate that MAGNET\nachieves state-of-the-art performance with an overall F1 score of 96.5\\% and\n99.2\\% on the two datasets, respectively. Ablation studies confirm the critical\ncontributions of multi-graph fusion and each attentional component. Our code is\navailable at https://github.com/ZixianReid/Multigraph_match", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.24422", "categories": ["cs.CR", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24422", "abs": "https://arxiv.org/abs/2510.24422", "authors": ["Bijeet Basak", "Nupur Patil", "Kurian Polachan", "Srinivas Vivek"], "title": "Attack on a PUF-based Secure Binary Neural Network", "comment": "Accepted at VLSID 2026. To be published in IEEE Xplore", "summary": "Binarized Neural Networks (BNNs) deployed on memristive crossbar arrays\nprovide energy-efficient solutions for edge computing but are susceptible to\nphysical attacks due to memristor nonvolatility. Recently, Rajendran et al.\n(IEEE Embedded Systems Letter 2025) proposed a Physical Unclonable Function\n(PUF)-based scheme to secure BNNs against theft attacks. Specifically, the\nweight and bias matrices of the BNN layers were secured by swapping columns\nbased on device's PUF key bits.\n  In this paper, we demonstrate that this scheme to secure BNNs is vulnerable\nto PUF-key recovery attack. As a consequence of our attack, we recover the\nsecret weight and bias matrices of the BNN. Our approach is motivated by\ndifferential cryptanalysis and reconstructs the PUF key bit-by-bit by observing\nthe change in model accuracy, and eventually recovering the BNN model\nparameters. Evaluated on a BNN trained on the MNIST dataset, our attack could\nrecover 85% of the PUF key, and recover the BNN model up to 93% classification\naccuracy compared to the original model's 96% accuracy. Our attack is very\nefficient and it takes a couple of minutes to recovery the PUF key and the\nmodel parameters.", "AI": {"tldr": "The paper analyzes the insecurity of a PUF-based method for protecting Binarized Neural Networks (BNNs) against physical attacks, demonstrating a key recovery attack using differential cryptanalysis to reconstruct the PUF key and model parameters efficiently with 85% key recovery and 93% model classification accuracy.", "motivation": "To evaluate the security of a recently proposed PUF-based scheme for securing BNNs on memristive crossbar arrays against theft attacks and to demonstrate its insecurity through a physical attack method.", "method": "An attack method inspired by differential cryptanalysis that reconstructs the PUF key by observing changes in BNN model accuracy. This process involves measuring the impact on model performance when the PUF key bits are altered during inference and using these measurements to infer the original key.", "result": "The PUF key recovery attack successfully recovers 85% of the PUF key and BNN model parameters. The attacked BNN produced 93% classification accuracy on the MNIST dataset compared to the original 96% accuracy, and the attack executed in a few minutes.", "conclusion": "The study concludes that the PUF-based security scheme for BNNs is vulnerable to efficient PUF-key recovery attacks. Differential cryptanalysis-based methods can reconstruct the PUF key and model without needing access to the complete model internals or the device containing the BNN."}}
{"id": "2510.24265", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.24265", "abs": "https://arxiv.org/abs/2510.24265", "authors": ["Sadia Afroz", "Zixuan Feng", "Katie Kimura", "Bianca Trinkenreich", "Igor Steinmacher", "Anita Sarma"], "title": "Developer Productivity with GenAI", "comment": null, "summary": "Generative AI (GenAI) tools are increasingly being adopted in software\ndevelopment as productivity aids. However, evidence regarding where and when\nthese tools actually enhance productivity is unclear. In this paper, we\ninvestigate how GenAI adoption affects different dimensions of developer\nproductivity. We surveyed 415 software practitioners to capture their\nperceptions of productivity changes associated with AI-assisted development\nusing the SPACE framework - Satisfaction and well-being, Performance, Activity,\nCommunication and collaboration, and Efficiency and flow. Our results,\ndisaggregated by frequency of AI usage, reveal limited overall productivity\nchange, highlighting the productivity paradox in which developers become faster\nbut do not necessarily create better software or feel more fulfilled.", "AI": {"tldr": "The study finds that while Generative AI tools are being adopted in software development to boost productivity, the actual improvements in various dimensions of developer productivity are limited, suggesting a productivity paradox.", "motivation": "There is a need to understand the real impact of Generative AI (GenAI) tools on software developer productivity, as current evidence is unclear regarding the specific contexts and aspects where these tools provide enhancement.", "method": "The paper involved surveying 415 software practitioners about their perceptions of GenAI's impact using the SPACE framework dimensions: Satisfaction, Performance, Activity, Communication, and Efficiency.", "result": "Results show limited overall changes in productivity dimensions, indicating a productivity paradox where developers become faster but do not necessarily improve software quality or fulfillment.", "conclusion": "Adoption of GenAI tools in software development does not consistently lead to better productivity outcomes as intended, with only minor beneficial impacts observed across the measured productivity factors."}}
{"id": "2510.24498", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24498", "abs": "https://arxiv.org/abs/2510.24498", "authors": ["Tejaswini Bollikonda"], "title": "Design and Optimization of Cloud Native Homomorphic Encryption Workflows for Privacy-Preserving ML Inference", "comment": "6 pages 2 figures, 2 tABLES", "summary": "As machine learning (ML) models become increasingly deployed through cloud\ninfrastructures, the confidentiality of user data during inference poses a\nsignificant security challenge. Homomorphic Encryption (HE) has emerged as a\ncompelling cryptographic technique that enables computation on encrypted data,\nallowing predictions to be generated without decrypting sensitive inputs.\nHowever, the integration of HE within large scale cloud native pipelines\nremains constrained by high computational overhead, orchestration complexity,\nand model compatibility issues.\n  This paper presents a systematic framework for the design and optimization of\ncloud native homomorphic encryption workflows that support privacy-preserving\nML inference. The proposed architecture integrates containerized HE modules\nwith Kubernetes-based orchestration, enabling elastic scaling and parallel\nencrypted computation across distributed environments. Furthermore,\noptimization strategies including ciphertext packing, polynomial modulus\nadjustment, and operator fusion are employed to minimize latency and resource\nconsumption while preserving cryptographic integrity. Experimental results\ndemonstrate that the proposed system achieves up to 3.2times inference\nacceleration and 40% reduction in memory utilization compared to conventional\nHE pipelines. These findings illustrate a practical pathway for deploying\nsecure ML-as-a-Service (MLaaS) systems that guarantee data confidentiality\nunder zero-trust cloud conditions.", "AI": {"tldr": "The paper introduces a systematic framework for cloud-native homomorphic encryption (HE) workflows to enhance privacy-preserving machine learning inference, achieving 3.2x faster inference and 40% less memory usage.", "motivation": "Ensuring data confidentiality in ML inference on public clouds is challenging due to security risks. Homomorphic encryption (HE) allows secure computation on encrypted data but faces issues with scalability, complexity, and performance in cloud environments.", "method": "The framework integrates containerized HE modules with Kubernetes orchestration for elastic scaling and parallel computation. It also employs optimization techniques such as ciphertext packing, polynomial modulus adjustment, and operator fusion to improve efficiency.", "result": "Experimental results show the framework provides 3.2 times faster inference and 40% lower memory consumption compared to traditional HE setups in cloud-native pipelines.", "conclusion": "The framework offers a practical solution for deploying secure and efficient ML-as-a-Service (MLaaS) under zero-trust cloud conditions, addressing the scalability and performance limitations of HE."}}
{"id": "2510.24358", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24358", "abs": "https://arxiv.org/abs/2510.24358", "authors": ["Lingyue Fu", "Bolun Zhang", "Hao Guan", "Yaoming Zhu", "Lin Qiu", "Weiwen Liu", "Xuezhi Cao", "Xunliang Cai", "Weinan Zhang", "Yong Yu"], "title": "Automatically Benchmarking LLM Code Agents through Agent-Driven Annotation and Evaluation", "comment": null, "summary": "Recent advances in code agents have enabled automated software development at\nthe project level, supported by large language models (LLMs) and widely adopted\ntools. However, existing benchmarks for code agent evaluation face two major\nlimitations: high annotation cost and expertise requirements, and rigid\nevaluation metrics that rely primarily on unit tests. To address these\nchallenges, we propose an agent-driven benchmark construction pipeline that\nleverages human supervision to efficiently generate diverse and challenging\nproject-level tasks. Based on this approach, we introduce PRDBench, a novel\nbenchmark comprising 50 real-world Python projects across 20 domains, each with\nstructured Product Requirement Document (PRD) requirements, comprehensive\nevaluation criteria, and reference implementations. PRDBench features rich data\nsources, high task complexity, and flexible metrics. We further employ an\nAgent-as-a-Judge paradigm to score agent outputs, enabling the evaluation of\nvarious test types beyond unit tests. Extensive experiments on PRDBench\ndemonstrate its effectiveness in assessing the capabilities of both code agents\nand evaluation agents, providing a scalable and robust framework for annotation\nand evaluation.", "AI": {"tldr": "The paper introduces PRDBench, a novel agent-driven benchmark for evaluating code agents with real-world project tasks.", "motivation": "Existing benchmarks for code agents have high annotation costs, expertise requirements, and rigid unit-test-based evaluation metrics, limiting effective project-level assessment.", "method": "Proposes an agent-driven pipeline with human supervision to generate diverse project-level tasks, creates PRDBench with 50 Python projects across 20 domains including PRD requirements and reference implementations, and employs an Agent-as-a-Judge paradigm for flexible metric evaluation.", "result": "Experiments demonstrate PRDBench effectively evaluates code agents' capabilities and provides a scalable framework for annotation and assessment across various test types.", "conclusion": "PRDBench addresses current benchmark limitations by offering rich data sources, high task complexity, and flexible metrics, enabling robust evaluation of code agents and evaluation agents in real-world scenarios."}}
{"id": "2510.24367", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24367", "abs": "https://arxiv.org/abs/2510.24367", "authors": ["Junda He", "Jieke Shi", "Terry Yue Zhuo", "Christoph Treude", "Jiamou Sun", "Zhenchang Xing", "Xiaoning Du", "David Lo"], "title": "LLM-as-a-Judge for Software Engineering: Literature Review, Vision, and the Road Ahead", "comment": null, "summary": "The rapid integration of Large Language Models (LLMs) into software\nengineering (SE) has revolutionized tasks like code generation, producing a\nmassive volume of software artifacts. This surge has exposed a critical\nbottleneck: the lack of scalable, reliable methods to evaluate these outputs.\nHuman evaluation is costly and time-consuming, while traditional automated\nmetrics like BLEU fail to capture nuanced quality aspects. In response, the\nLLM-as-a-Judge paradigm - using LLMs for automated evaluation - has emerged.\nThis approach leverages the advanced reasoning of LLMs, offering a path toward\nhuman-like nuance at automated scale. However, LLM-as-a-Judge research in SE is\nstill in its early stages. This forward-looking SE 2030 paper aims to steer the\ncommunity toward advancing LLM-as-a-Judge for evaluating LLM-generated software\nartifacts. We provide a literature review of existing SE studies, analyze their\nlimitations, identify key research gaps, and outline a detailed roadmap. We\nenvision these frameworks as reliable, robust, and scalable human surrogates\ncapable of consistent, multi-faceted artifact evaluation by 2030. Our work aims\nto foster research and adoption of LLM-as-a-Judge frameworks, ultimately\nimproving the scalability of software artifact evaluation.", "AI": {"tldr": "This paper addresses the scalability challenge in evaluating LLM-generated software artifacts, proposes LLM-as-a-Judge as a solution, and outlines a 2030 roadmap for developing reliable automated evaluation frameworks.", "motivation": "The surge in LLM-generated software artifacts requires scalable evaluation methods; human evaluation is impractical, and traditional metrics like BLEU lack nuance. LLM-as-a-Judge could bridge this gap but lacks systematic research in SE contexts.", "method": "Literature review of existing SE studies on LLM-as-a-Judge, analysis of limitations, and identification of research gaps to build a structured advancement roadmap.", "result": "Highlights key gaps (e.g., robustness, scalability, multi-dimensional evaluation criteria), validates LLM-as-a-Judge's potential, and proposes a vision for 2030 frameworks meeting human-like standards.", "conclusion": "LLM-as-a-Judge frameworks could revolutionize software artifact evaluation by 2030 through community-driven research focused on reliability, robustness, and multi-faceted automation."}}
{"id": "2510.24428", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24428", "abs": "https://arxiv.org/abs/2510.24428", "authors": ["Nguyen Hoang Anh", "Minh Le-Anh", "Bach Le", "Nghi D. Q. Bui"], "title": "CodeWiki: Automated Repository-Level Documentation at Scale", "comment": null, "summary": "Developers spend nearly 58% of their time understanding codebases, yet\nmaintaining comprehensive documentation remains challenging due to complexity\nand manual effort. While recent Large Language Models (LLMs) show promise for\nfunction-level documentation, they fail at the repository level, where\ncapturing architectural patterns and cross-module interactions is essential. We\nintroduce CodeWiki, the first open-source framework for holistic\nrepository-level documentation across seven programming languages. CodeWiki\nemploys three innovations: (i) hierarchical decomposition that preserves\narchitectural context, (ii) recursive agentic processing with dynamic\ndelegation, and (iii) synthesis of textual and visual artifacts including\narchitecture diagrams and data flows. We also present CodeWikiBench, the first\nrepository-level documentation benchmark with multi-level rubrics and agentic\nassessment. CodeWiki achieves 68.79% quality score with proprietary models and\n64.80% with open-source alternatives, outperforming existing closed-source\nsystems and demonstrating scalable, accurate documentation for real-world\nrepositories.", "AI": {"tldr": "CodeWiki is an open-source framework addressing repository-level documentation via three innovations, achieving high quality scores with proprietary and open-source LLMs while introducing a benchmark for evaluation.", "motivation": "Developers spend 58% of time understanding codebases; existing LLM tools fail at capturing repository-level architectural context and cross-module interactions due to complexity and manual effort.", "method": "CodeWiki employs (i) hierarchical decomposition for architectural context, (ii) recursive agentic processing with delegation, and (iii) synthesis of textual/visual artifacts. CodeWikiBench includes multi-level rubrics and agentic assessment.", "result": "Achieved 68.79% quality score with proprietary models and 64.80% with open-source models, outperforming closed-source systems across 7 languages and real-world repositories.", "conclusion": "CodeWiki is the first open-source framework to enable scalable, accurate repository-level documentation, demonstrating effectiveness through architectural analysis and benchmarking."}}
{"id": "2510.24483", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24483", "abs": "https://arxiv.org/abs/2510.24483", "authors": ["Michele Lanza"], "title": "The Divine Software Engineering Comedy -- Inferno: The Okinawa Files", "comment": null, "summary": "In June 2024 I co-organized the FUture of Software Engineering symposium in\nOkinawa, Japan. Me, Andrian Marcus, Takashi Kobayashi and Shinpei Hayashi were\ngeneral chairs, Nicole Novielli, Kevin Moran, Yutaro Kashiwa and Masanari Kondo\nwere program chairs, some members of my group, Carmen Armenti, Stefano\nCampanella, Roberto Minelli, were the tables, can't have a room with only\nchairs, after all. We invited a crowd of people to discuss what future software\nengineering has. FUSE became a 3-day marathon on whether there is actually a\nfuture at all for SE. This essay is a slightly dark take about what I saw at\nthat event, very loosely based on the discussions that took place, adding some\nhealthy sarcasm and cynicism, the intellectual salt and pepper I never seem to\nrun out of. I listened to the brilliant people who gathered to talk about where\nwe're headed, and distilled three nightmares headed in our direction: software\nmakers who don't know what they're doing, but get the job done anyway, a field\nmoving so fast it can't remember its own lessons, and technologies multiplying\nlike rabbits in Spring. So, let's start. The future, eh? The future of software\nengineering looks like a car crash in slow motion: you can see it coming but\nyou can't look away. The thing is...", "AI": {"tldr": "The paper is a reflective essay by an author who co-organized the FUSE symposium in June 2024. The author presents a critical and satirical perspective on the future of software engineering (SE), identifying three main challenges or 'nightmares' for the field.", "motivation": "The essay is motivated by the author's personal experience and observations during the FUSE symposium, where industry and academic experts engaged in discussions about the trajectory of software engineering. The author's critical viewpoint likely stems from a desire to highlight real concerns and issues within the field.", "method": "The author's method involves interpreting the content and opinions expressed at the symposium, combined with their own insight and critical thinking. The style is subjective and ironic, distilling complex ideas into an accessible narrative.", "result": "The main result is a critical analysis that identifies three primary issues facing the future of software engineering: 1) inexperienced software developers managing complex tasks. 2) A rapidly evolving field that forgets its lessons. 3) An uncontrolled proliferation of technologies.", "conclusion": "The author posits a bleak outlook for software engineering's future, emphasizing these critical challenges and the lack of a clear vision or solution for their resolution. The use of humor and cynicism underscores these serious points."}}
