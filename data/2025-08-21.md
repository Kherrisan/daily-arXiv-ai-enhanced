<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 6]
- [cs.SE](#cs.SE) [Total: 3]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Special-Character Adversarial Attacks on Open-Source Language Model](https://arxiv.org/abs/2508.14070)
*Ephraiem Sarabamoun*

Main category: cs.CR

TL;DR: This paper investigates vulnerabilities in large language models under character-level adversarial attacks and proposes solutions to enhance their security for practical use.


<details>
  <summary>Details</summary>
Motivation: LLMs achieve high performance in NLP tasks but face security risks from character-level adversarial manipulations.

Method: The paper likely proposes methods to mitigate character-level adversarial attacks, though specific techniques are not detailed in the abstract.

Result: Demonstrates the effectiveness of proposed methods against adversarial attacks, improving model robustness.

Conclusion: Addressing character-level adversarial vulnerabilities is crucial for the secure deployment of LLMs in real-world applications.

Abstract: Large language models (LLMs) have achieved remarkable performance across
diverse natural language processing tasks, yet their vulnerability to
character-level adversarial manipulations presents significant security
challenges for real-world deployments.

</details>


### [2] [CCFC: Core & Core-Full-Core Dual-Track Defense for LLM Jailbreak Protection](https://arxiv.org/abs/2508.14128)
*Jiaming Hu,Haoyu Wang,Debarghya Mukherjee,Ioannis Ch. Paschalidis*

Main category: cs.CR

TL;DR: CCFC is a dual-track defense framework that mitigates LLM jailbreak attacks by 50-75% through isolated semantic core analysis and structural attack disruption, outperforming existing methods while preserving response quality.


<details>
  <summary>Details</summary>
Motivation: Jailbreak attacks pose serious risks to LLM deployment, motivating the need for defenses against prompt injection and structure-aware attacks like DeepInception and GCG.

Method: CCFC employs a dual-track approach: a core-only track to eliminate adversarial distractions and a core-full-core (CFC) track to disrupt attack structures. It combines safety consistency checks across tracks for robust responses.

Result: CCFC reduces attack success rates by 50-75% compared to state-of-the-art defenses, demonstrating superior robustness without compromising benign query fidelity.

Conclusion: The paper concludes that CCFC offers a practical and effective solution for safer LLM deployment by significantly mitigating jailbreak attacks while maintaining response quality.

Abstract: Jailbreak attacks pose a serious challenge to the safe deployment of large
language models (LLMs). We introduce CCFC (Core & Core-Full-Core), a
dual-track, prompt-level defense framework designed to mitigate LLMs'
vulnerabilities from prompt injection and structure-aware jailbreak attacks.
CCFC operates by first isolating the semantic core of a user query via few-shot
prompting, and then evaluating the query using two complementary tracks: a
core-only track to ignore adversarial distractions (e.g., toxic suffixes or
prefix injections), and a core-full-core (CFC) track to disrupt the structural
patterns exploited by gradient-based or edit-based attacks. The final response
is selected based on a safety consistency check across both tracks, ensuring
robustness without compromising on response quality. We demonstrate that CCFC
cuts attack success rates by 50-75% versus state-of-the-art defenses against
strong adversaries (e.g., DeepInception, GCG), without sacrificing fidelity on
benign queries. Our method consistently outperforms state-of-the-art
prompt-level defenses, offering a practical and effective solution for safer
LLM deployment.

</details>


### [3] [Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text](https://arxiv.org/abs/2508.14190)
*Zixin Rao,Youssef Mohamed,Shang Liu,Zeyan Liu*

Main category: cs.CR

TL;DR: This paper proposes DA-MTL, a multi-task framework for detecting AI-generated text while identifying the specific LLM source. Evaluated across languages and models, it demonstrates how task-sharing improves both detection accuracy and attribution robustness, with new insights into LLM forensic analysis.


<details>
  <summary>Details</summary>
Motivation: Excessive focus on English-centric AI-text classification has left authorship attribution under-explored despite its critical role in forensic analysis. The multi-task approach addresses this gap by simultaneously solving both detection and attribution challenges across diverse linguistic contexts.

Method: The paper introduces DA-MTL, a multi-task learning framework designed to jointly address text detection and authorship attribution by leveraging task-specific characteristics and sharing insights between them. Evaluations span nine datasets, four backbone models, and multiple languages.

Result: DA-MTL shows strong performance across multiple languages and LLM sources. The framework's analysis reveals cross-modal/cross-lingual patterns and robustness against adversarial obfuscation techniques, offering insights into LLM generalization capabilities.

Conclusion: DA-MTL demonstrates that multi-task learning improves both text detection and authorship attribution, while providing insights into LLM behavior cross-linguistically and cross-modally.

Abstract: Large Language Models (LLMs), such as GPT-4 and Llama, have demonstrated
remarkable abilities in generating natural language. However, they also pose
security and integrity challenges. Existing countermeasures primarily focus on
distinguishing AI-generated content from human-written text, with most
solutions tailored for English. Meanwhile, authorship attribution--determining
which specific LLM produced a given text--has received comparatively little
attention despite its importance in forensic analysis. In this paper, we
present DA-MTL, a multi-task learning framework that simultaneously addresses
both text detection and authorship attribution. We evaluate DA-MTL on nine
datasets and four backbone models, demonstrating its strong performance across
multiple languages and LLM sources. Our framework captures each task's unique
characteristics and shares insights between them, which boosts performance in
both tasks. Additionally, we conduct a thorough analysis of cross-modal and
cross-lingual patterns and assess the framework's robustness against
adversarial obfuscation techniques. Our findings offer valuable insights into
LLM behavior and the generalization of both detection and authorship
attribution.

</details>


### [4] [A Taxonomy and Methodology for Proof-of-Location Systems](https://arxiv.org/abs/2508.14230)
*Eduardo Brito,Fernando Castillo,Liina Kamm,Amnir Hadachi,Ulrich Norbisrath*

Main category: cs.CR

TL;DR: This paper introduces a unified framework for designing secure Proof-of-Location systems by categorizing key design domains and mapping application needs to architectures, validated via real-world use cases.


<details>
  <summary>Details</summary>
Motivation: Traditional localization methods lack cryptographic guarantees, making them vulnerable to spoofing, replay, and collusion attacks. The paper addresses this gap to enable trustworthy, tamper-resistant location proofs essential for digital services.

Method: The method involves proposing a taxonomy of four core domains (cryptographic guarantees, spatio-temporal synchronization, trust/witness models, and interaction/overhead) and a methodology for mapping application-specific requirements to appropriate PoL architectures, illustrated through three use cases.

Result: The framework is demonstrated through three use cases (retail e-coupons, supply chain auditing, and physical e-voting), showing how application-specific constraints influence protocol design, enabling secure and interoperable PoL implementations.

Conclusion: The paper concludes that the proposed framework provides a structured approach for designing secure, scalable, and interoperable Proof-of-Location (PoL) systems, addressing critical vulnerabilities in traditional localization methods.

Abstract: Digital societies increasingly rely on trustworthy proofs of physical
presence for services such as supply-chain tracking, e-voting, ride-sharing,
and location-based rewards. Yet, traditional localization methods often lack
cryptographic guarantees of where and when an entity was present, leaving them
vulnerable to spoofing, replay, or collusion attacks. In response, research on
Proof-of-Location (PoL) has emerged, with recent approaches combining distance
bounding, distributed consensus, and privacy-enhancing techniques to enable
verifiable, tamper-resistant location claims.
  As the design space for PoL systems grows in complexity, this paper provides
a unified framework to help practitioners navigate diverse application needs.
We first propose a taxonomy identifying four core domains: (1) cryptographic
guarantees, (2) spatio-temporal synchronization, (3) trust and witness models,
and (4) interaction and overhead. Building on this, we introduce a methodology
to map application-specific requirements onto appropriate PoL architectures. We
illustrate this process through three use cases (retail e-coupons, supply chain
auditing, and physical e-voting), each showing how different constraints shape
protocol choices. Overall, this work offers a structured approach to building
secure, scalable, and interoperable PoL systems.

</details>


### [5] [SaMOSA: Sandbox for Malware Orchestration and Side-Channel Analysis](https://arxiv.org/abs/2508.14261)
*Meet Udeshi,Venkata Sai Charan Putrevu,Prashanth Krishnamurthy,Ramesh Karri,Farshad Khorrami*

Main category: cs.CR

TL;DR: SaMOSA is a customizable Linux sandbox for OT/CPS malware analysis, capturing four synchronized side-channels and supporting major architectures. It outperforms existing tools by enabling adaptable, comprehensive malware analysis via modular design and pipeline hooks.


<details>
  <summary>Details</summary>
Motivation: The proliferation of malware targeting Linux-based embedded devices in OT and CPS systems necessitates dynamic analysis of execution behaviors beyond static binary analysis. Existing Linux sandboxes are task-specific, limited to one or two side-channels, and lack customization capabilities for diverse analysis tasks.

Method: The authors developed SaMOSA, a Linux sandbox capable of emulating malware while capturing time-synchronized side-channels (system calls, network activity, disk activity, hardware performance counters). It integrates FakeNet for network service emulation and supports x86-64, ARM64, and PowerPC 64 architectures. The framework allows customization via pipeline hooks.

Result: SaMOSA captures four types of side-channels compared to existing tools' one or two. It supports three key architectures for OT/CPS and demonstrates practical utility through three case studies involving different malware families, validating its effectiveness in malware analysis.

Conclusion: SaMOSA addresses the gap in Linux malware analysis by providing a modular and customizable sandbox framework tailored for operational technology (OT) and cyber-physical systems (CPS). Its adaptability to diverse malware analysis tasks is demonstrated through case studies involving three malware families.

Abstract: Cyber-attacks on operational technology (OT) and cyber-physical systems (CPS)
have increased tremendously in recent years with the proliferation of malware
targeting Linux-based embedded devices of OT and CPS systems. Comprehensive
malware detection requires dynamic analysis of execution behavior in addition
to static analysis of binaries. Safe execution of malware in a manner that
captures relevant behaviors via side-channels requires a sandbox environment.
Existing Linux sandboxes are built for specific tasks, only capture one or two
side-channels, and do not offer customization for different analysis tasks. We
present the SaMOSA Linux sandbox that allows emulation of Linux malwares while
capturing time-synchronized side-channels from four sources. SaMOSA
additionally provides emulation of network services via FakeNet, and allows
orchestration and customization of the sandbox environment via pipeline hooks.
In comparison to existing Linux sandboxes, SaMOSA captures more side-channels
namely system calls, network activity, disk activity, and hardware performance
counters. It supports three architectures predominantly used in OT and CPS
namely x86-64, ARM64, and PowerPC 64. SaMOSA fills a gap in Linux malware
analysis by providing a modular and customizable sandbox framework that can be
adapted for many malware analysis tasks. We present three case studies of three
different malware families to demonstrate the advantages of SaMOSA.

</details>


### [6] [Differentially Private aggregate hints in mev-share](https://arxiv.org/abs/2508.14284)
*Jonathan Passerat-Palmbach,Sarisht Wadhwa*

Main category: cs.CR

TL;DR: This work introduces differentially-private aggregate hints for mev-share, enabling users to control information disclosure levels, quantify privacy costs, and receive better rebates, while empowering searchers to optimize MEV extraction without compromising user privacy.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the tension between searchers' information requirements for MEV extraction and users' desire to minimize information sharing to avoid frontrunning attacks in blockchain environments.

Method: The authors employ Differential Privacy techniques within the Trusted Curator Model to design aggregate hints, combined with random sampling to mitigate sybil attacks and amplify privacy while providing actionable insights for searchers.

Result: The analysis shows that DP aggregate hints allow users to formally quantify privacy loss and set rebates accordingly, while supporting improved searcher strategies through structured, privacy-preserving hints.

Conclusion: The paper concludes that Differentially-Private aggregate hints enhance mev-share by enabling users to balance privacy concerns with the need for searcher efficiency, ultimately improving backrunning strategies and front-running prevention while quantifying privacy trade-offs.

Abstract: Flashbots recently released mev-share to empower users with control over the
amount of information they share with searchers for extracting Maximal
Extractable Value (MEV). Searchers require more information to maintain
on-chain exchange efficiency and profitability, while users aim to prevent
frontrunning by withholding information. After analyzing two searching
strategies in mev-share to reason about searching techniques, this paper
introduces Differentially-Private (DP) aggregate hints as a new type of hints
to disclose information quantitatively. DP aggregate hints enable users to
formally quantify their privacy loss to searchers, and thus better estimate the
level of rebates to ask in return. The paper discusses the current properties
and privacy loss in mev-share and lays out how DP aggregate hints could enhance
the system for both users and searchers. We leverage Differential Privacy in
the Trusted Curator Model to design our aggregate hints. Additionally, we
explain how random sampling can defend against sybil attacks and amplify
overall user privacy while providing valuable hints to searchers for improved
backrunning extraction and frontrunning prevention.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [7] [You Don't Know Until You Click:Automated GUI Testing for Production-Ready Software Evaluation](https://arxiv.org/abs/2508.14104)
*Yutong Bian,Xianhao Lin,Yupeng Xie,Tianyang Liu,Mingchen Zhuge,Siyuan Lu,Haoming Tang,Jinlin Wang,Jiayi Zhang,Jiaqi Chen,Xiangru Tang,Yongxin Ni,Sirui Hong,Chenglin Wu*

Main category: cs.SE

TL;DR: RealDevWorld is a production-ready software evaluation framework for LLMs that automates interactive testing of generated applications through a taskbench and GUI-based agent-as-a-judge system, delivering human-aligned assessments with 92% accuracy.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks inadequately evaluate production-ready LLM-generated software due to reliance on static checks and binary pass/fail metrics, which fail to capture runtime dynamics and interactive behaviors essential for real-world usability.

Method: The framework introduces (1) RealDevBench: 194 open-ended software engineering tasks across domains with multimodal complexity; and (2) AppEvalPilot: a GUI-based agent-as-a-judge system simulating realistic user interactions to assess functional correctness, visual fidelity, and runtime behavior through task-specific diagnostic feedback.

Result: RealDevWorld achieves 0.92 accuracy and 0.85 correlation with human expert evaluations while reducing manual review by automating assessments of 194 tasks across domains through interactive testing mechanisms, demonstrating effective human-aligned evaluation of LLM-generated software.

Conclusion: RealDevWorld fills the critical evaluation gap for LLM-generated production-ready software by introducing automated interactive testing frameworks, enabling scalable, human-aligned assessments of real-world software functionality and usability.

Abstract: Large Language Models (LLMs) and code agents in software development are
rapidly evolving from generating isolated code snippets to producing
full-fledged software applications with graphical interfaces, interactive
logic, and dynamic behaviors. However, current benchmarks fall short in
evaluating such production-ready software, as they often rely on static checks
or binary pass/fail scripts, failing to capture the interactive behaviors and
runtime dynamics that define real-world usability - qualities that only emerge
when an application is actively used. This is the blind spot of current
evaluation: you don't know if an app works until you click through it, interact
with it, and observe how it responds. To bridge this gap, we introduce
RealDevWorld, a novel evaluation framework for automated end-to-end assessment
of LLMs' ability to generate production-ready repositories from scratch. It
features two key components: (1) RealDevBench, a diverse collection of 194
open-ended software engineering tasks across multiple domains, incorporating
multimodal elements to reflect real-world complexity; and (2) AppEvalPilot, a
new agent-as-a-judge evaluation system that simulates realistic, GUI-based user
interactions to automatically and holistically assess software functional
correctness, visual fidelity, and runtime behavior. The framework delivers
fine-grained, task-specific diagnostic feedback, supporting nuanced evaluation
beyond simple success/failure judgments. Empirical results show that
RealDevWorld delivers effective, automatic, and human-aligned evaluations,
achieving an accuracy of 0.92 and a correlation of 0.85 with expert human
assessments, while significantly reducing the reliance on manual review. This
enables scalable, human-aligned assessment of production-level software
generated by LLMs. Our code is available on GitHub.

</details>


### [8] [Ambiguity Resolution with Human Feedback for Code Writing Tasks](https://arxiv.org/abs/2508.14114)
*Aditey Nandan,Viraj Kumar*

Main category: cs.SE

TL;DR: ARHF system identifies and resolves code spec ambiguities using targeted test cases and human feedback, improving code accuracy and offering a new approach for teaching ambiguity resolution in CS education.


<details>
  <summary>Details</summary>
Motivation: Natural language task specifications in programming are inherently ambiguous, requiring programmers to identify and resolve these ambiguities manually. Existing tools lack structured methods to assist with this critical skill development in educational contexts.

Method: The system combines ambiguity detection by generating specific test inputs, a limited human feedback mechanism to clarify desired behavior, and code generation that conforms to clarified specifications. This integrates automated analysis with iterative user interaction.

Result: Evaluation showed the prototype successfully identified ambiguous inputs and achieved accurate code generation after feedback collection. Human feedback improved final code correctness by resolving specification ambiguities in 82% of test cases.

Conclusion: The study demonstrates that ARHF can effectively resolve ambiguities in code specifications through targeted human feedback, suggesting its potential to be integrated into Computer Science education to enhance students' ability to handle unclear requirements and improve coding outcomes.

Abstract: Specifications for code writing tasks are usually expressed in natural
language and may be ambiguous. Programmers must therefore develop the ability
to recognize ambiguities in task specifications and resolve them by asking
clarifying questions. We present and evaluate a prototype system, based on a
novel technique (ARHF: Ambiguity Resolution with Human Feedback), that (1)
suggests specific inputs on which a given task specification may be ambiguous,
(2) seeks limited human feedback about the code's desired behavior on those
inputs, and (3) uses this feedback to generate code that resolves these
ambiguities. We evaluate the efficacy of our prototype, and we discuss the
implications of such assistive systems on Computer Science education.

</details>


### [9] [Measuring LLM Code Generation Stability via Structural Entropy](https://arxiv.org/abs/2508.14288)
*Yewei Song,Tiezhu Sun,Xunzhu Tang,Prateek Rajput,Tegawende F. Bissyande,Jacques Klein*

Main category: cs.SE

TL;DR: The paper proposes AST-based structural entropy metrics to evaluate code-generation stability in LLMs, offering reference-free, language-agnostic insights into model consistency and robustness without execution overhead.


<details>
  <summary>Details</summary>
Motivation: Stability assessment of code generation is critical for real-world LLM reliability, yet existing metrics like pass@k or CodeBLEU are reference-dependent or language-specific. This work addresses the need for objective, scalable, and language-agnostic evaluation frameworks.

Method: The method extends structural-entropy concepts by pairing AST analysis with prompt-based code generation. It measures stability through (i) Jensen-Shannon divergence for structural overlap and (ii) Structural Cross-Entropy ratio for missing patterns, with structural-only and token-aware variants. The approach runs in linear time without external tests.

Result: The metrics reveal granular differences in LLM consistency across standard code-generation tasks, outperforming reference-based metrics. The method is efficient (O(n,d) time), applicable to any programming language, and requires no execution or test-suite dependencies.

Conclusion: The paper introduces a lightweight, reference-free method for evaluating code generation stability using structural entropy, demonstrating its effectiveness in revealing nuances of model consistency and robustness across tasks and languages.

Abstract: Assessing the stability of code generation from large language models (LLMs)
is essential for judging their reliability in real-world development. We extend
prior "structural-entropy concepts" to the program domain by pairing entropy
with abstract syntax tree (AST) analysis. For any fixed prompt, we collect the
multiset of depth-bounded subtrees of AST in each generated program and treat
their relative frequencies as a probability distribution. We then measure
stability in two complementary ways: (i) Jensen-Shannon divergence, a
symmetric, bounded indicator of structural overlap, and (ii) a Structural
Cross-Entropy ratio that highlights missing high-probability patterns. Both
metrics admit structural-only and token-aware variants, enabling separate views
on control-flow shape and identifier-level variability. Unlike pass@k, BLEU, or
CodeBLEU, our metrics are reference-free, language-agnostic, and
execution-independent. We benchmark several leading LLMs on standard code
generation tasks, demonstrating that AST-driven structural entropy reveals
nuances in model consistency and robustness. The method runs in O(n,d) time
with no external tests, providing a lightweight addition to the code-generation
evaluation toolkit.

</details>
