<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 24]
- [cs.SE](#cs.SE) [Total: 18]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Noisy Networks, Nosy Neighbors: Inferring Privacy Invasive Information from Encrypted Wireless Traffic](https://arxiv.org/abs/2510.13822)
*Bartosz Burgiel*

Main category: cs.CR

TL;DR: This paper shows encrypted smart home network traffic can reveal private behaviors and spatial layouts, exposing new privacy risks from passive observation by neighbors.


<details>
  <summary>Details</summary>
Motivation: To demonstrate how encrypted wireless traffic in smart homes can be exploited to infer sensitive information about inhabitants' behaviors and living spaces, even without data breaches.

Method: The study uses a setup mimicking a nosy neighbor's capabilities, analyzing 802.11 packets and Bluetooth Low Energy advertisements to identify devices, infer activity states, and approximate locations via RSSI-based trilateration.

Result: The research successfully detected multimedia device activity periods, inferred daily activities (sleeping, working, media consumption), and approximated the layout of a neighbor's apartment using solely encrypted network traffic.

Conclusion: Privacy risks in smart homes extend beyond traditional data breaches, as passive observation by a nosy neighbor can reveal privacy-invasive insights through encrypted network traffic.

Abstract: This thesis explores the extent to which passive observation of wireless
traffic in a smart home environment can be used to infer privacy-invasive
information about its inhabitants. Using a setup that mimics the capabilities
of a nosy neighbor in an adjacent flat, we analyze raw 802.11 packets and
Bluetooth Low Energy advertisemets. From this data, we identify devices, infer
their activity states and approximate their location using RSSI-based
trilateration. Despite the encrypted nature of the data, we demonstrate that it
is possible to detect active periods of multimedia devices, infer common
activities such as sleeping, working and consuming media, and even approximate
the layout of the neighbor's apartment. Our results show that privacy risks in
smart homes extend beyond traditional data breaches: a nosy neighbor behind the
wall can gain privacy-invasive insights into the lives of their neighbors
purely from encrypted network traffic.

</details>


### [2] [Multi-Layer Secret Sharing for Cross-Layer Attack Defense in 5G Networks: a COTS UE Demonstration](https://arxiv.org/abs/2510.13824)
*Wai Ming Chan,Remi Chou,Taejoon Kim*

Main category: cs.CR

TL;DR: The paper introduces the first implementation of multi-layer secret sharing on COTS 5G UE, ensuring perfect recovery and confidentiality even if one network operator and one relay are lost.


<details>
  <summary>Details</summary>
Motivation: The paper may be motivated by the need for secure data transmission in 5G networks without the complexities of infrastructure modification or pre-shared keys.

Method: The method is based on an XOR-based approach to distribute secret shares across different network operators and distributed relays. This allows for data confidentiality and perfect recovery despite potential loss of either a network operator or a relay, allowing for a robust communication system that can withstand DoS or unexpected attacks.

Result: The authors have successfully demonstrated the first real-world implementation of multi-layer secret sharing on COTS 5G user equipment. The approach ensures data confidentiality and allows perfect recovery even if both a network operator and a relay are simultaneously lost, which is a major concern in 5G communication security.

Conclusion: The authors have outlined a novel approach for secure, reliable multi-layer secret sharing for COTS 5G devices. This method does away with the traditional overheads of infrastructure change or prior key sharing and achieves data confidentiality while allowing perfect secret recovery under conditions of inconsiderable system component loss.

Abstract: This demo presents the first implementation of multi-layer secret sharing on
commercial-off-the-shelf (COTS) 5G user equipment (UE), operating without
infrastructure modifications or pre-shared keys. Our XOR-based approach
distributes secret shares across network operators and distributed relays,
ensuring perfect recovery and data confidentiality even if one network operator
and one relay are simultaneously lost (e.g., under denial of service (DoS) or
unanticipated attacks).

</details>


### [3] [A2AS: Agentic AI Runtime Security and Self-Defense](https://arxiv.org/abs/2510.13825)
*Eugene Neelou,Ivan Novikov,Max Moroz,Om Narayan,Tiffany Saade,Mika Ayenson,Ilya Kabanov,Jen Ozmen,Edward Lee,Vineeth Sai Narajala,Emmanuel Guilherme Junior,Ken Huang,Huseyin Gulsin,Jason Ross,Marat Vyshegorodtsev,Adelin Travers,Idan Habler,Rahul Jadav*

Main category: cs.CR

TL;DR: A2AS: HTTPS-like security for AI/LLMs, using the BASIC model to certify behavior, authenticate contexts, and enforce policies, enabling secure, scalable deployment without latency or complexity.


<details>
  <summary>Details</summary>
Motivation: The work addresses the need for a standardized security layer (like HTTPS for HTTP) to protect AI/LLM systems from adversarial attacks, ensure context integrity, and enforce secure behavior while avoiding model retraining and external dependencies.

Method: A2AS employs the BASIC model (Behavior certificates, Authenticated prompts, Security boundaries, In-context defenses, Codified policies) to enforce certified behavior, authenticate context, isolate risks, and apply application-specific rules without introducing latency, architectural changes, or operational overhead.

Result: The BASIC model and A2AS framework are formally defined, demonstrating their potential to establish a secure, scalable, and interoperable approach to AI and LLM security, positioning A2AS as a candidate industry standard.

Conclusion: The paper introduces the A2AS framework and BASIC security model as a foundational step toward establishing an industry standard for securing AI agents and LLM-powered applications, achieving defense-in-depth without compromising efficiency.

Abstract: The A2AS framework is introduced as a security layer for AI agents and
LLM-powered applications, similar to how HTTPS secures HTTP. A2AS enforces
certified behavior, activates model self-defense, and ensures context window
integrity. It defines security boundaries, authenticates prompts, applies
security rules and custom policies, and controls agentic behavior, enabling a
defense-in-depth strategy. The A2AS framework avoids latency overhead, external
dependencies, architectural changes, model retraining, and operational
complexity. The BASIC security model is introduced as the A2AS foundation: (B)
Behavior certificates enable behavior enforcement, (A) Authenticated prompts
enable context window integrity, (S) Security boundaries enable untrusted input
isolation, (I) In-context defenses enable secure model reasoning, (C) Codified
policies enable application-specific rules. This first paper in the series
introduces the BASIC security model and the A2AS framework, exploring their
potential toward establishing the A2AS industry standard.

</details>


### [4] [PIShield: Detecting Prompt Injection Attacks via Intrinsic LLM Features](https://arxiv.org/abs/2510.14005)
*Wei Zou,Yupei Liu,Yanting Wang,Ying Chen,Neil Gong,Jinyuan Jia*

Main category: cs.CR

TL;DR: PIShield detects prompt injections by analyzing LLM internal features from a critical layer, offering a computationally efficient yet powerful solution to secure LLM applications.


<details>
  <summary>Details</summary>
Motivation: Existing prompt injection detection methods suffer from suboptimal accuracy or high computational costs. Applications relying on LLMs require robust yet efficient solutions to prevent security breaches via poisoned inputs.

Method: Trains a linear classifier on internal representations of the final token from a pre-specified 'injection-critical' layer within LLMs, using labeled clean/contaminated prompt datasets to detect manipulations.

Result: PIShield outperforms 11 baselines across 5 datasets and 8 attack types, demonstrating high effectiveness, efficiency, and resistance to sophisticated adaptive attacks.

Conclusion: PIShield effectively addresses the limitations of existing prompt injection detection methods by leveraging internal representations from the injection-critical layer, achieving superior performance and efficiency while resisting adaptive attacks.

Abstract: LLM-integrated applications are vulnerable to prompt injection attacks, where
an attacker contaminates the input to inject malicious prompts, causing the LLM
to follow the attacker's intent instead of the original user's. Existing prompt
injection detection methods often have sub-optimal performance and/or high
computational overhead. In this work, we propose PIShield, a detection method
that is both effective and efficient. Our key observation is that the internal
representation of the final token in a prompt-extracted from a specific layer
of the LLM, which we term the injection-critical layer-captures distinguishing
features between clean and contaminated prompts. Leveraging this insight, we
train a simple linear classifier on these internal representations using a
labeled set of clean and contaminated prompts. We compare PIShield against 11
baselines across 5 diverse benchmark datasets and 8 prompt injection attacks.
The results demonstrate that PIShield is both highly effective and efficient,
substantially outperforming existing methods. Additionally, we show that
PIShield resists strong adaptive attacks.

</details>


### [5] [Quantitative Analysis of UAV Intrusion Mitigation for Border Security in 5G with LEO Backhaul Impairments](https://arxiv.org/abs/2510.14066)
*Rajendra Upadhyay,Al Nahian Bin Emran,Rajendra Paudyal,Lisa Donnan,Duminda Wijesekera*

Main category: cs.CR

TL;DR: This paper analyzes the detection-to-mitigation latency in a hybrid terrestrial-LEO satellite 5G system for rogue UAVs by modeling a simulation framework with satellite backhaul outages and fallback mechanisms.


<details>
  <summary>Details</summary>
Motivation: Uncooperative UAVs exploit cellular networks to disrupt infrastructure and border security. Hybrid 5G systems with satellite backhaul introduce risks due to stochastic outages and mitigation delays, necessitating strategies to ensure timely responses.

Method: An end-to-end simulation framework models terrestrial gNBs, LEO satellite backhaul with stochastic outages, and a detection-mitigation pipeline. Detection logic uses handover instability and signal variance; mitigation includes a lockdown with optional local fallback. Monte Carlo simulations evaluate UAV altitudes, speeds, and outage rates.

Result: Satellite outages cause long delays unless bounded by fallbacks; handover instability has negligible impact. Fallback mechanisms reduce mitigation delays, allowing patrol UEs to maintain terrestrial-like handover rates. Without fallback, outages prolong UAV presence in restricted areas, amplifying security risks.

Conclusion: Local fallback mechanisms are critical for robust mitigation of rogue UAVs in hybrid terrestrial-terrestrial 5G systems. Complementing non-terrestrial links with local control ensures timely responses to uncooperative UAV intrusions.

Abstract: Uncooperative unmanned aerial vehicles (UAVs) pose emerging threats to
critical infrastructure and border protection by operating as rogue user
equipment (UE) within cellular networks, consuming resources, creating
interference, and potentially violating restricted airspaces. This paper
presents minimal features of the operating space, yet an end-to-end simulation
framework to analyze detect-to-mitigate latency of such intrusions in a hybrid
terrestrial-non-terrestrial (LEO satellite) 5G system. The system model
includes terrestrial gNBs, satellite backhaul (with stochastic outages), and a
detection logic (triggered by handover instability and signal quality
variance). A lockdown mechanism is invoked upon detection, with optional local
fallback to cap mitigation delays. Monte Carlo sweeps across UAV altitudes,
speeds, and satellite outage rates yield several insights. First, satellite
backhaul outages can cause arbitrarily long mitigation delays, yet, to meet
fallback deadlines, they need to be effectively bounded. Second, while handover
instability was hypothesized, our results show that extra handovers have a
negligible effect within the range of parameters we considered. The main
benefit of resilience from fallback comes from the delay in limiting
mitigation. Third, patrol UEs experience negligible collateral impact, with
handover rates close to terrestrial baselines. Stress scenarios further
highlight that fallback is indispensable in preventing extreme control-plane
and physical security vulnerabilities: Without fallback, prolonged outages in
the satellite backhaul delay lockdown commands, allowing rogue UAVs to linger
inside restricted corridors for several seconds longer. These results
underscore the importance of complementing non-terrestrial links with local
control to ensure robust and timely response against uncooperative UAV
intrusions.

</details>


### [6] [Every Language Model Has a Forgery-Resistant Signature](https://arxiv.org/abs/2510.14086)
*Matthew Finlayson,Xiang Ren,Swabha Swayamdipta*

Main category: cs.CR

TL;DR: This paper introduces a novel forensic method for identifying language models via their geometric 'ellipse signature' output pattern, offering advantages over existing techniques.


<details>
  <summary>Details</summary>
Motivation: Current model-output analysis methods lack robustness and security; the proposed ellipse signature addresses these gaps with natural, unforgeable properties.

Method: Explores high-dimensional elliptical constraints inherently produced by all language models, develops extraction techniques for small models, and proposes a cryptographic-analogous verification protocol.

Result: Demonstrates ellipse signature extraction feasibility on small models, highlights practical challenges for production-scale models, and validates the signature's unique properties.

Conclusion: Ellipse signatures provide a secure, natural model identification mechanism with potential for output verification systems, though scalability challenges require further research.

Abstract: The ubiquity of closed-weight language models with public-facing APIs has
generated interest in forensic methods, both for extracting hidden model
details (e.g., parameters) and for identifying models by their outputs. One
successful approach to these goals has been to exploit the geometric
constraints imposed by the language model architecture and parameters. In this
work, we show that a lesser-known geometric constraint--namely, that language
model outputs lie on the surface of a high-dimensional ellipse--functions as a
signature for the model and can be used to identify the source model of a given
output. This ellipse signature has unique properties that distinguish it from
existing model-output association methods like language model fingerprints. In
particular, the signature is hard to forge: without direct access to model
parameters, it is practically infeasible to produce log-probabilities
(logprobs) on the ellipse. Secondly, the signature is naturally occurring,
since all language models have these elliptical constraints. Thirdly, the
signature is self-contained, in that it is detectable without access to the
model inputs or the full weights. Finally, the signature is compact and
redundant, as it is independently detectable in each logprob output from the
model. We evaluate a novel technique for extracting the ellipse from small
models and discuss the practical hurdles that make it infeasible for
production-scale models. Finally, we use ellipse signatures to propose a
protocol for language model output verification, analogous to cryptographic
symmetric-key message authentication systems.

</details>


### [7] [Power Grid Cybersecurity: Policy Analysis White Paper](https://arxiv.org/abs/2510.14171)
*Jack Vanlyssel*

Main category: cs.CR

TL;DR: This paper proposes a dual-policy strategy with information sharing, standardized cyber hygiene, and a unified framework to strengthen U.S. power grid cybersecurity and address fragmented, reactive policies.


<details>
  <summary>Details</summary>
Motivation: The U.S. power grid faces growing cyber risks from industrial control vulnerabilities, remote access weaknesses, and poor cyber hygiene. Current policies are fragmented and reactive, failing to ensure adequate protection for critical infrastructure supporting national security and economic stability.

Method: The paper recommends (1) enhanced government-utility information sharing for threat detection/response and (2) standardized cyber hygiene practices to reduce attack vectors, paired with a (3) Unified National Cybersecurity Framework to harmonize standards like NERC, IEC, IEEE, and NIST.

Result: The proposed policies align standards, eliminate regulatory overlap, improve threat detection, and create long-term resilience by adapting to evolving threats through unified frameworks.

Conclusion: The dual policy approach and Unified National Cybersecurity Framework proposed in the paper provide both immediate and sustainable improvements for safeguarding the U.S. power grid against evolving cyber threats.

Abstract: The U.S. power grid underpins national security, public safety, and economic
stability, but faces growing cyber risks from vulnerabilities in industrial
control systems, remote access, and poor cyber hygiene. Despite its critical
importance, current policy remains fragmented and reactive. This paper proposes
a dual policy approach to strengthen grid cybersecurity: enhanced information
sharing between government and private utilities to improve threat detection
and response, and standardized cyber hygiene practices to reduce common attack
vectors. For long-term resilience, a Unified National Cybersecurity Framework
is recommended to align existing NERC, IEC, IEEE, and NIST standards, eliminate
regulatory overlap, and adapt to evolving threats. Together, these policies
offer both immediate and sustainable improvements in safeguarding the nation's
most vital infrastructure.

</details>


### [8] [Securing U.S. Critical Infrastructure: Lessons from Stuxnet and the Ukraine Power Grid Attacks](https://arxiv.org/abs/2510.14185)
*Jack Vanlyssel*

Main category: cs.CR

TL;DR: This paper examines historical ICS cyber attacks to highlight ongoing vulnerabilities in U.S. critical infrastructure and recommends zero-trust policies and network segmentation to improve security.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the persistent cyber vulnerabilities in U.S. Industrial Control Systems, which are crucial for national security but remain exposed due to outdated practices and integration of digital technologies.

Method: The paper analyzes landmark cyber attacks (e.g., Stuxnet, Ukraine power grid) to identify recurring vulnerabilities and assess their relevance to current ICS environments.

Result: The analysis reveals that issues such as poor network segmentation, outdated software, weak authentication, and inadequate monitoring are still present in many U.S. ICS systems, making them susceptible to similar historical attacks.

Conclusion: The paper concludes that without immediate policy reforms, including the adoption of zero-trust architecture and better network segmentation, the U.S. risks facing similar catastrophic cyber incidents, and urges swift action to secure critical operational infrastructures.

Abstract: Industrial Control Systems (ICS) underpin the United States' critical
infrastructure, managing essential services such as power, water, and
transportation that are vital to national security and public safety. However,
increasing digital integration has exposed these systems to escalating cyber
threats. Historical attacks like Stuxnet and the Ukraine power grid incident
revealed exploitable weaknesses-poor network segmentation, outdated software,
weak authentication, and inadequate monitoring-that persist in many U.S. ICS
environments today. This paper analyzes these landmark attacks to identify
recurring vulnerabilities and assess their relevance to current U.S.
infrastructure. It argues that without immediate reforms, similar exploits
could lead to catastrophic disruptions and national security crises. To address
these risks, the paper proposes policy measures focused on implementing
zero-trust architecture and improved network segmentation to enhance system
resilience. These recommendations aim to guide policymakers and industry
leaders in securing the nation's most critical operational technologies against
future cyber threats.

</details>


### [9] [Infrastructure Patterns in Toll Scam Domains: A Comprehensive Analysis of Cybercriminal Registration and Hosting Strategies](https://arxiv.org/abs/2510.14198)
*Morium Akter Munny,Mahbub Alam,Sonjoy Kumar Paul,Daniel Timko,Muhammad Lutfor Rahman,Nitesh Saxena*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Toll scams involve criminals registering fake domains that pretend to be
legitimate transportation agencies to trick users into making fraudulent
payments. Although these scams are rapidly increasing and causing significant
harm, they have not been extensively studied. We present the first large-scale
analysis of toll scam domains, using a newly created dataset of 67,907
confirmed scam domains mostly registered in 2025. Our study reveals that
attackers exploit permissive registrars and less common top-level domains, with
86.9% of domains concentrated in just five non-mainstream TLDs and 72.9%
registered via a single provider. We also discover specific registration
patterns, including short bursts of activity that suggest automated,
coordinated attacks, with over half of domains registered in the first quarter
of 2025. This extreme temporal clustering reflects highly synchronized campaign
launches. Additionally, we build a simple predictive model using only domain
registration data to predict which scam domains are likely to be suspended -- a
proxy for confirmed abuse -- achieving 80.4% accuracy, and 92.3% sensitivity.
Our analysis reveals attacker strategies for evading detection -- such as
exploiting obscure TLDs, permissive registrars, and coordinated registration
bursts -- which can inform more targeted interventions by registrars, hosting
providers, and security platforms. However, our results suggest that
registration metadata alone may be insufficient, and incorporating features
from domain URLs and webpage content could further improve detection.

</details>


### [10] [An Information Asymmetry Game for Trigger-based DNN Model Watermarking](https://arxiv.org/abs/2510.14218)
*Chaoyue Huang,Gejian Zhao,Hanzhou Wu,Zhihua Xia,Asad Malik*

Main category: cs.CR

TL;DR: This paper explores robust watermarking for deep neural networks under information asymmetry and provides a game-theoretic analysis to design schemes that can resist watermarks removal via pruning or fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in the urgent need for effective technical measures to protect the intellectual property of deep neural networks, given the increasing threats posed by attacks.

Method: This paper models the watermarking and removal process as a game with an information asymmetry. It then defines the strategies, costs and utilities of both defender and attacker, derives the attacker's optimal pruning budget, and establishes bounds on watermark detection accuracy after attack.

Result: Experiments show the feasibility of the watermarked model and demonstrate that sparse watermarking can resist removal attempts with minimal impact on model accuracy.

Conclusion: The study emphasizes the effectiveness of game-theoretic approaches in guiding the development of robust watermarking for model copyright protection.

Abstract: As a valuable digital product, deep neural networks (DNNs) face increasingly
severe threats to the intellectual property, making it necessary to develop
effective technical measures to protect them. Trigger-based watermarking
methods achieve copyright protection by embedding triggers into the host DNNs.
However, the attacker may remove the watermark by pruning or fine-tuning. We
model this interaction as a game under conditions of information asymmetry,
namely, the defender embeds a secret watermark with private knowledge, while
the attacker can only access the watermarked model and seek removal. We define
strategies, costs, and utilities for both players, derive the attacker's
optimal pruning budget, and establish an exponential lower bound on the
accuracy of watermark detection after attack. Experimental results demonstrate
the feasibility of the watermarked model, and indicate that sparse watermarking
can resist removal with negligible accuracy loss. This study highlights the
effectiveness of game-theoretic analysis in guiding the design of robust
watermarking schemes for model copyright protection.

</details>


### [11] [RHINO: Guided Reasoning for Mapping Network Logs to Adversarial Tactics and Techniques with Large Language Models](https://arxiv.org/abs/2510.14233)
*Fanchao Meng,Jiaping Gui,Yunbo Li,Yue Wu*

Main category: cs.CR

TL;DR: RHINO is a framework enhancing LLM-based network intrusion analysis by decomposing attack detection into interpretable phases (behavioral abstraction, collaborative inference, validation), reducing hallucinations and improving accuracy (86.38%-88.45%) through structured reasoning aligned with MITRE ATT&CK.


<details>
  <summary>Details</summary>
Motivation: Current intrusion detection systems generate semantically fragmented alerts requiring manual correlation. Rule-based methods lack adaptability, ML methods lack contextual reasoning, and LLMs hallucinate technique names due to single-step classification.

Method: RHINO's three-phase approach: (1)application of LLM to transform raw logs into contextual narratives, (2)multiperspective technique generation using MITRE ATT&CK knowledge evaluation, and (3)predicate validation against official MITRE definitions to correct errors.

Result: Achieved 86.38%-88.45
% accuracy across four models with 24.25%-76.50
% relative performance gains on three benchmarks, demonstrating improved reliability and MITRE alignment compared to existing methods.

Conclusion: RHINO bridges the semantic gap between low-level observations and adversarial intent, establishing a scalable, interpretable framework for operationalizing LLMs in threat analysis while maintaining fidelity to established cybersecurity knowledge bases.

Abstract: Modern Network Intrusion Detection Systems generate vast volumes of low-level
alerts, yet these outputs remain semantically fragmented, requiring
labor-intensive manual correlation with high-level adversarial behaviors.
Existing solutions for automating this mapping-rule-based systems and machine
learning classifiers-suffer from critical limitations: rule-based approaches
fail to adapt to novel attack variations, while machine learning methods lack
contextual awareness and treat tactic-technique mapping as a syntactic matching
problem rather than a reasoning task. Although Large Language Models have shown
promise in cybersecurity tasks, preliminary experiments reveal that existing
LLM-based methods frequently hallucinate technique names or produce
decontextualized mappings due to their single-step classification approach.
  To address these challenges, we introduce RHINO, a novel framework that
decomposes LLM-based attack analysis into three interpretable phases mirroring
human reasoning: (1) behavioral abstraction, where raw logs are translated into
contextualized narratives; (2) multi-role collaborative inference, generating
candidate techniques by evaluating behavioral evidence against MITRE ATT&CK
knowledge; and (3) validation, cross-referencing predictions with official
MITRE definitions to rectify hallucinations. RHINO bridges the semantic gap
between low-level observations and adversarial intent while improving output
reliability through structured reasoning.
  We evaluate RHINO on three benchmarks across four backbone models. RHINO
achieved high accuracy, with model performance ranging from 86.38% to 88.45%,
resulting in relative gains from 24.25% to 76.50% across different models. Our
results demonstrate that RHINO significantly enhances the interpretability and
scalability of threat analysis, offering a blueprint for deploying LLMs in
operational security settings.

</details>


### [12] [Beyond a Single Perspective: Towards a Realistic Evaluation of Website Fingerprinting Attacks](https://arxiv.org/abs/2510.14283)
*Xinhao Deng,Jingyou Chen,Linxiao Yu,Yixiang Zhang,Zhongyi Gu,Changhao Qiu,Xiyuan Zhao,Ke Xu,Qi Li*

Main category: cs.CR

TL;DR: This paper evaluates Website Fingerprinting (WF) attacks across multiple realistic scenarios, showing that their performance drops in real-world conditions and introduces a multidimensional evaluation framework.


<details>
  <summary>Details</summary>
Motivation: Existing WF attacks are highly accurate in controlled environments but lack practical evaluation in realistic, complex conditions. The paper aims to bridge this gap by testing WF techniques under various real-world factors like defense mechanisms, traffic drift, and multi-tab browsing.

Method: The authors conducted a systematic evaluation of WF attacks under diverse realistic conditions, including defense mechanisms, traffic drift, multi-tab browsing, early-stage detection, open-world settings, and few-shot scenarios. They compared the performance of existing techniques across these scenarios.

Result: The experimental results demonstrate that many high-performing WF techniques in isolated settings experience significant degradation when exposed to multiple real-world challenges. The study found that current attacks are inadequate in environments where different factors coexist.

Conclusion: The paper concludes that existing WF attacks are limited in practical deployment due to their performance under combined real-world conditions. It introduces a multidimensional evaluation framework to better address these challenges and improve the robustness of future WF attacks.

Abstract: Website Fingerprinting (WF) attacks exploit patterns in encrypted traffic to
infer the websites visited by users, posing a serious threat to anonymous
communication systems. Although recent WF techniques achieve over 90% accuracy
in controlled experimental settings, most studies remain confined to single
scenarios, overlooking the complexity of real-world environments. This paper
presents the first systematic and comprehensive evaluation of existing WF
attacks under diverse realistic conditions, including defense mechanisms,
traffic drift, multi-tab browsing, early-stage detection, open-world settings,
and few-shot scenarios. Experimental results show that many WF techniques with
strong performance in isolated settings degrade significantly when facing other
conditions. Since real-world environments often combine multiple challenges,
current WF attacks are difficult to apply directly in practice. This study
highlights the limitations of WF attacks and introduces a multidimensional
evaluation framework, offering critical insights for developing more robust and
practical WF attacks.

</details>


### [13] [BinCtx: Multi-Modal Representation Learning for Robust Android App Behavior Detection](https://arxiv.org/abs/2510.14344)
*Zichen Liu,Shao Yang,Xusheng Xiao*

Main category: cs.CR

TL;DR: BINCTX is a multi-modal machine learning approach for detecting malicious mobile apps by analyzing bytecode, contextual metadata, and third-party library usage, achieving high accuracy (94.73% macro F1), robustness to obfuscation (84.2% post), and resistance to adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to detect stealthy malicious behaviors that rely on permission-unprotected APIs and can evade detection through UI/metadata camouflage. Current systems lack robustness to obfuscation and adversarial attacks.

Method: BINCTX synthesizes three app views: 1. Global bytecode-as-image capturing code semantics, 2. Contextual view combining manifest/actions/permissions/urls, 3. Third-party library usage patterns across inter-component call paths. These features are embedded/fused in a contextual-aware classifier.

Result: Achieved 94.73 macro F1 on realistic datasets, 14.92pp better than state-of-the-art. Maintains 84.2 F1 post-obfuscation and shows superior resistance to adversarial samples compared to bytecode-only models.

Conclusion: BINCTX demonstrates effective multi-modal analysis for app classification, with robustness under commercial obfuscation and adversarial conditions, addressing critical limitations in current mobile app security analyses.

Abstract: Mobile app markets host millions of apps, yet undesired behaviors (e.g.,
disruptive ads, illegal redirection, payment deception) remain hard to catch
because they often do not rely on permission-protected APIs and can be easily
camouflaged via UI or metadata edits. We present BINCTX, a learning approach
that builds multi-modal representations of an app from (i) a global
bytecode-as-image view that captures code-level semantics and family-style
patterns, (ii) a contextual view (manifested actions, components, declared
permissions, URL/IP constants) indicating how behaviors are triggered, and
(iii) a third-party-library usage view summarizing invocation frequencies along
inter-component call paths. The three views are embedded and fused to train a
contextual-aware classifier. On real-world malware and benign apps, BINCTX
attains a macro F1 of 94.73%, outperforming strong baselines by at least
14.92%. It remains robust under commercial obfuscation (F1 84%
post-obfuscation) and is more resistant to adversarial samples than
state-of-the-art bytecode-only systems.

</details>


### [14] [Match & Mend: Minimally Invasive Local Reassembly for Patching N-day Vulnerabilities in ARM Binaries](https://arxiv.org/abs/2510.14384)
*Sebastian Jänich,Merlin Sievers,Johannes Kinder*

Main category: cs.CR

TL;DR: This paper proposes automated binary-level firmware patching for IoT devices, achieving high success rates across benchmark and real-world datasets without requiring vendor updates.


<details>
  <summary>Details</summary>
Motivation: Low-cost IoT devices often run outdated, vulnerable software due to poor firmware update practices. Addressing this requires solutions that bypass vendor dependency.

Method: The authors introduce 'minimally invasive local reassembly,' a technique to automatically patch binary-level firmware with minimal side effects. Evaluations were conducted on MAGMA benchmarks and the KARONTE dataset to validate the method.

Result: The prototype successfully patched 83% of vulnerabilities in the MAGMA benchmarks and 96% in 30 real-world firmware images from the KARONTE dataset.

Conclusion: The approach provides an effective and practical solution for securing IoT devices, particularly those with poor update regimes, by enabling automated, vendor-independent firmware patching.

Abstract: Low-cost Internet of Things (IoT) devices are increasingly popular but often
insecure due to poor update regimes. As a result, many devices run outdated and
known-vulnerable versions of open-source software. We address this problem by
proposing to patch IoT firmware at the binary level, without requiring vendor
support. In particular, we introduce minimally invasive local reassembly, a new
technique for automatically patching known (n-day) vulnerabilities in IoT
firmware. Our approach is designed to minimize side effects and reduce the risk
of introducing breaking changes. We systematically evaluate our approach both
on 108 binaries within the controlled environment of the MAGMA benchmarks, as
well as on 30 real-world Linux-based IoT firmware images from the KARONTE
dataset. Our prototype successfully patches 83% of targeted vulnerabilities in
MAGMA and 96% in the firmware dataset.

</details>


### [15] [Stealthy Dual-Trigger Backdoors: Attacking Prompt Tuning in LM-Empowered Graph Foundation Models](https://arxiv.org/abs/2510.14470)
*Xiaoyu Xue,Yuni Lai,Chenxi Huang,Yulin Zhu,Gaolei Li,Xiaoge Zhang,Kai Zhou*

Main category: cs.CR

TL;DR: This paper reveals vulnerabilities in graph foundation models using text-attributed graphs, proposing a dual-trigger backdoor attack framework that achieves high attack success without explicit text attribute optimization through strategic text pool usage.


<details>
  <summary>Details</summary>
Motivation: Current graph backdoor attacks fail in attribute-inaccessible TAG environments without trigger node text optimization, exposing unique security vulnerabilities in LM-integrated GFMs during unsecured prompt tuning.

Method: A dual-trigger backdoor attack framework leveraging both text-level and struct-level triggers was proposed, utilizing a pre-established text pool to bypass explicit node attribute optimization constraints in TAG systems.

Result: Experimental results demonstrated superior clean accuracy and high attack success rates, including for concealed single-trigger node scenarios, confirming the effectiveness of dual-trigger attacks without attribute optimization.

Conclusion: The work highlights critical backdoor risks in LM-empowered GFMs and emphasizes the need for robust supervision mechanisms in open-source platforms to address security vulnerabilities in foundation models.

Abstract: The emergence of graph foundation models (GFMs), particularly those
incorporating language models (LMs), has revolutionized graph learning and
demonstrated remarkable performance on text-attributed graphs (TAGs). However,
compared to traditional GNNs, these LM-empowered GFMs introduce unique security
vulnerabilities during the unsecured prompt tuning phase that remain
understudied in current research. Through empirical investigation, we reveal a
significant performance degradation in traditional graph backdoor attacks when
operating in attribute-inaccessible constrained TAG systems without explicit
trigger node attribute optimization. To address this, we propose a novel
dual-trigger backdoor attack framework that operates at both text-level and
struct-level, enabling effective attacks without explicit optimization of
trigger node text attributes through the strategic utilization of a
pre-established text pool. Extensive experimental evaluations demonstrate that
our attack maintains superior clean accuracy while achieving outstanding attack
success rates, including scenarios with highly concealed single-trigger nodes.
Our work highlights critical backdoor risks in web-deployed LM-empowered GFMs
and contributes to the development of more robust supervision mechanisms for
open-source platforms in the era of foundation models.

</details>


### [16] [Certifying optimal MEV strategies with Lean](https://arxiv.org/abs/2510.14480)
*Massimo Bartoletti,Riccardo Marchesin,Roberto Zunino*

Main category: cs.CR

TL;DR: Formalizes MEV analysis in Lean theorem prover, enables machine-checked proofs for DeFi protocol security, and provides first mechanized verification of sandwich attack optimality in AMMs.


<details>
  <summary>Details</summary>
Motivation: The vast space of adversarial strategies in MEV attacks makes empirical studies and manual analysis insufficient for rigorous verification. This motivates the need for mechanized formalization to ensure protocol security guarantees.

Method: The authors introduce a methodology for constructing machine-checked proofs of MEV bounds using the Lean theorem prover, enabling formal verification of adversarial strategy limits in DeFi protocols.

Result: The approach is demonstrated through modeling two DeFi protocols and producing the first machine-checked proof of sandwich attack optimality in Automated Market Makers (AMMs), a foundational DeFi primitive.

Conclusion: The paper concludes that the proposed mechanized formalization in Lean provides stronger correctness guarantees for verifying MEV bounds compared to existing techniques, advancing the rigorous analysis of blockchain security in DeFi protocols.

Abstract: Maximal Extractable Value (MEV) refers to a class of attacks to decentralized
applications where the adversary profits by manipulating the ordering,
inclusion, or exclusion of transactions in a blockchain. Decentralized Finance
(DeFi) protocols are a primary target of these attacks, as their logic depends
critically on transaction sequencing. To date, MEV attacks have already
extracted billions of dollars in value, underscoring their systemic impact on
blockchain security. Verifying the absence of MEV attacks requires determining
suitable upper bounds, i.e. proving that no adversarial strategy can extract
more value (if any) than expected by protocol designers. This problem is
notoriously difficult: the space of adversarial strategies is extremely vast,
making empirical studies and pen-and-paper reasoning insufficiently rigorous.
In this paper, we present the first mechanized formalization of MEV in the Lean
theorem prover. We introduce a methodology to construct machine-checked proofs
of MEV bounds, providing correctness guarantees beyond what is possible with
existing techniques. To demonstrate the generality of our approach, we model
and analyse the MEV of two paradigmatic DeFi protocols. Notably, we develop the
first machine-checked proof of the optimality of sandwich attacks in Automated
Market Makers, a fundamental DeFi primitive.

</details>


### [17] [Lexo: Eliminating Stealthy Supply-Chain Attacks via LLM-Assisted Program Regeneration](https://arxiv.org/abs/2510.14522)
*Evangelos Lamprou,Julian Dai,Grigoris Ntousakis,Martin C. Rinard,Nikos Vasilakis*

Main category: cs.CR

TL;DR: Lexo is a framework designed to counter software supply-chain attacks in open source ecosystems by automatically regenerating code to eliminate malicious behavior while preserving original functionality.


<details>
  <summary>Details</summary>
Motivation: Software supply-chain attacks are a critical issue in the open source software ecosystem as they can introduce hidden malicious functionalities that remain dormant until they reach the target environment.

Method: Lexo uses a process where it generates input-output pairs to model a component's behavior, then synthesizes a new version of the component without malicious code by consulting multiple Large Language Models (LLMs), using correctness and coverage metrics, and implementing guardrails for the LLM outputs.

Result: Lexo was evaluated on over 100 real-world software packages, including those involved in high profile stealthy supply-chain attacks. It showed scalability across multiple domains, efficient code regeneration (under 100 seconds on average), compatibility maintenance, and success in eliminating malicious code, even where state-of-the-art LLMs failed.

Conclusion: Lexo provides an effective, scalable, and automated solution for mitigating stealthy supply-chain attacks in open source software, maintaining functionality and compatibility while successfully removing malicious content.

Abstract: Software supply-chain attacks are an important and ongoing concern in the
open source software ecosystem. These attacks maintain the standard
functionality that a component implements, but additionally hide malicious
functionality activated only when the component reaches its target environment.
Lexo addresses such stealthy attacks by automatically learning and regenerating
vulnerability-free versions of potentially malicious components. Lexo first
generates a set of input-output pairs to model a component's full observable
behavior, which it then uses to synthesize a new version of the original
component. The new component implements the original functionality but avoids
stealthy malicious behavior. Throughout this regeneration process, Lexo
consults several distinct instances of Large Language Models (LLMs), uses
correctness and coverage metrics to shepherd these instances, and guardrails
their results. Our evaluation on 100+ real-world packages, including high
profile stealthy supply-chain attacks, indicates that Lexo scales across
multiple domains, regenerates code efficiently (<100s on average), maintains
compatibility, and succeeds in eliminating malicious code in several real-world
supply-chain-attacks, even in cases when a state-of-the-art LLM fails to
eliminate malicious code when prompted to do so.

</details>


### [18] [Symbolic verification of Apple's Find My location-tracking protocol](https://arxiv.org/abs/2510.14589)
*Vaishnavi Sundararajan,Rithwik*

Main category: cs.CR

TL;DR: Formal verification confirms Apple's Find My protocol's security claims.


<details>
  <summary>Details</summary>
Motivation: The paper addresses privacy concerns in proprietary tracking systems like Apple's Find My, aiming to validate Apple's security claims despite the system's closed-source nature.

Method: The authors used symbolic modeling of the Find My protocol, formalized desirable security properties, and conducted machine-checkable proofs via the Tamarin prover.

Result: Automated proofs in Tamarin verified that the Find My protocol satisfies its specified privacy and security properties.

Conclusion: The paper concludes that Apple's Find My system can be formally verified using symbolic modeling and Tamarin prover to confirm its privacy and security claims.

Abstract: Tracking devices, while designed to help users find their belongings in case
of loss/theft, bring in new questions about privacy and surveillance of not
just their own users, but in the case of crowd-sourced location tracking, even
that of others even orthogonally associated with these platforms. Apple's Find
My is perhaps the most ubiquitous such system which can even locate devices
which do not possess any cellular support or GPS, running on millions of
devices worldwide. Apple claims that this system is private and secure, but the
code is proprietary, and such claims have to be taken on faith. It is well
known that even with perfect cryptographic guarantees, logical flaws might
creep into protocols, and allow undesirable attacks. In this paper, we present
a symbolic model of the Find My protocol, as well as a precise formal
specification of desirable properties, and provide automated, machine-checkable
proofs of these properties in the Tamarin prover.

</details>


### [19] [Improving Cybercrime Detection and Digital Forensics Investigations with Artificial Intelligence](https://arxiv.org/abs/2510.14638)
*Silvia Lucia Sanna,Leonardo Regano,Davide Maiorca,Giorgio Giacinto*

Main category: cs.CR

TL;DR: The paper explores the intersection of cybercrime, digital forensics (DF), and AI, proposing AI integration to enhance cybercrime detection and DF analysis while highlighting risks of cybercriminal misuse through a chatbot-based steganography case study.


<details>
  <summary>Details</summary>
Motivation: Cybercrime remains prevalent in Europe, necessitating advanced tools like AI-driven DF analysis to improve prevention, detection, and analysis. Existing systems could benefit from better AI integration to combat evolving threats.

Method: The study reviews AI applications in cyberattack detection and DF analysis, proposes enhancing these systems with AI, and demonstrates a case study using Gemini, Copilot, and ChatGPT to generate Python code for steganographic image encoding/decoding as an anti-forensics example.

Result: AI can significantly improve cybercrime detection and DF analysis efficacy. However, chatbots can also be weaponized for steganography, demonstrating dual-use risks where similar techniques might be exploited by cybercriminals for anti-forensics.

Conclusion: AI offers transformative potential for cybercrime analysis and DF procedures but requires careful implementation to prevent exploitation by malicious actors. Understanding both technical capabilities and misuse risks is critical for developing robust cybersecurity frameworks.

Abstract: According to a recent EUROPOL report, cybercrime is still recurrent in
Europe, and different activities and countermeasures must be taken to limit,
prevent, detect, analyze, and fight it. Cybercrime must be prevented with
specific measures, tools, and techniques, for example through automated network
and malware analysis. Countermeasures against cybercrime can also be improved
with proper \df analysis in order to extract data from digital devices trying
to retrieve information on the cybercriminals. Indeed, results obtained through
a proper \df analysis can be leveraged to train cybercrime detection systems to
prevent the success of similar crimes. Nowadays, some systems have started to
adopt Artificial Intelligence (AI) algorithms for cyberattack detection and \df
analysis improvement. However, AI can be better applied as an additional
instrument in these systems to improve the detection and in the \df analysis.
For this reason, we highlight how cybercrime analysis and \df procedures can
take advantage of AI. On the other hand, cybercriminals can use these systems
to improve their skills, bypass automatic detection, and develop advanced
attack techniques. The case study we presented highlights how it is possible to
integrate the use of the three popular chatbots {\tt Gemini}, {\tt Copilot} and
{\tt chatGPT} to develop a Python code to encode and decoded images with
steganographic technique, even though their presence is not an indicator of
crime, attack or maliciousness but used by a cybercriminal as anti-forensics
technique.

</details>


### [20] [AEX-NStep: Probabilistic Interrupt Counting Attacks on Intel SGX](https://arxiv.org/abs/2510.14675)
*Nicolas Dutly,Friederike Groschupp,Ivan Puddu,Kari Kostiainen,Srdjan Capkun*

Main category: cs.CR

TL;DR: AEX-NStep bypasses Intel's AEX-Notify by introducing probabilistic interrupt counting attacks, exposing flaws in its security guarantees and enabling practical ECDSA key leakage in SGX enclaves.


<details>
  <summary>Details</summary>
Motivation: The paper aims to assess the efficacy of Intel's AEX-Notify mitigation against interrupt-based stepping attacks and extend its security analysis to identify critical vulnerabilities, guiding future defenses.

Method: The authors developed two new probabilistic interrupt counting attacks, exploiting AEX-Notify's failure to ensure obfuscated forward progress, which enabled a practical ECDSA key leakage attack against AEX-Notify-enabled enclaves.

Result: The paper successfully executed a key leakage attack against SGX enclaves using the new probabilistic attacks, proving AEX-Notify's insufficiency to prevent such exploits.

Conclusion: The study reveals that AEX-Notify does not fully prevent interrupt counting attacks, demonstrating that probabilistic methods can bypass it, thus necessitating improved mitigation strategies for SGX enclave security.

Abstract: To mitigate interrupt-based stepping attacks (notably using SGX-Step), Intel
introduced AEX-Notify, an ISA extension to Intel SGX that aims to prevent
deterministic single-stepping. In this work, we introduce AEX-NStep, the first
interrupt counting attack on AEX-Notify-enabled Enclaves. We show that
deterministic single-stepping is not required for interrupt counting attacks to
be practical and that, therefore, AEX-Notify does not entirely prevent such
attacks. We specifically show that one of AEX-Notify's security guarantees,
obfuscated forward progress, does not hold, and we introduce two new
probabilistic interrupt counting attacks. We use these attacks to construct a
practical ECDSA key leakage attack on an AEX-Notify-enabled SGX enclave. Our
results extend the original security analysis of AEX-Notify and inform the
design of future mitigations.

</details>


### [21] [FibRace: a large-scale benchmark of client-side proving on mobile devices](https://arxiv.org/abs/2510.14693)
*Simon Malatrait,Alex Sirac*

Main category: cs.CR

TL;DR: FibRace proves smartphones can reliably generate ZK proofs (2.2M collected), with performance tied to RAM/SoC, enabling mobile privacy apps.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the practicality of mobile devices for ZK proofs, enabling privacy-preserving applications while collecting empirical data on performance limitations.

Method: A gamified mobile challenge where 6,047 participants generated over 2 million Fibonacci proofs across 1,420 device models, benchmarking performance through real-world usage (Sep 2025).

Result: 89% of modern smartphones completed proofs in <5 seconds; RAM capacity and SoC performance (A19 Pro/M-series) determined stability; blockchain natively verified all proofs without congestion.

Conclusion: Modern smartphones can reliably generate zero-knowledge proofs on-chain without specialized hardware, establishing FibRace as a foundational dataset for mobile proving research.

Abstract: FibRace, jointly developed by KKRT Labs and Hyli, was the first large-scale
experiment to test client-side proof generation on smartphones using Cairo M.
Presented as a mobile game in which players proved Fibonacci numbers and
climbed a leaderboard, FibRace served a dual purpose: to engage the public and
to provide empirical benchmarking. Over a three-week campaign (September 11-30,
2025), 6,047 players across 99 countries generated 2,195,488 proofs on 1,420
unique device models. The results show that most modern smartphones can
complete a proof in under 5 seconds, confirming that *mobile devices are now
capable of producing zero-knowledge proofs reliably*, without the need for
remote provers or specialized hardware. Performance was correlated primarily
with RAM capacity and SoC (System on Chip) performance: devices with at least 3
GB of RAM proved stably, when Apple's A19 Pro and M-series chips achieved the
fastest proving times. Hyli's blockchain natively verified every proof onchain
without congestion. FibRace provides the most comprehensive dataset to date on
mobile proving performance, establishing a practical baseline for future
research in lightweight provers, proof-powered infrastructure, and
privacy-preserving mobile applications.

</details>


### [22] [SLIE: A Secure and Lightweight Cryptosystem for Data Sharing in IoT Healthcare Services](https://arxiv.org/abs/2510.14708)
*Ha Xuan Son,Nguyen Quoc Anh,Phat T. Tran-Truong,Le Thanh Tuan,Pham Thanh Nghiem*

Main category: cs.CR

TL;DR: This paper proposes SLIE, a lightweight, secure identity-based encryption system for the Internet of Medical Things (IoMT) that addresses device management and communication vulnerabilities through scalable trust, end-to-end encryption, and energy efficiency improvements over RSA.


<details>
  <summary>Details</summary>
Motivation: Service-oriented IoMT models face critical security risks in device management and sensitive medical data transmission, necessitating a cryptosystem that ensures scalability, interoperability, and robustness against attacks while optimizing for resource-constrained devices.

Method: SLIE utilizes Wildcard Key Derivation Identity-Based Encryption (WKD-IBE) with hierarchical access control, constant-time operations, memory obfuscation, and expiry-based key revocation. It enforces secure, omnidirectional communication and lightweight key management tailored for constrained environments while meeting HIPAA/GDPR compliance.

Result: SLIE achieves 84.54% faster encryption and 99.70% faster decryption than RSA (0.936ms/0.217ms for 1KB data), with energy efficiency of 0.014 J/KB. It effectively mitigates side-channel, man-in-the-middle, and unauthorized access attacks.

Conclusion: SLIE provides a scalable, lightweight, and secure solution for IoMT, outperforming RSA in speed, energy efficiency, and attack resistance while ensuring compliance with critical healthcare regulations and enabling robust, interoperable medical IoT services.

Abstract: The Internet of Medical Things (IoMT) has revolutionized healthcare by
transforming medical operations into standardized, interoperable services.
However, this service-oriented model introduces significant security
vulnerabilities in device management and communication, which are especially
critical given the sensitivity of medical data. To address these risks, this
paper proposes SLIE (Secure and Lightweight Identity Encryption), a novel
cryptosystem based on Wildcard Key Derivation Identity-Based Encryption
(WKD-IBE). SLIE ensures scalable trust and secure omnidirectional communication
through end-to-end encryption, hierarchical access control, and a lightweight
key management system designed for resource-constrained devices. It
incorporates constant-time operations, memory obfuscation, and expiry-based key
revocation to counter side-channel, man-in-the-middle, and unauthorized access
attacks, thereby ensuring compliance with standards like HIPAA and GDPR.
Evaluations show that SLIE significantly outperforms RSA, with encryption and
decryption times of 0.936ms and 0.217ms for 1KB of data, an 84.54% improvement
in encryption speed, a 99.70% improvement in decryption speed, and an energy
efficiency of 0.014 J/KB.

</details>


### [23] [Secure Sparse Matrix Multiplications and their Applications to Privacy-Preserving Machine Learning](https://arxiv.org/abs/2510.14894)
*Marc Damie,Florian Hahn,Andreas Peter,Jan Ramon*

Main category: cs.CR

TL;DR: The paper proposes sparsity-aware MPC algorithms for secure matrix multiplication, addressing memory and communication inefficiencies in sparse ML applications while adapting to realistic data distributions.


<details>
  <summary>Details</summary>
Motivation: Existing MPC frameworks are not optimized for sparse data, which is prevalent in applications like recommender systems and genomics. Dense data representations lead to impractical memory usage, and current assumptions about sparsity patterns (e.g., fixed row non-zero bounds) often contradict real-world statistics (e.g., power laws).

Method: 1) Develops MPC algorithms for sparse matrix multiplication avoiding dense representations. 2) Introduces a safe upper bound for non-zero entries in rows/columns based on power law distributions. 3) Focuses on reducing both memory overhead and communication costs (achieving up to 1000× reduction in experiments).

Result: Validated in two ML applications where existing methods are impractical. Demonstrated significant communication cost improvements and practicality using realistic sparsity patterns. Addressed privacy concerns in runtime leakage for sparse data processing.

Conclusion: The framework enables efficient and secure processing of high-dimensional sparse data in MPC, bridging a critical gap for privacy-preserving ML applications with realistic sparsity distributions.

Abstract: To preserve privacy, multi-party computation (MPC) enables executing Machine
Learning (ML) algorithms on secret-shared or encrypted data. However, existing
MPC frameworks are not optimized for sparse data. This makes them unsuitable
for ML applications involving sparse data, e.g., recommender systems or
genomics. Even in plaintext, such applications involve high-dimensional sparse
data, that cannot be processed without sparsity-related optimizations due to
prohibitively large memory requirements.
  Since matrix multiplication is central in ML algorithms, we propose MPC
algorithms to multiply secret sparse matrices. On the one hand, our algorithms
avoid the memory issues of the "dense" data representation of classic secure
matrix multiplication algorithms. On the other hand, our algorithms can
significantly reduce communication costs (some experiments show a factor 1000)
for realistic problem sizes. We validate our algorithms in two ML applications
in which existing protocols are impractical.
  An important question when developing MPC algorithms is what assumptions can
be made. In our case, if the number of non-zeros in a row is a sensitive piece
of information then a short runtime may reveal that the number of non-zeros is
small. Existing approaches make relatively simple assumptions, e.g., that there
is a universal upper bound to the number of non-zeros in a row. This often
doesn't align with statistical reality, in a lot of sparse datasets the amount
of data per instance satisfies a power law. We propose an approach which allows
adopting a safe upper bound on the distribution of non-zeros in rows/columns of
sparse matrices.

</details>


### [24] [A Hard-Label Black-Box Evasion Attack against ML-based Malicious Traffic Detection Systems](https://arxiv.org/abs/2510.14906)
*Zixuan Liu,Yi Zhao,Zhuotao Liu,Qi Li,Chuanpu Fu,Guangmeng Zhou,Ke Xu*

Main category: cs.CR

TL;DR: This paper proposes NetMasquerade, an RL-driven adversarial traffic evasion framework using Traffic-BERT that bypasses ML-based security systems with >96% success in 80 scenarios while maintaining operational practicality through real-time traffic generation.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial traffic evasion methods require impractical conditions (encrypted protocols, prior model knowledge) or specialized setups, leaving the feasibility of stealthy black-box attacks across diverse tasks and protocols unaddressed.

Method: The approach combines (1) Traffic-BERT - a pre-trained network-specialized model with attention mechanisms for benign traffic pattern extraction, and (2) a reinforcement learning framework that manipulates malicious packet sequences to mimic benign traffic patterns with minimal modifications.

Result: Achieves >96.65% success rate in evading 6 detection systems across 80 scenarios, including those considered robust against existing attacks, with 100% evasion of certifiably robust models and real-time performance (low-latency generation).

Conclusion: NetMasquerade effectively demonstrates a practical hard-label black-box evasion attack framework using reinforcement learning and Traffic-BERT, challenging state-of-the-art robust ML-based traffic detection systems while maintaining real-world operational feasibility through low-latency adversarial traffic generation.

Abstract: Machine Learning (ML)-based malicious traffic detection is a promising
security paradigm. It outperforms rule-based traditional detection by
identifying various advanced attacks. However, the robustness of these ML
models is largely unexplored, thereby allowing attackers to craft adversarial
traffic examples that evade detection. Existing evasion attacks typically rely
on overly restrictive conditions (e.g., encrypted protocols, Tor, or
specialized setups), or require detailed prior knowledge of the target (e.g.,
training data and model parameters), which is impractical in realistic
black-box scenarios. The feasibility of a hard-label black-box evasion attack
(i.e., applicable across diverse tasks and protocols without internal target
insights) thus remains an open challenge. To this end, we develop
NetMasquerade, which leverages reinforcement learning (RL) to manipulate attack
flows to mimic benign traffic and evade detection. Specifically, we establish a
tailored pre-trained model called Traffic-BERT, utilizing a network-specialized
tokenizer and an attention mechanism to extract diverse benign traffic
patterns. Subsequently, we integrate Traffic-BERT into the RL framework,
allowing NetMasquerade to effectively manipulate malicious packet sequences
based on benign traffic patterns with minimal modifications. Experimental
results demonstrate that NetMasquerade enables both brute-force and stealthy
attacks to evade 6 existing detection methods under 80 attack scenarios,
achieving over 96.65% attack success rate. Notably, it can evade the methods
that are either empirically or certifiably robust against existing evasion
attacks. Finally, NetMasquerade achieves low-latency adversarial traffic
generation, demonstrating its practicality in real-world scenarios.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [25] [From Craft to Constitution: A Governance-First Paradigm for Principled Agent Engineering](https://arxiv.org/abs/2510.13857)
*Qiang Xu,Xiangyu Wen,Changran Xu,Zeju Li,Jianyuan Zhong*

Main category: cs.SE

TL;DR: This paper addresses the 'crisis of craft' in deploying Large Language Model (LLM)-based agents into mission-critical applications by proposing a governance-first paradigm called ArbiterOS, which bridges the gap between LLMs probabilistic nature and deterministic software engineering approaches.


<details>
  <summary>Details</summary>
Motivation: The transition from LLM-based agents from prototypes to reliable production systems is hindered by brittleness and unpredictability caused by a fundamental paradigm mismatch between probabilistic LLMs and deterministic software engineering mental models.

Method: The authors introduce a formal architecture called ArbiterOS grounded in a governance-first paradigm, emphasizing principled agent engineering to address the inherent challenges of managing LLMs probabilistic behavior.

Result: ArbiterOS provides a structured framework to mitigate the crisis of craft by aligning agent development with the probabilistic nature of LLMs, although specific quantitative results are not detailed in the abstract.

Conclusion: The paper underscores the need to move beyond deterministic software engineering paradigms and adopt governance-driven approaches like ArbiterOS to ensure trustworthy, mission-critical LLM-based agents.

Abstract: The advent of powerful Large Language Models (LLMs) has ushered in an ``Age
of the Agent,'' enabling autonomous systems to tackle complex goals. However,
the transition from prototype to production is hindered by a pervasive ``crisis
of craft,'' resulting in agents that are brittle, unpredictable, and ultimately
untrustworthy in mission-critical applications. This paper argues this crisis
stems from a fundamental paradigm mismatch -- attempting to command inherently
probabilistic processors with the deterministic mental models of traditional
software engineering. To solve this crisis, we introduce a governance-first
paradigm for principled agent engineering, embodied in a formal architecture we
call ArbiterOS.

</details>


### [26] [Benchmarking Correctness and Security in Multi-Turn Code Generation](https://arxiv.org/abs/2510.13859)
*Ruchit Rawal,Jeffrey Yang Fan Chiang,Chihao Shen,Jeffery Siyuan Tian,Aastha Mahajan,Tom Goldstein,Yizheng Chen*

Main category: cs.SE

TL;DR: The paper introduces MT-Sec, a new benchmark for evaluating correctness and security in multi-turn coding scenarios, revealing a significant drop in model performance compared to single-turn settings across 32 models and different agent scaffolds.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLM-generated code focus on correctness and security in single-turn tasks but do not account for the iterative nature of real development.

Method: MT-Sec is built using a synthetic data pipeline to transform single-turn tasks into multi-turn sequences. It evaluates 32 open- and closed-source models with three agent scaffolding configurations across both full-program and code-diff generations.

Result: Models show a 20-27% decline in correct and secure outputs in multi-turn settings, especially in code-diff generation, with higher rates of errors. Agent scaffolding helps in single-turn tasks but less so in multi-turn ones.

Conclusion: Multi-turn coding evaluation shows critical performance gaps in LLMs, emphasizing the need for benchmarking real-world workflows involving correctness and security.

Abstract: AI coding assistants powered by large language models (LLMs) have transformed
software development, significantly boosting productivity. While existing
benchmarks evaluate the correctness and security of LLM-generated code, they
are typically limited to single-turn tasks that do not reflect the iterative
nature of real-world development. We introduce MT-Sec, the first benchmark to
systematically evaluate both correctness and security in multi-turn coding
scenarios. We construct this using a synthetic data pipeline that transforms
existing single-turn tasks into semantically aligned multi-turn interaction
sequences, allowing reuse of original test suites while modeling the complexity
of real-world coding processes. We evaluate 32 open- and closed-source models,
and three agent-scaffolding on MT-Sec and observe a consistent 20-27% drop in
"correct and secure" outputs from single-turn to multi-turn settings -- even
among state-of-the-art models. Beyond full-program generation, we also evaluate
models on multi-turn code-diff generation -- an unexplored yet practically
relevant setting -- and find that models perform worse here, with increased
rates of functionally incorrect and insecure outputs. Finally, we find that
while agent scaffoldings boost single-turn code generation performance, they
are not quite as effective in multi-turn evaluations. Together, these findings
highlight the need for benchmarks that jointly evaluate correctness and
security in multi-turn, real-world coding workflows.

</details>


### [27] [A11YN: aligning LLMs for accessible web UI code generation](https://arxiv.org/abs/2510.13914)
*Janghan Yoon,Jaegwan Cho,Junhyeok Kim,Jiwan Chung,Jaehyun Jeon,Youngjae Yu*

Main category: cs.SE

TL;DR: Researchers develop A11yn, the first method to align LLMs with accessibility standards, achieving a 60% reduction in accessibility flaws using a WCAG violation penalty reward function and specialized datasets.


<details>
  <summary>Details</summary>
Motivation: Current LLMs replicate accessibility flaws from training data, excluding users with diverse needs. There is a pressing need to align code generation with accessibility standards.

Method: A11yn optimizes a reward function penalizing WCAG violations based on severity, supported by two datasets (UIReq-6.8K for training and RealUIReq-300 as a benchmark) for training and evaluation.

Result: A11yn reduces the Inaccessibility Rate by 60% compared to baseline models while maintaining UI semantic fidelity and visual quality.

Conclusion: The study demonstrates that accessibility in code-generating LLMs can be systematically optimized through structured alignment with accessibility guidelines, achieving significant improvements without compromising UI quality.

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
in generating functional and aesthetic web interfaces directly from
instructions. However, these models often replicate accessibility flaws from
their training data, resulting in interfaces that exclude users with diverse
needs and contexts. To address this gap, we introduce A11yn, the first method
that aligns code-generating LLMs to reliably produce accessibility-compliant
web UIs. A11yn optimizes a novel reward function that penalizes violations of
the Web Content Accessibility Guidelines (WCAG), with penalties scaled to the
severity of each violation as identified by an accessibility testing engine. To
support training, we construct UIReq-6.8K, a dataset of 6,800 diverse
instructions for web UI generation. For evaluation, we introduce RealUIReq-300,
a benchmark of 300 real-world web UI requests grounded and manually curated
from public web pages, spanning a broad range of use cases. Empirical results
show that A11yn significantly outperforms strong baselines, lowering the
Inaccessibility Rate by 60% over the base model while preserving semantic
fidelity and visual quality of generated UIs. These findings demonstrate that
accessibility can be systematically optimized within LLMs, showing the
feasibility of aligning code generation for accessibility.

</details>


### [28] [Signature in Code Backdoor Detection, how far are we?](https://arxiv.org/abs/2510.13992)
*Quoc Hung Le,Thanh Le-Cong,Bach Le,Bowen Xu*

Main category: cs.SE

TL;DR: This paper reevaluates Spectral Signatures for code model backdoor detection, identifies suboptimal practices, and proposes a new metric to assess defense effectiveness without model retraining.


<details>
  <summary>Details</summary>
Motivation: Recent studies indicate Spectral Signatures are inadequately effective for code models, necessitating revisiting their applicability in backdoor attack scenarios where poisoned data manipulation is a critical threat.

Method: The authors systematically evaluated Spectral Signature-based defenses under diverse attack scenarios and configurations, analyzing their effectiveness through key factor settings.

Result: Identified suboptimal usage of Spectral Signatures in code backdoor detection, discovered a new proxy metric for accurate performance estimation without requiring post-defense model retraining.

Conclusion: The paper concludes that existing Spectral Signature settings are often suboptimal for code model backdoor detection, but introduces a novel proxy metric to estimate defense performance without model retraining.

Abstract: As Large Language Models (LLMs) become increasingly integrated into software
development workflows, they also become prime targets for adversarial attacks.
Among these, backdoor attacks are a significant threat, allowing attackers to
manipulate model outputs through hidden triggers embedded in training data.
Detecting such backdoors remains a challenge, and one promising approach is the
use of Spectral Signature defense methods that identify poisoned data by
analyzing feature representations through eigenvectors. While some prior works
have explored Spectral Signatures for backdoor detection in neural networks,
recent studies suggest that these methods may not be optimally effective for
code models. In this paper, we revisit the applicability of Spectral
Signature-based defenses in the context of backdoor attacks on code models. We
systematically evaluate their effectiveness under various attack scenarios and
defense configurations, analyzing their strengths and limitations. We found
that the widely used setting of Spectral Signature in code backdoor detection
is often suboptimal. Hence, we explored the impact of different settings of the
key factors. We discovered a new proxy metric that can more accurately estimate
the actual performance of Spectral Signature without model retraining after the
defense.

</details>


### [29] [One Bug, Hundreds Behind: LLMs for Large-Scale Bug Discovery](https://arxiv.org/abs/2510.14036)
*Qiushi Wu,Yue Xiao,Dhilung Kirat,Kevin Eykholt,Jiyong Jang,Douglas Lee Schales*

Main category: cs.SE

TL;DR: This paper explores Recurring Pattern Bugs (RPBs) in large programs, proposing BugStone—an LLVM and LLM-powered system—to automatically detect and address them. Large-scale evaluation shows it identifies 22K+ potential issues in the Linux kernel with 92.2% precision, highlighting a significant vulnerability pattern in software security.


<details>
  <summary>Details</summary>
Motivation: Manual bug fixing is error-prone and time-consuming. Unfixed RPBs widen attack surfaces as attackers exploit recurring patterns. Existing practices miss similar bugs in other code segments, compromising software security at scale.

Method: BugStone analyzes code for patterns from patched RPB instances (e.g., API misuse). It uses LLVM for code analysis and an LLM to refine patterns. The system scans entire codebases, identifies similar vulnerabilities, and validates findings via manual analysis of 1.9K security bug datasets and 22K+ Linux kernel issues.

Result: BugStone found 246 valid issues from 400 analyzed in Linux kernel. It achieved 92.2% precision and 79.1% pairwise accuracy on a manually annotated dataset of 1,900 bugs. The system identified 80 recurring patterns and 850 fixes across top-tier security research.

Conclusion: BugStone demonstrates automated, cost-effective detection of RPBs, significantly improving software security. Its approach reduces manual labor and mitigates risks from exploitable patterns, with the annotated dataset enabling future research on recurring vulnerabilities.

Abstract: Fixing bugs in large programs is a challenging task that demands substantial
time and effort. Once a bug is found, it is reported to the project
maintainers, who work with the reporter to fix it and eventually close the
issue. However, across the program, there are often similar code segments,
which may also contain the bug, but were missed during discovery. Finding and
fixing each recurring bug instance individually is labor intensive. Even more
concerning, bug reports can inadvertently widen the attack surface as they
provide attackers with an exploitable pattern that may be unresolved in other
parts of the program.
  In this paper, we explore these Recurring Pattern Bugs (RPBs) that appear
repeatedly across various code segments of a program or even in different
programs, stemming from a same root cause, but are unresolved. Our
investigation reveals that RPBs are widespread and can significantly compromise
the security of software programs. This paper introduces BugStone, a program
analysis system empowered by LLVM and a Large Language Model (LLM). The key
observation is that many RPBs have one patched instance, which can be leveraged
to identify a consistent error pattern, such as a specific API misuse. By
examining the entire program for this pattern, it is possible to identify
similar sections of code that may be vulnerable. Starting with 135 unique RPBs,
BugStone identified more than 22K new potential issues in the Linux kernel.
Manual analysis of 400 of these findings confirmed that 246 were valid. We also
created a dataset from over 1.9K security bugs reported by 23 recent top-tier
conference works. We manually annotate the dataset, identify 80 recurring
patterns and 850 corresponding fixes. Even with a cost-efficient model choice,
BugStone achieved 92.2% precision and 79.1% pairwise accuracy on the dataset.

</details>


### [30] [David vs. Goliath: A comparative study of different-sized LLMs for code generation in the domain of automotive scenario generation](https://arxiv.org/abs/2510.14115)
*Philipp Bauerfeind,Amir Salarpour,David Fernandez,Pedram MohajerAnsari,Johannes Reschke,Mert D. Pesé*

Main category: cs.SE

TL;DR: NL2Scenic introduces a standardized framework for NL-to-Scenic code generation in autonomous driving, showing mid-size open-source models like Qwen2.5Coder are practical, cost-effective alternatives to larger models.


<details>
  <summary>Details</summary>
Motivation: NL-to-Scenic generation with LLMs suffers from scarce data, limited reproducibility, and inconsistent metrics, requiring standardized evaluation for autonomous driving scenario simulations.

Method: Introduced NL2Scenic, an open dataset and framework with 146 NL/Scenic pairs, difficulty-stratified test split, Example Retriever, and 14 prompting variants (ZS, FS, CoT, SP, MoT). Evaluated 13 models using text and execution metrics, plus expert judgment (n=11). Proposed EDIT-COMP (combined F1 of EDIT-SIM and compilation) as a robust proxy.

Result: GPT-4o performs best; Qwen2.5Coder-14B reaches 88% of expert scores on local hardware. Retrieval-augmented prompting (FSER) improves smaller models. Open-source models outperform CodeLlama at comparable scales. EDIT-SIM correlates best with human judgment, while EDIT-COMP improves ranking fidelity.

Conclusion: NL2Scenic and EDIT-COMP provide a reproducible basis for evaluating Scenic code generation, highlighting mid-size open-source models as practical, cost-effective options for autonomous-driving scenario programming.

Abstract: Scenario simulation is central to testing autonomous driving systems. Scenic,
a domain-specific language (DSL) for CARLA, enables precise and reproducible
scenarios, but NL-to-Scenic generation with large language models (LLMs)
suffers from scarce data, limited reproducibility, and inconsistent metrics. We
introduce NL2Scenic, an open dataset and framework with 146 NL/Scenic pairs, a
difficulty-stratified 30-case test split, an Example Retriever, and 14
prompting variants (ZS, FS, CoT, SP, MoT). We evaluate 13 models: four
proprietary (GPT-4o, GPT-5, Claude-Sonnet-4, Gemini-2.5-pro) and nine
open-source code models (Qwen2.5Coder 0.5B-32B; CodeLlama 7B/13B/34B), using
text metrics (BLEU, ChrF, EDIT-SIM, CrystalBLEU) and execution metrics
(compilation and generation), and compare them with an expert study (n=11).
EDIT-SIM correlates best with human judgments; we also propose EDIT-COMP (F1 of
EDIT-SIM and compilation) as a robust dataset-level proxy that improves ranking
fidelity. GPT-4o performs best overall, while Qwen2.5Coder-14B reaches about 88
percent of its expert score on local hardware. Retrieval-augmented prompting,
Few-Shot with Example Retriever (FSER), consistently boosts smaller models, and
scaling shows diminishing returns beyond mid-size, with Qwen2.5Coder
outperforming CodeLlama at comparable scales. NL2Scenic and EDIT-COMP offer a
standardized, reproducible basis for evaluating Scenic code generation and
indicate that mid-size open-source models are practical, cost-effective options
for autonomous-driving scenario programming.

</details>


### [31] [Caruca: Effective and Efficient Specification Mining for Opaque Software Components](https://arxiv.org/abs/2510.14279)
*Evangelos Lamprou,Seong-Heon Jung,Mayank Keoliya,Lukas Lazarek,Konstantinos Kallas,Michael Greenberg,Nikos Vasilakis*

Main category: cs.SE

TL;DR: Caruca automates specification mining for shell commands by translating docs into structured syntax, executing valid invocations, and extracting properties. It eliminates manual effort and works correctly for 98% of tested commands.


<details>
  <summary>Details</summary>
Motivation: Manual creation of partial specifications for opaque commands is laborious, error-prone, and limits the practicality of specification-dependent systems. Automating this process improves scalability and reliability.

Method: Caruca uses a large language model to translate command documentation into structured syntax, explores valid command invocations and environments, concretely executes them, and extracts properties via system-call and filesystem interposition to generate specifications.

Result: Caruca successfully generated correct specifications for 59 out of 60 tested commands (GNU Coreutils, POSIX, third-party), fully automating the process and enabling immediate integration with existing tools like a state-of-the-art static analysis system.

Conclusion: Caruca effectively automates specification mining for opaque commands, making specification-dependent systems more practical by eliminating manual effort and demonstrating correctness across most tested cases.

Abstract: A wealth of state-of-the-art systems demonstrate impressive improvements in
performance, security, and reliability on programs composed of opaque
components, such as Unix shell commands. To reason about commands, these
systems require partial specifications. However, creating such specifications
is a manual, laborious, and error-prone process, limiting the practicality of
these systems. This paper presents Caruca, a system for automatic specification
mining for opaque commands. To overcome the challenge of language diversity
across commands, Caruca first instruments a large language model to translate a
command's user-facing documentation into a structured invocation syntax. Using
this representation, Caruca explores the space of syntactically valid command
invocations and execution environments. Caruca concretely executes each
command-environment pair, interposing at the system-call and filesystem level
to extract key command properties such as parallelizability and filesystem pre-
and post-conditions. These properties can be exported in multiple specification
formats and are immediately usable by existing systems. Applying Caruca across
60 GNU Coreutils, POSIX, and third-party commands across several
specification-dependent systems shows that Caruca generates correct
specifications for all but one case, completely eliminating manual effort from
the process and currently powering the full specifications for a
state-of-the-art static analysis tool.

</details>


### [32] [A Hybrid, Knowledge-Guided Evolutionary Framework for Personalized Compiler Auto-Tuning](https://arxiv.org/abs/2510.14292)
*Haolin Pan,Hongbin Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: A Hybrid, Knowledge-Guided Evolutionary Framework improves compiler pass auto-tuning by combining offline knowledge extraction and online genetic algorithms, achieving 11.0% better LLVM IR instruction reduction than -Oz.


<details>
  <summary>Details</summary>
Motivation: Traditional one-size-fits-all compiler optimizations (like -O3/-Oz) fail to unlock programs' full performance potential due to the NP-hard nature of finding optimal pass sequences.

Method: The framework uses offline analysis to create a compilation knowledge base with Pass Behavioral Vectors, Pass Groups, Synergy Pass Graphs, and Prototype Sequences, then applies knowledge-infused genetic operators in an online stage for personalized optimization.

Result: Achieved 11.0% average additional LLVM IR instruction reduction over the optimized -Oz baseline across seven datasets.

Conclusion: The hybrid approach demonstrates state-of-the-art performance in discovering personalized, high-efficiency compiler optimization sequences.

Abstract: Compiler pass auto-tuning is critical for enhancing software performance, yet
finding the optimal pass sequence for a specific program is an NP-hard problem.
Traditional, general-purpose optimization flags like -O3 and -Oz adopt a
one-size-fits-all approach, often failing to unlock a program's full
performance potential. To address this challenge, we propose a novel Hybrid,
Knowledge-Guided Evolutionary Framework. This framework intelligently guides
online, personalized optimization using knowledge extracted from a large-scale
offline analysis phase. During the offline stage, we construct a comprehensive
compilation knowledge base composed of four key components: (1) Pass Behavioral
Vectors to quantitatively capture the effectiveness of each optimization; (2)
Pass Groups derived from clustering these vectors based on behavior similarity;
(3) a Synergy Pass Graph to model beneficial sequential interactions; and (4) a
library of Prototype Pass Sequences evolved for distinct program types. In the
online stage, a bespoke genetic algorithm leverages this rich knowledge base
through specially designed, knowledge-infused genetic operators. These
operators transform the search by performing semantically-aware recombination
and targeted, restorative mutations. On a suite of seven public datasets, our
framework achieves an average of 11.0% additional LLVM IR instruction reduction
over the highly-optimized opt -Oz baseline, demonstrating its state-of-the-art
capability in discovering personalized, high-performance optimization
sequences.

</details>


### [33] [A Systematic Study of Time Limit Exceeded Errors in Online Programming Assignments](https://arxiv.org/abs/2510.14339)
*Jialu Zhang,Jialiang Gu,Wangmeiyu Zhang,José Pablo Cambronero,John Kolesar,Ruzica Piskac,Daming Li,Hanyuan Shi*

Main category: cs.SE

TL;DR: Through a 1000-submission analysis of TLE errors, researchers identify algorithm inefficiency, infinite loops, and improper I/O as root causes. Their Nettle system combines LLMs with test feedback to automatically fix TLE errors at 98.5% success rate, validated by both custom and platform evaluators.


<details>
  <summary>Details</summary>
Motivation: TLE errors in online programming platforms lack diagnostic support, forcing users to abandon submissions after repeated failures. Existing tools provide minimal assistance, necessitating automated solutions for reliable TLE error resolution.

Method: The authors conducted a manual analysis of 1000 Codeforces submissions with TLE errors to identify root causes, then developed Nettle - an automated repair tool combining LLMs with compiler/test-case feedback - and Nettle-Eval, a dedicated evaluation framework for TLE repairs.

Result: Nettle achieved a 98.5% repair success rate on 1000 real-world TLE cases, significantly outperforming LLM baselines while maintaining 100% functionality correctness via Nettle-Eval verification and official platform testing.

Conclusion: The paper challenges the conventional view that TLE errors are purely performance issues and demonstrates the effectiveness of their automated repair framework Nettle, achieving a 98.5% fix rate with reliable repairs validated by both their evaluator and platform checkers.

Abstract: Online programming platforms such as Codeforces and LeetCode attract millions
of users seeking to learn to program or refine their skills for industry
interviews. A major challenge for these users is the Time Limit Exceeded (TLE)
error, triggered when a program exceeds the execution time bound. Although
designed as a performance safeguard, TLE errors are difficult to resolve: error
messages provide no diagnostic insight, platform support is minimal, and
existing debugging tools offer little help. As a result, many users abandon
their submissions after repeated TLE failures.
  This paper presents the first large-scale empirical study of TLE errors in
online programming. We manually analyzed 1000 Codeforces submissions with TLE
errors, classified their root causes, and traced how users attempted to fix
them. Our analysis shows that TLE errors often arise not only from inefficient
algorithms but also from infinite loops, improper data structure use, and
inefficient I/O, challenging the conventional view that TLEs are purely
performance issues.
  Guided by these findings, we introduce Nettle, the first automated repair
tool specifically designed for TLE errors, and Nettle-Eval, the first framework
for evaluating TLE repairs. Integrating LLMs with targeted automated feedback
generated by the compiler and test cases, Nettle produces small, correct code
edits that eliminate TLEs while preserving functionality. Evaluated on the same
1000 real-world cases, Nettle achieves a 98.5% fix rate, far exceeding the
strongest LLM baseline, and all of its repairs pass both Nettle-Eval and the
platform's official checker, confirming the reliability of our framework.

</details>


### [34] [PathFix: Automated Program Repair with Expected Path](https://arxiv.org/abs/2510.14341)
*Xu He,Shu Wang,Kun Sun*

Main category: cs.SE

TL;DR: PathFix is a new APR method that uses path-sensitive constraints from correct execution paths to generate accurate patches, outperforming existing APR methods especially for complex code structures.


<details>
  <summary>Details</summary>
Motivation: Existing APR methods generate too many plausible patch candidates and overfit to partial test cases due to the challenge of generating precise specifications.

Method: PathFix uses path-sensitive constraints from correct execution paths, operating in four steps: tracing fault paths, deriving expected paths from desired correct outputs on the control flow graph, generating and evaluating patches by solving state constraints along the paths, and validating generated patches. It also integrates a large language model to improve performance and scalability.

Result: PathFix outperforms existing solutions, particularly in complex program structures like loops and recursion.

Conclusion: PathFix provides a better approach to APR by focusing on correct paths and integrating LLMs, reducing overfitting and improving handling of complex structures.

Abstract: Automated program repair (APR) techniques are effective in fixing inevitable
defects in software, enhancing development efficiency and software robustness.
However, due to the difficulty of generating precise specifications, existing
APR methods face two main challenges: generating too many plausible patch
candidates and overfitting them to partial test cases. To tackle these
challenges, we introduce a new APR method named PathFix, which leverages
path-sensitive constraints extracted from correct execution paths to generate
patches for repairing buggy code. It is based on one observation: if a buggy
program is repairable, at least one expected path is supposed to replace the
fault path in the patched program. PathFix operates in four main steps. First,
it traces fault paths reaching the fault output in the buggy program. Second,
it derives expected paths by analyzing the desired correct output on the
control flow graph, where an expected path defines how a feasible patch leads
to the correct execution. Third, PathFix generates and evaluates patches by
solving state constraints along the expected path. Fourth, we validate the
correctness of the generated patch. To further enhance repair performance and
mitigate scalability issues introduced by path-sensitive analysis, we integrate
a large language model (LLM) into our framework. Experimental results show that
PathFix outperforms existing solutions, particularly in handling complex
program structures such as loops and recursion.

</details>


### [35] [Towards Automated Governance: A DSL for Human-Agent Collaboration in Software Projects](https://arxiv.org/abs/2510.14465)
*Adem Ait,Gwendal Jouneaux,Javier Luis Cánovas Izquierdo,Jordi Cabot*

Main category: cs.SE

TL;DR: Presents a DSL for governing多元 stakeholder collaboration in OSS by formalizing policy enforcement mechanisms for human-AI teams.


<details>
  <summary>Details</summary>
Motivation: Modern software development involves increasingly diverse stakeholders, including AI agents, yet OSS projects lack explicit governance policies. This creates ambiguity and inefficiency in managing contributions and conflicts.

Method: Design and conceptualization of a Domain-Specific Language (DSL) tailored for defining and enforcing governance policies in systems with heterogeneous stakeholders (humans and AI agents).

Result: A novel DSL framework for governance policy specification, enabling structured enforcement of rules for collaborative systems with mixed human-AI participation.

Conclusion: The proposed DSL offers a pathway towards robust, adaptable, and automated governance for diverse stakeholders in software projects, particularly enhancing collaboration in Open-Source Software (OSS) contexts.

Abstract: The stakeholders involved in software development are becoming increasingly
diverse, with both human contributors from varied backgrounds and AI-powered
agents collaborating together in the process. This situation presents unique
governance challenges, particularly in Open-Source Software (OSS) projects,
where explicit policies are often lacking or unclear. This paper presents the
vision and foundational concepts for a novel Domain-Specific Language (DSL)
designed to define and enforce rich governance policies in systems involving
diverse stakeholders, including agents. This DSL offers a pathway towards more
robust, adaptable, and ultimately automated governance, paving the way for more
effective collaboration in software projects, especially OSS ones.

</details>


### [36] [E2Edev: Benchmarking Large Language Models in End-to-End Software Development Task](https://arxiv.org/abs/2510.14509)
*Jingyao Liu,Chen Huang,Zhizhao Guan,Wenqiang Lei,Yang Deng*

Main category: cs.SE

TL;DR: E2EDev is an end-to-end testing framework with a human-in-the-loop annotation system (HITL-MAA), revealing current E2ESD frameworks' limitations. The paper underscores the need for better E2ESD solutions.


<details>
  <summary>Details</summary>
Motivation: Existing E2ESD frameworks struggle with effectiveness and cost-efficiency in automated testing. Annotation efforts require reduction while maintaining quality.

Method: 1. Developed E2EDev: fine-grained requirements, BDD test scenarios with Python implementations, and Behave-based automation. 2. Introduced HITL-MAA to reduce annotation effort and improve quality. 3. Evaluated E2ESD frameworks and LLM backbones using E2EDev.

Result: Analysis shows current frameworks consistently underperform on the E2EDev benchmark, highlighting unresolved challenges in E2ESD tasks.

Conclusion: Current E2ESD solutions are inadequate; the open-source E2EDev benchmark and findings call for improved frameworks and cost-effective methodologies.

Abstract: E2EDev comprises (i) a fine-grained set of user requirements, (ii) {multiple
BDD test scenarios with corresponding Python step implementations for each
requirement}, and (iii) a fully automated testing pipeline built on the Behave
framework. To ensure its quality while reducing the annotation effort, E2EDev
leverages our proposed Human-in-the-Loop Multi-Agent Annotation Framework
(HITL-MAA). {By evaluating various E2ESD frameworks and LLM backbones with
E2EDev}, our analysis reveals a persistent struggle to effectively solve these
tasks, underscoring the critical need for more effective and cost-efficient
E2ESD solutions. Our codebase and benchmark are publicly available at
https://github.com/SCUNLP/E2EDev.

</details>


### [37] [Software Testing Education and Industry Needs - Report from the ENACTEST EU Project](https://arxiv.org/abs/2510.14625)
*Mehrdad Saadatmand,Abbas Khan,Beatriz Marin,Ana C. R Paiva,Nele Van Asch,Graham Moran,Felix Cammaerts,Monique Snoeck,Alexandra Mendes*

Main category: cs.SE

TL;DR: This collaboration between industry and academia identifies growing competencies needed in software testing, revealing gaps in teaching AI/ML testing, security testing, and soft skills. Based on interviews, focus groups, and a scoping review, the paper calls for curriculum updates to address industry needs like secure systems testing and AI-driven verification techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the evolving software development landscape requiring testers to adapt to new tools and practices, while current testing education appears insufficient in addressing industrial competency needs and emerging gaps.

Method: The study used two focus groups, industry interviews (railway, healthcare, software consulting), and a curated scoping review. The instrument was co-designed with the ENACTEST project consortium through iterative refinement to ensure alignment with industry and educational needs.

Result: Key results include insights into training methodologies, challenges in industrial training delivery, evaluation frameworks for training quality, mismatches between academic education and industry requirements, future trends (e.g., AI/ML testing), and gaps in knowledge transfer practices within companies.

Conclusion: The study concludes that there are significant knowledge gaps in areas like AI testing, security testing, and soft skills, highlighting the need for updated testing education that aligns with industry demands and emerging trends.

Abstract: The evolving landscape of software development demands that software testers
continuously adapt to new tools, practices, and acquire new skills. This study
investigates software testing competency needs in industry, identifies
knowledge gaps in current testing education, and highlights competencies and
gaps not addressed in academic literature. This is done by conducting two focus
group sessions and interviews with professionals across diverse domains,
including railway industry, healthcare, and software consulting and performing
a curated small-scale scoping review. The study instrument, co-designed by
members of the ENACTEST project consortium, was developed collaboratively and
refined through multiple iterations to ensure comprehensive coverage of
industry needs and educational gaps. In particular, by performing a thematic
qualitative analysis, we report our findings and observations regarding:
professional training methods, challenges in offering training in industry,
different ways of evaluating the quality of training, identified knowledge gaps
with respect to academic education and industry needs, future needs and trends
in testing education, and knowledge transfer methods within companies. Finally,
the scoping review results confirm knowledge gaps in areas such as AI testing,
security testing and soft skills.

</details>


### [38] [ATGen: Adversarial Reinforcement Learning for Test Case Generation](https://arxiv.org/abs/2510.14635)
*Qingyao Li,Xinyi Dai,Weiwen Liu,Xiangyang Li,Yasheng Wang,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.SE

TL;DR: ATGen breaks the fixed-difficulty ceiling of static test generation by using adversarial reinforcement learning to dynamically evolve test cases, improving code reliability and enabling practical applications as an inference filter and training reward source for code generation models.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the critical bottleneck of static test datasets in code generation, which impose a 'fixed-difficulty ceiling' that prevents uncovering novel or more complex bugs beyond the training scope of existing test generation methods.

Method: ATGen employs adversarial reinforcement learning with a curriculum framework. A test case generator is trained against an adversarial code generator that creates progressively harder-to-detect bugs, while being optimized to maximize both output accuracy and attack success through a dual-objective reinforcement learning strategy.

Result: Experiments show ATGen significantly outperforms state-of-the-art baselines. It demonstrates practical value as both a stronger Best-of-N filter for generated code and a higher-quality reward source for training code generation models.

Conclusion: This paper introduces ATGen, an adversarial reinforcement learning framework that surpasses the limitations of static test generation methods by dynamically evolving test cases to uncover increasingly complex bugs in code generated by Large Language Models (LLMs). The approach establishes a new paradigm for improving LLM-generated code reliability and demonstrates practical utility in filtering inference outputs and training reward models.

Abstract: Large Language Models (LLMs) excel at code generation, yet their outputs
often contain subtle bugs, for which effective test cases are a critical
bottleneck. Existing test generation methods, whether based on prompting or
supervised fine-tuning, rely on static datasets. This imposes a
``fixed-difficulty ceiling'', fundamentally limiting their ability to uncover
novel or more complex bugs beyond their training scope. To overcome this, we
introduce ATGen, a framework that trains a test case generator via adversarial
reinforcement learning. ATGen pits a test generator against an adversarial code
generator that continuously crafts harder bugs to evade the current policy.
This dynamic loop creates a curriculum of increasing difficulty challenging
current policy. The test generator is optimized via Reinforcement Learning (RL)
to jointly maximize ``Output Accuracy'' and ``Attack Success'', enabling it to
learn a progressively stronger policy that breaks the fixed-difficulty ceiling
of static training. Extensive experiments demonstrate that ATGen significantly
outperforms state-of-the-art baselines. We further validate its practical
utility, showing it serves as both a more effective filter for Best-of-N
inference and a higher-quality reward source for training code generation
models. Our work establishes a new, dynamic paradigm for improving the
reliability of LLM-generated code.

</details>


### [39] [Requirement Identification for Traffic Simulations in Driving Simulators](https://arxiv.org/abs/2510.14653)
*Sven Tarlowski,Lutz Eckstein*

Main category: cs.SE

TL;DR: The paper introduces a methodology for systematic identification of traffic simulation requirements to ensure realistic traffic conditions in automotive studies.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the need for high fidelity in traffic simulations to ensure valid experimental outcomes and maintain participant engagement in automotive development and testing.

Method: The methodology employs a structured approach based on sub-goals defined in each phase of a study. It identifies specific technical requirements for microscopic levels, agent models, and visual representation through these sub-goals.

Result: The result is a framework that links study objectives directly to traffic simulation design, enabling a more methodical and precise setup for simulations that are realistic and efficient.

Conclusion: The paper concludes that the proposed approach significantly supports automotive development by ensuring realistic traffic conditions through systematic simulation design.

Abstract: This paper addresses the challenge of ensuring realistic traffic conditions
by proposing a methodology that systematically identifies traffic simulation
requirements. Using a structured approach based on sub-goals in each study
phase, specific technical needs are derived for microscopic levels, agent
models, and visual representation. The methodology aims to maintain a high
degree of fidelity, enhancing both the validity of experimental outcomes and
participant engagement. By providing a clear link between study objectives and
traffic simulation design, this approach supports robust automotive development
and testing.

</details>


### [40] [LLM Agents for Automated Web Vulnerability Reproduction: Are We There Yet?](https://arxiv.org/abs/2510.14700)
*Bin Liu,Yanjie Zhao,Guoai Xu,Haoyu Wang*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language model (LLM) agents have demonstrated remarkable capabilities
in software engineering and cybersecurity tasks, including code generation,
vulnerability discovery, and automated testing. One critical but underexplored
application is automated web vulnerability reproduction, which transforms
vulnerability reports into working exploits. Although recent advances suggest
promising potential, challenges remain in applying LLM agents to real-world web
vulnerability reproduction scenarios. In this paper, we present the first
comprehensive evaluation of state-of-the-art LLM agents for automated web
vulnerability reproduction. We systematically assess 20 agents from software
engineering, cybersecurity, and general domains across 16 dimensions, including
technical capabilities, environment adaptability, and user experience factors,
on 3 representative web vulnerabilities. Based on the results, we select three
top-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation
on our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types
and 6 web technologies. Our results reveal that while LLM agents achieve
reasonable success on simple library-based vulnerabilities, they consistently
fail on complex service-based vulnerabilities requiring multi-component
environments. Complex environment configurations and authentication barriers
create a gap where agents can execute exploit code but fail to trigger actual
vulnerabilities. We observe high sensitivity to input guidance, with
performance degrading by over 33% under incomplete authentication information.
Our findings highlight the significant gap between current LLM agent
capabilities and the demands of reliable automated vulnerability reproduction,
emphasizing the need for advances in environmental adaptation and autonomous
problem-solving capabilities.

</details>


### [41] [Leveraging Code Cohesion Analysis to Identify Source Code Supply Chain Attacks](https://arxiv.org/abs/2510.14778)
*Maor Reuben,Ido Mendel,Or Feldman,Moshe Kravchik,Mordehai Guri,Rami Puzis*

Main category: cs.SE

TL;DR: This paper introduces an unsupervised method to detect supply chain attacks by analyzing cohesion disruptions in source code using a name-prediction-based cohesion (NPC) metric. Experimental results show reduced cohesion and altered naming patterns in malicious code injections, with detection precision metrics under imbalanced attack scenarios.


<details>
  <summary>Details</summary>
Motivation: Supply chain attacks pose severe software security risks but are rare and challenging to detect automatically due to their deceptive nature and the need to interpret code intentions. Current tools struggle with contextual understanding of code changes.

Method: The study proposes an NPC metric to quantify function cohesion changes, comparing malicious injections to natural code updates. A large-scale analysis of 54,707 functions from C++ repositories is performed, evaluating detection effectiveness under extreme class imbalance.

Result: Code injections significantly decrease cohesion and shift naming patterns toward shorter names. The method achieves 36.41%% Precision@100 at 1:1,000 attack ratio and 12.47%% at 1:10,000, demonstrating effectiveness in imbalanced datasets.

Conclusion: Automated cohesion measurement, particularly NPC, shows potential for identifying supply chain attacks by detecting cohesion disruptions, offering an improved approach to maintaining source code integrity.

Abstract: Supply chain attacks significantly threaten software security with malicious
code injections within legitimate projects. Such attacks are very rare but may
have a devastating impact. Detecting spurious code injections using automated
tools is further complicated as it often requires deciphering the intention of
both the inserted code and its context. In this study, we propose an
unsupervised approach for highlighting spurious code injections by quantifying
cohesion disruptions in the source code. Using a name-prediction-based cohesion
(NPC) metric, we analyze how function cohesion changes when malicious code is
introduced compared to natural cohesion fluctuations. An analysis of 54,707
functions over 369 open-source C++ repositories reveals that code injection
reduces cohesion and shifts naming patterns toward shorter, less descriptive
names compared to genuine function updates. Considering the sporadic nature of
real supply-chain attacks, we evaluate the proposed method with extreme
test-set imbalance and show that monitoring high-cohesion functions with NPC
can effectively detect functions with injected code, achieving a Precision@100
of 36.41% at a 1:1,000 ratio and 12.47% at 1:10,000. These results suggest that
automated cohesion measurements, in general, and name-prediction-based
cohesion, in particular, may help identify supply chain attacks, improving
source code integrity.

</details>


### [42] [Instruction Set Migration at Warehouse Scale](https://arxiv.org/abs/2510.14928)
*Eric Christopher,Kevin Crossan,Wolff Dobson,Chris Kennelly,Drew Lewis,Kun Lin,Martin Maas,Parthasarathy Ranganathan,Emma Rapati,Brian Yang*

Main category: cs.SE

TL;DR: The paper analyzes Google's large-scale migration from x86 to Arm (30,000+ code commits) to identify a taxonomy of tasks involved in modern ISA migration, emphasizing the new challenges beyond binary translation and the role of AI in automation.


<details>
  <summary>Details</summary>
Motivation: There has been limited academic focus on ISA migrations like x86 to Arm migration mainly scoped on binary translation, missing the new challenges introduced by recompiling from open-source.

Method: The analysis of Google's extensive x86 to Arm migration by examining the full set of code commits. 

Result: A new taxonomy of tasks for modern ISA migration is identified, AI's role in automation is demonstrated, and challenging areas are highlighted.

Conclusion: Modern ISA migrations present new multi-faceted challenges requiring beyond binary translation, AI can help but further research is needed for remaining challenges.

Abstract: Migrating codebases from one instruction set architecture (ISA) to another is
a major engineering challenge. A recent example is the adoption of Arm (in
addition to x86) across the major Cloud hyperscalers. Yet, this problem has
seen limited attention by the academic community. Most work has focused on
static and dynamic binary translation, and the traditional conventional wisdom
has been that this is the primary challenge.
  In this paper, we show that this is no longer the case. Modern ISA migrations
can often build on a robust open-source ecosystem, making it possible to
recompile all relevant software from scratch. This introduces a new and
multifaceted set of challenges, which are different from binary translation.
  By analyzing a large-scale migration from x86 to Arm at Google, spanning
almost 40,000 code commits, we derive a taxonomy of tasks involved in ISA
migration. We show how Google automated many of the steps involved, and
demonstrate how AI can play a major role in automatically addressing these
tasks. We identify tasks that remain challenging and highlight research
challenges that warrant further attention.

</details>
