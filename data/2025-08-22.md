<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [On the need to perform comprehensive evaluations of automated program repair benchmarks: Sorald case study](https://arxiv.org/abs/2508.15135)
*Sumudu Liyanage,Sherlock A. Licorish,Markus Wagner,Stephen G. MacDonell*

Main category: cs.SE

TL;DR: This study evaluates APR tool Sorald using a comprehensive framework, revealing introduced faults and code degradation despite violation fixes. It advocates for holistic evaluation methods to ensure safe APR tool adoption.


<details>
  <summary>Details</summary>
Motivation: Existing APR tool evaluations focus narrowly on violation clearance, neglecting potential new faults, functional changes, and structural degradation. This study addresses the need for holistic evaluation frameworks to assess APR tool impacts comprehensively.

Method: The researchers evaluated Sorald, a state-of-the-art APR tool, by analyzing its performance in repairing 3,529 SonarQube violations across 2,393 Java code snippets. They assessed introduced faults, functional correctness via unit test failure rates, and code structure degradation.

Result: Sorald introduced 2,120 new faults (32 bugs, 2,088 code smells), exhibited a 24% unit test failure rate, and degraded code structure while fixing violations. These findings demonstrate the limitations of current APR tool evaluation practices.

Conclusion: This study emphasizes the need for comprehensive evaluation frameworks for APR tools to capture both their benefits and introduced issues, ensuring safe and effective adoption.

Abstract: In supporting the development of high-quality software, especially necessary
in the era of LLMs, automated program repair (APR) tools aim to improve code
quality by automatically addressing violations detected by static analysis
profilers. Previous research tends to evaluate APR tools only for their ability
to clear violations, neglecting their potential introduction of new (sometimes
severe) violations, changes to code functionality and degrading of code
structure. There is thus a need for research to develop and assess
comprehensive evaluation frameworks for APR tools. This study addresses this
research gap, and evaluates Sorald (a state-of-the-art APR tool) as a proof of
concept. Sorald's effectiveness was evaluated in repairing 3,529 SonarQube
violations across 30 rules within 2,393 Java code snippets extracted from Stack
Overflow. Outcomes show that while Sorald fixes specific rule violations, it
introduced 2,120 new faults (32 bugs, 2088 code smells), reduced code
functional correctness--as evidenced by a 24% unit test failure rate--and
degraded code structure, demonstrating the utility of our framework. Findings
emphasize the need for evaluation methodologies that capture the full spectrum
of APR tool effects, including side effects, to ensure their safe and effective
adoption.

</details>


### [2] [Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems](https://arxiv.org/abs/2508.15411)
*Frederik Vandeputte*

Main category: cs.SE

TL;DR: This paper proposes integrating GenAI with traditional software principles using five design pillars and architectural patterns to build reliable, adaptive systems, while advocating for further research and validation.


<details>
  <summary>Details</summary>
Motivation: GenAI's unpredictability and inefficiency hinder reliable system development, necessitating a paradigm shift to integrate cognitive capabilities with traditional software engineering principles.

Method: The authors introduce five foundational GenAI-native design principles (reliability, excellence, evolvability, self-reliance, assurance) and propose architectural patterns (GenAI-native cells, organic substrates, programmable routers) to create robust systems.

Result: They outline a GenAI-native software stack, analyze technical/user/economic/legal impacts, and highlight the importance of validation through experimentation.

Conclusion: The paper aims to inspire future research and encourage the implementation and refinement of the proposed GenAI-native conceptual framework, emphasizing the need for further validation and experimentation.

Abstract: Generative AI (GenAI) has emerged as a transformative technology,
demonstrating remarkable capabilities across diverse application domains.
However, GenAI faces several major challenges in developing reliable and
efficient GenAI-empowered systems due to its unpredictability and inefficiency.
This paper advocates for a paradigm shift: future GenAI-native systems should
integrate GenAI's cognitive capabilities with traditional software engineering
principles to create robust, adaptive, and efficient systems.
  We introduce foundational GenAI-native design principles centered around five
key pillars -- reliability, excellence, evolvability, self-reliance, and
assurance -- and propose architectural patterns such as GenAI-native cells,
organic substrates, and programmable routers to guide the creation of resilient
and self-evolving systems. Additionally, we outline the key ingredients of a
GenAI-native software stack and discuss the impact of these systems from
technical, user adoption, economic, and legal perspectives, underscoring the
need for further validation and experimentation. Our work aims to inspire
future research and encourage relevant communities to implement and refine this
conceptual framework.

</details>
