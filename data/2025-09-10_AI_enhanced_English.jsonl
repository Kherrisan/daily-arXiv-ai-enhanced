{"id": "2509.07016", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07016", "abs": "https://arxiv.org/abs/2509.07016", "authors": ["Muhammad Arif Hakimi Zamrai", "Kamaludin Mohd Yusof"], "title": "Random Forest Stratified K-Fold Cross Validation on SYN DoS Attack SD-IoV", "comment": null, "summary": "In response to the prevalent concern of TCP SYN flood attacks within the\ncontext of Software-Defined Internet of Vehicles (SD-IoV), this study addresses\nthe significant challenge of network security in rapidly evolving vehicular\ncommunication systems. This research focuses on optimizing a Random Forest\nClassifier model to achieve maximum accuracy and minimal detection time,\nthereby enhancing vehicular network security. The methodology involves\npreprocessing a dataset containing SYN attack instances, employing feature\nscaling and label encoding techniques, and applying Stratified K-Fold\ncross-validation to target key metrics such as accuracy, precision, recall, and\nF1-score. This research achieved an average value of 0.999998 for all metrics\nwith a SYN DoS attack detection time of 0.24 seconds. Results show that the\nfine-tuned Random Forest model, configured with 20 estimators and a depth of\n10, effectively differentiates between normal and malicious traffic with high\naccuracy and minimal detection time, which is crucial for SD-IoV networks. This\napproach marks a significant advancement and introduces a state-of-the-art\nalgorithm in detecting SYN flood attacks, combining high accuracy with minimal\ndetection time. It contributes to vehicular network security by providing a\nrobust solution against TCP SYN flood attacks while maintaining network\nefficiency and reliability.", "AI": {"tldr": "Optimized Random Forest model achieves near-perfect metrics and 0.24s detection time for TCP SYN flood attacks in SD-IoV networks.", "motivation": "Addressing network security vulnerabilities in dynamic Software-Defined Internet of Vehicles (SD-IoV) systems threatened by TCP SYN flood attacks, requiring high-accuracy, low-latency solutions to maintain both security and network efficiency.", "method": "A Random Forest Classifier was optimized through feature scaling, label encoding preprocessing, and Stratified K-Fold cross-validation, using 20 estimators and depth-10 tree configuration.", "result": "Achieved 0.999998 average accuracy/precision/recall/F1-score with 0.24s detection time, effectively distinguishing normal and malicious traffic in vehicular networks.", "conclusion": "This approach establishes a state-of-the-art solution for SYN flood detection in SD-IoV, balancing perfect accuracy with real-time performance requirements critical for vehicular communication systems."}}
{"id": "2509.07053", "categories": ["cs.CR", "cs.CY", "J.4; K.4.1; K.4.3; K.5.0; K.5.2; K.6.5"], "pdf": "https://arxiv.org/pdf/2509.07053", "abs": "https://arxiv.org/abs/2509.07053", "authors": ["Paul Benjamin Lowry", "Gregory D. Moody", "Robert Willison", "Clay Posey"], "title": "The Signalgate Case is Waiving a Red Flag to All Organizational and Behavioral Cybersecurity Leaders, Practitioners, and Researchers: Are We Receiving the Signal Amidst the Noise?", "comment": null, "summary": "The Signalgate incident of March 2025, wherein senior US national security\nofficials inadvertently disclosed sensitive military operational details via\nthe encrypted messaging platform Signal, highlights critical vulnerabilities in\norganizational security arising from human error, governance gaps, and the\nmisuse of technology. Although smaller in scale when compared to historical\nbreaches involving billions of records, Signalgate illustrates critical\nsystemic issues often overshadowed by a focus on external cyber threats.\nEmploying a case-study approach and systematic review grounded in the NIST\nCybersecurity Framework, we analyze the incident to identify patterns of\nhuman-centric vulnerabilities and governance challenges common to\norganizational security failures. Findings emphasize three critical points. (1)\nOrganizational security depends heavily on human behavior, with internal actors\noften serving as the weakest link despite advanced technical defenses; (2)\nLeadership tone strongly influences organizational security culture and\nefficacy, and (3) widespread reliance on technical solutions without sufficient\ninvestments in human and organizational factors leads to ineffective practices\nand wasted resources. From these observations, we propose actionable\nrecommendations for enhancing organizational and national security, including\nstrong leadership engagement, comprehensive adoption of zero-trust\narchitectures, clearer accountability structures, incentivized security\nbehaviors, and rigorous oversight. Particularly during periods of\norganizational transition, such as mergers or large-scale personnel changes,\nadditional measures become particularly important. Signalgate underscores the\nneed for leaders and policymakers to reorient cybersecurity strategies toward\naddressing governance, cultural, and behavioral risks.", "AI": {"tldr": "The Signalgate incident exemplifies how human error and governance flaws can undermine organizational security despite robust technical defenses. This paper uses a NIST-based analysis to propose that security strategies must shift toward proactive governance, leadership-driven culture, and human-centric investments to address systemic risks.", "motivation": "The Signalgate incident revealed systemic vulnerabilities in organizational security stemming from human error, governance gaps, and technology misuse. The paper aims to shift the focus from external cyber threats to internal human and organizational factors often overlooked in security practices.", "method": "The study employs a case-study approach and systematic review grounded in the NIST Cybersecurity Framework to analyze patterns of human-centric vulnerabilities and governance challenges associated with the Signalgate incident.", "result": "The analysis identifies three key findings: (1) Human behavior is the critical vulnerability in organizational security, (2) Leadership tone directly shapes security culture and efficacy, and (3) Overreliance on technical solutions without addressing human/organizational factors leads to ineffective practices. Recommendations include zero-trust architectures, leadership engagement, and structured accountability frameworks.", "conclusion": "The paper concludes that organizational security strategies must prioritize addressing governance, cultural, and behavioral risks, particularly during transitions like mergers. Leaders and policymakers need to adopt holistic approaches emphasizing zero-trust architectures, leadership engagement, and human-centric solutions to mitigate systemic vulnerabilities highlighted by the Signalgate incident."}}
{"id": "2509.07055", "categories": ["cs.CR", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.07055", "abs": "https://arxiv.org/abs/2509.07055", "authors": ["Tom\u00e1s Gonz\u00e1lez", "Mateo Dulce-Rubio", "Aaditya Ramdas", "M\u00f3nica Ribero"], "title": "Sequentially Auditing Differential Privacy", "comment": null, "summary": "We propose a practical sequential test for auditing differential privacy\nguarantees of black-box mechanisms. The test processes streams of mechanisms'\noutputs providing anytime-valid inference while controlling Type I error,\novercoming the fixed sample size limitation of previous batch auditing methods.\nExperiments show this test detects violations with sample sizes that are orders\nof magnitude smaller than existing methods, reducing this number from 50K to a\nfew hundred examples, across diverse realistic mechanisms. Notably, it\nidentifies DP-SGD privacy violations in \\textit{under} one training run, unlike\nprior methods needing full model training.", "AI": {"tldr": "This paper introduces an efficient sequential test for differential privacy auditing that reduces sample requirements by 1000\u00d7 and detects violations faster than existing methods, including identifying DP-SGD issues in under one training run.", "motivation": "Existing batch auditing methods suffer from fixed sample size requirements and inefficiencies (e.g., needing 50K samples or full model training) which limits practical differential privacy auditing.", "method": "A sequential testing framework processing output streams with anytime-valid inference and Type I error control to audit differential privacy in black-box mechanisms.", "result": "Achieves orders-of-magnitude reduction in sample size (50K \u2192 hundreds) and detects DP-SGD violations within a single training run, unlike prior methods requiring full training cycles.", "conclusion": "The proposed sequential test outperforms existing methods by significantly reducing required sample sizes and detecting DP violations efficiently, including in under one training run for DP-SGD."}}
{"id": "2509.07131", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07131", "abs": "https://arxiv.org/abs/2509.07131", "authors": ["Nicol\u00f2 Romandini", "Carlo Mazzocca", "Kai Otsuki", "Rebecca Montanari"], "title": "SoK: Security and Privacy of AI Agents for Blockchain", "comment": "This work has been accepted to the 7th International Conference on\n  Blockchain Computing and Applications (BCCA 2025)", "summary": "Blockchain and smart contracts have garnered significant interest in recent\nyears as the foundation of a decentralized, trustless digital ecosystem,\nthereby eliminating the need for traditional centralized authorities. Despite\ntheir central role in powering Web3, their complexity still presents\nsignificant barriers for non-expert users. To bridge this gap, Artificial\nIntelligence (AI)-based agents have emerged as valuable tools for interacting\nwith blockchain environments, supporting a range of tasks, from analyzing\non-chain data and optimizing transaction strategies to detecting\nvulnerabilities within smart contracts. While interest in applying AI to\nblockchain is growing, the literature still lacks a comprehensive survey that\nfocuses specifically on the intersection with AI agents. Most of the related\nwork only provides general considerations, without focusing on any specific\ndomain. This paper addresses this gap by presenting the first Systematization\nof Knowledge dedicated to AI-driven systems for blockchain, with a special\nfocus on their security and privacy dimensions, shedding light on their\napplications, limitations, and future research directions.", "AI": {"tldr": "This paper is the first to systematically review AI agents for blockchain security and privacy, addressing prior gaps in focused literature and outlining applications, limitations, and future research paths.", "motivation": "The paper is motivated by the identified gap in literature: while blockchain and AI agent integration is growing, no existing surveys specifically focus on AI-driven systems for blockchain, particularly in security and privacy, leaving non-experts and researchers without structured guidance.", "method": "The authors employ a Systematization of Knowledge (SoK) approach, systematically reviewing and organizing existing research at the intersection of AI agents and blockchain, with an emphasis on security and privacy.", "result": "The result is a structured SoK that synthesizes current AI applications in blockchain ecosystems, evaluates their security and privacy implications, and highlights limitations and future research directions in the field.", "conclusion": "The paper concludes that by presenting the first Systematization of Knowledge on AI-driven blockchain systems, particularly security and privacy aspects, the study offers a comprehensive foundation for future research and applications, addressing the previous lack of focused surveys in this domain."}}
{"id": "2509.07449", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.07449", "abs": "https://arxiv.org/abs/2509.07449", "authors": ["Mterorga Ukor"], "title": "Aspect-Oriented Programming in Secure Software Development: A Case Study of Security Aspects in Web Applications", "comment": "10 pages, 3 figures", "summary": "Security remains a critical challenge in modern web applications, where\nthreats such as unauthorized access, data breaches, and injection attacks\ncontinue to undermine trust and reliability. Traditional Object-Oriented\nProgramming (OOP) often intertwines security logic with business functionality,\nleading to code tangling, scattering, and reduced maintainability. This study\ninvestigates the role of Aspect-Oriented Programming (AOP) in enhancing secure\nsoftware development by modularizing cross-cutting security concerns. Using a\ncase study approach, we compare AOP-based implementations of security features\nincluding authentication, authorization, input validation, encryption, logging,\nand session management with conventional OOP or middleware-based approaches.\nData collection involves analyzing code quality metrics (e.g., lines of code,\ncoupling, cohesion, modularity index, reusability), performance metrics\n(response time, throughput, memory usage), and maintainability indicators.\nDeveloper feedback is also incorporated to assess integration and debugging\nexperiences. Statistical methods, guided by the ISO/IEC 25010 software quality\nmodel, are applied to evaluate differences across implementations. The findings\ndemonstrate that AOP enhances modularity, reusability, and maintainability of\nsecurity mechanisms, while introducing only minimal performance overhead. The\nstudy contributes practical insights for software engineers and researchers\nseeking to balance security with software quality in web application\ndevelopment.", "AI": {"tldr": "This study evaluates how Aspect-Oriented Programming (AOP) improves security modularity and maintainability in web applications through case studies and metrics, showing AOP introduces minimal performance overhead while enhancing code quality versus traditional OOP and middleware approaches.", "motivation": "Traditional OOP intertwines security logic with business functionality, causing code tangling and reduced maintainability, while web applications face persistent security threats like unauthorized access, data breaches, and injection attacks.", "method": "The study uses a case study approach comparing AOP-based implementations of security features with conventional OOP or middleware-based approaches, evaluating code quality metrics, performance metrics, and developer feedback using statistical methods aligned with the ISO/IEC 25010 software quality model.", "result": "AOP enhances modularity, reusability, and maintainability of security mechanisms with minimal performance overhead compared to conventional approaches.", "conclusion": "The study contributes practical insights for software engineers and researchers seeking to balance security with software quality in web application development."}}
{"id": "2509.07225", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.07225", "abs": "https://arxiv.org/abs/2509.07225", "authors": ["Ze Sheng", "Qingxiao Xu", "Jianwei Huang", "Matthew Woodcock", "Heqing Huang", "Alastair F. Donaldson", "Guofei Gu", "Jeff Huang"], "title": "All You Need Is A Fuzzing Brain: An LLM-Powered System for Automated Vulnerability Detection and Patching", "comment": "14 pages, 5 figures", "summary": "Our team, All You Need Is A Fuzzing Brain, was one of seven finalists in\nDARPA's Artificial Intelligence Cyber Challenge (AIxCC), placing fourth in the\nfinal round. During the competition, we developed a Cyber Reasoning System\n(CRS) that autonomously discovered 28 security vulnerabilities - including six\npreviously unknown zero-days - in real-world open-source C and Java projects,\nand successfully patched 14 of them. The complete CRS is open source at\nhttps://github.com/o2lab/afc-crs-all-you-need-is-a-fuzzing-brain. This paper\nprovides a detailed technical description of our CRS, with an emphasis on its\nLLM-powered components and strategies. Building on AIxCC, we further introduce\na public leaderboard for benchmarking state-of-the-art LLMs on vulnerability\ndetection and patching tasks, derived from the AIxCC dataset. The leaderboard\nis available at https://o2lab.github.io/FuzzingBrain-Leaderboard/.", "AI": {"tldr": "Team 'All You Need Is A Fuzzing Brain' placed 4th in DARPA's AIxCC by developing an open-source LLM-powered Cyber Reasoning System that found and patched 28 vulnerabilities, including 6 zero-days, while introducing a new LLM benchmarking leaderboard.", "motivation": "To advance cybersecurity by creating autonomous systems capable of discovering and mitigating vulnerabilities in real-world codebases, as required by DARPA's AIxCC competition.", "method": "The team developed an autonomous Cyber Reasoning System (CRS) leveraging Large Language Models (LLMs) to detect vulnerabilities and generate patches, combining fuzzing techniques with AI-based analysis.", "result": "The CRS discovered 28 security vulnerabilities in the AIxCC competition (six zero-days) and patched 14, achieving 4th place among seven finalists. They also established a public leaderboard for benchmarking LLMs on vulnerability tasks.", "conclusion": "The team's work demonstrates the effectiveness of LLM-powered Cyber Reasoning Systems in identifying and patching security vulnerabilities, contributing to open-source tools and a new public leaderboard for benchmarking future research."}}
{"id": "2509.07498", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.07498", "abs": "https://arxiv.org/abs/2509.07498", "authors": ["Hai Dinh-Tuan"], "title": "CRACI: A Cloud-Native Reference Architecture for the Industrial Compute Continuum", "comment": null, "summary": "The convergence of Information Technology (IT) and Operational Technology\n(OT) in Industry 4.0 exposes the limitations of traditional, hierarchical\narchitectures like ISA-95 and RAMI 4.0. Their inherent rigidity, data silos,\nand lack of support for cloud-native technologies impair the development of\nscalable and interoperable industrial systems. This paper addresses this issue\nby introducing CRACI, a Cloud-native Reference Architecture for the Industrial\nCompute Continuum. Among other features, CRACI promotes a decoupled and\nevent-driven model to enable flexible, non-hierarchical data flows across the\ncontinuum. It embeds cross-cutting concerns as foundational pillars: Trust,\nGovernance & Policy, Observability, and Lifecycle Management, ensuring quality\nattributes are core to the design. The proposed architecture is validated\nthrough a two-fold approach: (1) a comparative theoretical analysis against\nestablished standards, operational models, and academic proposals; and (2) a\nquantitative evaluation based on performance data from previously published\nreal-world smart manufacturing implementations. The results demonstrate that\nCRACI provides a viable, state-of-the-art architecture that utilizes the\ncompute continuum to overcome the structural limitations of legacy models and\nenable scalable, modern industrial systems.", "AI": {"tldr": "This paper introduces CRACI, a cloud-native architecture addressing industry 4.0's need for flexible, interoperable systems by overcoming the rigidity of traditional IT/OT frameworks.", "motivation": "Traditional hierarchical IT/OT architectures (ISA-95, RAMI 4.0) are rigid, create data silos, and lack cloud-native support, impeding scalable, interoperable Industry 4.0 systems.", "method": "Proposes CRACI (Cloud-native Reference Architecture) with decoupled, event-driven design and embedded cross-cutting pillars (Trust, Governance, Observability, Lifecycle Management). Validated through theoretical comparison with standards and quantitative real-world smart manufacturing data.", "result": "Demonstrated CRACI's ability to overcome structural limitations of legacy models through theoretical validation and real-world performance comparisons, proving its effectiveness in enabling modern industrial compute continua.", "conclusion": "CRACI provides a viable, state-of-the-art architecture that addresses the limitations of traditional models by utilizing the compute continuum to enable scalable, modern industrial systems."}}
{"id": "2509.07287", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07287", "abs": "https://arxiv.org/abs/2509.07287", "authors": ["Yan Pang", "Wenlong Meng", "Xiaojing Liao", "Tianhao Wang"], "title": "Paladin: Defending LLM-enabled Phishing Emails with a New Trigger-Tag Paradigm", "comment": "20 pages", "summary": "With the rapid development of large language models, the potential threat of\ntheir malicious use, particularly in generating phishing content, is becoming\nincreasingly prevalent. Leveraging the capabilities of LLMs, malicious users\ncan synthesize phishing emails that are free from spelling mistakes and other\neasily detectable features. Furthermore, such models can generate\ntopic-specific phishing messages, tailoring content to the target domain and\nincreasing the likelihood of success.\n  Detecting such content remains a significant challenge, as LLM-generated\nphishing emails often lack clear or distinguishable linguistic features. As a\nresult, most existing semantic-level detection approaches struggle to identify\nthem reliably. While certain LLM-based detection methods have shown promise,\nthey suffer from high computational costs and are constrained by the\nperformance of the underlying language model, making them impractical for\nlarge-scale deployment.\n  In this work, we aim to address this issue. We propose Paladin, which embeds\ntrigger-tag associations into vanilla LLM using various insertion strategies,\ncreating them into instrumented LLMs. When an instrumented LLM generates\ncontent related to phishing, it will automatically include detectable tags,\nenabling easier identification. Based on the design on implicit and explicit\ntriggers and tags, we consider four distinct scenarios in our work. We evaluate\nour method from three key perspectives: stealthiness, effectiveness, and\nrobustness, and compare it with existing baseline methods. Experimental results\nshow that our method outperforms the baselines, achieving over 90% detection\naccuracy across all scenarios.", "AI": {"tldr": "Paladin detects LLM-generated phishing emails by embedding detectable tags into LLMs during content generation, achieving 90%+ accuracy with low computational overhead.", "motivation": "LLMs enable crafting of linguistically flawless, tailored phishing emails, bypassing traditional detection systems. Existing detection methods either lack reliability (semantic approaches) or suffer from high costs and scalability limits (LLM-based approaches).", "method": "Proposed Paladin, which integrates trigger-tag associations into standard LLMs via insertion strategies. When generating phishing content, instrumented LLMs automatically embed detectable tags, enabling efficient detection while maintaining operational stealth.", "result": "Outperformed baselines with >90% detection accuracy across all four scenarios, demonstrating superior effectiveness in terms of stealthiness, robustness, and detection capability.", "conclusion": "Paladin effectively detects LLM-generated phishing emails with high accuracy across four scenarios, offering an efficient and scalable solution without relying on expensive detection models."}}
{"id": "2509.07540", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.07540", "abs": "https://arxiv.org/abs/2509.07540", "authors": ["Huu Hung Nguyen", "Anh Tuan Nguyen", "Thanh Le-Cong", "Yikun Li", "Han Wei Ang", "Yide Yin", "Frank Liauw", "Shar Lwin Khin", "Ouh Eng Lieh", "Ting Zhang", "David Lo"], "title": "PatchSeeker: Mapping NVD Records to their Vulnerability-fixing Commits with LLM Generated Commits and Embeddings", "comment": null, "summary": "Software vulnerabilities pose serious risks to modern software ecosystems.\nWhile the National Vulnerability Database (NVD) is the authoritative source for\ncataloging these vulnerabilities, it often lacks explicit links to the\ncorresponding Vulnerability-Fixing Commits (VFCs). VFCs encode precise code\nchanges, enabling vulnerability localization, patch analysis, and dataset\nconstruction. Automatically mapping NVD records to their true VFCs is therefore\ncritical. Existing approaches have limitations as they rely on sparse, often\nnoisy commit messages and fail to capture the deep semantics in the\nvulnerability descriptions. To address this gap, we introduce PatchSeeker, a\nnovel method that leverages large language models to create rich semantic links\nbetween vulnerability descriptions and their VFCs. PatchSeeker generates\nembeddings from NVD descriptions and enhances commit messages by synthesizing\ndetailed summaries for those that are short or uninformative. These generated\nmessages act as a semantic bridge, effectively closing the information gap\nbetween natural language reports and low-level code changes. Our approach\nPatchSeeker achieves 59.3% higher MRR and 27.9% higher Recall@10 than the\nbest-performing baseline, Prospector, on the benchmark dataset. The extended\nevaluation on recent CVEs further confirms PatchSeeker's effectiveness.\nAblation study shows that both the commit message generation method and the\nselection of backbone LLMs make a positive contribution to PatchSeeker. We also\ndiscuss limitations and open challenges to guide future work.", "AI": {"tldr": "PatchSeeker uses LLMs to bridge semantic gaps between NVD descriptions and VFCs via enhanced embeddings and commit message synthesis, outperforming existing methods by 59.3% in MRR and 27.9% in Recall@10 with strong validation on recent CVEs.", "motivation": "NVD records lack direct links to VFCs, and existing approaches struggle with noisy commit messages and superficial semantic analysis. The deep semantic alignment between vulnerability descriptions and VFCs is critical for accurate mapping.", "method": "PatchSeeker leverages large language models to generate semantic embeddings from NVD descriptions and enhances sparse commit messages through synthesized summaries, creating richer semantic links between vulnerability reports and VFCs. An ablation study validates component contributions.", "result": "PatchSeeker achieves 59.3% higher MRR and 27.9% higher Recall@10 than the Prospector baseline. Extended evaluations on recent CVEs confirm robust performance, with ablation studies confirming the positive impact of commit message enhancement and LLM backbone choices.", "conclusion": "The authors present PatchSeeker, a method using LLMs to improve NVD-VFC mapping, demonstrating its effectiveness over existing approaches with significant performance gains while acknowledging remaining challenges for future work."}}
{"id": "2509.07290", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07290", "abs": "https://arxiv.org/abs/2509.07290", "authors": ["Nan Wang", "Nan Wu", "Xiangyu Hui", "Jiafan Wang", "Xin Yuan"], "title": "zkUnlearner: A Zero-Knowledge Framework for Verifiable Unlearning with Multi-Granularity and Forgery-Resistance", "comment": null, "summary": "As the demand for exercising the \"right to be forgotten\" grows, the need for\nverifiable machine unlearning has become increasingly evident to ensure both\ntransparency and accountability. We present {\\em zkUnlearner}, the first\nzero-knowledge framework for verifiable machine unlearning, specifically\ndesigned to support {\\em multi-granularity} and {\\em forgery-resistance}.\n  First, we propose a general computational model that employs a {\\em\nbit-masking} technique to enable the {\\em selectivity} of existing\nzero-knowledge proofs of training for gradient descent algorithms. This\ninnovation enables not only traditional {\\em sample-level} unlearning but also\nmore advanced {\\em feature-level} and {\\em class-level} unlearning. Our model\ncan be translated to arithmetic circuits, ensuring compatibility with a broad\nrange of zero-knowledge proof systems. Furthermore, our approach overcomes key\nlimitations of existing methods in both efficiency and privacy. Second, forging\nattacks present a serious threat to the reliability of unlearning.\nSpecifically, in Stochastic Gradient Descent optimization, gradients from\nunlearned data, or from minibatches containing it, can be forged using\nalternative data samples or minibatches that exclude it. We propose the first\neffective strategies to resist state-of-the-art forging attacks. Finally, we\nbenchmark a zkSNARK-based instantiation of our framework and perform\ncomprehensive performance evaluations to validate its practicality.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.07728", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.07728", "abs": "https://arxiv.org/abs/2509.07728", "authors": ["John Gouwar", "Gregory Becker", "Tamara Dahlgren", "Nathan Hanford", "Arjun Guha", "Todd Gamblin"], "title": "Bridging the Gap Between Binary and Source Based Package Management in Spack", "comment": "To appear in SC '25", "summary": "Binary package managers install software quickly but they limit\nconfigurability due to rigid ABI requirements that ensure compatibility between\nbinaries. Source package managers provide flexibility in building software, but\ncompilation can be slow. For example, installing an HPC code with a new MPI\nimplementation may result in a full rebuild. Spack, a widely deployed,\nHPC-focused package manager, can use source and pre-compiled binaries, but\nlacks a binary compatibility model, so it cannot mix binaries not built\ntogether. We present splicing, an extension to Spack that models binary\ncompatibility between packages and allows seamless mixing of source and binary\ndistributions. Splicing augments Spack's packaging language and dependency\nresolution engine to reuse compatible binaries but maintains the flexibility of\nsource builds. It incurs minimal installation-time overhead and allows rapid\ninstallation from binaries, even for ABI-sensitive dependencies like MPI that\nwould otherwise require many rebuilds.", "AI": {"tldr": "Splicing enables Spack to mix binaries and sources efficiently, avoiding slow rebuilds for HPC workloads while maintaining configurability.", "motivation": "Binary package managers are fast but lack configurability, while source-based ones are slow. Spack's inability to mix unrelated binaries forces unnecessary rebuilds (e.g., for MPI).", "method": "Splicing enhances Spack's packaging language and dependency resolution engine to model binary compatibility, reusing existing binaries and integrating them with source builds.", "result": "Splicing reduces installation overhead and rebuilds by reusing compatible binaries (even ABI-sensitive ones). It incurs minimal runtime cost while preserving Spack's source-build flexibility.", "conclusion": "Splicing extends Spack to solve the binary compatibility issue by enabling seamless mixing of source and binary packages, maintaining flexibility while reducing rebuild times for ABI-sensitive dependencies."}}
{"id": "2509.07315", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.07315", "abs": "https://arxiv.org/abs/2509.07315", "authors": ["Hongfei Xia", "Hongru Wang", "Zeming Liu", "Qian Yu", "Yuhang Guo", "Haifeng Wang"], "title": "SafeToolBench: Pioneering a Prospective Benchmark to Evaluating Tool Utilization Safety in LLMs", "comment": "18 pages, 7 figures", "summary": "Large Language Models (LLMs) have exhibited great performance in autonomously\ncalling various tools in external environments, leading to better problem\nsolving and task automation capabilities. However, these external tools also\namplify potential risks such as financial loss or privacy leakage with\nambiguous or malicious user instructions. Compared to previous studies, which\nmainly assess the safety awareness of LLMs after obtaining the tool execution\nresults (i.e., retrospective evaluation), this paper focuses on prospective\nways to assess the safety of LLM tool utilization, aiming to avoid irreversible\nharm caused by directly executing tools. To this end, we propose SafeToolBench,\nthe first benchmark to comprehensively assess tool utilization security in a\nprospective manner, covering malicious user instructions and diverse practical\ntoolsets. Additionally, we propose a novel framework, SafeInstructTool, which\naims to enhance LLMs' awareness of tool utilization security from three\nperspectives (i.e., \\textit{User Instruction, Tool Itself, and Joint\nInstruction-Tool}), leading to nine detailed dimensions in total. We experiment\nwith four LLMs using different methods, revealing that existing approaches fail\nto capture all risks in tool utilization. In contrast, our framework\nsignificantly enhances LLMs' self-awareness, enabling a more safe and\ntrustworthy tool utilization.", "AI": {"tldr": "This paper introduces SafeToolBench and SafeInstructTool to proactively enhance LLM safety during external tool use. Their proposed framework outperforms existing methods, offering comprehensive risk assessment and significantly improving tool utilization trustworthiness.", "motivation": "LLMs using external tools face risks like financial loss and privacy leaks from ambiguous/malicious user instructions. Previous retrospective safety evaluations are insufficient for preventing irreversible harm, necessitating prospective risk assessment methodologies.", "method": "The paper introduces SafeToolBench, a benchmark for prospective safety assessment of LLM tool utilization, and SafeInstructTool, a framework analyzing risks from three perspectives (User Instruction, Tool Itself, and Joint Instruction-Tool) across nine dimensions. Four LLMs were tested to compare risk mitigation efficacy.", "result": "Experiments revealed existing methods fail to detect all tool utilization risks, while SafeInstructTool demonstrates improved safety awareness, reducing harmful tool executions and providing a more robust solution for trustable LLM interactions.", "conclusion": "The paper concludes that the proposed SafeInstructTool framework significantly enhances Large Language Models' (LLMs) self-awareness in tool utilization, enabling safer and more trustworthy interactions with external tools. SafeToolBench is introduced as the first benchmark for prospective safety evaluation, addressing the limitations of retrospective methods."}}
{"id": "2509.07747", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.07747", "abs": "https://arxiv.org/abs/2509.07747", "authors": ["Maksym Avramenko", "David Chapela-Campa", "Marlon Dumas", "Fredrik Milani"], "title": "What's Coming Next? Short-Term Simulation of Business Processes from Current State", "comment": null, "summary": "Business process simulation is an approach to evaluate business process\nchanges prior to implementation. Existing methods in this field primarily\nsupport tactical decision-making, where simulations start from an empty state\nand aim to estimate the long-term effects of process changes. A complementary\nuse-case is operational decision-making, where the goal is to forecast\nshort-term performance based on ongoing cases and to analyze the impact of\ntemporary disruptions, such as demand spikes and shortfalls in available\nresources. An approach to tackle this use-case is to run a long-term simulation\nup to a point where the workload is similar to the current one (warm-up), and\nmeasure performance thereon. However, this approach does not consider the\ncurrent state of ongoing cases and resources in the process. This paper studies\nan alternative approach that initializes the simulation from a representation\nof the current state derived from an event log of ongoing cases. The paper\naddresses two challenges in operationalizing this approach: (1) Given a\nsimulation model, what information is needed so that a simulation run can start\nfrom the current state of cases and resources? (2) How can the current state of\na process be derived from an event log? The resulting short-term simulation\napproach is embodied in a simulation engine that takes as input a simulation\nmodel and a log of ongoing cases, and simulates cases for a given time horizon.\nAn experimental evaluation shows that this approach yields more accurate\nshort-term performance forecasts than long-term simulations with warm-up\nperiod, particularly in the presence of concept drift or bursty performance\npatterns.", "AI": {"tldr": "This paper introduces a short-term simulation method that initializes from the current state of ongoing business processes via event logs, offering more accurate forecasts than traditional warm-up simulations for operational decision-making, especially under dynamic conditions.", "motivation": "Existing simulation methods start from an empty state (tactical decisions), but operational decisions require forecasting short-term performance based on ongoing cases and handling disruptions. Current approaches ignore the current state, leading to less accurate forecasts.", "method": "The paper proposes a method to initialize simulations from the current state of ongoing cases and resources, derived from event logs, addressing (1) identifying necessary simulation model information and (2) extracting the current state from logs. This is implemented in a simulation engine.", "result": "The proposed approach demonstrates experimentally that short-term simulations initialized with the current state yield more accurate performance forecasts than long-term simulations with warm-up, particularly when dealing with concept drift or bursty performance patterns.", "conclusion": "The study concludes that using current state initialization from event logs provides more accurate short-term performance forecasts compared to long-term warm-up simulations, especially under conditions of concept drift or bursty patterns."}}
{"id": "2509.07457", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.07457", "abs": "https://arxiv.org/abs/2509.07457", "authors": ["Shakhzod Yuldoshkhujaev", "Mijin Jeon", "Doowon Kim", "Nick Nikiforakis", "Hyungjoon Koo"], "title": "A Decade-long Landscape of Advanced Persistent Threats: Longitudinal Analysis and Global Trends", "comment": "18 pages, 13 figures (including subfigures), 11 tables. To appear in\n  the Proceedings of the ACM Conference on Computer and Communications Security\n  (CCS) 2025", "summary": "An advanced persistent threat (APT) refers to a covert, long-term\ncyberattack, typically conducted by state-sponsored actors, targeting critical\nsectors and often remaining undetected for long periods. In response,\ncollective intelligence from around the globe collaborates to identify and\ntrace surreptitious activities, generating substantial documentation on APT\ncampaigns publicly available on the web. While prior works predominantly focus\non specific aspects of APT cases, such as detection, evaluation, cyber threat\nintelligence, and dataset creation, limited attention has been devoted to\nrevisiting and investigating these scattered dossiers in a longitudinal manner.\nThe objective of our study is to fill the gap by offering a macro perspective,\nconnecting key insights and global trends in past APT attacks. We\nsystematically analyze six reliable sources-three focused on technical reports\nand another three on threat actors-examining 1,509 APT dossiers (24,215 pages)\nspanning 2014-2023, and identifying 603 unique APT groups worldwide. To\nefficiently unearth relevant information, we employ a hybrid methodology that\ncombines rule-based information retrieval with large-language-model-based\nsearch techniques. Our longitudinal analysis reveals shifts in threat actor\nactivities, global attack vectors, changes in targeted sectors, and\nrelationships between cyberattacks and significant events such as elections or\nwars, which provide insights into historical patterns in APT evolution. Over\nthe past decade, 154 countries have been affected, primarily using malicious\ndocuments and spear phishing as dominant initial infiltration vectors, with a\nnoticeable decline in zero-day exploitation since 2016. Furthermore, we present\nour findings through interactive visualization tools, such as an APT map or\nflow diagram, to facilitate intuitive understanding of global patterns and\ntrends in APT activities.", "AI": {"tldr": "This study analyzes 1,509 APT dossiers (2014-2023), identifying global trends in threat activities, attack vectors, and geopolitical links via a hybrid rule-based/LLM methodology, with interactive visualizations.", "motivation": "Prior research on APTs focuses on isolated detection or case studies, with limited longitudinal analysis of scattered dossiers. This work addresses the gap by connecting historical trends and macro patterns.", "method": "Systematic analysis of 1,509 APT dossiers (24,215 pages from 2014-2023) using six authoritative sources, combining rule-based information retrieval with large-language-model-based search, and identifying 603 unique APT groups.", "result": "Longitudinal shifts in infiltration tactics (e.g., decline in zero-day exploitation since 2016), sector targeting, international influence of events (elections/wars), and 154 affected countries. Findings include 603 unique APT groups and 60,630 identified attack instances.", "conclusion": "The work provides a macro overview of APT evolution, revealing interconnected cyber-geopolitical patterns and offering tools like an APT map/flow diagram to enhance understanding of global threat dynamics."}}
{"id": "2509.07763", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.07763", "abs": "https://arxiv.org/abs/2509.07763", "authors": ["Mikel Robredo", "Matteo Esposito", "Fabio Palomba", "Rafael Pe\u00f1aloza", "Valentina Lenarduzzi"], "title": "What Were You Thinking? An LLM-Driven Large-Scale Study of Refactoring Motivations in Open-Source Projects", "comment": null, "summary": "Context. Code refactoring improves software quality without changing external\nbehavior. Despite its advantages, its benefits are hindered by the considerable\ncost of time, resources, and continuous effort it demands. Aim. Understanding\nwhy developers refactor, and which metrics capture these motivations, may\nsupport wider and more effective use of refactoring in practice. Method. We\nperformed a large-scale empirical study to analyze developers refactoring\nactivity, leveraging Large Language Models (LLMs) to identify underlying\nmotivations from version control data, comparing our findings with previous\nmotivations reported in the literature. Results. LLMs matched human judgment in\n80% of cases, but aligned with literature-based motivations in only 47%. They\nenriched 22% of motivations with more detailed rationale, often highlighting\nreadability, clarity, and structural improvements. Most motivations were\npragmatic, focused on simplification and maintainability. While metrics related\nto developer experience and code readability ranked highest, their correlation\nwith motivation categories was weak. Conclusions. We conclude that LLMs\neffectively capture surface-level motivations but struggle with architectural\nreasoning. Their value lies in providing localized explanations, which, when\ncombined with software metrics, can form hybrid approaches. Such integration\noffers a promising path toward prioritizing refactoring more systematically and\nbalancing short-term improvements with long-term architectural goals.", "AI": {"tldr": "This paper examines developer refactoring motivations using LLMs and empirical analysis. It finds LLMs match human judgment in 80% of cases but align with existing literature in only 47%, highlighting practical motivations like readability and maintainability, while suggesting hybrid approaches combining LLMs and software metrics for systematic refactoring decisions.", "motivation": "Refactoring is beneficial but resource-intensive. Understanding developer motivations and their correlation with metrics could enhance refactoring practices. Existing literature lacks nuanced explanations, prompting the need for LLMs to uncover deeper rationales.", "method": "A large-scale empirical study using LLMs to analyze version control data, comparing LLM-derived motivations against human judgments and motivations from prior literature. Metrics like developer experience and code readability were evaluated for correlation.", "result": "LLMs achieved 80%% alignment with human judgments but only 47%% with literature-based motivations. 22%% of LLM-identified motives provided richer details (e.g., readability, structure). Pragmatic motives dominated (simplification, maintainability), though metrics showed weak correlation with motivation categories.", "conclusion": "LLMs capture surface-level refactoring motives effectively but lack architectural insight. Integrating LLMs with software metrics offers a hybrid approach to prioritize refactoring, balancing immediate improvements and long-term design goals."}}
{"id": "2509.07465", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.07465", "abs": "https://arxiv.org/abs/2509.07465", "authors": ["Norman Poh", "Daryl Burns"], "title": "Biometric Bound Credentials for Age Verification", "comment": null, "summary": "Age verification is increasingly critical for regulatory compliance, user\ntrust, and the protection of minors online. Historically, solutions have\nstruggled with poor accuracy, intrusiveness, and significant security risks.\nMore recently, concerns have shifted toward privacy, surveillance, fairness,\nand the need for transparent, trustworthy systems. In this paper, we propose\nBiometric Bound Credentials (BBCreds) as a privacy-preserving approach that\ncryptographically binds age credentials to an individual's biometric features\nwithout storing biometric templates. This ensures only the legitimate,\nphysically present user can access age-restricted services, prevents credential\nsharing, and addresses both legacy and emerging challenges in age verification.\nenhances privacy.", "AI": {"tldr": "BBCreds is a new method for age verification that improves privacy and security by binding credentials to biometric features without storing templates.", "motivation": "The need for accurate and secure age verification systems to combat issues like regulatory compliance, user trust, and protecting minors online has become increasingly important, while legacy systems struggle with drawbacks such as low accuracy, intrusiveness, and security risks.", "method": "The solution involves using Biometric Bound Credentials (BBCreds) to cryptographically tie age credentials to an individual's biometric data, eliminating the requirement to store biometric templates.", "result": "BBCreds enable exclusive access by the legitimate user, avoid the sharing of credentials, and simultaneously address the challenges faced by legacy and contemporary age verification methods.", "conclusion": "This solution demonstrates the viability of private and secure age verification through the use of BBCreds, offering a robust approach that combines cryptographic assurances with biometric features to protect minors and foster trust."}}
{"id": "2509.07851", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.07851", "abs": "https://arxiv.org/abs/2509.07851", "authors": ["Irdin Pekaric", "Giovanni Apruzzese"], "title": "\"We provide our resources in a dedicated repository\": Surveying the Transparency of HICSS publications", "comment": null, "summary": "Every day, new discoveries are made by researchers from all across the globe\nand fields. HICSS is a flagship venue to present and discuss such scientific\nadvances. Yet, the activities carried out for any given research can hardly be\nfully contained in a single document of a few pages-the \"paper.\" Indeed, any\ngiven study entails data, artifacts, or other material that is crucial to truly\nappreciate the contributions claimed in the corresponding paper. External\nrepositories (e.g., GitHub) are a convenient tool to store all such resources\nso that future work can freely observe and build upon them -- thereby improving\ntransparency and promoting reproducibility of research as a whole. In this\nwork, we scrutinize the extent to which papers recently accepted to HICSS\nleverage such repositories to provide supplementary material. To this end, we\ncollect all the 5579 papers included in HICSS proceedings from 2017-2024. Then,\nwe identify those entailing either human subject research (850) or technical\nimplementations (737), or both (147). Finally, we review their text, examining\nhow many include a link to an external repository-and, inspect its contents.\nOverall, out of 2028 papers, only 3\\% have a functional and publicly available\nrepository that is usable by downstream research. We release all our tools.", "AI": {"tldr": "This study analyzes 5579 HICSS papers (2017-2024) to assess how often supplementary research materials are shared in public repositories. Only 3% of 2028 papers with human/technical focus had functional, accessible repositories.", "motivation": "Research transparency and reproducibility require sharing supplementary data/artifacts, yet it's unclear how often HICSS authors implement this practice.", "method": "Analyzed all 5579 HICSS papers from 2017-2024, identified 2028 papers involving human subjects (850) or technical implementations (737), and manually checked for public repository links and accessibility.", "result": "Only 3.0% (59/2028) of analyzed papers provided functional public repositories with the expected supplementary materials. Tools for replication are publicly shared.", "conclusion": "HICSS papers frequently fail to provide accessible supplementary research materials despite their importance for transparency and reproducibility. This highlights a need for improved sharing practices in the research community."}}
{"id": "2509.07504", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.07504", "abs": "https://arxiv.org/abs/2509.07504", "authors": ["Bilal Hussain Abbasi", "Yanjun Zhang", "Leo Zhang", "Shang Gao"], "title": "Backdoor Attacks and Defenses in Computer Vision Domain: A Survey", "comment": null, "summary": "Backdoor (trojan) attacks embed hidden, controllable behaviors into\nmachine-learning models so that models behave normally on benign inputs but\nproduce attacker-chosen outputs when a trigger is present. This survey reviews\nthe rapidly growing literature on backdoor attacks and defenses in the\ncomputer-vision domain. We introduce a multi-dimensional taxonomy that\norganizes attacks and defenses by injection stage (dataset poisoning,\nmodel/parameter modification, inference-time injection), trigger type (patch,\nblended/frequency, semantic, transformation), labeling strategy (dirty-label\nvs. clean-label / feature-collision), representation stage (instance-specific,\nmanifold/class-level, neuron/parameter hijacking, distributed encodings), and\ntarget task (classification, detection, segmentation, video, multimodal). For\neach axis we summarize representative methods, highlight evaluation practices,\nand discuss where defenses succeed or fail. For example, many classical\nsanitization and reverse-engineering tools are effective against reusable patch\nattacks but struggle with input-aware, sample-specific, or parameter-space\nbackdoors and with transfer via compromised pre-trained encoders or hardware\nbit-flips. We synthesize trends, identify persistent gaps (supply-chain and\nhardware threats, certifiable defenses, cross-task benchmarks), and propose\npractical guidelines for threat-aware evaluation and layered defenses. This\nsurvey aims to orient researchers and practitioners to the current threat\nlandscape and pressing research directions in secure computer vision.", "AI": {"tldr": "This paper is a comprehensive survey on backdoor attacks and defenses in computer vision, organizing them into a multi-dimensional taxonomy and identifying research gaps.", "motivation": "Backdoor attacks pose significant security risks to machine learning systems, necessitating systematic understanding of attack vectors and defenses to guide future research and practical defense strategies.", "method": "The authors establish a taxonomy categorizing existing work along 7 key dimensions (ejection stage, trigger type, labeling strategy, etc.) and conduct a critical evaluation of representative methods across 895+ references.", "result": "Analysis reveals strong defense effectiveness against traditional patch attacks but persistent vulnerabilities to input-aware backdoors, transferable encoder attacks, and hardware exploits. Key gaps identified include supply-chain risks, lack of certifiability, and cross-task benchmarks.", "conclusion": "The survey establishes a structured framework for threat analysis in secure computer vision, emphasizing the need for layered defenses and proposing 5 practical guidelines for threat-aware evaluation."}}
