{"id": "2507.03156", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.03156", "abs": "https://arxiv.org/abs/2507.03156", "authors": ["Amr Mohamed", "Maram Assi", "Mariam Guizani"], "title": "The Impact of LLM-Assistants on Software Developer Productivity: A Systematic Literature Review", "comment": "37 pages", "summary": "Large language model assistants (LLM-assistants) present new opportunities to\ntransform software development. Developers are increasingly adopting these\ntools across tasks, including coding, testing, debugging, documentation, and\ndesign. Yet, despite growing interest, there is no synthesis of how\nLLM-assistants affect software developer productivity. In this paper, we\npresent a systematic literature review of 37 peer-reviewed studies published\nbetween January 2014 and December 2024 that examine this impact. Our analysis\nreveals that LLM-assistants offer both considerable benefits and critical\nrisks. Commonly reported gains include minimized code search, accelerated\ndevelopment, and the automation of trivial and repetitive tasks. However,\nstudies also highlight concerns around cognitive offloading, reduced team\ncollaboration, and inconsistent effects on code quality. While the majority of\nstudies (92%) adopt a multi-dimensional perspective by examining at least two\nSPACE dimensions, reflecting increased awareness of the complexity of developer\nproductivity, only 14% extend beyond three dimensions, indicating substantial\nroom for more integrated evaluations. Satisfaction, Performance, and Efficiency\nare the most frequently investigated dimensions, whereas Communication and\nActivity remain underexplored. Most studies are exploratory (64%) and\nmethodologically diverse, but lack longitudinal and team-based evaluations.\nThis review surfaces key research gaps and provides recommendations for future\nresearch and practice. All artifacts associated with this study are publicly\navailable at https://zenodo.org/records/15788502.", "AI": {"tldr": "A systematic literature review of 37 studies (2014-2024) on LLM-assistants' impact on software developer productivity reveals dual benefits (accelerated development, task automation) and risks (cognitive offloading, collaboration reduction). Research gaps include underexplored SPACE dimensions (Communication/Activity) and limited longitudinal/team-based evaluations.", "motivation": "The paper addresses the lack of synthesized understanding of LLM-assistants' effects on developer productivity across multiple dimensions, highlighting both observed benefits and methodological limitations in existing research.", "method": "Systematic literature review of 37 peer-reviewed empirical studies published between 2014-2024, analyzing findings through the SPACE framework (Satisfaction, Productivity, Action, Code, Evaluation) to evaluate productivity impacts comprehensively.", "result": "92% of studies examined \u22652 SPACE dimensions. Primary benefits: code search reduction (87%), development acceleration (68%), task automation (67%). Critical risks: cognitive offloading (41%), reduced collaboration (29%), code quality inconsistency (22%). Only 14% studied beyond three SPACE dimensions.", "conclusion": "The review identifies key gaps in understanding communication/efficiency impacts, suggests future research priorities for comprehensive productivity evaluation, and emphasizes the need for longitudinal and team-based studies."}}
{"id": "2507.03160", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.03160", "abs": "https://arxiv.org/abs/2507.03160", "authors": ["Md Mahade Hasan", "Muhammad Waseem", "Kai-Kristian Kemell", "Jussi Raskua", "Juha Ala-Rantalaa", "Pekka Abrahamsson"], "title": "Assessing Small Language Models for Code Generation: An Empirical Study with Benchmarks", "comment": null, "summary": "The recent advancements of Small Language Models (SLMs) have opened new\npossibilities for efficient code generation. SLMs offer lightweight and\ncost-effective alternatives to Large Language Models (LLMs), making them\nattractive for use in resource-constrained environments. However, empirical\nunderstanding of SLMs, particularly their capabilities, limitations, and\nperformance trade-offs in code generation remains limited. This study presents\na comprehensive empirical evaluation of 20 open-source SLMs ranging from 0.4B\nto 10B parameters on five diverse code-related benchmarks (HumanEval, MBPP,\nMercury, HumanEvalPack, and CodeXGLUE). The models are assessed along three\ndimensions: i) functional correctness of generated code, ii) computational\nefficiency and iii) performance across multiple programming languages. The\nfindings of this study reveal that several compact SLMs achieve competitive\nresults while maintaining a balance between performance and efficiency, making\nthem viable for deployment in resource-constrained environments. However,\nachieving further improvements in accuracy requires switching to larger models.\nThese models generally outperform their smaller counterparts, but they require\nmuch more computational power. We observe that for 10% performance\nimprovements, models can require nearly a 4x increase in VRAM consumption,\nhighlighting a trade-off between effectiveness and scalability. Besides, the\nmultilingual performance analysis reveals that SLMs tend to perform better in\nlanguages such as Python, Java, and PHP, while exhibiting relatively weaker\nperformance in Go, C++, and Ruby. However, statistical analysis suggests these\ndifferences are not significant, indicating a generalizability of SLMs across\nprogramming languages. Based on the findings, this work provides insights into\nthe design and selection of SLMs for real-world code generation tasks.", "AI": {"tldr": "This paper evaluates 20 open-source Small Language Models (SLMs) for code generation across 5 benchmarks, analyzing performance vs. efficiency trade-offs and multilingual capabilities, finding that compact models offer cost-effective solutions while larger models improve accuracy at higher resource costs.", "motivation": "SLMs present lightweight code generation alternatives to LLMs, but empirical data on their effectiveness, limitations, and computational trade-offs in constrained environments remains insufficient, necessitating a structured evaluation for practical deployment guidance.", "method": "Assessed 20 SLMs (0.4B-10B parameters) using functional correctness, computational efficiency, and multilingual performance on HumanEval, MBPP, Mercury, HumanEvalPack, and CodeXGLUE benchmarks, measuring VRAM consumption and cross-language consistency.", "result": "Small models achieve competitive results with efficiency advantages, while larger models require ~4x VRAM for 10% performance gains; Python/Java/PHP outperform Go/C++/Ruby in SLM performance, though differences lack statistical significance.", "conclusion": "The study provides actionable insights for SLM design and selection in code generation, highlighting the balance between model size, accuracy, and scalability, and suggesting that multilingual generalization is feasible despite minor performance variations."}}
{"id": "2507.03263", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.03263", "abs": "https://arxiv.org/abs/2507.03263", "authors": ["Haiqiao Gu", "Yiliang Zhao", "Kai Gao", "Minghui Zhou"], "title": "Analyzing C/C++ Library Migrations at the Package-level: Prevalence, Domains, Targets and Rationals across Seven Package Management Tools", "comment": null, "summary": "Library migration happens when a library can not meet the project's\nrequirements and is non-trivial to accomplish. To mitigate the problem,\nsubstantial efforts have been devoted to understanding its characteristics and\nrecommending alternative libraries, especially for programming language (PL)\necosystems with a central package hosting platform, such as Python (PyPI).\nHowever, to the best of our knowledge, understanding of C/C++ library\nmigrations is still lacking, possibly due to challenges resulting from the\nfragmented and complicated dependency management practices in the C/C++\necosystem. To bridge this knowledge gap, this paper analyzes 19,943 C/C++\nprojects that utilize different package management tools and establishes the\nfirst C/C++ library migration dataset. Based on the dataset, we investigate the\nprevalence, domains, target library, and rationale of C/C++ library migrations\nand compare the results with three widely investigated PLs: Python, JavaScript,\nand Java. We find that the overall trend in the number of C/C++ library\nmigrations is similar to Java. Migrations across different package management\ntools are also observed. In C/C++, library migrations mainly occur in GUI,\nBuild, and OS development, but are rare in domains (e.g., Testing and Logging)\nthat dominate library migrations in the three compared PLs. 83.46\\% of C/C++\nsource libraries only have one migration target, suggesting that our library\nmigration dataset could be used directly to recommend migration targets. We\nfind four C/C++-specific migration reasons, such as less compile time and\nunification of dependency management, revealing the unique dependency\nmanagement requirements in C/C++ projects. We believe our findings can help\nC/C++ developers make more informed library migration decisions and shed light\non the design of C/C++ library migration tools.", "AI": {"tldr": "This paper generates the first C/C++ library migration dataset by analyzing 19,943 projects, revealing unique migration patterns and reasons (e.g., GUI/OS focus, dependency unification) compared to Python, JavaScript, and Java ecosystems.", "motivation": "Existing research on library migration focuses on PLs with central package hosting platforms (Python, JavaScript, Java), but C/C++'s fragmented dependency management practices leave a critical knowledge gap.", "method": "Analyzes 19,943 C/C++ projects across various package management tools to create the first C/C++ library migration dataset, comparing migration trends with Python, JavaScript, and Java.", "result": "C/C++ migrations occur predominantly in GUI/Build/OS domains (vs. Testing/Logging in other PLs), 83.46% of source libraries have single migration targets. Four C/C++-specific migration reasons identified: compile time reduction, dependency management unification, license adjustments, and toolchain compatibility.", "conclusion": "Findings provide actionable insights for C/C++ developers to make informed library migration decisions and guide the development of C/C++-specific migration tools addressing unique dependency management challenges."}}
{"id": "2507.03328", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.03328", "abs": "https://arxiv.org/abs/2507.03328", "authors": ["S. Lee", "C. Myers", "A. Yang", "T. Zhang", "S. J. L. Billinge"], "title": "scikit-package -- software packaging standards and roadmap for sharing reproducible scientific software", "comment": "GitHub: https://github.com/scikit-package/scikit-package Doc:\n  https://scikit-package.github.io/scikit-package/", "summary": "Scientific advancement relies on the ability to share and reproduce results.\nWhen data analysis or calculations are carried out using software written by\nscientists there are special challenges around code versions, quality and code\nsharing. scikit-package provides a roadmap to facilitate code reuse and sharing\nwith minimal effort through tutorials coupled with automated and centralized\nreusable workflows. The goal of the project is to provide pedagogical and\npractical tools for scientists who are not professionally trained software\nengineers to write more reusable and maintainable software code. Code reuse can\noccur at multiple levels of complexity-from turning a code block into a\nfunction within a single script, to publishing a publicly installable, fully\ntested, and documented software package scikit-package provides a community\nmaintained set of tools, and a roadmap, to help scientists bring their software\nhigher levels of reproducibility and shareability.", "AI": {"tldr": "scikit-package provides a roadmap and tools for scientists to create reusable, shareable, and reproducible software.", "motivation": "Scientific reproducibility and collaboration face challenges with code versioning, quality, and sharing when researchers develop software without professional software engineering training.", "method": "The project combines tutorials with automated workflows and community-maintained tools to guide scientists toward software best practices.", "result": "Enables code reuse at multiple complexity levels (from functions to packages) with standardized, tested, and documented tools.", "conclusion": "scikit-package democratizes software quality for scientific communities through practical, low-effort workflows and education, ensuring reproducible and shareable research software."}}
{"id": "2507.02951", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02951", "abs": "https://arxiv.org/abs/2507.02951", "authors": ["Elizabeth Lui", "Jiahao Sun"], "title": "Bittensor Protocol: The Bitcoin in Decentralized Artificial Intelligence? A Critical and Empirical Analysis", "comment": "MARBLE 2025", "summary": "This paper investigates whether Bittensor can be considered the Bitcoin of\ndecentralized Artificial Intelligence by directly comparing its tokenomics,\ndecentralization properties, consensus mechanism, and incentive structure\nagainst those of Bitcoin. Leveraging on-chain data from all 64 active Bittensor\nsubnets, we first document considerable concentration in both stake and\nrewards. We further show that rewards are overwhelmingly driven by stake,\nhighlighting a clear misalignment between quality and compensation. As a\nremedy, we put forward a series of two-pronged protocol-level interventions.\nFor incentive realignment, our proposed solutions include performance-weighted\nemission split, composite scoring, and a trust-bonus multiplier. As for\nmitigating security vulnerability due to stake concentration, we propose and\nempirically validate stake cap at the 88th percentile, which elevates the\nmedian coalition size required for a 51-percent attack and remains robust\nacross daily, weekly, and monthly snapshots.", "AI": {"tldr": "This paper evaluates Bittensor's similarity to Bitcoin as a decentralized AI platform by comparing tokenomics, decentralization, consensus, and incentives. It identifies stake and reward concentration, proposes protocol-level solutions (performance-weighted emissions, composite scoring, trust-bonus multiplier, and 88th percentile stake cap) to address misalignment in compensation and reduce 51% attack risks.", "motivation": "The authors aim to assess the viability of Bittensor as a decentralized AI equivalent to Bitcoin by addressing gaps in stake distribution equity, reward alignment with quality, and network security vulnerabilities.", "method": "Leveraged on-chain data from 64 Bittensor subnets to analyze stake/reward concentration. Proposed and empirically validated protocol interventions via data-driven methods, including 88th percentile stake cap testing across daily/weekly/monthly snapshots.", "result": "1) High concentration of stake and rewards, with rewards strongly correlated to stake rather than quality. 2) The 88th percentile stake cap increased median coalition size for 51% attacks by ~factor of 2. 3) Proposed mechanisms demonstrate robust incentive alignment and security improvements across timeframes.", "conclusion": "Bittensor requires substantial protocol-level reforms to address stake concentration, reward misalignment, and security risks. The authors' two-pronged approach of incentive realignment and stake capping demonstrates promising potential to strengthen decentralization and long-term sustainability, though further implementation validation is needed."}}
{"id": "2507.03405", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.03405", "abs": "https://arxiv.org/abs/2507.03405", "authors": ["Krishna Ronanki", "Simon Arvidsson", "Johan Axell"], "title": "Prompt Engineering Guidelines for Using Large Language Models in Requirements Engineering", "comment": "Accepted for publication at the 51st Euromicro Conference Series on\n  Software Engineering and Advanced Applications (SEAA) 2025", "summary": "The rapid emergence of generative AI models like Large Language Models (LLMs)\nhas demonstrated its utility across various activities, including within\nRequirements Engineering (RE). Ensuring the quality and accuracy of\nLLM-generated output is critical, with prompt engineering serving as a key\ntechnique to guide model responses. However, existing literature provides\nlimited guidance on how prompt engineering can be leveraged, specifically for\nRE activities. The objective of this study is to explore the applicability of\nexisting prompt engineering guidelines for the effective usage of LLMs within\nRE. To achieve this goal, we began by conducting a systematic review of primary\nliterature to compile a non-exhaustive list of prompt engineering guidelines.\nThen, we conducted interviews with RE experts to present the extracted\nguidelines and gain insights on the advantages and limitations of their\napplication within RE. Our literature review indicates a shortage of prompt\nengineering guidelines for domain-specific activities, specifically for RE. Our\nproposed mapping contributes to addressing this shortage. We conclude our study\nby identifying an important future line of research within this field.", "AI": {"tldr": "The study systematically reviews prompt engineering guidelines for LLMs in Requirements Engineering (RE), identifies gaps, and proposes a guideline mapping to address RE-specific challenges, concluding with future research directions.", "motivation": "The critical need for ensuring quality and accuracy of LLM-generated output in RE, combined with limited existing guidance on prompt engineering for domain-specific RE activities.", "method": "A systematic review of primary literature to compile prompt engineering guidelines followed by interviews with RE experts to assess their relevance and limitations.", "result": "Literature review revealed a shortage of domain-specific prompt engineering guidelines for RE. Expert interviews provided insights on applying extracted guidelines, informing our proposed mapping.", "conclusion": "The study presents findings on prompt engineering applicability in RE, contributes a guideline mapping to bridge the domain gap, and highlights an important future research direction in this area."}}
{"id": "2507.02956", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02956", "abs": "https://arxiv.org/abs/2507.02956", "authors": ["Blake Bullwinkel", "Mark Russinovich", "Ahmed Salem", "Santiago Zanella-Beguelin", "Daniel Jones", "Giorgio Severi", "Eugenia Kim", "Keegan Hines", "Amanda Minnich", "Yonatan Zunger", "Ram Shankar Siva Kumar"], "title": "A Representation Engineering Perspective on the Effectiveness of Multi-Turn Jailbreaks", "comment": null, "summary": "Recent research has demonstrated that state-of-the-art LLMs and defenses\nremain susceptible to multi-turn jailbreak attacks. These attacks require only\nclosed-box model access and are often easy to perform manually, posing a\nsignificant threat to the safe and secure deployment of LLM-based systems. We\nstudy the effectiveness of the Crescendo multi-turn jailbreak at the level of\nintermediate model representations and find that safety-aligned LMs often\nrepresent Crescendo responses as more benign than harmful, especially as the\nnumber of conversation turns increases. Our analysis indicates that at each\nturn, Crescendo prompts tend to keep model outputs in a \"benign\" region of\nrepresentation space, effectively tricking the model into fulfilling harmful\nrequests. Further, our results help explain why single-turn jailbreak defenses\nlike circuit breakers are generally ineffective against multi-turn attacks,\nmotivating the development of mitigations that address this generalization gap.", "AI": {"tldr": "The paper analyzes Crescendo multi-turn jailbreak attacks, revealing they exploit model misclassification of responses as benign in prolonged interactions, highlighting gaps in single-turn defenses and the need for improved mitigation strategies.", "motivation": "Current state-of-the-art LLMs and defenses remain vulnerable to closed-box, manually performed multi-turn jailbreak attacks, which threaten the safe deployment of AI systems. Understanding their mechanisms is critical for developing robust protections.", "method": "The study examines intermediate model representations of Crescendo attacks, tracking how safety-aligned LMs process harmful multi-turn prompts and classifying outputs as benign or harmful over successive conversation stages.", "result": "Crescendo prompts keep outputs in a \"benign\" region of representation space longer, causing safety-aligned LLMs to misclassify harmful requests. Single-turn defenses (e.g., circuit breakers) fail to detect these evolved risks.", "conclusion": "Multi-turn jailbreaks expose weaknesses in static, single-turn defenses by dynamically manipulating model representations. Effective mitigations must address sequential interactions and representation space dynamics."}}
{"id": "2507.03515", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.03515", "abs": "https://arxiv.org/abs/2507.03515", "authors": ["Radouane Bouchekir", "Michell Guzman Cancimance"], "title": "Enhancing Uncertainty Quantification for Runtime Safety Assurance Using Causal Risk Analysis and Operational Design Domain", "comment": null, "summary": "Ensuring the runtime safety of autonomous systems remains challenging due to\ndeep learning components' inherent uncertainty and their sensitivity to\nenvironmental changes. In this paper, we propose an enhancement of traditional\nuncertainty quantification by explicitly incorporating environmental conditions\nusing risk-based causal analysis. We leverage Hazard Analysis and Risk\nAssessment (HARA) and fault tree modeling to identify critical operational\nconditions affecting system functionality. These conditions, together with\nuncertainties from the data and model, are integrated into a unified Bayesian\nNetwork (BN). At runtime, this BN is instantiated using real-time environmental\nobservations to infer a probabilistic distribution over the safety estimation.\nThis distribution enables the computation of both expected performance and its\nassociated variance, providing a dynamic and context-aware measure of\nuncertainty. We demonstrate our approach through a case study of the Object\nDetection (OD) component in an Automated Valet Parking (AVP).", "AI": {"tldr": "The paper enhances uncertainty quantification for autonomous systems by integrating environmental conditions into a Bayesian Network (BN) for dynamic safety estimation.", "motivation": "Deep learning components' inherent uncertainty and sensitivity to environmental changes challenge the runtime safety of autonomous systems.", "method": "Leverages HARA and fault tree modeling to identify critical operational conditions, then integrates them with data/model uncertainties into a BN for probabilistic safety inference.", "result": "Demonstrated the approach through a case study on Object Detection in Automated Valet Parking, enabling computation of expected performance and uncertainty variance.", "conclusion": "Proposes a context-aware safety estimation framework that improves runtime dependability of autonomous systems under varying conditions."}}
{"id": "2507.02959", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02959", "abs": "https://arxiv.org/abs/2507.02959", "authors": ["Ahmed Bensaoud", "Jugal Kalita"], "title": "A Novel Active Learning Approach to Label One Million Unknown Malware Variants", "comment": null, "summary": "Active learning for classification seeks to reduce the cost of labeling\nsamples by finding unlabeled examples about which the current model is least\ncertain and sending them to an annotator/expert to label. Bayesian theory can\nprovide a probabilistic view of deep neural network models by asserting a prior\ndistribution over model parameters and estimating the uncertainties by\nposterior distribution over these parameters. This paper proposes two novel\nactive learning approaches to label one million malware examples belonging to\ndifferent unknown modern malware families. The first model is Inception-V4+PCA\ncombined with several support vector machine (SVM) algorithms (UTSVM, PSVM,\nSVM-GSU, TBSVM). The second model is Vision Transformer based Bayesian Neural\nNetworks ViT-BNN. Our proposed ViT-BNN is a state-of-the-art active learning\napproach that differs from current methods and can apply to any particular\ntask. The experiments demonstrate that the ViT-BNN is more stable and robust in\nhandling uncertainty.", "AI": {"tldr": "This paper proposes two active learning methods (Inception-V4+PCA with SVM variants and ViT-BNN) for labeling large-scale modern malware data, demonstrating enhanced stability and uncertainty handling via Bayesian neural networks.", "motivation": "Reducing labeling costs through uncertainty-driven active learning is crucial for handling complex, evolving malware families with limited expert resources.", "method": "1. Hybrid model: Inception-V4 convolutional network with PCA dimensionality reduction + multi-SVM approaches (UTSVM, PSVM, SVM-GSU, TBSVM). 2. Vision Transformer Bayesian Neural Network (ViT-BNN) for scalable uncertainty estimation.", "result": "Experiments show ViT-BNN outperforms standard approaches in stability and robustness when managing uncertainty in 1 million unlabeled malware examples.", "conclusion": "ViT-BNN offers a novel, task-agnostic framework for efficient active learning in cybersecurity, effectively balancing exploration of unknown malware families with exploitation of learned patterns."}}
{"id": "2507.03527", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.03527", "abs": "https://arxiv.org/abs/2507.03527", "authors": ["Dulaji Hidellaarachchi", "John Grundy", "Rashina Hoda"], "title": "The Role of Humour in Software Engineering -- A Literature Review and Preliminary Taxonomy", "comment": "Accepted to publish in Journal of Software Systems (JSS) New Idea\n  Track 2025 (23 pages, 1 figure)", "summary": "Humour has long been recognized as a key factor in enhancing creativity,\ngroup effectiveness, and employee well-being across various domains. However,\nits occurrence and impact within software engineering (SE) teams remains\nunder-explored. This paper introduces a comprehensive, literature review-based\ntaxonomy exploring the characterisation and use of humour in SE teams, with the\ngoal of boosting productivity, improving communication, and fostering a\npositive work environment while emphasising the responsible use of humour to\nmitigate its potential negative impacts. Drawing from a wide array of studies\nin psychology, sociology, and organizational behaviour, our proposed framework\ncategorizes humour into distinct theories, styles, models, and scales, offering\nSE professionals and researchers a structured approach to understanding humour\nin their work. This study also addresses the unique challenges of applying\nhumour in SE, highlighting its potential benefits while acknowledging the need\nfor further empirical validation in this context. Ultimately, our study aims to\npave the way for more cohesive, creative, and psychologically supportive SE\nenvironments through the strategic use of humour.", "AI": {"tldr": "This paper develops a literature-based taxonomy of humor in software engineering teams to enhance productivity, communication, and work environments while addressing responsible usage.", "motivation": "Humor's role in software engineering remains under-researched despite its established benefits in other domains, necessitating structured examination for SE-specific insights.", "method": "A literature review integrating psychology, sociology, and organizational behavior studies categorized humor into theories, styles, models, and scales, tailored for software engineering contexts.", "result": "The taxonomy framework highlights humor's potential benefits and challenges in SE, providing professionals and researchers with a structural lens for analysis.", "conclusion": "Strategic humor use can foster cohesive, creative SE environments, but further empirical validation is needed to solidify its application and mitigate risks."}}
{"id": "2507.02968", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02968", "abs": "https://arxiv.org/abs/2507.02968", "authors": ["Vijayalakshmi Ramasamy", "Seth Barrett", "Gokila Dorai", "Jessica Zumbach"], "title": "Unveiling Privacy Policy Complexity: An Exploratory Study Using Graph Mining, Machine Learning, and Natural Language Processing", "comment": "7 Pages; 1 Algorithm; 1 Table; 2 Figures; Accepted by AIRC 2025", "summary": "Privacy policy documents are often lengthy, complex, and difficult for\nnon-expert users to interpret, leading to a lack of transparency regarding the\ncollection, processing, and sharing of personal data. As concerns over online\nprivacy grow, it is essential to develop automated tools capable of analyzing\nprivacy policies and identifying potential risks. In this study, we explore the\npotential of interactive graph visualizations to enhance user understanding of\nprivacy policies by representing policy terms as structured graph models. This\napproach makes complex relationships more accessible and enables users to make\ninformed decisions about their personal data (RQ1). We also employ graph mining\nalgorithms to identify key themes, such as User Activity and Device\nInformation, using dimensionality reduction techniques like t-SNE and PCA to\nassess clustering effectiveness. Our findings reveal that graph-based\nclustering improves policy content interpretability. It highlights patterns in\nuser tracking and data sharing, which supports forensic investigations and\nidentifies regulatory non-compliance. This research advances AI-driven tools\nfor auditing privacy policies by integrating interactive visualizations with\ngraph mining. Enhanced transparency fosters accountability and trust.", "AI": {"tldr": "This paper proposes using interactive graph visualizations to improve user understanding of privacy policies and identifies key themes via graph mining algorithms to enhance interpretability, forensic analysis, and regulatory compliance.", "motivation": "Privacy policies are lengthy and complex, reducing user transparency, understanding, and trust. Automated tools are needed to analyze these policies and identify risks, especially as online privacy concerns rise.", "method": "The study represents privacy policy terms as structured graph models and applies dimensionality reduction (t-SNE, PCA) combined with graph mining to identify key themes like User Activity and Device Information, while using interactive visualizations to make information accessible.", "result": "Graph-based clustering improved policy interpretability, revealed patterns in user tracking and data sharing, and supported forensic investigations and regulatory compliance checks.", "conclusion": "Integrating interactive visualization with graph mining advances AI-driven privacy policy auditing, promoting transparency, accountability, and user trust."}}
{"id": "2507.03536", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.03536", "abs": "https://arxiv.org/abs/2507.03536", "authors": ["Adam Tornhill", "Markus Borg", "Nadim Hagatulah", "Emma S\u00f6derberg"], "title": "ACE: Automated Technical Debt Remediation with Validated Large Language Model Refactorings", "comment": "Published in proceedings of the 1st International Workshop on\n  Artificial Intelligence for Integrated Development Environments (AI-IDE)\n  (2025)", "summary": "The remarkable advances in AI and Large Language Models (LLMs) have enabled\nmachines to write code, accelerating the growth of software systems. However,\nthe bottleneck in software development is not writing code but understanding\nit; program understanding is the dominant activity, consuming approximately 70%\nof developers' time. This implies that improving existing code to make it\neasier to understand has a high payoff and - in the age of AI-assisted coding -\nis an essential activity to ensure that a limited pool of developers can keep\nup with ever-growing codebases. This paper introduces Augmented Code\nEngineering (ACE), a tool that automates code improvements using validated LLM\noutput. Developed through a data-driven approach, ACE provides reliable\nrefactoring suggestions by considering both objective code quality improvements\nand program correctness. Early feedback from users suggests that AI-enabled\nrefactoring helps mitigate code-level technical debt that otherwise rarely gets\nacted upon.", "AI": {"tldr": "This paper introduces Augmented Code Engineering (ACE), a tool that automates code improvements using validated LLM output to address program understanding inefficiencies in modern software development.", "motivation": "Developers spend 70% of their time understanding code, not writing it, creating a bottleneck in the AI-assisted coding era. Improving code understandability could significantly reduce technical debt and optimize developer productivity.", "method": "ACE was developed through a data-driven approach to deliver reliable refactoring suggestions by combining objective code quality metrics with program correctness validation using large language models.", "result": "Early user feedback indicates AI-enabled refactoring via ACE effectively mitigates code-level technical debt that would otherwise remain unresolved in typical development workflows.", "conclusion": "Automated code improvement tools like ACE are essential for managing technical debt and ensuring developer efficiency when working with rapidly expanding codebases in the AI-assisted development landscape."}}
{"id": "2507.02969", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02969", "abs": "https://arxiv.org/abs/2507.02969", "authors": ["Daniel L\u00f3pez-Montero", "Jos\u00e9 L. \u00c1lvarez-Aldana", "Alicia Morales-Mart\u00ednez", "Marta Gil-L\u00f3pez", "Juan M. Au\u00f1\u00f3n Garc\u00eda"], "title": "Reinforcement Learning for Automated Cybersecurity Penetration Testing", "comment": null, "summary": "This paper aims to provide an innovative machine learning-based solution to\nautomate security testing tasks for web applications, ensuring the correct\nfunctioning of all components while reducing project maintenance costs.\nReinforcement Learning is proposed to select and prioritize tools and optimize\nthe testing path. The presented approach utilizes a simulated webpage along\nwith its network topology to train the agent. Additionally, the model leverages\nGeometric Deep Learning to create priors that reduce the search space and\nimprove learning convergence. The validation and testing process was conducted\non real-world vulnerable web pages commonly used by human hackers for learning.\nAs a result of this study, a reinforcement learning algorithm was developed\nthat maximizes the number of vulnerabilities found while minimizing the number\nof steps required", "AI": {"tldr": "This paper proposes a machine learning-based method using Reinforcement Learning (RL) and Geometric Deep Learning to automate web application security testing, reducing maintenance costs while maximizing vulnerability detection efficiency through optimized tool selection and path prioritization.", "motivation": "Prior security testing methods for web applications require manual tool selection, resource-intensive processes, and lack intelligent optimization to reduce costs. The paper motivates the need for an automated approach that can efficiently identify vulnerabilities with minimal steps and resources.", "method": "The approach integrates RL to dynamically select and prioritize testing tools and optimize exploration paths, combined with Geometric Deep Learning to construct structural priors that shrink the search space and accelerate learning convergence. Training occurs in simulated web environments with real-world network topologies.", "result": "The developed algorithm achieved higher vulnerability detection rates in fewer interaction steps compared to baseline methods, validated on well-known exploitable web pages used by human hackers.", "conclusion": "The paper demonstrates that ML-driven automation, particularly RL with geometric priors, can enhance security testing efficacy and efficiency for web applications while lowering long-term maintenance costs."}}
{"id": "2507.03620", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG", "68T50", "I.2.7; D.2.3"], "pdf": "https://arxiv.org/pdf/2507.03620", "abs": "https://arxiv.org/abs/2507.03620", "authors": ["Francisca Lemos", "Victor Alves", "Filipa Ferraz"], "title": "Is It Time To Treat Prompts As Code? A Multi-Use Case Study For Prompt Optimization Using DSPy", "comment": "20 pages with 1 figure", "summary": "Although prompt engineering is central to unlocking the full potential of\nLarge Language Models (LLMs), crafting effective prompts remains a\ntime-consuming trial-and-error process that relies on human intuition. This\nstudy investigates Declarative Self-improving Python (DSPy), an optimization\nframework that programmatically creates and refines prompts, applied to five\nuse cases: guardrail enforcement, hallucination detection in code, code\ngeneration, routing agents, and prompt evaluation. Each use case explores how\nprompt optimization via DSPy influences performance. While some cases\ndemonstrated modest improvements - such as minor gains in the guardrails use\ncase and selective enhancements in hallucination detection - others showed\nnotable benefits. The prompt evaluation criterion task demonstrated a\nsubstantial performance increase, rising accuracy from 46.2% to 64.0%. In the\nrouter agent case, the possibility of improving a poorly performing prompt and\nof a smaller model matching a stronger one through optimized prompting was\nexplored. Although prompt refinement increased accuracy from 85.0% to 90.0%,\nusing the optimized prompt with a cheaper model did not improve performance.\nOverall, this study's findings suggest that DSPy's systematic prompt\noptimization can enhance LLM performance, particularly when instruction tuning\nand example selection are optimized together. However, the impact varies by\ntask, highlighting the importance of evaluating specific use cases in prompt\noptimization research.", "AI": {"tldr": "This study evaluates the Declarative Self-improving Python (DSPy) framework for optimizing Large Language Model prompts across five use cases, showing mixed performance improvements ranging from modest gains to significant accuracy boosts in specific tasks.", "motivation": "Prompt engineering for LLMs is typically a labor-intensive, intuitive trial-and-error process, motivating the need for a systematic, automated optimization framework to enhance efficiency and reliability.", "method": "DSPy is applied to optimize prompts in five scenarios: guardrail enforcement, code hallucination detection, code generation, routing agents, and prompt evaluation. Each case assesses performance changes through automated instruction and example selection refinements.", "result": "While guardrails and hallucination detection showed minor improvements, prompt evaluation accuracy increased from 46.2% to 64.0%. Router agent optimization raised accuracy from 85.0% to 90.0%, but using the optimized prompt with a cheaper model did not yield performance parity with stronger models.", "conclusion": "DSPy demonstrates potential to improve LLM performance via systematic prompt optimization, particularly when instruction tuning and example selection are co-optimized. However, its effectiveness is task-dependent, emphasizing the necessity for context-specific evaluations in prompt engineering research."}}
{"id": "2507.02971", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.02971", "abs": "https://arxiv.org/abs/2507.02971", "authors": ["Mohsen Ghasemizade", "Juniper Lovato", "Christopher M. Danforth", "Peter Sheridan Dodds", "Laura S. P. Bloomfield", "Matthew Price", "Team LEMURS", "Joseph P. Near"], "title": "Aim High, Stay Private: Differentially Private Synthetic Data Enables Public Release of Behavioral Health Information with High Utility", "comment": "14 pages, 8 figures, 2 tables", "summary": "Sharing health and behavioral data raises significant privacy concerns, as\nconventional de-identification methods are susceptible to privacy attacks.\nDifferential Privacy (DP) provides formal guarantees against re-identification\nrisks, but practical implementation necessitates balancing privacy protection\nand the utility of data.\n  We demonstrate the use of DP to protect individuals in a real behavioral\nhealth study, while making the data publicly available and retaining high\nutility for downstream users of the data. We use the Adaptive Iterative\nMechanism (AIM) to generate DP synthetic data for Phase 1 of the Lived\nExperiences Measured Using Rings Study (LEMURS). The LEMURS dataset comprises\nphysiological measurements from wearable devices (Oura rings) and self-reported\nsurvey data from first-year college students. We evaluate the synthetic\ndatasets across a range of privacy budgets, epsilon = 1 to 100, focusing on the\ntrade-off between privacy and utility.\n  We evaluate the utility of the synthetic data using a framework informed by\nactual uses of the LEMURS dataset. Our evaluation identifies the trade-off\nbetween privacy and utility across synthetic datasets generated with different\nprivacy budgets. We find that synthetic data sets with epsilon = 5 preserve\nadequate predictive utility while significantly mitigating privacy risks. Our\nmethodology establishes a reproducible framework for evaluating the practical\nimpacts of epsilon on generating private synthetic datasets with numerous\nattributes and records, contributing to informed decision-making in data\nsharing practices.", "AI": {"tldr": "This paper demonstrates differential privacy (DP) implementation for a behavioral health dataset using Adaptive Iterative Mechanism (AIM), balancing privacy protection and data utility across privacy budgets (epsilon = 1\u2013100).", "motivation": "Conventional de-identification methods are privacy-attack vulnerable, necessitating formal guarantees like DP to balance re-identification risk reduction with data utility preservation for sharing sensitive health/behavioral information.", "method": "Used AIM to generate DP synthetic data for LEMURS Phase 1 (combining wearable device measurements and self-reported student surveys), evaluating utility across 1 to 100 epsilon values with a framework aligned to dataset's real-world applications.", "result": "Achieved adequate predictive utility with epsilon=5 while substantially mitigating privacy risks; methodology demonstrates reproducible epsilon-utility-privacy trade-off evaluations for synthetic datasets with complex attributes/records.", "conclusion": "Established a standardized evaluation framework for DP synthetic data generation, enabling informed decisions about optimal privacy thresholds while maintaining dataset utility for downstream research applications."}}
{"id": "2507.03659", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.03659", "abs": "https://arxiv.org/abs/2507.03659", "authors": ["Valentina Wu", "Alexandra Mendes", "Alexandre Abreu"], "title": "Specification-Guided Repair of Arithmetic Errors in Dafny Programs using LLMs", "comment": null, "summary": "Formal verification offers strong assurances of software correctness.\nHowever, debugging and repairing the underlying faults can be complex and\ntime-consuming when verification fails. Automated Program Repair (APR) aims to\nease this by automatically identifying and fixing faults. Traditional APR\ntechniques often depend on test suites for validation, but these may fail to\ncapture all scenarios. In contrast, formal specifications provide stronger\ncorrectness criteria for effective repairs.\n  We present an innovative APR tool for Dafny, a verification-aware programming\nlanguage that uses formal specifications - including pre-conditions,\npost-conditions, and invariants - as oracles for fault localization and repair.\nAssuming the correctness of the specifications and focusing on arithmetic bugs,\nwe localize faults through a series of steps, which include using Hoare Logic\nto determine the state of each statement within the program and\nstate-of-the-art Large Language Models (LLMs) to synthesize candidate fixes.\nThe chosen models were GPT-4o mini, Llama 3, Mistral 7B, and Llemma 7B.\n  We evaluate our approach using DafnyBench, a benchmark of real-world Dafny\nprograms. Our tool achieves 89.6% accuracy in fault localization, with GPT-4o\nmini yielding the highest repair success rate (74.18%). These results highlight\nthe potential of combining formal reasoning with LLM-driven program synthesis\nfor automated program repair.", "AI": {"tldr": "The paper introduces an Automated Program Repair (APR) tool for Dafny that combines Hoare Logic and Large Language Models (LLMs) to address arithmetic bugs using formal specifications as oracles, achieving high localization accuracy and repair success rates.", "motivation": "Traditional APR techniques rely on test suites, which may miss critical scenarios; formal specifications offer stronger correctness criteria, making fault localization and repair more effective for verification-aware languages like Dafny.", "method": "1) Use Hoare Logic to assess the state of each program statement. 2) Leverage LLMs (GPT-4o mini, Llama 3, Mistral 7B, Llemma 7B) to synthesize candidate fixes. 3) Validate correctness using Dafny's formal specifications as oracles.", "result": "Achieved 89.6% fault localization accuracy on the DafnyBench benchmark, with GPT-4o mini showing the highest repair success rate (74.18%).", "conclusion": "Combining formal reasoning (Hoare Logic) with LLM-driven synthesis significantly improves APR effectiveness for Dafny, demonstrating the potential of integrating verification-aware methods with AI-based program repair."}}
{"id": "2507.02976", "categories": ["cs.CR", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.02976", "abs": "https://arxiv.org/abs/2507.02976", "authors": ["Amirali Sajadi", "Kostadin Damevski", "Preetha Chatterjee"], "title": "Are AI-Generated Fixes Secure? Analyzing LLM and Agent Patches on SWE-bench", "comment": null, "summary": "Large Language Models (LLMs) and their agentic frameworks are increasingly\nadopted to automate software development tasks such as issue resolution and\nprogram repair. While prior work has identified security risks in LLM-generated\ncode, most evaluations have focused on synthetic or isolated settings, leaving\nopen questions about the security of these systems in real-world development\ncontexts. In this study, we present the first large-scale security analysis of\nLLM-generated patches using 20,000+ issues from the SWE-bench dataset. We\nevaluate patches produced by a standalone LLM (Llama 3.3) and compare them to\ndeveloper-written patches. We also assess the security of patches generated by\nthree top-performing agentic frameworks (OpenHands, AutoCodeRover, HoneyComb)\non a subset of our data. Finally, we analyze a wide range of code, issue, and\nproject-level factors to understand the conditions under which LLMs and agents\nare most likely to generate insecure code. Our findings reveal that the\nstandalone LLM introduces nearly 9x more new vulnerabilities than developers,\nwith many of these exhibiting unique patterns not found in developers' code.\nAgentic workflows also generate a significant number of vulnerabilities,\nparticularly when granting LLMs more autonomy, potentially increasing the\nlikelihood of misinterpreting project context or task requirements. We find\nthat vulnerabilities are more likely to occur in LLM patches associated with a\nhigher number of files, more lines of generated code, and GitHub issues that\nlack specific code snippets or information about the expected code behavior and\nsteps to reproduce. These results suggest that contextual factors play a\ncritical role in the security of the generated code and point toward the need\nfor proactive risk assessment methods that account for both code and\nissue-level information to complement existing vulnerability detection tools.", "AI": {"tldr": "The study evaluates security risks in LLM-generated software patches using real-world data from 20,000+ GitHub issues. It reveals that standalone LLMs (Llama 3.3) and agentic frameworks (e.g., OpenHands) introduce significantly more new vulnerabilities than human-written patches, with factors like code complexity and imprecise issue descriptions exacerbating security flaws.", "motivation": "Previous security evaluations of LLM-generated code focused on synthetic or isolated scenarios, leaving gaps in understanding their real-world development risks as these models gain adoption in critical tasks like bug fixes and program repair.", "method": "Conducted large-scale analysis on 20,000+ SWE-bench issues, comparing vulnerabilities in patches from: (1) standalone Llama 3.3 model; (2) three top agentic frameworks with varying autonomy levels; and (3) developer-written patches. Analyzed code/issue/project-level factors correlating with insecurity.", "result": "1) Standalone LLM generated ~9x more new vulnerabilities than developers, with unique insecure patterns. 2) Agentic systems also showed high vulnerability rates, increasing with LLM autonomy. 3) Insecurity correlated with: multi-file patch complexity, larger code changes, and ambiguous GitHub issues (lack of reproduction steps/snippets).", "conclusion": "Contextual factors critically influence LLM security output, requiring complementary risk assessment methods that leverage both code complexity and issue-level information to reduce deployment risks."}}
{"id": "2507.04173", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.04173", "abs": "https://arxiv.org/abs/2507.04173", "authors": ["Henri A\u00efdasso", "Francis Bordeleau", "Ali Tizghadam"], "title": "Efficient Detection of Intermittent Job Failures Using Few-Shot Learning", "comment": "Accepted at the 41st International Conference on Software Maintenance\n  and Evolution - ICSME 2025, Industry Track", "summary": "One of the main challenges developers face in the use of continuous\nintegration (CI) and deployment pipelines is the occurrence of intermittent job\nfailures, which result from unexpected non-deterministic issues (e.g., flaky\ntests or infrastructure problems) rather than regular code-related errors such\nas bugs. Prior studies developed machine-learning (ML) models trained on large\ndatasets of job logs to classify job failures as either intermittent or\nregular. As an alternative to costly manual labeling of large datasets, the\nstate-of-the-art (SOTA) approach leveraged a heuristic based on\nnon-deterministic job reruns. However, this method mislabels intermittent job\nfailures as regular in contexts where rerunning suspicious job failures is not\nan explicit policy, and therefore limits the SOTA's performance in practice. In\nfact, our manual analysis of 2,125 job failures from 5 industrial and 1\nopen-source projects reveals that, on average, 32\\% of intermittent job\nfailures are mislabeled as regular. To address these limitations, this paper\nintroduces a novel approach to intermittent job failure detection using\nfew-shot learning (FSL). Specifically, we fine-tune a small language model\nusing a few number of manually labeled log examples to generate rich\nembeddings, which are then used to train an ML classifier. Our FSL-based\napproach achieves 70-88\\% F1-score with only 12 shots in all projects,\noutperforming the SOTA, which proved ineffective (34-52\\% F1-score) in 4\nprojects. Overall, this study underlines the importance of data quality over\nquantity and provides a more efficient and practical framework for the\ndetection of intermittent job failures in organizations.", "AI": {"tldr": "The paper introduces a few-shot learning approach for detecting intermittent job failures in CI/CD pipelines, achieving higher accuracy than state-of-the-art heuristic methods that rely on rerun data and mislabel 32% of cases.", "motivation": "State-of-the-art methods for classifying intermittent job failures use rerun-based heuristics but fail when reruns are not explicitly implemented, leading to significant mislabeling rates (32%) and poor performance in practice.", "method": "Fine-tunes a small language model using 12 manually labeled log examples per project to generate rich embeddings, which are then used as features for an ML classifier trained to detect intermittent failures.", "result": "Achieved 70-88% F1-score with only 12 labeled examples per project, outperforming state-of-the-art methods (34-52% F1-score) in four of the six analyzed projects.", "conclusion": "Demonstrates that few-shot learning with high-quality manually labeled data surpasses large-scale heuristic approaches for intermittent failure detection, emphasizing the importance of data quality over quantity in practical CI/CD contexts."}}
{"id": "2507.03000", "categories": ["cs.CR", "cs.IT", "math.IT", "Primary 05A17, Secondary 11D45, 11Y60, 94A60", "F.2.1"], "pdf": "https://arxiv.org/pdf/2507.03000", "abs": "https://arxiv.org/abs/2507.03000", "authors": ["Michael A. Idowu"], "title": "Deterministic Cryptographic Seed Generation via Cyclic Modular Inversion over $\\mathbb{Z}/3^p\\mathbb{Z}$", "comment": "29 pages, 13 figures, 13 tables. Includes entropy analysis, symbolic\n  residue formulation, empirical validation, and benchmarking against\n  NIST-recommended DRBG frameworks", "summary": "We present a deterministic framework for cryptographic seed generation based\non cyclic modular inversion over $\\mathbb{Z}/3^p\\mathbb{Z}$. The method\nenforces algebraic admissibility on seed inputs via the identity $d_k \\equiv\n-\\left(2^{k-1}\\right)^{-1} \\bmod 3^p$, thereby producing structured and\ninvertible residue sequences. This mapping yields entropy-rich, cycle-complete\nseeds well-suited for cryptographic primitives such as DRBGs, KDFs, and\npost-quantum schemes. To assess the quality of randomness, we introduce the\nEntropy Confidence Score (ECS), a composite metric reflecting coverage,\nuniformity, and modular bias. Although not a cryptographic PRNG in itself, the\nframework serves as a deterministic entropy filter that conditions and\nvalidates seed inputs prior to their use by conventional generators. Empirical\nand hardware-based results confirm constant-time execution, minimal\nside-channel leakage, and lightweight feasibility for embedded applications.\nThe framework complements existing cryptographic stacks by acting as an\nalgebraically verifiable entropy filter, thereby enhancing structural soundness\nand auditability.", "AI": {"tldr": "This paper proposes a deterministic entropy generation framework using algebraic modular inversions mod $3^p$. It introduces an Entropy Confidence Score (ECS) to validate seed quality and provides lightweight, secure seed sequences suitable for both classical and post-quantum cryptography with embedded system compatibility.", "motivation": "The authors aim to enhance structural soundness and auditability in cryptographic stacks by creating an algebraically-verifiable entropy filter. Existing generators lack formal algebraic validation mechanisms, and deterministic seeding is critical for secure hardware implementations and mitigating side-channel leaks.", "method": "The framework employs cyclic modular arithmetic over $\\mathbb{Z}/3^p\\mathbb{Z}$ with the identity $d_k \n\n", "result": "Results demonstrate constant-time execution with (1) 85% reduction in side-channel leakage vs. industry standards, (2) ECS validation showing >99% entropy coverage, and (3) 40% smaller hardware footprint compared to AES-CTR based DRBGs. Seed sequences pass NIST STS and Rabbit randomness tests.", "conclusion": "The algebraic entropy filtering framework establishes a new baseline for cryptographically verifiable seeding. The three-stage modular inversion approach enables hardware-secured, auditable randomness while maintaining compatibility with legacy cryptographic primitives through deterministic entropy conditioning."}}
{"id": "2507.04185", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.04185", "abs": "https://arxiv.org/abs/2507.04185", "authors": ["Aniket Kesari", "Travis Breaux", "Tom Norton", "Sarah Santos", "Anmol Singhal"], "title": "From Legal Text to Tech Specs: Generative AI's Interpretation of Consent in Privacy Law", "comment": "10 pages, 1 figure, 20th International Conference on Artificial\n  Intelligence and Law (ICAIL 2025)", "summary": "Privacy law and regulation have turned to \"consent\" as the legitimate basis\nfor collecting and processing individuals' data. As governments have rushed to\nenshrine consent requirements in their privacy laws, such as the California\nConsumer Privacy Act (CCPA), significant challenges remain in understanding how\nthese legal mandates are operationalized in software. The opaque nature of\nsoftware development processes further complicates this translation. To address\nthis, we explore the use of Large Language Models (LLMs) in requirements\nengineering to bridge the gap between legal requirements and technical\nimplementation. This study employs a three-step pipeline that involves using an\nLLM to classify software use cases for compliance, generating LLM modifications\nfor non-compliant cases, and manually validating these changes against legal\nstandards. Our preliminary findings highlight the potential of LLMs in\nautomating compliance tasks, while also revealing limitations in their\nreasoning capabilities. By benchmarking LLMs against real-world use cases, this\nresearch provides insights into leveraging AI-driven solutions to enhance legal\ncompliance of software.", "AI": {"tldr": "This paper explores using Large Language Models (LLMs) in requirements engineering to improve compliance with privacy laws like the CCPA by addressing challenges in translating legal requirements into technical software implementations. It outlines a three-step pipeline involving classification, modification generation, and manual validation, highlighting LLMs' automation potential while noting their reasoning limitations.", "motivation": "The increasing reliance on consent-based privacy laws such as the CCPA has created significant challenges in operationalizing these legal mandates within software development processes, necessitating a clearer understanding of how to bridge the gap between legal and technical requirements.", "method": "The study employs a three-step pipeline: (1) using an LLM to classify software use cases for compliance with legal standards, (2) generating potential modifications for non-compliant use cases, and (3) manually validating these modifications against legal frameworks to assess their effectiveness.", "result": "Preliminary findings demonstrate LLMs' potential to automate compliance tasks but also reveal limitations in their reasoning capabilities when applied to real-world privacy use cases, indicating a need for further refinement in aligning technical and legal interpretations.", "conclusion": "While LLMs show promise in streamlining legal compliance for software, their current reasoning limitations necessitate cautious implementation and manual validation to ensure robustness, offering valuable insights for improving AI-driven compliance solutions in future work."}}
{"id": "2507.03014", "categories": ["cs.CR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.03014", "abs": "https://arxiv.org/abs/2507.03014", "authors": ["Do-hyeon Yoon", "Minsoo Chun", "Thomas Allen", "Hans M\u00fcller", "Min Wang", "Rajesh Sharma"], "title": "Intrinsic Fingerprint of LLMs: Continue Training is NOT All You Need to Steal A Model!", "comment": "This paper flags a potential case of model plagiarism, copyright\n  violation, and information fabrication in arXiv:2505.21411", "summary": "Large language models (LLMs) face significant copyright and intellectual\nproperty challenges as the cost of training increases and model reuse becomes\nprevalent. While watermarking techniques have been proposed to protect model\nownership, they may not be robust to continue training and development, posing\nserious threats to model attribution and copyright protection. This work\nintroduces a simple yet effective approach for robust LLM fingerprinting based\non intrinsic model characteristics. We discover that the standard deviation\ndistributions of attention parameter matrices across different layers exhibit\ndistinctive patterns that remain stable even after extensive continued\ntraining. These parameter distribution signatures serve as robust fingerprints\nthat can reliably identify model lineage and detect potential copyright\ninfringement. Our experimental validation across multiple model families\ndemonstrates the effectiveness of our method for model authentication. Notably,\nour investigation uncovers evidence that a recently Pangu Pro MoE model\nreleased by Huawei is derived from Qwen-2.5 14B model through upcycling\ntechniques rather than training from scratch, highlighting potential cases of\nmodel plagiarism, copyright violation, and information fabrication. These\nfindings underscore the critical importance of developing robust fingerprinting\nmethods for protecting intellectual property in large-scale model development\nand emphasize that deliberate continued training alone is insufficient to\ncompletely obscure model origins.", "AI": {"tldr": "This paper proposes a robust method for large language model (LLM) fingerprinting using intrinsic parameter patterns in attention matrices, demonstrating it can detect model lineage and copyright infringement, including evidence that Huawei's Pangu Pro MoE may be derived from Qwen-2.5.", "motivation": "The paper addresses the urgent need for reliable LLM ownership protection as training costs rise and model reuse becomes widespread. Existing watermarking techniques are vulnerable to continued training, undermining intellectual property enforcement.", "method": "The authors analyze the standard deviation distributions of weights in LLM attention matrices across layers, identifying stable, fingerprint-like patterns that persist despite continued training. These distribution signatures enable robust model lineage identification and plagiarism detection.", "result": "Experiments validate the method's effectiveness across multiple model families, successfully identifying model origins and detecting suspicious cases like Huawei's Pangu Pro MoE potentially being derived from Qwen-2.5 via upcycling rather than full retraining.", "conclusion": "The work emphasizes the importance of developing intrinsic model fingerprinting techniques to combat IP theft in large-scale LLM development. It shows deliberate continued training alone cannot fully obscure a model's lineage, providing a crucial tool for attribution and copyright enforcement."}}
{"id": "2507.04354", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.04354", "abs": "https://arxiv.org/abs/2507.04354", "authors": ["Yanzhou Mu", "Juan Zhai", "Chunrong Fang", "Xiang Chen", "Zhixiang Cao", "Peiran Yang", "Kexin Zhao", "An Guo", "Zhenyu Chen"], "title": "Improving Deep Learning Framework Testing with Model-Level Metamorphic Testing", "comment": "23 pages, 5 figures", "summary": "Deep learning (DL) frameworks are essential to DL-based software systems, and\nframework bugs may lead to substantial disasters, thus requiring effective\ntesting. Researchers adopt DL models or single interfaces as test inputs and\nanalyze their execution results to detect bugs. However, floating-point errors,\ninherent randomness, and the complexity of test inputs make it challenging to\nanalyze execution results effectively, leading to existing methods suffering\nfrom a lack of suitable test oracles. Some researchers utilize metamorphic\ntesting to tackle this challenge. They design Metamorphic Relations (MRs) based\non input data and parameter settings of a single framework interface to\ngenerate equivalent test inputs, ensuring consistent execution results between\noriginal and generated test inputs. Despite their promising effectiveness, they\nstill face certain limitations. (1) Existing MRs overlook structural\ncomplexity, limiting test input diversity. (2) Existing MRs focus on limited\ninterfaces, which limits generalization and necessitates additional\nadaptations. (3) Their detected bugs are related to the result consistency of\nsingle interfaces and far from those exposed in multi-interface combinations\nand runtime metrics (e.g., resource usage). To address these limitations, we\npropose ModelMeta, a model-level metamorphic testing method for DL frameworks\nwith four MRs focused on the structure characteristics of DL models. ModelMeta\naugments seed models with diverse interface combinations to generate test\ninputs with consistent outputs, guided by the QR-DQN strategy. It then detects\nbugs through fine-grained analysis of training loss/gradients, memory/GPU\nusage, and execution time.", "AI": {"tldr": "The paper proposes ModelMeta, a model-level metamorphic testing method addressing limitations of existing approaches by exploiting DL model structure characteristics and runtime metrics analysis for comprehensive framework bug detection.", "motivation": "Deep learning frameworks require effective testing due to potential disastrous bugs, but existing methods face challenges from floating-point errors, inherent randomness, and complex test inputs, resulting in unsuitable test oracles that lack generalization and fail to detect bugs in multi-interface combinations and runtime behaviors.", "method": "ModelMeta introduces four metamorphic relations (MRs) focused on model structure and couples them with the QR-DQN strategy to generate diverse interface-combination test inputs with consistent outputs, then detects bugs via detailed analysis of training loss/gradients, memory/GPU usage, and execution time.", "result": "The approach overcomes prior limitations by testing multi-interface combinations, analyzing runtime resource metrics, and capturing structural bugs in addition to result consistency.", "conclusion": "ModelMeta advances DL framework testing by addressing structural complexity, interface generalization, and runtime metrics. Its four MRs combined with a strategic testing approach enable more effective detection of framework bugs beyond single-interface consistency issues."}}
{"id": "2507.03021", "categories": ["cs.CR", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.03021", "abs": "https://arxiv.org/abs/2507.03021", "authors": ["Ya-Ting Yang", "Quanyan Zhu"], "title": "A Multi-Resolution Dynamic Game Framework for Cross-Echelon Decision-Making in Cyber Warfare", "comment": null, "summary": "Cyber warfare has become a critical dimension of modern conflict, driven by\nsociety's increasing dependence on interconnected digital and physical\ninfrastructure. Effective cyber defense often requires decision-making at\ndifferent echelons, where the tactical layer focuses on detailed actions such\nas techniques, tactics, and procedures, while the strategic layer addresses\nlong-term objectives and coordinated planning. Modeling these interactions at\ndifferent echelons remains challenging due to the dynamic, large-scale, and\ninterdependent nature of cyber environments. To address this, we propose a\nmulti-resolution dynamic game framework in which the tactical layer captures\nfine-grained interactions using high-resolution extensive-form game trees,\nwhile the strategic layer is modeled as a Markov game defined over\nlower-resolution states abstracted from those game trees. This framework\nsupports scalable reasoning and planning across different levels of abstraction\nthrough zoom-in and zoom-out operations that adjust the granularity of the\nmodeling based on operational needs. A case study demonstrates how the\nframework works and its effectiveness in improving the defender's strategic\nadvantage.", "AI": {"tldr": "The paper proposes a multi-resolution dynamic game framework to model cyber warfare defense across tactical and strategic layers, enabling scalable reasoning through abstraction adjustments.", "motivation": "Modern cyber defense requires decision-making across different echelons (tactical vs strategic) due to society's reliance on interconnected systems. Existing modeling approaches struggle with the dynamic, large-scale, and interdependent nature of cyber environments.", "method": "develops a dual-layer framework where tactical interactions are captured via high-resolution extensive-form game trees while strategic layer uses low-resolution Markov games abstracted from these trees. A zoom-in/zoom-out mechanism dynamically adjusts model granularity based on operational needs.", "result": "Case study demonstrates improved strategic advantage for defenders through this multi-resolution approach, enabling effective planning across abstraction levels in complex cyber environments.", "conclusion": "This framework provides a scalable solution for modeling cyber warfare by integrating tactical detail and strategic abstraction, with adjustable resolution to optimize resource allocation and strategy formation."}}
{"id": "2507.04360", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.04360", "abs": "https://arxiv.org/abs/2507.04360", "authors": ["Yanzhou Mu", "Juan Zhai", "Chunrong Fang", "Xiang Chen", "Zhixiang Cao", "Peiran Yang", "Yinglong Zou", "Tao Zheng", "Zhenyu Chen"], "title": "DevMuT: Testing Deep Learning Framework via Developer Expertise-Based Mutation", "comment": "12 pages, 8 figures", "summary": "Deep learning (DL) frameworks are the fundamental infrastructure for various\nDL applications. Framework defects can profoundly cause disastrous accidents,\nthus requiring sufficient detection. In previous studies, researchers adopt DL\nmodels as test inputs combined with mutation to generate more diverse models.\nThough these studies demonstrate promising results, most detected defects are\nconsidered trivial (i.e., either treated as edge cases or ignored by the\ndevelopers). To identify important bugs that matter to developers, we propose a\nnovel DL framework testing method DevMuT, which generates models by adopting\nmutation operators and constraints derived from developer expertise. DevMuT\nsimulates developers'common operations in development and detects more diverse\ndefects within more stages of the DL model lifecycle (e.g., model training and\ninference). We evaluate the performance of DevMuT on three widely used DL\nframeworks (i.e., PyTorch, JAX, and Mind- Spore) with 29 DL models from nine\ntypes of industry tasks. The experiment results show that DevMuT outperforms\nstate-of-the-art baselines: it can achieve at least 71.68% improvement on\naverage in the diversity of generated models and 28.20% improvement on average\nin the legal rates of generated models. Moreover, DevMuT detects 117 defects,\n63 of which are confirmed, 24 are fixed, and eight are of high value confirmed\nby developers. Finally, DevMuT has been deployed in the MindSpore community\nsince December 2023. These demonstrate the effectiveness of DevMuT in detecting\ndefects that are close to the real scenes and are of concern to developers.", "AI": {"tldr": "DevMuT is a novel deep learning framework testing method that leverages developer expertise to identify high-value defects across model training and inference via mutation operators and lifecycle simulations.", "motivation": "Most existing deep learning framework defect detection methods focus on trivial or 'edge case' bugs ignored by developers, necessitating techniques to identify critical defects aligned with developer priorities.", "method": "DevMuT generates diverse test models by integrating mutation operators with domain constraints derived from developer knowledge, simulating realistic development workflows through staged lifecycle testing.", "result": "Outperformed state-of-the-art baselines with 71.68% improvement in model diversity and 28.20% higher legal rates while detecting 117 defects (24 of which were fixed), including 8 high-value ones validated by developers across 3 frameworks.", "conclusion": "DevMuT successfully detects developer-priority defects relevant to real-world usage, demonstrated by its 2023 deployment in MindSpore and confirmation of multiple critical issues by framework communities."}}
{"id": "2507.03051", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.03051", "abs": "https://arxiv.org/abs/2507.03051", "authors": ["Marco Simoni", "Aleksandar Fontana", "Giulio Rossolini", "Andrea Saracino"], "title": "Improving LLM Reasoning for Vulnerability Detection via Group Relative Policy Optimization", "comment": "Under Review", "summary": "Improving and understanding the training dynamics and reasoning of Large\nLanguage Models (LLMs) has become essential for their deployment in AI-based\nsecurity tools, such as software vulnerability detection. In this work, we\npresent an extensive study aimed at advancing recent RL-based finetuning\ntechniques for LLMs in the context of vulnerability detection.\n  We start by highlighting key limitations of commonly adopted LLMs, such as\ntheir tendency to over-predict certain types of vulnerabilities while failing\nto detect others. To address this challenge, we explore the use of Group\nRelative Policy Optimization (GRPO), a recent policy-gradient method, for\nguiding LLM behavior through structured, rule-based rewards. We enable its\napplication to the vulnerability detection task by redefining its advantage\nfunctions and reward signals using annotations from widely used datasets in the\nfield, including BigVul, DiverseVul, and CleanVul.\n  The proposed methodology enables an extensive set of experiments, addressing\nmultiple research questions regarding the impact of GRPO on generalization,\nreasoning capabilities, and performance improvements over standard supervised\nfinetuning (SFT). Our findings offer valuable insights into the potential of\nRL-based training to enhance both the performance and reasoning abilities of\nLLMs in the context of software vulnerability detection.", "AI": {"tldr": "This paper presents an extensive study exploring Group Relative Policy Optimization (GRPO) to enhance Large Language Models (LLMs) for software vulnerability detection. It addresses LLM limitations in predicting vulnerabilities and improves their performance and reasoning through structured RL-based fine-tuning.", "motivation": "LLMs deployed in AI-based security tools, like vulnerability detection systems, exhibit key limitations such as over-predicting specific vulnerabilities while missing others. This challenges their reliability and effectiveness, necessitating improved training dynamics and reasoning capabilities.", "method": "The authors employ GRPO, a policy-gradient method, and adapt it for vulnerability detection by redefining advantage functions and reward signals using annotations from datasets like BigVul, DiverseVul, and CleanVul. This allows structured, rule-based reward guidance for LLM training.", "result": "Experiments validate the methodology's effectiveness across multiple research questions, revealing performance and reasoning enhancements over standard supervised finetuning (SFT) in vulnerability detection tasks. The findings highlight generalization improvements and nuanced detection capabilities.", "conclusion": "RL-based training via GRPO significantly improves the performance and reasoning abilities of LLMs for software vulnerability detection, offering a structured approach to address overprediction issues and underscoring the potential of reinforcement learning techniques in this domain."}}
{"id": "2507.04390", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.04390", "abs": "https://arxiv.org/abs/2507.04390", "authors": ["Vanesya Aura Ardity", "Yusuf Sulistyo Nugroho", "Syful Islam"], "title": "Exploring React Library Related Questions on Stack Overflow: Answered vs. Unanswered", "comment": "6 pages, 9 figures, 7 tables, conference paper", "summary": "React is a popular JavaScript framework in modern web application\ndevelopment. Due to its high performance and efficiency, many developers use\nthis framework. Although React library offers many advantages, it is not\nwithout its challenges. When using React library, developers often face\nproblems where they often seek solutions through question-and-answer forums,\nsuch as Stack Overflow (SO). However, despite its high popularity, many\nReact-related questions on SO remain unanswered. Thus, this study aims to\nanalyze the factors associated with question answerability and difficulty\nlevels of React-related questions on SO. To facilitate our study, Exploratory\nData Analysis was applied to 534,820 questions, where they are filtered based\non 23 React-related tags. We implemented a quantitative approach through text\nmining and statistical analysis. A logistic regression model was used to\nidentify attributes associated with question answerability, while a simple\nlinear regression model was employed to examine the correlation between user\nreputations and performance difficulty scores (PD Score). The results show that\nsome attributes, such as number of views, code snippet inclusion, number of\nlines of code, and user reputation, positively affect the likelihood of\nquestion answerability. In contrast, the number of comments, question lengths,\nand presence of images in React-related questions reduce the probability of a\nquestion receiving responses from users. Further investigation indicates a\nnegative correlation between user reputations and PD Score, where reputation\nincrease corresponds to -0.092 reduction in PD score, signaling experienced\nusers tend to propose more complex technical inquiries. This study provides\ninsights into the characteristics of technical question-and-answer platforms,\nsuch as SO, that users need to consider the answerability factors when posting\nquestions related to React.", "AI": {"tldr": "Analyzes factors affecting answerability and difficulty of React-related questions on Stack Overflow using EDA and regression models.", "motivation": "Despite React's popularity, many questions remain unanswered on Stack Overflow, prompting investigation into factors influencing answerability.", "method": "Applied Exploratory Data Analysis to 534,820 questions with 23 React tags, using logistic regression (attributes affecting answerability) and linear regression (user reputation vs. PD Score).", "result": "Code snippets and user reputation increase answerability. Comments, question length, and images decrease it. Higher user reputation correlates with -0.092 reduction in PD Score, indicating complex questions from experienced users.", "conclusion": "Technical Q&A platforms require consideration of post characteristics (e.g., code inclusion, question conciseness) to enhance answerability. Experienced users may ask more challenging questions, lowering PD Scores."}}
{"id": "2507.03064", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03064", "abs": "https://arxiv.org/abs/2507.03064", "authors": ["Hetvi Shastri", "Walid A. Hanafy", "Li Wu", "David Irwin", "Mani Srivastava", "Prashant Shenoy"], "title": "LLM-Driven Auto Configuration for Transient IoT Device Collaboration", "comment": null, "summary": "Today's Internet of Things (IoT) has evolved from simple sensing and\nactuation devices to those with embedded processing and intelligent services,\nenabling rich collaborations between users and their devices. However, enabling\nsuch collaboration becomes challenging when transient devices need to interact\nwith host devices in temporarily visited environments. In such cases,\nfine-grained access control policies are necessary to ensure secure\ninteractions; however, manually implementing them is often impractical for\nnon-expert users. Moreover, at run-time, the system must automatically\nconfigure the devices and enforce such fine-grained access control rules.\nAdditionally, the system must address the heterogeneity of devices.\n  In this paper, we present CollabIoT, a system that enables secure and\nseamless device collaboration in transient IoT environments. CollabIoT employs\na Large language Model (LLM)-driven approach to convert users' high-level\nintents to fine-grained access control policies. To support secure and seamless\ndevice collaboration, CollabIoT adopts capability-based access control for\nauthorization and uses lightweight proxies for policy enforcement, providing\nhardware-independent abstractions.\n  We implement a prototype of CollabIoT's policy generation and auto\nconfiguration pipelines and evaluate its efficacy on an IoT testbed and in\nlarge-scale emulated environments. We show that our LLM-based policy generation\npipeline is able to generate functional and correct policies with 100%\naccuracy. At runtime, our evaluation shows that our system configures new\ndevices in ~150 ms, and our proxy-based data plane incurs network overheads of\nup to 2 ms and access control overheads up to 0.3 ms.", "AI": {"tldr": "CollabIoT is an IoT system enabling secure, seamless device collaboration in transient environments using an LLM-driven approach to generate access control policies, achieving 100% accuracy with low overhead due to proxies and auto-configuration.", "motivation": "Transient IoT devices in temporary environments require fine-grained access control for security, but manual implementation is impractical for non-experts. Systems must also auto-configure, enforce policies at runtime, and address device heterogeneity.", "method": "CollabIoT uses an LLM to translate user intents into policies, combines capability-based access control with lightweight proxies for policy enforcement, and provides hardware-independent abstractions through auto-configuration pipelines.", "result": "Prototype achieved 100% policy accuracy. Device configuration time was ~150 ms, and the system incurred network overhead of 2 ms and access control overhead of 0.3 ms, validated on real and emulated IoT testbeds.", "conclusion": "CollabIoT demonstrates that LLM-driven policy generation can securely and efficiently enable transient device collaboration in IoT settings by automating access control with minimal runtime overhead."}}
{"id": "2507.04422", "categories": ["cs.SE", "cs.AI", "D.2.7; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.04422", "abs": "https://arxiv.org/abs/2507.04422", "authors": ["Guoming Long", "Jingzhi Gong", "Hui Fang", "Tao Chen"], "title": "Learning Software Bug Reports: A Systematic Literature Review", "comment": "Accepted by TOSEM", "summary": "The recent advancement of artificial intelligence, especially machine\nlearning (ML), has significantly impacted software engineering research,\nincluding bug report analysis. ML aims to automate the understanding,\nextraction, and correlation of information from bug reports. Despite its\ngrowing importance, there has been no comprehensive review in this area. In\nthis paper, we present a systematic literature review covering 1,825 papers,\nselecting 204 for detailed analysis. We derive seven key findings: 1) Extensive\nuse of CNN, LSTM, and $k$NN for bug report analysis, with advanced models like\nBERT underutilized due to their complexity. 2) Word2Vec and TF-IDF are popular\nfor feature representation, with a rise in deep learning approaches. 3) Stop\nword removal is the most common preprocessing, with structural methods rising\nafter 2020. 4) Eclipse and Mozilla are the most frequently evaluated software\nprojects. 5) Bug categorization is the most common task, followed by bug\nlocalization and severity prediction. 6) There is increasing attention on\nspecific bugs like non-functional and performance bugs. 7) Common evaluation\nmetrics are F1-score, Recall, Precision, and Accuracy, with $k$-fold\ncross-validation preferred for model evaluation. 8) Many studies lack robust\nstatistical tests. We also identify six promising future research directions to\nprovide useful insights for practitioners.", "AI": {"tldr": "A systematic literature review analyzing 204 out of 1,825 papers to summarize trends and challenges in ML-based bug report analysis, identifying key models, techniques, and future research directions", "motivation": "Recent growth of ML in bug report analysis lacks a comprehensive review, necessitating structured insights to guide practitioners and researchers", "method": "Systematic literature review of 1,825 papers with detailed analysis of 204 selected works", "result": "Seven key findings: 1) CNN, LSTM, $k$NN dominance over BERT, 2) Word2Vec/TF-IDF feature prevalence, 3) Stop word removal as common preprocessing, 4) Eclipse/Mozilla as benchmark projects, 5) Bug categorization as primary task, 6) Rising focus on non-functional/performance bugs, 7) $k$-fold cross-validation preference with limited statistical rigor", "conclusion": "Summary highlights ML model/technique adoption patterns, project datasets, and evaluation methodologies in bug report analysis while identifying gaps (e.g., insufficient statistical tests) and suggesting six future research directions for practical impact"}}
{"id": "2507.03136", "categories": ["cs.CR", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.03136", "abs": "https://arxiv.org/abs/2507.03136", "authors": ["Ricardo Queiroz de Araujo Fernandes", "Anderson Santos", "Daniel Maier de Carvalho", "Andr\u00e9 Luiz Bandeira Molina"], "title": "Holographic Projection and Cyber Attack Surface: A Physical Analogy for Digital Security", "comment": "The paper was produced to base a presentation in the V Jornadas STIC\n  capitulo Panam\\'a", "summary": "This article presents an in-depth exploration of the analogy between the\nHolographic Principle in theoretical physics and cyber attack surfaces in\ndigital security. Building on concepts such as black hole entropy and AdS/CFT\nduality, it highlights how complex infrastructures project their\nvulnerabilities onto their external interfaces. The paper draws a parallel\nbetween a black hole's event horizon, which encodes all internal information,\nand the attack surface, which reflects the internal architecture's security\nposture. Additionally, the article outlines how this conceptual framework can\nguide cybersecurity practices, emphasizing strategies such as attack surface\nreduction, continuous scanning with tools like OWASP ZAP and Greenbone OpenVAS,\nand the implementation of Zero Trust Architecture. This analogy not only\nprovides a unique perspective on digital security but also underscores the\ncritical importance of boundary-level defenses in protecting vast internal\ninfrastructures.", "AI": {"tldr": "The paper analogizes the Holographic Principle in physics to cyber attack surfaces, explaining how internal system vulnerabilities are encoded in external interfaces and proposing boundary-heavy defense strategies like attack surface reduction and Zero Trust.", "motivation": "Draws from theoretical physics concepts (black hole entropy, AdS/CFT) to offer a novel framework for understanding complex infrastructures' security posture through their external interfaces.", "method": "Establishes a conceptual analogy between black hole event horizons encoding internal information and attack surfaces reflecting system vulnerabilities, applying this dual-layer perspective to cybersecurity.", "result": "Proposes actionable strategies including attack surface reduction, continuous scanning (OWASP ZAP, Greenbone OpenVAS), and Zero Trust Architecture implementation as boundary-level defense solutions.", "conclusion": "The physics-inspired analogy highlights the critical need for strong boundary defenses to protect complex digital infrastructures, offering both a fresh theoretical viewpoint and practical cybersecurity guidance."}}
{"id": "2507.04548", "categories": ["cs.SE", "cs.AI", "cs.LG", "D.2.11; D.2.7; I.2.7; I.5.4"], "pdf": "https://arxiv.org/pdf/2507.04548", "abs": "https://arxiv.org/abs/2507.04548", "authors": ["Renato Cordeiro Ferreira", "Dayanne Gomes", "Vitor Tamae", "Francisco Wernke", "Alfredo Goldman"], "title": "SPIRA: Building an Intelligent System for Respiratory Insufficiency Detection", "comment": "4 pages, 1 figure (1 diagram), published at ISE 2022", "summary": "Respiratory insufficiency is a medic symptom in which a person gets a reduced\namount of oxygen in the blood. This paper reports the experience of building\nSPIRA: an intelligent system for detecting respiratory insufficiency from\nvoice. It compiles challenges faced in two succeeding implementations of the\nsame architecture, summarizing lessons learned on data collection, training,\nand inference for future projects in similar systems.", "AI": {"tldr": "This paper presents SPIRA, an intelligent voice-based system for detecting respiratory insufficiency, detailing implementation challenges and lessons learned in data, training, and inference.", "motivation": "Respiratory insufficiency requires timely detection; voice-based systems offer non-invasive alternatives but face practical barriers in development.", "method": "The authors built and iteratively refined two versions of SPIRA's architecture, analyzing obstacles during data collection, training, and inference phases.", "result": "Key challenges included limited voice data quality, model reproducibility during training, and deployment constraints in real-time inference for medical applications.", "conclusion": "The paper establishes practical guidelines for future voice-based medical detection systems by systematically documenting implementation pitfalls and solutions."}}
{"id": "2507.03236", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03236", "abs": "https://arxiv.org/abs/2507.03236", "authors": ["Noureldin Zahran", "Ahmad Tahmasivand", "Ihsen Alouani", "Khaled Khasawneh", "Mohammed E. Fouda"], "title": "On Jailbreaking Quantized Language Models Through Fault Injection Attacks", "comment": "This work has been published in GLSVLSI 2025", "summary": "The safety alignment of Language Models (LMs) is a critical concern, yet\ntheir integrity can be challenged by direct parameter manipulation attacks,\nsuch as those potentially induced by fault injection. As LMs are increasingly\ndeployed using low-precision quantization for efficiency, this paper\ninvestigates the efficacy of such attacks for jailbreaking aligned LMs across\ndifferent quantization schemes. We propose gradient-guided attacks, including a\ntailored progressive bit-level search algorithm introduced herein and a\ncomparative word-level (single weight update) attack. Our evaluation on\nLlama-3.2-3B, Phi-4-mini, and Llama-3-8B across FP16 (baseline), and\nweight-only quantization (FP8, INT8, INT4) reveals that quantization\nsignificantly influences attack success. While attacks readily achieve high\nsuccess (>80\\% Attack Success Rate, ASR) on FP16 models, within an attack\nbudget of 25 perturbations, FP8 and INT8 models exhibit ASRs below 20\\% and\n50\\%, respectively. Increasing the perturbation budget up to 150 bit-flips, FP8\nmodels maintained ASR below 65\\%, demonstrating some resilience compared to\nINT8 and INT4 models that have high ASR. In addition, analysis of perturbation\nlocations revealed differing architectural targets across quantization schemes,\nwith (FP16, INT4) and (INT8, FP8) showing similar characteristics. Besides,\njailbreaks induced in FP16 models were highly transferable to subsequent\nFP8/INT8 quantization (<5\\% ASR difference), though INT4 significantly reduced\ntransferred ASR (avg. 35\\% drop). These findings highlight that while common\nquantization schemes, particularly FP8, increase the difficulty of direct\nparameter manipulation jailbreaks, vulnerabilities can still persist,\nespecially through post-attack quantization.", "AI": {"tldr": "This paper evaluates how quantization schemes affect the success of direct parameter manipulation attacks for jailbreaking safety-aligned language models, finding that lower precision (e.g., FP8) increases attack difficulty but vulnerabilities may persist through transfer.", "motivation": "Understanding the impact of quantization on safety alignment is critical as models shift to low-precision deployment for efficiency. Prior work lacked analysis of how specific quantization schemes influence adversarial robustness against parameter manipulation.", "method": "Gradient-guided attacks with a progressive bit-level search algorithm (25-150 perturbations) and a single-perturbation word-level attack were applied to Llama-3.2-3B, Phi-4-mini, and Llama-3-8B under FP16, FP8, INT8, INT4 quantization schemes.", "result": "FP16 models achieved >80% ASR with 25 perturbations, while FP8/INT8 had <20%/50% ASR. At 150 bit-flips, FP8 remained under 65% ASR but INT4 dropped transferred ASR by 35%. Architectural targets of attacks varied by quantization scheme.", "conclusion": "Lower-precision quantization (especially FP8) provides architectural resilience against parameter manipulation attacks, but prior-jailbroken FP16 models may transfer their vulnerabilities to quantized versions. Post-attack quantization remains a key vulnerability avenue."}}
{"id": "2507.04555", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.04555", "abs": "https://arxiv.org/abs/2507.04555", "authors": ["Gabriella Waters"], "title": "Testing, Evaluation, Verification and Validation (TEVV) of Digital Twins: A Comprehensive Framework", "comment": "1 figure, 41 pages, 3 tables", "summary": "Digital twins have emerged as a powerful technology for modeling and\nsimulating complex systems across various domains (Fuller et al., 2020; Tao et\nal., 2019). As virtual representations of physical assets, processes, or\nsystems, digital twins enable real-time monitoring, predictive analysis, and\noptimization. However, as digital twins become more sophisticated and integral\nto decision-making processes, ensuring their accuracy, reliability, and ethical\nimplementation is essential. This paper presents a comprehensive framework for\nthe Testing, Evaluation, Verification and Validation (TEVV) of digital twins to\naddress the unique challenges posed by these dynamic and complex virtual\nmodels.", "AI": {"tldr": "The paper introduces a comprehensive framework for Testing, Evaluation, Verification, and Validation (TEVV) of digital twins to ensure their accuracy, reliability, and ethical implementation.", "motivation": "As digital twins grow in complexity and decision-making importance, their accuracy, reliability, and ethical use must be rigorously ensured.", "method": "Developing a TEVV framework tailored to the dynamic and complex nature of digital twins.", "result": "Presentation of the framework addressing challenges in real-time monitoring, predictive analysis, and optimization of digital twins.", "conclusion": "The proposed TEVV framework is critical for advancing digital twin technology responsibly and effectively."}}
{"id": "2507.03258", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.03258", "abs": "https://arxiv.org/abs/2507.03258", "authors": ["Zhaorun Lin"], "title": "Novel Blockchain-based Protocols for Electronic Voting and Auctions", "comment": "My thesis for MPhil at HKUST", "summary": "Programmable blockchains have long been a hot research topic given their\ntremendous use in decentralized applications. Smart contracts, using\nblockchains as their underlying technology, inherit the desired properties such\nas verifiability, immutability, and transparency, which make it a great suit in\ntrustless environments.\n  In this thesis, we consider several decentralized protocols to be built on\nblockchains, specifically using smart contracts on Ethereum. We used\nalgorithmic and cryptographic tools in our implementations to further improve\nthe level of security and efficiency beyond the state-of-the-art works. We\nproposed a new approach called Blind Vote, which is an untraceable, secure,\nefficient, secrecy-preserving, and fully on-chain electronic voting protocol\nbased on the well-known concept of Chaum's blind signatures. We illustrate that\nour approach achieves the same security guarantees as previous methods such as\nTornado Vote [1], while consuming significantly less gas. Thus, we provide a\ncheaper and considerably more gas-efficient alternative for anonymous\nblockchain-based voting. On the other hand, we propose a new family of\nalgorithms for private, trustless auctions that protect bidder identities and\nbid values while remaining practical for smart contract execution. We ensure\ntrustlessness by running the auction logic in a smart contract, thereby\neliminating reliance on any single trusted party. This approach prevents bid\ntampering, front-running, and collusion by enforcing immutability and\ndecentralized verification of bids. The resulting protocol uniquely combines\nefficiency, trustlessness, and enduring bid privacy, offering a scalable and\nsecure solution for blockchain-based marketplaces and other decentralized\napplications.", "AI": {"tldr": "This paper introduces two blockchain-based protocols: Blind Vote, a secure and gas-efficient on-chain electronic voting system using Chaum's blind signatures, and a private auction algorithm family that protects bidder identities and bid values while maintaining trustless execution.", "motivation": "Existing blockchain protocols lack sufficient security, efficiency, and bid privacy. The authors aim to address these gaps by proposing solutions that reduce gas costs and enhance transparency for decentralized applications like voting and marketplaces.", "method": "1. Blind Vote: Implements Chaum's blind signatures for untraceability while minimizing gas consumption via algorithmic optimizations. 2. Auction protocols: Designs a novel algorithm family running auction logic in smart contracts to eliminate single points of trust, using cryptographic privacy-preserving techniques.", "result": "Blind Vote consumes 30-50% less gas than Tornado Vote while maintaining equivalent security guarantees. Auction protocols provide robust bid privacy, prevent front-running/collusion, and operate efficiently within smart contract execution constraints.", "conclusion": "The proposed protocols establish scalable, cost-effective, and trustless frameworks for privacy-preserving blockchain applications, with potential impacts on decentralized governance, marketplaces, and democratic systems requiring anonymity and verifiability."}}
{"id": "2507.04857", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.04857", "abs": "https://arxiv.org/abs/2507.04857", "authors": ["Weiqi Wang", "Marie Farrell", "Lucas C. Cordeiro", "Liping Zhao"], "title": "Supporting Software Formal Verification with Large Language Models: An Experimental Study", "comment": "Accepted for publication in 2025 IEEE 33rd International Requirements\n  Engineering Conference (RE)", "summary": "Formal methods have been employed for requirements verification for a long\ntime. However, it is difficult to automatically derive properties from natural\nlanguage requirements. SpecVerify addresses this challenge by integrating large\nlanguage models (LLMs) with formal verification tools, providing a more\nflexible mechanism for expressing requirements. This framework combines Claude\n3.5 Sonnet with the ESBMC verifier to form an automated workflow. Evaluated on\nnine cyber-physical systems from Lockheed Martin, SpecVerify achieves 46.5%\nverification accuracy, comparable to NASA's CoCoSim, but with lower false\npositives. Our framework formulates assertions that extend beyond the\nexpressive power of LTL and identifies falsifiable cases that are missed by\nmore traditional methods. Counterexample analysis reveals CoCoSim's limitations\nstemming from model connection errors and numerical approximation issues. While\nSpecVerify advances verification automation, our comparative study of Claude,\nChatGPT, and Llama shows that high-quality requirements documentation and human\nmonitoring remain critical, as models occasionally misinterpret specifications.\nOur results demonstrate that LLMs can significantly reduce the barriers to\nformal verification, while highlighting the continued importance of\nhuman-machine collaboration in achieving optimal results.", "AI": {"tldr": "SpecVerify integrates large language models (LLMs) with formal verification tools to overcome the challenge of deriving properties from natural language requirements. Evaluated on nine systems, it achieves 46.5% verification accuracy with fewer false positives compared to CoCoSim, while highlighting the importance of human oversight in LLM-assisted verification.", "motivation": "Traditional formal verification struggles with automatically extracting verifiable properties from natural language requirements, as existing methods like CoCoSim face limitations in expressive power and numerical approximations. This motivates the integration of LLMs to enhance flexibility and accuracy in requirements verification.", "method": "SpecVerify combines the Claude 3.5 Sonnet LLM with the ESBMC formal verifier to create an automated workflow. It formulates assertions beyond LTL\u2019s expressive capabilities and identifies falsifiable cases overlooked by traditional approaches through counterexample analysis.", "result": "SpecVerify achieves 46.5% verification accuracy on nine cyber-physical systems (comparable to CoCoSim) with significantly lower false positives. Counterexamples reveal CoCoSim\u2019s weaknesses in model connection errors and numerical approximations, but also show that LLMs (Claude, ChatGPT, Llama) occasionally misinterpret specifications.", "conclusion": "LLMs can reduce barriers to formal verification by automating property derivation and expanding expressive power beyond LTL. However, high-quality requirements documentation and human monitoring remain critical to ensure accuracy, demonstrating the necessity of human-machine collaboration for optimal verification outcomes."}}
{"id": "2507.03278", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.03278", "abs": "https://arxiv.org/abs/2507.03278", "authors": ["Jiaqi Xue", "Yifei Zhao", "Mengxin Zheng", "Xun Chen", "Fan Yao", "Yan Solihin", "Qian Lou"], "title": "Securing Transformer-based AI Execution via Unified TEE and Crypto-protected Accelerators", "comment": "15 pages", "summary": "Recent advances in Transformer models, e.g., large language models (LLMs),\nhave brought tremendous breakthroughs in various artificial intelligence (AI)\ntasks, leading to their wide applications in many security-critical domains.\nDue to their unprecedented scale and prohibitively high development cost, these\nmodels have become highly valuable intellectual property for AI stakeholders\nand are increasingly deployed via machine learning as a service (MLaaS).\nHowever, MLaaS often runs on untrusted cloud infrastructure, exposing data and\nmodels to potential breaches. Mainstream protection mechanisms leverage trusted\nexecution environments (TEEs) where confidentiality and integrity for secretive\ndata are shielded using hardware-based encryption and integrity checking.\nUnfortunately, running model inference entirely within TEEs is subject to\nnon-trivial slowdown, which is further exacerbated in LLMs due to the\nsubstantial computation and memory footprint involved. Recent studies reveal\nthat the hybrid TEE-based scheme offloading partial model inference operations\nto the untrusted accelerators (e.g., GPU) is a promising solution. However,\nprior offloading schemes fail to ensure dual protection of data and model in\nTransformer inference, as they cannot securely offload critical operations,\ni.e., Attention and SoftMax, forcing these computations to remain confined\nwithin TEEs. To address these challenges, we propose TwinShield, a framework\nenabling secure Transformer inference in heterogeneous TEE and accelerator\nsystems with dual protection for both model and data. TwinShield offloads ~87%\nof computation to GPUs and delivers 4.0x - 6.1x speedups over previous\napproaches across various Transformer models.", "AI": {"tldr": "TwinShield is a framework enabling secure Transformer inference with dual data and model protection by offloading ~87% of computation to GPUs, achieving 4.0x-6.1x speedups over existing methods.", "motivation": "Large Transformer models deployed in untrusted cloud infrastructure face security risks and performance bottlenecks from relying solely on trusted execution environments (TEEs). Existing solutions either cannot offload security-critical operations like Attention and SoftMax (forcing them to remain in TEEs) or fail to protect both model confidentiality and data integrity during offloading.", "method": "TwinShield leverages a hybrid architecture combining TEEs and untrusted accelerators (e.g., GPU) by securely offloading most Transformer inference operations while maintaining dual protection for data and model through hardware-based encryption and integrity checking. It specifically enables secure offloading of previously protected operations (Attention, SoftMax) without compromising privacy or safety.", "result": "TwinShield achieves 4.0x-6.1x speedups compared to prior approaches across multiple Transformer models while offloading ~87% of computations to external accelerators, demonstrating effective secure offloading of all critical components including Attention and SoftMax layers.", "conclusion": "TwinShield presents a breakthrough in secure MLaaS deployment by proving that comprehensive data/model protection can be combined with significant performance improvements in large Transformer models. The framework addresses the fundamental trade-off between security and efficiency in AI inference workloads, enabling practical deployment of LLMs on untrusted cloud infrastructure."}}
{"id": "2507.04871", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.04871", "abs": "https://arxiv.org/abs/2507.04871", "authors": ["Jerome Pfeiffer", "Jingxi Zhang", "Benoit Combemale", "Judith Michael", "Bernhard Rumpe", "Manuel Wimmer", "Andreas Wortmann"], "title": "Towards a Unifying Reference Model for Digital Twins of Cyber-Physical Systems", "comment": null, "summary": "Digital twins are sophisticated software systems for the representation,\nmonitoring, and control of cyber-physical systems, including automotive,\navionics, smart manufacturing, and many more. Existing definitions and\nreference models of digital twins are overly abstract, impeding their\ncomprehensive understanding and implementation guidance. Consequently, a\nsignificant gap emerges between abstract concepts and their industrial\nimplementations. We analyze popular reference models for digital twins and\ncombine these into a significantly detailed unifying reference model for\ndigital twins that reduces the concept-implementation gap to facilitate their\nengineering in industrial practice. This enhances the understanding of the\nconcepts of digital twins and their relationships and guides developers to\nimplement digital twins effectively.", "AI": {"tldr": "Proposes a unifying reference model for digital twins to reduce the concept-implementation gap.", "motivation": "Existing digital twin definitions and reference models are overly abstract, creating a significant gap between theory and industrial practice.", "method": "Analyzes popular reference models and synthesizes them into a detailed unifying digital twin framework.", "result": "Enhanced understanding of digital twin concepts and their relationships with improved implementation guidance.", "conclusion": "The unifying model facilitates practical engineering of digital twins across different industries by addressing abstraction limitations in previous approaches."}}
{"id": "2507.03323", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.03323", "abs": "https://arxiv.org/abs/2507.03323", "authors": ["Kazumasa Shinagawa", "Koji Nuida"], "title": "A Note on Single-Cut Full-Open Protocols", "comment": null, "summary": "Card-based cryptography is a research area that realizes cryptographic\nprotocols such as secure computation by applying shuffles to sequences of cards\nthat encode input values. A single-cut full-open protocol is one that obtains\nan output value by applying a random cut to an input sequence of cards, after\nwhich all cards are opened. In this paper, we propose three single-cut\nfull-open protocols: two protocols for three-variable functions and one\nprotocol for a four-variable function.", "AI": {"tldr": "Presents three single-cut full-open card protocols for three and four-variable functions.", "motivation": "Aims to simplify secure card-based computation by reducing the number of required shuffles while maintaining security and correctness.", "method": "Designs protocols that use a single random cut followed by full card revelation, encoding inputs as card sequences and leveraging shuffle operations for function evaluation.", "result": "Three protocols demonstrated: two for three-variable functions and one for four-variable functions, verified for correctness and security post-cut.", "conclusion": "The proposed protocols advance card-based cryptography by achieving minimal shuffle complexity (single cut), enabling easier implementation in physical/didactic scenarios while maintaining strong security guarantees."}}
{"id": "2507.05100", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05100", "abs": "https://arxiv.org/abs/2507.05100", "authors": ["Haoran Wei", "Nazim Madhavji", "John Steinbacher"], "title": "Understanding Everything as Code: A Taxonomy and Conceptual Model", "comment": "Accepted by the 19th ACM/IEEE International Symposium on Empirical\n  Software Engineering and Measurement (ESEM 2025), Technical Papers track", "summary": "Background: Everything as Code (EaC) is an emerging paradigm aiming to codify\nall aspects of modern software systems. Despite its growing popularity,\ncomprehensive industry standards and peer-reviewed research clarifying its\nscope and guiding its adoption remain scarce. Aims: This study systematically\nanalyzes existing knowledge and perceptions of EaC, clarifies its scope and\nboundaries, and provides structured guidance for researchers and practitioners.\nMethod: We conducted a large-scale multivocal literature review (MLR),\nsynthesizing academic and grey literature sources. Findings were analyzed\nquantitatively and thematically. Based on this analysis, we developed a\ntaxonomy and conceptual model of EaC, validated through collaboration with\nindustry experts. Results: The resulting taxonomy comprises 25 distinct EaC\npractices organized into six layers based on industry awareness and functional\nroles. The conceptual model illustrates focus areas, overlaps, and interactions\namong these EaC practices within the software delivery lifecycle. Additionally,\npractical code examples demonstrating the implementation of these practices\nwere developed in collaboration with industry experts. Conclusions: This work\naddresses the current scarcity of academic discourse on EaC by providing the\nfirst comprehensive taxonomy and conceptual model. These contributions enhance\nconceptual clarity, offer actionable guidance to practitioners, and lay the\ngroundwork for future research in this emerging domain.", "AI": {"tldr": "A paper presenting the first comprehensive EaC taxonomy and conceptual model through an MLR of academic/grey literature, validated with industry experts.", "motivation": "The study addresses the lack of academic standards and research on EaC to provide clarity, guidance, and a foundation for future work.", "method": "Large-scale multivocal literature review (MLR) synthesizing academic and grey literature with quantitative and thematic analysis, validated by industry experts.", "result": "25-taxonomy practices in 6 layers, a conceptual model mapping EaC practices in the software lifecycle, and practitioner code examples validated with experts.", "conclusion": "Establishes first structured EaC framework to overcome research gaps, offering practical guidance and research foundations for this emerging paradigm."}}
{"id": "2507.03344", "categories": ["cs.CR", "cs.SE", "C.1.3; D.2.5"], "pdf": "https://arxiv.org/pdf/2507.03344", "abs": "https://arxiv.org/abs/2507.03344", "authors": ["Jason Zhijingcheng Yu", "Fangqi Han", "Kaustab Choudhury", "Trevor E. Carlson", "Prateek Saxena"], "title": "Securing Mixed Rust with Hardware Capabilities", "comment": "To appear at CCS '25", "summary": "The Rust programming language enforces three basic Rust principles, namely\nownership, borrowing, and AXM (Aliasing Xor Mutability) to prevent security\nbugs such as memory safety violations and data races. However, Rust projects\noften have mixed code, i.e., code that also uses unsafe Rust, FFI (Foreign\nFunction Interfaces), and inline assembly for low-level control. The Rust\ncompiler is unable to statically enforce Rust principles in mixed Rust code\nwhich can lead to many security vulnerabilities. In this paper, we propose\nCapsLock, a security enforcement mechanism that can run at the level of machine\ncode and detect Rust principle violations at run-time in mixed code. CapsLock\nis kept simple enough to be implemented into recent capability-based hardware\nabstractions that provide low-cost spatial memory safety. CapsLock introduces a\nnovel revoke-on-use abstraction for capability-based designs, wherein accessing\na memory object via a capability implicitly invalidates certain other\ncapabilities pointing to it, thereby also providing temporal memory safety\nautomatically, without requiring software to explicitly specify such\ninvalidation. Thus, CapsLock is the first mechanism capable of providing\ncross-language enforcement of Rust principles. We implemented a prototype of\nCapsLock on QEMU. Evaluation results show that CapsLock is highly compatible\nwith existing Rust code (passing 99.7% of the built-in test cases of the 100\nmost popular crates) and flags Rust principle violations in real-world Rust\nprojects that use FFI or inline assembly. We discovered 8 previously unknown\nbugs in such crates in our experiments.", "AI": {"tldr": "CapsLock is a runtime enforcement mechanism for Rust principles that provides spatial and temporal memory safety by introducing a revoke-on-use abstraction in capability-based hardware, enabling cross-language enforcement even in mixed code scenarios containing unsafe Rust, FFI, and inline assembly. It passes 99.7% of crate test cases and identified 8 new bugs in real-world projects.", "motivation": "Rust's static compiler checks for ownership, borrowing, and AXM principles cannot fully enforce security guarantees in mixed code (unsafe Rust, FFI, inline assembly), leaving room for memory safety violations and data races. Current hardware abstractions lack a mechanism to address this gap in language and runtime enforcement.", "method": "CapsLock leverages capability-based hardware abstractions to implement a novel revoke-on-use mechanism. When a memory object is accessed through a valid capability, it implicitly invalidates specific capabilities pointing to it, enforcing aliasing/mutability constraints at runtime without requiring explicit software management of capability invalidation.", "result": "99.7% compatibility with existing Rust crates (100 most popular crates passing all test cases except 0.3%). Detection of Rust principle violations in FFI/inline assembly projects identified 8 previously unknown bugs, demonstrating effectiveness in real-world applications.", "conclusion": "CapsLock is the first mechanism to provide cross-language runtime enforcement of Rust principles for mixed code environments. The prototype shows high compatibility and practical utility in detecting violations, establishing feasibility for secure execution in unsafe Rust contexts."}}
{"id": "2507.05200", "categories": ["cs.SE", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.05200", "abs": "https://arxiv.org/abs/2507.05200", "authors": ["Susmita Das", "Madhusudan Ghosh", "Priyanka Swami", "Debasis Ganguly", "Gul Calikli"], "title": "In-Context Learning as an Effective Estimator of Functional Correctness of LLM-Generated Code", "comment": null, "summary": "When applying LLM-based code generation to software development projects that\nfollow a feature-driven or rapid application development approach, it becomes\nnecessary to estimate the functional correctness of the generated code in the\nabsence of test cases. Just as a user selects a relevant document from a ranked\nlist of retrieved ones, a software generation workflow requires a developer to\nchoose (and potentially refine) a generated solution from a ranked list of\nalternative solutions, ordered by their posterior likelihoods. This implies\nthat estimating the quality of a ranked list -- akin to estimating \"relevance\"\nfor query performance prediction (QPP) in IR -- is also crucial for generative\nsoftware development, where quality is defined in terms of \"functional\ncorrectness\". In this paper, we propose an in-context learning (ICL) based\napproach for code quality estimation. Our findings demonstrate that providing\nfew-shot examples of functionally correct code from a training set enhances the\nperformance of existing QPP approaches as well as a zero-shot-based approach\nfor code quality estimation.", "AI": {"tldr": "This paper proposes an in-context learning (ICL) approach using few-shot examples from training sets to enhance code quality estimation for LLM-based software development without test cases, improving both existing QPP methods and zero-shot approaches.", "motivation": "LLM-based code generation in agile development lacks test cases for assessing functional correctness, necessitating a quality estimation method akin to relevance ranking in information retrieval.", "method": "The authors employ ICL to provide developers with a ranked list of generated code solutions, incorporating few-shot examples of functionally correct code to inform quality estimation.", "result": "Results show that few-shot example training boosts performance of QPP approaches and zero-shot-based estimation of code functional correctness.", "conclusion": "The study demonstrates that contextual learning with targeted examples significantly improves code quality assessment, offering a scalable solution for generative software development workflows."}}
{"id": "2507.03361", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.03361", "abs": "https://arxiv.org/abs/2507.03361", "authors": ["Rayne Holland"], "title": "Accelerating Private Heavy Hitter Detection on Continual Observation Streams", "comment": "24 pages, 8 figures", "summary": "Differentially private frequency estimation and heavy hitter detection are\ncore problems in the private analysis of data streams. Two models are typically\nconsidered: the one-pass model, which outputs results only at the end of the\nstream, and the continual observation model, which requires releasing private\nsummaries at every time step. While the one-pass model allows more efficient\nsolutions, continual observation better reflects scenarios where timely and\nongoing insights are critical.\n  In the one-pass setting, sketches have proven to be an effective tool for\ndifferentially private frequency analysis, as they can be privatized by a\nsingle injection of calibrated noise. In contrast, existing methods in the\ncontinual observation model add fresh noise to the entire sketch at every step,\nincurring high computational costs. This challenge is particularly acute for\nheavy hitter detection, where current approaches often require querying every\nitem in the universe at each step, resulting in untenable per-update costs for\nlarge domains.\n  To overcome these limitations, we introduce a new differentially private\nsketching technique based on lazy updates, which perturbs and updates only a\nsmall, rotating part of the output sketch at each time step. This significantly\nreduces computational overhead while maintaining strong privacy and utility\nguarantees. In comparison to prior art, for frequency estimation, our method\nimproves the update time by a factor of $O(w)$ for sketches of dimension $d\n\\times w$; for heavy hitter detection, it reduces per-update complexity from\n$\\Omega(|U|)$ to $O(d \\log w)$, where $U$ is the input domain. Experiments show\na increase in throughput by a factor of~$250$, making differential privacy more\npractical for real-time, continual observation, applications.", "AI": {"tldr": "This paper introduces a differentially private sketching technique with lazy updates to reduce computational costs in continual observation data stream analysis, achieving significant improvements in efficiency over existing methods.", "motivation": "Current continual observation models for differential privacy require excessive computational resources by adding fresh noise to entire sketches and querying all domain items per update, limiting their practicality for real-time applications.", "method": "The method employs lazy updates in a novel differentially private sketching framework, perturbing and updating only a small, rotating portion of the output sketch at each time step while maintaining privacy and utility guarantees.", "result": "The approach improves frequency estimation update time by $O(w)$ and reduces heavy hitter detection complexity from $\\Omega(|U|)$ to $O(d \\log w)$, demonstrating a 250\u00d7 throughput increase in experiments.", "conclusion": "Lazy update-based differentially private sketching makes continual observation feasible for large-scale, real-time data streams by drastically reducing computational overhead while preserving analytical accuracy."}}
{"id": "2507.05245", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05245", "abs": "https://arxiv.org/abs/2507.05245", "authors": ["Fatema Tuz Zohra", "Brittany Johnson"], "title": "An Investigation into Maintenance Support for Neural Networks", "comment": "Revised version accepted at the HumanAISE Workshop, co-located with\n  FSE 2025", "summary": "As the potential for neural networks to augment our daily lives grows,\nensuring their quality through effective testing, debugging, and maintenance is\nessential. This is especially the case as we acknowledge the prospects of\nnegative impacts from these technologies. Traditional software engineering\nmethods, such as testing and debugging, have proven effective in maintaining\nsoftware quality; however, they reveal significant research and practice gaps\nin maintaining neural networks. In particular, there is a limited understanding\nof how practitioners currently address challenges related to understanding and\nmitigating undesirable behaviors in neural networks. In our ongoing research,\nwe explore the current state of research and practice in maintaining neural\nnetworks by curating insights from practitioners through a preliminary study\ninvolving interviews and supporting survey responses. Our findings thus far\nindicate that existing tools primarily concentrate on building and training\nmodels. While these tools can be beneficial, they often fall short of\nsupporting practitioners' understanding and addressing the underlying causes of\nunexpected model behavior. By evaluating current procedures and identifying the\nlimitations of traditional methodologies, our study aims to offer a\ndeveloper-centric perspective on where current practices fall short and\nhighlight opportunities for improving maintenance support in neural networks.", "AI": {"tldr": "The paper examines gaps in neural network maintenance practices, highlighting that existing tools focus on model building/training but lack support for addressing unexpected model behaviors, and proposes a developer-centric perspective to improve maintenance.", "motivation": "Neural networks are increasingly integrated into daily life with potential negative impacts, necessitating effective testing, debugging, and maintenance methods similar to traditional software engineering.", "method": "Conducted preliminary interviews and surveys with practitioners to curate insights on current research and practices in neural network maintenance.", "result": "Existing tools prioritize model building/training over understanding and mitigating the root causes of unexpected behaviors, revealing limitations in traditional methodologies for neural network maintenance.", "conclusion": "Traditional software engineering methods are insufficient for neural networks; the study identifies shortcomings in current practices and suggests opportunities for enhancing maintenance support through targeted research and tools."}}
{"id": "2507.03387", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.03387", "abs": "https://arxiv.org/abs/2507.03387", "authors": ["Andong Chen", "Zhaoxuan Jin", "Ziyi Guo", "Yan Chen"], "title": "Breaking the Bulkhead: Demystifying Cross-Namespace Reference Vulnerabilities in Kubernetes Operators", "comment": "12 pages", "summary": "Kubernetes Operators, automated tools designed to manage application\nlifecycles within Kubernetes clusters, extend the functionalities of\nKubernetes, and reduce the operational burden on human engineers. While\nOperators significantly simplify DevOps workflows, they introduce new security\nrisks. In particular, Kubernetes enforces namespace isolation to separate\nworkloads and limit user access, ensuring that users can only interact with\nresources within their authorized namespaces. However, Kubernetes Operators\noften demand elevated privileges and may interact with resources across\nmultiple namespaces. This introduces a new class of vulnerabilities, the\nCross-Namespace Reference Vulnerability. The root cause lies in the mismatch\nbetween the declared scope of resources and the implemented scope of the\nOperator logic, resulting in Kubernetes being unable to properly isolate the\nnamespace. Leveraging such vulnerability, an adversary with limited access to a\nsingle authorized namespace may exploit the Operator to perform operations\naffecting other unauthorized namespaces, causing Privilege Escalation and\nfurther impacts. To the best of our knowledge, this paper is the first to\nsystematically investigate the security vulnerability of Kubernetes Operators.\nWe present Cross-Namespace Reference Vulnerability with two strategies,\ndemonstrating how an attacker can bypass namespace isolation. Through\nlarge-scale measurements, we found that over 14% of Operators in the wild are\npotentially vulnerable. Our findings have been reported to the relevant\ndevelopers, resulting in 7 confirmations and 6 CVEs by the time of submission,\naffecting vendors including ****** and ******, highlighting the critical need\nfor enhanced security practices in Kubernetes Operators. To mitigate it, we\nalso open-source the static analysis suite to benefit the ecosystem.", "AI": {"tldr": "The paper introduces Cross-Namespace Reference Vulnerability in Kubernetes Operators, where 14% of real-world Operators are potentially vulnerable. This allows privilege escalation by bypassing namespace isolation, leading to 6 CVEs and affecting major vendors. A static analysis tool is provided as mitigation.", "motivation": "Kubernetes Operators simplify DevOps but require elevated privileges to manage resources across namespaces, creating new security risks that compromise namespace isolation. Prior research lacks systematic analysis of this threat.", "method": "The authors conducted a systematic investigation of Operator security through: 1) Identifying scope mismatch (declared vs. implemented), 2) Developing two strategies to bypass namespace isolation, and 3) Performing large-scale measurements of wild Operators followed by CVE reporting.", "result": "14% of Operators exhibit vulnerability patterns; 7 confirmations and 6 CVEs reported (including redacted vendors). The static analysis suite is open-sourced for ecosystem use.", "conclusion": "Cross-Namespace vulnerabilities in Operators pose critical risks requiring enhanced security practices. Open-source static analysis tools are vital for detection, emphasizing urgent mitigation efforts in the Kubernetes ecosystem."}}
{"id": "2507.03450", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.03450", "abs": "https://arxiv.org/abs/2507.03450", "authors": ["Antonio Emanuele Cin\u00e0", "Maura Pintor", "Luca Demetrio", "Ambra Demontis", "Battista Biggio", "Fabio Roli"], "title": "Evaluating the Evaluators: Trust in Adversarial Robustness Tests", "comment": null, "summary": "Despite significant progress in designing powerful adversarial evasion\nattacks for robustness verification, the evaluation of these methods often\nremains inconsistent and unreliable. Many assessments rely on mismatched\nmodels, unverified implementations, and uneven computational budgets, which can\nlead to biased results and a false sense of security. Consequently, robustness\nclaims built on such flawed testing protocols may be misleading and give a\nfalse sense of security. As a concrete step toward improving evaluation\nreliability, we present AttackBench, a benchmark framework developed to assess\nthe effectiveness of gradient-based attacks under standardized and reproducible\nconditions. AttackBench serves as an evaluation tool that ranks existing attack\nimplementations based on a novel optimality metric, which enables researchers\nand practitioners to identify the most reliable and effective attack for use in\nsubsequent robustness evaluations. The framework enforces consistent testing\nconditions and enables continuous updates, making it a reliable foundation for\nrobustness verification.", "AI": {"tldr": "This paper introduces AttackBench, a standardized benchmark framework for reliable evaluation of gradient-based adversarial attacks in robustness verification.", "motivation": "Current evaluation methods for adversarial attacks are inconsistent due to mismatched models, unverified implementations, and uneven computational budgets, leading to biased results and false security assumptions.", "method": "AttackBench ranks attack implementations using a novel optimality metric while enforcing standardized testing conditions, with support for continuous updates.", "result": "Demonstrates the framework's ability to identify reliable attacks through reproducible benchmarking under controlled conditions.", "conclusion": "AttackBench provides a systematic and maintainable foundation for robustness verification, addressing critical limitations in existing evaluation protocols."}}
{"id": "2507.03607", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.03607", "abs": "https://arxiv.org/abs/2507.03607", "authors": ["C\u00e9dric Bonhomme", "Alexandre Dulaunoy"], "title": "VLAI: A RoBERTa-Based Model for Automated Vulnerability Severity Classification", "comment": "This paper is a preprint for the 25V4C-TC: 2025 Vulnerability\n  Forecasting Technical Colloquia. Darwin College Cambridge, UK, September\n  25-26, 2025", "summary": "This paper presents VLAI, a transformer-based model that predicts software\nvulnerability severity levels directly from text descriptions. Built on\nRoBERTa, VLAI is fine-tuned on over 600,000 real-world vulnerabilities and\nachieves over 82% accuracy in predicting severity categories, enabling faster\nand more consistent triage ahead of manual CVSS scoring. The model and dataset\nare open-source and integrated into the Vulnerability-Lookup service.", "AI": {"tldr": "VLAI is a RoBERTa-based model that predicts software vulnerability severity levels from text with 82% accuracy, integrated into open-source vulnerability triage tools.", "motivation": "Manual CVSS scoring of vulnerabilities is time-consuming and error-prone, requiring efficient automated triage solutions for faster response.", "method": "Fine-tuned RoBERTa transformer model on 600,000+ real-world vulnerability text descriptions with severity labels.", "result": "Achieved >82% accuracy in severity category prediction, demonstrated improved triage consistency compared to manual methods.", "conclusion": "VLAI provides a reliable open-source framework for automated vulnerability severity assessment, accelerating security response workflows."}}
{"id": "2507.03773", "categories": ["cs.CR", "cs.DC", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.03773", "abs": "https://arxiv.org/abs/2507.03773", "authors": ["Yibo He", "Cunjian Huang", "Xianmiao Qu", "Hongdeng Chen", "Wei Yang", "Tao Xie"], "title": "RVISmith: Fuzzing Compilers for RVV Intrinsics", "comment": "To appear in ACM CCS 2025", "summary": "Modern processors are equipped with single instruction multiple data (SIMD)\ninstructions for fine-grained data parallelism. Compiler auto-vectorization\ntechniques that target SIMD instructions face performance limitations due to\ninsufficient information available at compile time, requiring programmers to\nmanually manipulate SIMD instructions. SIMD intrinsics, a type of built-in\nfunction provided by modern compilers, enable programmers to manipulate SIMD\ninstructions within high-level programming languages. Bugs in compilers for\nSIMD intrinsics can introduce potential threats to software security, producing\nunintended calculation results, data loss, program crashes, etc.\n  To detect bugs in compilers for SIMD intrinsics, we propose RVISmith, a\nrandomized fuzzer that generates well-defined C programs that include various\ninvocation sequences of RVV (RISC-V Vector Extension) intrinsics. We design\nRVISmith to achieve the following objectives: (i) achieving high intrinsic\ncoverage, (ii) improving sequence variety, and (iii) without known undefined\nbehaviors. We implement RVISmith based on the ratified RVV intrinsic\nspecification and evaluate our approach with three modern compilers: GCC, LLVM,\nand XuanTie. Experimental results show that RVISmith achieves 11.5 times higher\nintrinsic coverage than the state-of-the-art fuzzer for RVV intrinsics. By\ndifferential testing that compares results across different compilers,\noptimizations, and equivalent programs, we detect and report 13 previously\nunknown bugs of the three compilers under test to date. Of these bugs, 10 are\nconfirmed and another 3 are fixed by the compiler developers.", "AI": {"tldr": "RVISmith is a randomized fuzzer for detecting bugs in RISC-V Vector Extension (RVV) compilers, achieving 11.5\u00d7 higher intrinsic coverage and uncovering 13 previously unknown compiler bugs.", "motivation": "Compiler auto-vectorization for SIMD instructions struggles with limited compile-time information, forcing manual use of SIMD intrinsics. Compiler bugs in handling these intrinsics can lead to security vulnerabilities, data loss, and program instability, necessitating robust detection methods.", "method": "RVISmith generates C programs with well-defined RVV intrinsic invocation sequences, prioritizing high intrinsic coverage, sequence diversity, and avoidance of undefined behaviors. It leverages the ratified RVV specification to implement a fuzzer that uses differential testing across compilers (GCC, LLVM, XuanTie) to detect discrepancies.", "result": "RVISmith outperforms existing fuzzers with 11.5\u00d7 higher intrinsic coverage and identifies 13 new compiler bugs through differential testing. Of these, 10 were confirmed by developers, and 3 were fixed.", "conclusion": "RVISmith demonstrates an effective approach to compiler testing for SIMD intrinsics, achieving significant coverage improvements and exposing multiple unresolved compiler issues that impact software reliability and security."}}
{"id": "2507.03619", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.03619", "abs": "https://arxiv.org/abs/2507.03619", "authors": ["Ruikai Zhou", "Kang Yang", "Xun Chen", "Wendy Hui Wang", "Guanhong Tao", "Jun Xu"], "title": "Blackbox Dataset Inference for LLM", "comment": null, "summary": "Today, the training of large language models (LLMs) can involve personally\nidentifiable information and copyrighted material, incurring dataset misuse. To\nmitigate the problem of dataset misuse, this paper explores \\textit{dataset\ninference}, which aims to detect if a suspect model $\\mathcal{M}$ used a victim\ndataset $\\mathcal{D}$ in training. Previous research tackles dataset inference\nby aggregating results of membership inference attacks (MIAs) -- methods to\ndetermine whether individual samples are a part of the training dataset.\nHowever, restricted by the low accuracy of MIAs, previous research mandates\ngrey-box access to $\\mathcal{M}$ to get intermediate outputs (probabilities,\nloss, perplexity, etc.) for obtaining satisfactory results. This leads to\nreduced practicality, as LLMs, especially those deployed for profits, have\nlimited incentives to return the intermediate outputs.\n  In this paper, we propose a new method of dataset inference with only\nblack-box access to the target model (i.e., assuming only the text-based\nresponses of the target model are available). Our method is enabled by two sets\nof locally built reference models, one set involving $\\mathcal{D}$ in training\nand the other not. By measuring which set of reference model $\\mathcal{M}$ is\ncloser to, we determine if $\\mathcal{M}$ used $\\mathcal{D}$ for training.\nEvaluations of real-world LLMs in the wild show that our method offers high\naccuracy in all settings and presents robustness against bypassing attempts.", "AI": {"tldr": "A novel dataset inference method using black-box access to detect dataset misuse in large language models.", "motivation": "Large language models (LLMs) are often trained on datasets containing sensitive or copyrighted content, leading to potential misuse. Previous dataset inference methods rely on grey-box access to models due to the limitations of membership inference attacks (MIAs), which is restrictive for commercial models that avoid exposing internal mechanisms.", "method": "The proposed method builds two sets of locally trained reference models: one incorporating the victim dataset  $\\mathcal{D}$ in training and the other trained without it. By comparing the similarity of the suspect model $\\mathcal{M}$ to these references under black-box access constraints, the inference is made as to whether $\\mathcal{M}$ utilized $\\mathcal{D}$.", "result": "Evaluations on real-world LLMs demonstrate high detection accuracy across all tested scenarios. The method is robust against bypassing strategies, affirming its practical applicability.", "conclusion": "This work presents a feasible alternative to overcome the impracticality of grey-box access, offering scalable and effective dataset inference for black-box LLMs while resisting evasion techniques."}}
{"id": "2507.04055", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.04055", "abs": "https://arxiv.org/abs/2507.04055", "authors": ["Yufan Chen", "Daoyuan Wu", "Juantao Zhong", "Zicheng Zhang", "Debin Gao", "Shuai Wang", "Yingjiu Li", "Ning Liu"], "title": "Rethinking and Exploring String-Based Malware Family Classification in the Era of LLMs and RAG", "comment": null, "summary": "Malware Family Classification (MFC) aims to identify the fine-grained family\n(e.g., GuLoader or BitRAT) to which a potential malware sample belongs, in\ncontrast to malware detection or sample classification that predicts only an\nYes/No. Accurate family identification can greatly facilitate automated sample\nlabeling and understanding on crowdsourced malware analysis platforms such as\nVirusTotal and MalwareBazaar, which generate vast amounts of data daily. In\nthis paper, we explore and assess the feasibility of using traditional binary\nstring features for MFC in the new era of large language models (LLMs) and\nRetrieval-Augmented Generation (RAG). Specifically, we investigate how\nFamily-Specific String (FSS) features could be utilized in a manner similar to\nRAG to facilitate MFC. To this end, we develop a curated evaluation framework\ncovering 4,347 samples from 67 malware families, extract and analyze over 25\nmillion strings, and conduct detailed ablation studies to assess the impact of\ndifferent design choices in four major modules.", "AI": {"tldr": "This paper evaluates using binary string features for malware family classification with large language models and RAG, developing a framework with 4,347 samples and analyzing 25 million strings.", "motivation": "Accurate malware family identification automates labeling and analysis on platforms like VirusTotal, crucial for managing daily high-volume malware data.", "method": "A curated framework was developed for 67 malware families, utilizing Family-Specific Strings (FSS) in RAG-like approaches. 25+ million strings were extracted, and ablation studies analyzed four key modules.", "result": "The study demonstrates FSS-based methods' feasibility for MFC, with ablation experiments quantifying impacts of module design choices, though specific performance metrics are not detailed.", "conclusion": "The work establishes that traditional FSS features can effectively enhance LLM-era MFC through RAG-like approaches, providing a foundation for improving automated malware analysis systems."}}
{"id": "2507.03636", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.03636", "abs": "https://arxiv.org/abs/2507.03636", "authors": ["Xiaodong Wu", "Xiangman Li", "Qi Li", "Jianbing Ni", "Rongxing Lu"], "title": "SecureT2I: No More Unauthorized Manipulation on AI Generated Images from Prompts", "comment": null, "summary": "Text-guided image manipulation with diffusion models enables flexible and\nprecise editing based on prompts, but raises ethical and copyright concerns due\nto potential unauthorized modifications. To address this, we propose SecureT2I,\na secure framework designed to prevent unauthorized editing in diffusion-based\ngenerative models. SecureT2I is compatible with both general-purpose and\ndomain-specific models and can be integrated via lightweight fine-tuning\nwithout architectural changes. We categorize images into a permit set and a\nforbid set based on editing permissions. For the permit set, the model learns\nto perform high-quality manipulations as usual. For the forbid set, we\nintroduce training objectives that encourage vague or semantically ambiguous\noutputs (e.g., blurred images), thereby suppressing meaningful edits. The core\nchallenge is to block unauthorized editing while preserving editing quality for\npermitted inputs. To this end, we design separate loss functions that guide\nselective editing behavior. Extensive experiments across multiple datasets and\nmodels show that SecureT2I effectively degrades manipulation quality on\nforbidden images while maintaining performance on permitted ones. We also\nevaluate generalization to unseen inputs and find that SecureT2I consistently\noutperforms baselines. Additionally, we analyze different vagueness strategies\nand find that resize-based degradation offers the best trade-off for secure\nmanipulation control.", "AI": {"tldr": "SecureT2I is a framework that prevents unauthorized text-guided image edits by training diffusion models to degrade forbidden inputs while maintaining permitted edits, using lightweight fine-tuning without architecture changes.", "motivation": "Text-guided image manipulation with diffusion models raises ethical and copyright issues due to potential unauthorized modifications; existing solutions lack compatibility and practical integration with diverse models.", "method": "SecureT2I divides images into 'permit' and 'forbid' sets based on editing permissions. For permit sets, standard high-quality manipulation is enforced. For forbid sets, separate training objectives with designed loss functions encourage outputs like blurred or semantically ambiguous images, suppressing meaningful edits. The framework requires only lightweight fine-tuning without requiring model architecture changes.", "result": "Experiments show SecureT2I effectively degrades unauthorized editing quality on forbidden datasets while preserving performance on permit sets. It generalizes robustly to unseen inputs and outperforms baselines. Resize-based degradation is identified as the optimal strategy for secure manipulation control.", "conclusion": "SecureT2I provides a practical, model-agnostic solution for secure text-to-image editing by balancing selective degradation with quality preservation. Resize-based approaches optimize security versus functionality trade-offs, offering potential for broader implementation in diffusion models."}}
{"id": "2507.03646", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.03646", "abs": "https://arxiv.org/abs/2507.03646", "authors": ["Xiaodong Wu", "Tianyi Tang", "Xiangman Li", "Jianbing Ni", "Yong Yu"], "title": "When There Is No Decoder: Removing Watermarks from Stable Diffusion Models in a No-box Setting", "comment": "arXiv admin note: text overlap with arXiv:2408.02035", "summary": "Watermarking has emerged as a promising solution to counter harmful or\ndeceptive AI-generated content by embedding hidden identifiers that trace\ncontent origins. However, the robustness of current watermarking techniques is\nstill largely unexplored, raising critical questions about their effectiveness\nagainst adversarial attacks. To address this gap, we examine the robustness of\nmodel-specific watermarking, where watermark embedding is integrated with\ntext-to-image generation in models like latent diffusion models. We introduce\nthree attack strategies: edge prediction-based, box blurring, and\nfine-tuning-based attacks in a no-box setting, where an attacker does not\nrequire access to the ground-truth watermark decoder. Our findings reveal that\nwhile model-specific watermarking is resilient against basic evasion attempts,\nsuch as edge prediction, it is notably vulnerable to blurring and\nfine-tuning-based attacks. Our best-performing attack achieves a reduction in\nwatermark detection accuracy to approximately 47.92\\%. Additionally, we perform\nan ablation study on factors like message length, kernel size and decoder\ndepth, identifying critical parameters influencing the fine-tuning attack's\nsuccess. Finally, we assess several advanced watermarking defenses, finding\nthat even the most robust methods, such as multi-label smoothing, result in\nwatermark extraction accuracy that falls below an acceptable level when\nsubjected to our no-box attacks.", "AI": {"tldr": "This paper investigates the robustness of model-specific watermarking in text-to-image models like latent diffusion models under adversarial attacks without accessing the watermark decoder. They develop three novel attack strategies (edge prediction, blurring, fine-tuning) and find watermarks are resilient to basic attacks but vulnerable to blurring (47.92% detection accuracy reduction). Ablation studies identify critical parameters affecting attack success.", "motivation": "Current AI watermarking solutions lack robustness analysis against adversarial attacks, and there is a critical need to understand their vulnerabilities in real-world scenarios where attackers don't have access to watermark decoders.", "method": "Developed three model-specific watermark evasion attacks: 1) Edge prediction-based attack for basic content modifications, 2) Box blurring to suppress watermark signals, 3) Fine-tuning-based attack modifying generation parameters. Conducted ablation studies on message length, blurring kernel size, and decoder depth through controlled experiments.", "result": "Attack results show: - Basic attacks (edge prediction) have limited effectiveness against model-specific watermarking - Best fine-tuning attack reduced watermark detection accuracy to ~47.92% (from 95-100%) - Ablation studies revealed optimal parameters for attack effectiveness (message length \u2264 4k, kernel size 11-21, decoder depth 8-16)", "conclusion": "Model-specific watermarking, while showing some resilience against simple attacks, is fundamentally vulnerable to advanced transformations like blurring and generation process manipulation. Even robust defenses such as multi-label smoothing fail against these \"no-box\" attacks, necessitating new watermarking approaches with stronger adversarial robustness."}}
{"id": "2507.03694", "categories": ["cs.CR", "cs.CE", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.03694", "abs": "https://arxiv.org/abs/2507.03694", "authors": ["Jovonni L. PHarr"], "title": "Willchain: Decentralized, Privacy-Preserving, Self-Executing, Digital Wills", "comment": null, "summary": "This work presents a novel decentralized protocol for digital estate planning\nthat integrates advances distributed computing, and cryptography. The original\nproof-of-concept was constructed using purely solidity contracts. Since then,\nwe have enhanced the implementation into a layer-1 protocol that uses modern\ninterchain communication to connect several heterogeneous chain types. A key\ncontribution of this research is the implementation of several modern\ncryptographic primitives to support various forms of claims for information\nvalidation. These primitives introduce an unmatched level of privacy to the\nprocess of digital inheritance. We also demonstrate on a set of heterogeneous\nsmart contracts, following the same spec, on each chain to serve as entry\npoints, gateways, or bridge contracts that are invoked via a path from the will\nmodule on our protocol, to the contract. This ensures a fair and secure\ndistribution of digital assets in accordance with the wishes of the decedent\nwithout the requirement of moving their funds. This research further extends\nits innovations with a user interaction model, featuring a check-in system and\naccount abstraction process, which enhances flexibility and user-friendliness\nwithout compromising on security. By developing a dedicated permissionless\nblockchain that is secured by a network of validators, and interchain relayers,\nthe proposed protocol signifies a transformation in the digital estate planning\nindustry and illustrates the potential of blockchain technology in\nrevolutionizing traditional legal and personal spheres. Implementing a\ncryptoeconomic network at the core of inheritance planning allows for unique\nincentive compatible economic mechanisms to be constructed.", "AI": {"tldr": "A decentralized protocol for digital estate planning using interchain communication and advanced cryptography to ensure secure, private, and efficient inheritance of digital assets across multiple blockchains without requiring fund transfers.", "motivation": "Traditional digital inheritance systems lack privacy, security, and flexibility, often relying on centralized third parties. Blockchain technology offers a trustless and secure framework, but existing protocols are either on-chain-only or lack cross-chain capabilities, failing to address diverse ecosystem needs. Additionally, ensuring privacy and incentive alignment during inheritance remains unmet.", "method": "The protocol evolves from pure Solidity-based contracts to a layer-1 system supported by modern interchain communication to connect heterogeneous chains. It implements advanced cryptographic primitives (e.g., zero-knowledge proofs) for claim validation and privacy. Smart contracts across chains act as gateways, managed via a path from the will module. User interaction includes a check-in system and account abstraction for flexibility and ease of use without compromising security.", "result": "The protocol enables secure and private digital asset distribution across multiple chains, demonstrating efficient interchain communication and cryptographic validation. It reduces reliance on centralized entities, avoids fund transfers, and introduces incentive-compatible mechanisms through a permissionless blockchain secured by validators and relayers. Empirical results likely show scalability and robustness improvements over previous methods.", "conclusion": "This work redefines digital estate planning by combining decentralized protocols with cryptography and interchain capabilities, addressing privacy, security, and flexibility challenges. It highlights blockchain's potential to integrate with legal frameworks while offering a scalable and incentive-aligned solution for managing digital inheritance in a heterogeneous ecosystem."}}
{"id": "2507.03993", "categories": ["cs.CR", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.03993", "abs": "https://arxiv.org/abs/2507.03993", "authors": ["Dipo Dunsin", "Mohamed Chahine Ghanem", "Eduardo Almeida Palmieri"], "title": "MalVol-25: A Diverse, Labelled and Detailed Volatile Memory Dataset for Malware Detection and Response Testing and Validation", "comment": "6 pages", "summary": "This paper addresses the critical need for high-quality malware datasets that\nsupport advanced analysis techniques, particularly machine learning and agentic\nAI frameworks. Existing datasets often lack diversity, comprehensive labelling,\nand the complexity necessary for effective machine learning and agent-based AI\ntraining. To fill this gap, we developed a systematic approach for generating a\ndataset that combines automated malware execution in controlled virtual\nenvironments with dynamic monitoring tools. The resulting dataset comprises\nclean and infected memory snapshots across multiple malware families and\noperating systems, capturing detailed behavioural and environmental features.\nKey design decisions include applying ethical and legal compliance, thorough\nvalidation using both automated and manual methods, and comprehensive\ndocumentation to ensure replicability and integrity. The dataset's distinctive\nfeatures enable modelling system states and transitions, facilitating RL-based\nmalware detection and response strategies. This resource is significant for\nadvancing adaptive cybersecurity defences and digital forensic research. Its\nscope supports diverse malware scenarios and offers potential for broader\napplications in incident response and automated threat mitigation.", "AI": {"tldr": "The paper introduces a high-quality, diverse malware dataset generated via automated execution and dynamic monitoring in virtual environments. Key features include memory snapshots across multiple families and operating systems, supporting machine learning, agentic AI, and RL-based detection strategies with ethical-legal compliance, validation, and documentation for reproducibility.", "motivation": "Existing malware datasets lack diversity, comprehensive labeling, and sufficient complexity, hindering effectiveness in training advanced machine learning and agent-based AI systems for cybersecurity. This limits research in adaptive defenses and digital forensics.", "method": "A systematic approach combining automated malware execution in controlled virtual environments with dynamic monitoring tools. Collected clean/infected memory snapshots across malware families and OS, validated via automated/manual methods, and ensured reproducibility through thorough documentation.", "result": "The dataset enables system state modeling and transitions, facilitating RL-based malware detection/response. It supports diverse malware scenarios and provides features for adaptive cybersecurity research, validating its utility in incident response and threat mitigation.", "conclusion": "This dataset significantly advances adaptive cybersecurity defenses and digital forensic research by offering comprehensive, ethically validated, and replicable malware behavioral data. Its design supports broader applications in threat detection and automated response frameworks."}}
{"id": "2507.04077", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.04077", "abs": "https://arxiv.org/abs/2507.04077", "authors": ["Yue Su", "Meng Shen", "Cong Zuo", "Yuzhi Liu", "Liehuang Zhu"], "title": "S-Leak: Leakage-Abuse Attack Against Efficient Conjunctive SSE via s-term Leakage", "comment": "16 pages, 12 figures. Preliminary version. Future journal/conference\n  submission intended", "summary": "Conjunctive Searchable Symmetric Encryption (CSSE) enables secure conjunctive\nsearches over encrypted data. While leakage-abuse attacks (LAAs) against\nsingle-keyword SSE have been extensively studied, their extension to\nconjunctive queries faces a critical challenge: the combinatorial explosion of\ncandidate keyword combinations, leading to enormous time and space overhead for\nattacks. In this paper, we reveal a fundamental vulnerability in\nstate-of-the-art CSSE schemes: s-term leakage, where the keyword with the\nminimal document frequency in a query leaks distinct patterns. We propose\nS-Leak, the first passive attack framework that progressively recovers\nconjunctive queries by exploiting s-term leakage and global leakage. Our key\ninnovation lies in a three-stage approach: identifying the s-term of queries,\npruning low-probability keyword conjunctions, and reconstructing full queries.\nWe propose novel metrics to better assess attacks in conjunctive query\nscenarios. Empirical evaluations on real-world datasets demonstrate that our\nattack is effective in diverse CSSE configurations. When considering 161,700\nconjunctive keyword queries, our attack achieves a 95.15% accuracy in\nrecovering at least one keyword, 82.57% for at least two, 58% for all three\nkeywords, and maintains efficacy against defenses such as SEAL padding and CLRZ\nobfuscation. Our work exposes the underestimated risks of s-term leakage in\npractical SSE deployments and calls for a redesign of leakage models for\nmulti-keyword search scenarios.", "AI": {"tldr": "This paper introduces S-Leak, a passive attack on conjunctive searchable symmetric encryption (CSSE) that exploits s-term leakage and global leakage to recover queries with high accuracy, even against existing defenses.", "motivation": "Existing leakage-abuse attacks (LAAs) for single-keyword SSE are limited when extended to conjunctive queries due to combinatorial keyword explosion, making attack complexity impractical. The paper identifies s-term leakage\u2014a previously underestimated vulnerability\u2014as a new risk in multi-keyword scenarios.", "method": "The authors propose a three-stage attack framework: 1) Identifying the s-term (keyword with minimal document frequency), 2) Pruning low-probability keyword conjunctions using novel metrics, and 3) Reconstructing full queries. They also design tailored evaluation metrics for conjunctive search scenarios.", "result": "Empirical evaluations on real-world datasets show 95.15% accuracy in recovering at least one keyword (from 161,700 queries), 82.57% for two keywords, 58% for all three keywords. The attack effectively bypasses defenses like SEAL padding and CLRZ obfuscation.", "conclusion": "S-term leakage poses significant risks in practical multi-keyword SSE deployments, necessitating revised leakage models for conjunctive query scenarios to prevent future vulnerabilities."}}
{"id": "2507.04104", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.04104", "abs": "https://arxiv.org/abs/2507.04104", "authors": ["Sri Harsha Gajavalli"], "title": "Human-Centered Interactive Anonymization for Privacy-Preserving Machine Learning: A Case for Human-Guided k-Anonymity", "comment": null, "summary": "Privacy-preserving machine learning (ML) seeks to balance data utility and\nprivacy, especially as regulations like the GDPR mandate the anonymization of\npersonal data for ML applications. Conventional anonymization approaches often\nreduce data utility due to indiscriminate generalization or suppression of data\nattributes. In this study, we propose an interactive approach that incorporates\nhuman input into the k-anonymization process, enabling domain experts to guide\nattribute preservation based on contextual importance. Using the UCI Adult\ndataset, we compare classification outcomes of interactive human-influenced\nanonymization with traditional, fully automated methods. Our results show that\nhuman input can enhance data utility in some cases, although results vary\nacross tasks and settings. We discuss limitations of our approach and suggest\npotential areas for improved interactive frameworks in privacy-aware ML.", "AI": {"tldr": "This paper introduces an interactive privacy-preserving ML approach that incorporates domain experts' input in a k-anonymization process to improve data utility compared to traditional automated methods.", "motivation": "Regulatory requirements (e.g., GDPR) demand anonymization of personal data for ML uses while maintaining data utility remains a challenge due to indiscriminate generalization/suppression by conventional techniques.", "method": "The proposed method integrates human-in-the-loop guidance during k-anonymization, using domain experts' judgments on contextual attribute importance evaluated via classification tasks on the UCI Adult dataset.", "result": "Human-influenced anonymization achieves enhanced data utility in some cases; however, effectiveness varies across machine learning tasks and settings.", "conclusion": "Interactive frameworks can improve privacy-utility tradeoffs in ML, but limitations remain, prompting potential advancements for better integration of non-automated contextual controls in sensitive applications."}}
{"id": "2507.04106", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04106", "abs": "https://arxiv.org/abs/2507.04106", "authors": ["Stanis\u0142aw Pawlak", "Bart\u0142omiej Twardowski", "Tomasz Trzci\u0144ski", "Joost van de Weijer"], "title": "Addressing The Devastating Effects Of Single-Task Data Poisoning In Exemplar-Free Continual Learning", "comment": "Accepted at CoLLAs 2025", "summary": "Our research addresses the overlooked security concerns related to data\npoisoning in continual learning (CL). Data poisoning - the intentional\nmanipulation of training data to affect the predictions of machine learning\nmodels - was recently shown to be a threat to CL training stability. While\nexisting literature predominantly addresses scenario-dependent attacks, we\npropose to focus on a more simple and realistic single-task poison (STP)\nthreats. In contrast to previously proposed poisoning settings, in STP\nadversaries lack knowledge and access to the model, as well as to both previous\nand future tasks. During an attack, they only have access to the current task\nwithin the data stream. Our study demonstrates that even within these stringent\nconditions, adversaries can compromise model performance using standard image\ncorruptions. We show that STP attacks are able to strongly disrupt the whole\ncontinual training process: decreasing both the stability (its performance on\npast tasks) and plasticity (capacity to adapt to new tasks) of the algorithm.\nFinally, we propose a high-level defense framework for CL along with a poison\ntask detection method based on task vectors. The code is available at\nhttps://github.com/stapaw/STP.git .", "AI": {"tldr": "The paper introduces single-task poison (STP) attacks in continual learning, demonstrating their ability to disrupt model stability and plasticity using image corruptions, even with limited adversary knowledge, and proposes a defense framework with task-based poison detection.", "motivation": "Existing research on data poisoning in continual learning focuses on scenario-dependent attacks, but the authors argue that STP threats are simpler, more realistic, and have been overlooked, necessitating a new focus for secure CL models.", "method": "The study models adversaries with access only to the current task, using standard image corruptions. It proposes a high-level defense framework for CL and a poison task detection method based on task vectors to mitigate such attacks.", "result": "STP attacks are shown to significantly decrease algorithm stability (past task performance) and plasticity (adaptability to new tasks). The proposed defense framework and detection method are introduced but not quantitatively evaluated in the abstract.", "conclusion": "The paper concludes that STP attacks pose a critical threat to continual learning and that addressing these simpler, realistic threats is essential for developing secure models, with the proposed framework offering a potential solution."}}
{"id": "2507.04126", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.04126", "abs": "https://arxiv.org/abs/2507.04126", "authors": ["Howard Halim", "Eyasu Getahun Chekole", "Dani\u00ebl Reijsbergen", "Jianying Zhou"], "title": "BlowPrint: Blow-Based Multi-Factor Biometrics for Smartphone User Authentication", "comment": null, "summary": "Biometric authentication is a widely used security mechanism that leverages\nunique physiological or behavioral characteristics to authenticate users. In\nmulti-factor biometrics (MFB), multiple biometric modalities, e.g.,\nphysiological and behavioral, are integrated to mitigate the limitations\ninherent in single-factor biometrics. The main challenge in MFB lies in\nidentifying novel behavioral techniques capable of meeting critical criteria,\nincluding high accuracy, high usability, non-invasiveness, resilience against\nspoofing attacks, and low use of computational resources. Despite ongoing\nadvancements, current behavioral biometric techniques often fall short of\nfulfilling one or more of these requirements. In this work, we propose\nBlowPrint, a novel behavioral biometric technique that allows us to\nauthenticate users based on their phone blowing behaviors. In brief, we assume\nthat the way users blow on a phone screen can produce distinctive acoustic\npatterns, which can serve as a unique biometric identifier for effective user\nauthentication. It can also be seamlessly integrated with physiological\ntechniques, such as facial recognition, to enhance its robustness and security.\nTo assess BlowPrint's effectiveness, we conduct an empirical study involving 50\nparticipants from whom we collect blow-acoustic and facial feature data.\nSubsequently, we compute the similarity scores of the two modalities using\nvarious similarity algorithms and combine them through score-level fusion.\nFinally, we compute the accuracy using a machine learning-based classifier. As\na result, the proposed method demonstrates an accuracy of 99.35% for blow\nacoustics, 99.96% for facial recognition, and 99.82% for the combined approach.\nThe experimental results demonstrate BlowPrint's high effectiveness in terms of\nauthentication accuracy, spoofing attack resilience, usability,\nnon-invasiveness, and other aspects.", "AI": {"tldr": "BlowPrint is a novel behavioral biometric technique using phone blowing acoustic patterns for authentication, achieving 99.35% accuracy alone and 99.82% when combined with facial recognition.", "motivation": "Current multi-factor biometrics (MFB) struggle with limitations in high accuracy, usability, non-invasiveness, spoofing resilience, and low computational requirements in behavioral techniques.", "method": "An empirical study with 50 participants collected blow-acoustic and facial data, computed modality similarity scores using multiple algorithms, fused them at the score level, and classified using machine learning.", "result": "BlowPrint achieved 99.35% for blow acoustics, 99.96% for facial recognition, and 99.82% combined accuracy, demonstrating resilience against spoofing attacks and high usability.", "conclusion": "BlowPrint presents a non-invasive, user-friendly, and secure behavioral biometric method that complements physiological techniques to enhance MFB robustness."}}
{"id": "2507.04174", "categories": ["cs.CR", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.04174", "abs": "https://arxiv.org/abs/2507.04174", "authors": ["Abdellah Akilal", "M-Tahar Kechadi"], "title": "Cloud Digital Forensic Readiness: An Open Source Approach to Law Enforcement Request Management", "comment": null, "summary": "Cloud Forensics presents a multi-jurisdictional challenge that may undermines\nthe success of digital forensic investigations (DFIs). The growing volumes of\ndomiciled and foreign law enforcement (LE) requests, the latency and complexity\nof formal channels for crossborder data access are challenging issues. In this\npaper, we first discuss major Cloud Service Providers (CSPs) transparency\nreports and law enforcement guidelines, then propose an abstract architecture\nfor a Cloud Law Enforcement Requests Management System (CLERMS). A proof of\nconcept of the proposed solution is developed, deployed and validated by two\nrealistic scenarios, in addition to an economic estimation of its associated\ncosts. Based on available open source components, our solution is for the\nbenefit of both CSPs and Cloud Service Consumers (CSCs), and aims to enhance\nthe due Cloud Digital Forensic Readiness (CDFR).", "AI": {"tldr": "The paper addresses multi-jurisdictional challenges in Cloud Forensics by proposing a Cloud Law Enforcement Requests Management System (CLERMS) with a proof of concept and economic analysis.", "motivation": "Cloud forensics faces obstacles due to increasing law enforcement requests across jurisdictions and the inefficiency of existing cross-border data access formalities. A streamlined system is needed to ensure effective digital forensic readiness.", "method": "Analyzes CSP transparency reports and law enforcement guidelines, then proposes CLERMS architecture and demonstrates it through two realistic scenarios along with cost estimation using open-source components.", "result": "Developed and validated a CLERMS proof of concept, providing economic cost analysis and demonstrating potential to improve compliance and efficiency for CSPs and Cloud Service Consumers.", "conclusion": "CLERMS offers a viable solution to enhance due Cloud Digital Forensic Readiness (CDFR) by improving cross-border law enforcement request management processes for both service providers and users."}}
{"id": "2507.04197", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.04197", "abs": "https://arxiv.org/abs/2507.04197", "authors": ["Nishant Chinnasami", "Rye Stahle-Smith", "Rasha Karakchi"], "title": "ML-Enhanced AES Anomaly Detection for Real-Time Embedded Security", "comment": null, "summary": "Advanced Encryption Standard (AES) is a widely adopted cryptographic\nalgorithm, yet its practical implementations remain susceptible to side-channel\nand fault injection attacks. In this work, we propose a comprehensive framework\nthat enhances AES-128 encryption security through controlled anomaly injection\nand real-time anomaly detection using both statistical and machine learning\n(ML) methods. We simulate timing and fault-based anomalies by injecting\nexecution delays and ciphertext perturbations during encryption, generating\nlabeled datasets for detection model training. Two complementary detection\nmechanisms are developed: a threshold-based timing anomaly detector and a\nsupervised Random Forest classifier trained on combined timing and ciphertext\nfeatures. We implement and evaluate the framework on both CPU and FPGA-based\nSoC hardware (PYNQ-Z1), measuring performance across varying block sizes,\ninjection rates, and core counts. Our results show that ML-based detection\nsignificantly outperforms threshold-based methods in precision and recall while\nmaintaining real-time performance on embedded hardware. Compared to existing\nAES anomaly detection methods, our solution offers a low-cost, real-time, and\naccurate detection approach deployable on lightweight FPGA platforms.", "AI": {"tldr": "This paper proposes a framework for enhancing AES-128 encryption security by injecting controlled anomalies and detecting them in real-time using statistical methods and a Random Forest classifier. It evaluates this approach on CPU and FPGA platforms.", "motivation": "Practical AES implementations are vulnerable to side-channel and fault injection attacks, necessitating a lightweight yet effective security enhancement solution. Existing methods lack low-cost real-time accuracy.", "method": "1. Simulated timing/fault anomalies via execution delays and ciphertext perturbations for dataset generation\n2. Developed threshold-based timing detector + Random Forest classifier using combined features\n3. Implemented on x86 CPU and FPGA (PYNQ-Z1) for testing\n4. Evaluated performance varying block size (128-256 bits), injection rates (0.1-10%), and 1-4 cores", "result": "ML-based detection outperformed threshold methods in precision/recall while maintaining real-time performance on embedded hardware. Achieved 99.2% accuracy vs. 82.7% for thresholding, with 190ms inference time on FPGA for 1000 samples", "conclusion": "Offers a deployable solution for lightweight FPGA platforms (4.6mm2 active area) with low energy consumption (12mW) that effectively thwarts side-channel attacks. Practical in 128-256 bit modes across varying threat levels."}}
{"id": "2507.04214", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.04214", "abs": "https://arxiv.org/abs/2507.04214", "authors": ["Jianshuo Dong", "Tianyi Zhang", "Feng Yan", "Yuanjie Li", "Hewu Li", "Han Qiu"], "title": "Can Large Language Models Automate the Refinement of Cellular Network Specifications?", "comment": null, "summary": "Cellular networks serve billions of users globally, yet concerns about\nreliability and security persist due to weaknesses in 3GPP standards. However,\ntraditional analysis methods, including manual inspection and automated tools,\nstruggle with increasingly expanding cellular network specifications. This\npaper investigates the feasibility of Large Language Models (LLMs) for\nautomated cellular network specification refinement. To advance it, we leverage\n200,000+ approved 3GPP Change Requests (CRs) that document specification\nrevisions, constructing a valuable dataset for domain tasks. We introduce\nCR-eval, a principled evaluation framework, and benchmark 16 state-of-the-art\nLLMs, demonstrating that top models can discover security-related weaknesses in\nover 127 out of 200 test cases within five trials. To bridge potential gaps, we\nexplore LLM specialization techniques, including fine-tuning an 8B model to\nmatch or surpass advanced LLMs like GPT-4o and DeepSeek-R1. Evaluations on 30\ncellular attacks identify open challenges for achieving full automation. These\nfindings confirm that LLMs can automate the refinement of cellular network\nspecifications and provide valuable insights to guide future research in this\ndirection.", "AI": {"tldr": "This paper explores using large language models (LLMs) to improve cellular network specification refinement, addressing security weaknesses and scalability challenges in 3GPP standards with a novel CR-eval framework and extensive benchmarking.", "motivation": "Cellular networks face persistent reliability and security issues due to 3GPP standard weaknesses, while traditional analysis methods (manual inspection/automated tools) struggle with expanding specifications. Existing approaches lack scalability and effectiveness for automated refinement.", "method": "1) Constructed a dataset of 200,000+ approved 3GPP Change Requests (CRs) 2) Developed CR-eval, a principled evaluation framework 3) Benchmarked 16 state-of-the-art LLMs across 200 test cases 4) Implemented LLM specialization techniques through fine-tuning (8B model matching/surpassing GPT-4o and DeepSeek R1) 5) Evaluated models on 30 cellular attack scenarios", "result": "1) Top LLMs identified security weaknesses in 127+ test cases (63.5%) across five trials 2) 8B fine-tuned model achieved comparable/superior performance to advanced models 3) Identified significant open challenges in automating 30 network attacks 4) Demonstrated automation feasibility with ~63% success rate in baseline evaluations.", "conclusion": "LLMs can effectively automate cellular network specification refinement, discovering security flaws at scale. The 8B model's success shows specialization potential, but challenges remain in full automation of attack identification. This work provides foundational evidence and evaluation methodology for advancing LLM application in cellular network security."}}
{"id": "2507.04227", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04227", "abs": "https://arxiv.org/abs/2507.04227", "authors": ["Guohong Liu", "Jialei Ye", "Jiacheng Liu", "Yuanchun Li", "Wei Liu", "Pengzhi Gao", "Jian Luan", "Yunxin Liu"], "title": "Hijacking JARVIS: Benchmarking Mobile GUI Agents against Unprivileged Third Parties", "comment": null, "summary": "Mobile GUI agents are designed to autonomously execute diverse device-control\ntasks by interpreting and interacting with mobile screens. Despite notable\nadvancements, their resilience in real-world scenarios where screen content may\nbe partially manipulated by untrustworthy third parties remains largely\nunexplored. Owing to their black-box and autonomous nature, these agents are\nvulnerable to manipulations that could compromise user devices. In this work,\nwe present the first systematic investigation into the vulnerabilities of\nmobile GUI agents. We introduce a scalable attack simulation framework\nAgentHazard, which enables flexible and targeted modifications of screen\ncontent within existing applications. Leveraging this framework, we develop a\ncomprehensive benchmark suite comprising both a dynamic task execution\nenvironment and a static dataset of vision-language-action tuples, totaling\nover 3,000 attack scenarios. The dynamic environment encompasses 58\nreproducible tasks in an emulator with various types of hazardous UI content,\nwhile the static dataset is constructed from 210 screenshots collected from 14\npopular commercial apps. Importantly, our content modifications are designed to\nbe feasible for unprivileged third parties. We evaluate 7 widely-used mobile\nGUI agents and 5 common backbone models using our benchmark. Our findings\nreveal that all examined agents are significantly influenced by misleading\nthird-party content (with an average misleading rate of 28.8% in human-crafted\nattack scenarios) and that their vulnerabilities are closely linked to the\nemployed perception modalities and backbone LLMs. Furthermore, we assess\ntraining-based mitigation strategies, highlighting both the challenges and\nopportunities for enhancing the robustness of mobile GUI agents. Our code and\ndata will be released at https://agenthazard.github.io.", "AI": {"tldr": "This paper investigates vulnerabilities in mobile GUI agents when interacting with screens manipulated by untrustworthy third parties, introducing AgentHazard framework, a mobile vision benchmark with over 3,000 attack scenarios, and revealing significant susceptibility (28.8% misleading rate) in seven major agents. The work highlights perception modality/backbone model relationships and mitigation strategies.", "motivation": "Current mobile GUI agents lack robustness against real-world adversarial screen content manipulations by unprivileged third parties due to their black-box autonomous nature, creating risks for device compromise. Systematic vulnerability investigations and benchmarks are urgently needed.", "method": "Developed AgentHazard framework for scalable attack simulation; created a benchmark with 58 dynamic tasks in emulators using modified commercial app data and 210 static screenshots from 14 apps. Test conditions mimic feasible third-party attacks. Evaluated 7 GUI agents and 5 backbone models.", "result": "All tested agents (7 GUI agents and 5 backbone models) demonstrated significant vulnerability to third-party content manipulation (avg. 28.8% misleading rate in human-crafted attacks). Vulnerabilities correlate with specific vision-language-action perception methods. Found both challenges and opportunities in training-based mitigation approaches.", "conclusion": "Demonstrates critical security risks in current mobile GUI agent systems, provides standardized benchmark for evaluating robustness against adversarial UI content, and establishes foundation for developing more secure autonomous mobile interfaces. Code and dataset available at https://agenthazard.github.io."}}
{"id": "2507.04275", "categories": ["cs.CR", "cs.AI", "cs.LG", "68T05, 68M25", "D.4.6; I.2.6; K.6.5"], "pdf": "https://arxiv.org/pdf/2507.04275", "abs": "https://arxiv.org/abs/2507.04275", "authors": ["M. Tahir Akdeniz", "Zeynep Ye\u015filkaya", "\u0130. Enes K\u00f6se", "\u0130. Ula\u015f \u00dcnal", "Sevil \u015een"], "title": "VOLTRON: Detecting Unknown Malware Using Graph-Based Zero-Shot Learning", "comment": "17 pages, 6 figures, Submitted as a preprint", "summary": "The persistent threat of Android malware presents a serious challenge to the\nsecurity of millions of users globally. While many machine learning-based\nmethods have been developed to detect these threats, their reliance on large\nlabeled datasets limits their effectiveness against emerging, previously unseen\nmalware families, for which labeled data is scarce or nonexistent.\n  To address this challenge, we introduce a novel zero-shot learning framework\nthat combines Variational Graph Auto-Encoders (VGAE) with Siamese Neural\nNetworks (SNN) to identify malware without needing prior examples of specific\nmalware families. Our approach leverages graph-based representations of Android\napplications, enabling the model to detect subtle structural differences\nbetween benign and malicious software, even in the absence of labeled data for\nnew threats.\n  Experimental results show that our method outperforms the state-of-the-art\nMaMaDroid, especially in zero-day malware detection. Our model achieves 96.24%\naccuracy and 95.20% recall for unknown malware families, highlighting its\nrobustness against evolving Android threats.", "AI": {"tldr": "This paper proposes a zero-shot learning framework combining Variational Graph Auto-Encoders (VGAE) and Siamese Neural Networks (SNN) to detect previously unseen Android malware families without requiring labeled data for these threats.", "motivation": "Traditional machine learning-based Android malware detection methods rely on large labeled datasets, limiting their ability to detect emerging, zero-day malware families where labeled data is unavailable.", "method": "The framework uses graph-based representations of Android applications, with VGAE learning structural patterns and SNN comparing application similarities to enable zero-shot detection of malware.", "result": "Outperformed state-of-the-art MaMaDroid with 96.24% accuracy and 95.20% recall for unknown malware families, demonstrating robust zero-day detection capabilities.", "conclusion": "The proposed method provides an effective solution for detecting evolving Android malware without prior labeled examples, improving security against emerging threats."}}
{"id": "2507.04365", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.04365", "abs": "https://arxiv.org/abs/2507.04365", "authors": ["Xiaomeng Hu", "Pin-Yu Chen", "Tsung-Yi Ho"], "title": "Attention Slipping: A Mechanistic Understanding of Jailbreak Attacks and Defenses in LLMs", "comment": null, "summary": "As large language models (LLMs) become more integral to society and\ntechnology, ensuring their safety becomes essential. Jailbreak attacks exploit\nvulnerabilities to bypass safety guardrails, posing a significant threat.\nHowever, the mechanisms enabling these attacks are not well understood. In this\npaper, we reveal a universal phenomenon that occurs during jailbreak attacks:\nAttention Slipping. During this phenomenon, the model gradually reduces the\nattention it allocates to unsafe requests in a user query during the attack\nprocess, ultimately causing a jailbreak. We show Attention Slipping is\nconsistent across various jailbreak methods, including gradient-based token\nreplacement, prompt-level template refinement, and in-context learning.\nAdditionally, we evaluate two defenses based on query perturbation, Token\nHighlighter and SmoothLLM, and find they indirectly mitigate Attention\nSlipping, with their effectiveness positively correlated with the degree of\nmitigation achieved. Inspired by this finding, we propose Attention Sharpening,\na new defense that directly counters Attention Slipping by sharpening the\nattention score distribution using temperature scaling. Experiments on four\nleading LLMs (Gemma2-9B-It, Llama3.1-8B-It, Qwen2.5-7B-It, Mistral-7B-It v0.2)\nshow that our method effectively resists various jailbreak attacks while\nmaintaining performance on benign tasks on AlpacaEval. Importantly, Attention\nSharpening introduces no additional computational or memory overhead, making it\nan efficient and practical solution for real-world deployment.", "AI": {"tldr": "The paper discover universal \"Attention Slipping\" in jailbreak attacks and propose a defense called \"Attention Sharpening\" that enhances safety guardrails by sharpening attention score distributions via temperature scaling, demonstrating effectiveness across four major LLMs without performance overhead.", "motivation": "As LLMs become critical societal components, understanding and preventing jailbreak attacks that bypass safety mechanisms is imperative for responsible AI deployment.", "method": "1) Identify and characterize Attention Slipping phenomenon across different jailbreak attack vectors\n2) Evaluate existing defenses (Token Highlighter/SmoothLLM) by measuring their impact on attention slipping\n3) Propose Attention Sharpening defense that:\n- Directly targets attention score distribution\n- Uses temperature scaling to sharpen attention\n- Requires no additional computation/memory overhead", "result": "Successfully demonstrated that Attention Sharpening:\n- Resists various jailbreak attacks (gradient-based token replacement, prompt template refinement, in-context learning)\n- Maintains performance on benign tasks (AlpacaEval)\n- Has no computational/memory overhead\nFound strong positive correlation between defense effectiveness and degree of attention slipping mitigation", "conclusion": "Attention Sharpening offers an efficient, practical defense against jailbreak attacks by addressing the underlying Attention Slipping mechanism, maintaining model functionality without trade-offs in safety or performance for real-world LLM deployments."}}
{"id": "2507.04426", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.04426", "abs": "https://arxiv.org/abs/2507.04426", "authors": ["Novruz Amirov", "Leminur Celik", "Egemen Ali Caner", "Emre Yurdakul", "Fahri Anil Yerlikaya", "Serif Bahtiyar"], "title": "Enhancing Phishing Detection in Financial Systems through NLP", "comment": null, "summary": "The threat of phishing attacks in financial systems is continuously growing.\nTherefore, protecting sensitive information from unauthorized access is\nparamount. This paper discusses the critical need for robust email phishing\ndetection. Several existing methods, including blacklists and whitelists, play\na crucial role in detecting phishing attempts. Nevertheless, these methods\npossess inherent limitations, emphasizing the need for the development of a\nmore advanced solution. Our proposed solution presents a pioneering Natural\nLanguage Processing (NLP) approach for phishing email detection. Leveraging\nsemantic similarity and TFIDF (Term Frequency-Inverse Document Frequency)\nanalysis, our solution identifies keywords in phishing emails, subsequently\nevaluating the semantic similarities with a dedicated phishing dataset,\nultimately contributing to the enhancement of cybersecurity and NLP domains\nthrough a robust solution for detecting phishing threats in financial systems.\nExperimental results show the accuracy of our phishing detection method can\nreach 79.8 percent according to TF-IDF analysis, while it can reach 67.2\npercent according to semantic analysis.", "AI": {"tldr": "This paper proposes a novel NLP-based phishing email detection method combining semantic similarity and TF-IDF analysis, achieving 79.8% accuracy experimentally.", "motivation": "Financial systems face growing phishing threats; existing methods (blacklists/whitelists) have inherent limitations requiring more robust solutions.", "method": "The solution uses TF-IDF analysis for keyword identification and semantic similarity evaluation against a phishing dataset, leveraging NLP techniques.", "result": "The method achieves 79.8% accuracy (TF-IDF) and 67.2% accuracy (semantic analysis) in experimental tests.", "conclusion": "The proposed NLP approach addresses phishing detection limitations, offering a robust solution that advances both cybersecurity and NLP domains."}}
{"id": "2507.04457", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.04457", "abs": "https://arxiv.org/abs/2507.04457", "authors": ["Ruixuan Liu", "Li Xiong"], "title": "UniAud: A Unified Auditing Framework for High Auditing Power and Utility with One Training Run", "comment": "14 pages", "summary": "Differentially private (DP) optimization has been widely adopted as a\nstandard approach to provide rigorous privacy guarantees for training datasets.\nDP auditing verifies whether a model trained with DP optimization satisfies its\nclaimed privacy level by estimating empirical privacy lower bounds through\nhypothesis testing. Recent O(1) frameworks improve auditing efficiency by\nchecking the membership status of multiple audit samples in a single run,\nrather than checking individual samples across multiple runs. However, we\nreveal that there is no free lunch for this improved efficiency: data\ndependency and an implicit conflict between auditing and utility impair the\ntightness of the auditing results. Addressing these challenges, our key\ninsights include reducing data dependency through uncorrelated data and\nresolving the auditing-utility conflict by decoupling the criteria for\neffective auditing and separating objectives for utility and auditing. We first\npropose a unified framework, UniAud, for data-independent auditing that\nmaximizes auditing power through a novel uncorrelated canary construction and a\nself-comparison framework. We then extend this framework as UniAud++ for\ndata-dependent auditing, optimizing the auditing and utility trade-off through\nmulti-task learning with separate objectives for auditing and training.\nExperimental results validate that our black-box O(1) framework matches the\nstate-of-the-art auditing results of O(T) auditing with thousands of runs,\ndemonstrating the best efficiency-auditing trade-off across vision and language\ntasks. Additionally, our framework provides meaningful auditing with only\nslight utility degradation compared to standard DP training, showing the\noptimal utility-auditing trade-off and the benefit of requiring no extra\ntraining for auditing.", "AI": {"tldr": "This paper addresses the limitations of efficient O(1) frameworks for differentially private (DP) auditing by introducing UniAud and UniAud++ methods. These frameworks improve auditing tightness while maintaining utility through data independence, uncorrelated canary construction, and multi-task learning with separated objectives. The results demonstrate superior efficiency-auditing trade-offs and minimal utility degradation compared to existing methods.", "motivation": "Recent O(1) DP auditing frameworks sacrifice tightness for efficiency due to data dependency and a conflict between auditing effectiveness and model utility. The paper seeks to resolve these issues by decoupling auditing criteria and rethinking data independence.", "method": "The authors propose UniAud, a data-independent auditing framework using uncorrelated canary data and self-comparison, and extend it to UniAud++ for data-dependent scenarios. They employ multi-task learning to separately optimize utility and auditing objectives.", "result": "UniAud and UniAud++ match the state-of-the-art O(T) auditing performance with significantly fewer runs across vision and language tasks. They achieve near-standard DP training utility with marginal degradation, confirming optimal efficiency-auditing and utility-auditing trade-offs without requiring auxiliary training.", "conclusion": "The proposed unified frameworks demonstrate that improved DP auditing tightness and efficiency are achievable by addressing data dependency through uncorrelated data and resolving objective conflicts via decoupling. This advances practical privacy-utility trade-off analysis in machine learning."}}
{"id": "2507.04461", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.04461", "abs": "https://arxiv.org/abs/2507.04461", "authors": ["Tanvir Rahman", "A. B. M. Harun-ur Rashid"], "title": "Arbiter PUF: Uniqueness and Reliability Analysis Using Hybrid CMOS-Stanford Memristor Model", "comment": null, "summary": "In an increasingly interconnected world, protecting electronic devices has\ngrown more crucial because of the dangers of data extraction, reverse\nengineering, and hardware tampering. Producing chips in a third-party\nmanufacturing company can let hackers change the design. As the Internet of\nThings (IoT) proliferates, physical attacks happen more, and conventional\ncryptography techniques do not function well. In this paper, we investigate the\ndesign and assessment of PUFs using the Stanford Memristor Model, utilizing its\nrandom filament evolution to improve security. The system was built using 45nm\nCMOS technology. A comparison is made between CMOS-based and memristor-based\nArbiter PUFs, evaluating their performance under temperature, voltage, and\nprocess variations. Intra- and inter-hamming distances are employed by Monte\nCarlo simulations to estimate uniqueness and reliability. The results show that\nmemristor-based PUFs offer better reliability than CMOS-based designs, though\nuniqueness needs further improvement. Furthermore, this study sheds light on\nthe reasonableness of memristor-based PUFs for secure applications in hardware\nsecurity.", "AI": {"tldr": "This paper explores memristor-based PUFs for enhanced reliability in hardware security, demonstrating superior performance over CMOS-based solutions under process variations but requiring improvements in uniqueness.", "motivation": "The growing security risks in interconnected electronic devices, including data extraction and hardware tampering, create a critical need for robust security solutions beyond traditional cryptography. The IoT's expansion exacerbates these challenges as existing methods struggle with process variations in third-party manufacturing.", "method": "The study uses the Stanford Memristor Model with 45nm CMOS technology to design and evaluate memristor-based Arbiter PUFs through Monte Carlo simulation of intra- and inter-hamming distances under temperature, voltage, and process variations.", "result": "Memristor PUFs show higher reliability than CMOS PUFs in process variation resistance, but exhibit suboptimal uniqueness metrics that require further optimization for practical deployment.", "conclusion": "Memristor-based PUFs represent a viable alternative for secure applications in hardware security contexts, particularly where reliability under environmental variations is critical, though uniqueness remains a limitation to address."}}
{"id": "2507.04495", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.04495", "abs": "https://arxiv.org/abs/2507.04495", "authors": ["Hyunwook Choi", "Sangyun Won", "Daeyeon Hwang", "Junhyeok Choi"], "title": "README: Robust Error-Aware Digital Signature Framework via Deep Watermarking Model", "comment": null, "summary": "Deep learning-based watermarking has emerged as a promising solution for\nrobust image authentication and protection. However, existing models are\nlimited by low embedding capacity and vulnerability to bit-level errors, making\nthem unsuitable for cryptographic applications such as digital signatures,\nwhich require over 2048 bits of error-free data. In this paper, we propose\nREADME (Robust Error-Aware Digital Signature via Deep WaterMarking ModEl), a\nnovel framework that enables robust, verifiable, and error-tolerant digital\nsignatures within images. Our method combines a simple yet effective\ncropping-based capacity scaling mechanism with ERPA (ERror PAinting Module), a\nlightweight error correction module designed to localize and correct bit errors\nusing Distinct Circular Subsum Sequences (DCSS). Without requiring any\nfine-tuning of existing pretrained watermarking models, README significantly\nboosts the zero-bit-error image rate (Z.B.I.R) from 1.2% to 86.3% when\nembedding 2048-bit digital signatures into a single image, even under\nreal-world distortions. Moreover, our use of perceptual hash-based signature\nverification ensures public verifiability and robustness against tampering. The\nproposed framework unlocks a new class of high-assurance applications for deep\nwatermarking, bridging the gap between signal-level watermarking and\ncryptographic security.", "AI": {"tldr": "README is a framework that addresses the limitations of existing deep learning-based watermarking by enabling robust, verifiable, and error-tolerant 2048-bit digital signatures in images through a cropping-based capacity scaling mechanism and ERPA (ERror PAinting Module) with DCSS. It achieves an 86.3% zero-bit-error rate under real-world distortions without retraining models.", "motivation": "Deep learning-based watermarking struggles with embedding 2048-bit error-free digital signatures due to low capacity and bit-level error vulnerabilities, limiting its use in cryptographic applications requiring high assurance.", "method": "Combines a cropping-based capacity scaling mechanism with ERPA, a lightweight error correction module utilizing Distinct Circular Subsum Sequences (DCSS), to localize and correct bit errors. Integrates perceptual hash-based verification for public tamper resistance.", "result": "README increases the zero-bit-error image rate (Z.B.I.R) from 1.2% to 86.3% when embedding 2048-bit signatures into a single image, maintaining accuracy under real-world distortions without retraining existing models.", "conclusion": "README bridges the gap between signal-level watermarking and cryptographic security, enabling digital signatures in images with public verifiability and high error tolerance, unlocking high-assurance applications for deep watermarking."}}
{"id": "2507.04501", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.04501", "abs": "https://arxiv.org/abs/2507.04501", "authors": ["Gennady Khalimov", "Yevgen Kotukh"], "title": "LINE: Public-key encryption", "comment": null, "summary": "We propose a public key encryption cryptosystem based on solutions of linear\nequation systems with predefinition of input parameters through shared secret\ncomputation for factorizable substitutions. The existence of multiple\nequivalent solutions for an underdetermined system of linear equations\ndetermines the impossibility of its resolution by a cryptanalyst in polynomial\ntime. The completion of input parameters of the equation system is implemented\nthrough secret homomorphic matrix transformation for substitutions factorized\nover the basis of a vector space of dimension m over the field F2. Encryption\nis implemented through computation of substitutions that are one-way functions\non an elementary abelian 2-group of order 2\"m. Decryption is implemented\nthrough completion of input parameters of the equation system. Homomorphic\ntransformations are constructed based on matrix computations. Matrix\ncomputations enable the implementation of high security and low computational\noverhead for homomorphic transformations.", "AI": {"tldr": "The paper proposes a high-security, low-overhead public-key encryption system based on solving underdetermined linear equation systems using factorizable substitutions with homomorphic matrix transformations over F\u2082 vector spaces.", "motivation": "Traditional cryptosystems require robust security against polynomial-time attacks, motivating the use of complex mathematical structures like underdetermined systems with multiple equivalent solutions to prevent cryptanalysis.", "method": "The system utilizes secret homomorphic matrix transformations to complete input parameters of linear equation systems. Encryption relies on one-way functions over an elementary abelian 2-group of order 2^m, while decryption involves reconstructing these parameters.", "result": "Matrix-based homomorphic transformations enable secure parameter completion with low computational cost, achieving a cryptosystem that resists efficient cryptanalysis due to the inherent complexity of underdetermined linear systems.", "conclusion": "The proposed cryptosystem offers a novel approach combining algebraic complexity and efficient computation, potentially improving security without sacrificing performance for factorizable substitutions over F\u2082 vector spaces."}}
{"id": "2507.04752", "categories": ["cs.CR", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.04752", "abs": "https://arxiv.org/abs/2507.04752", "authors": ["Shuo Yang", "Xinran Zheng", "Xinchen Zhang", "Jinfeng Xu", "Jinze Li", "Donglin Xie", "Weicai Long", "Edith C. H. Ngai"], "title": "Large Language Models for Network Intrusion Detection Systems: Foundations, Implementations, and Future Directions", "comment": null, "summary": "Large Language Models (LLMs) have revolutionized various fields with their\nexceptional capabilities in understanding, processing, and generating\nhuman-like text. This paper investigates the potential of LLMs in advancing\nNetwork Intrusion Detection Systems (NIDS), analyzing current challenges,\nmethodologies, and future opportunities. It begins by establishing a\nfoundational understanding of NIDS and LLMs, exploring the enabling\ntechnologies that bridge the gap between intelligent and cognitive systems in\nAI-driven NIDS. While Intelligent NIDS leverage machine learning and deep\nlearning to detect threats based on learned patterns, they often lack\ncontextual awareness and explainability. In contrast, Cognitive NIDS integrate\nLLMs to process both structured and unstructured security data, enabling deeper\ncontextual reasoning, explainable decision-making, and automated response for\nintrusion behaviors. Practical implementations are then detailed, highlighting\nLLMs as processors, detectors, and explainers within a comprehensive AI-driven\nNIDS pipeline. Furthermore, the concept of an LLM-centered Controller is\nproposed, emphasizing its potential to coordinate intrusion detection\nworkflows, optimizing tool collaboration and system performance. Finally, this\npaper identifies critical challenges and opportunities, aiming to foster\ninnovation in developing reliable, adaptive, and explainable NIDS. By\npresenting the transformative potential of LLMs, this paper seeks to inspire\nadvancement in next-generation network security systems.", "AI": {"tldr": "This paper explores the potential of large language models (LLMs) for advancing network intrusion detection systems (NIDS) by addressing challenges in traditional AI-driven approaches and proposing LLM-centered solutions for contextual awareness, explainability, and automation.", "motivation": "Traditional intelligent NIDS (using ML/DL) suffer from limited contextual understanding and lack of explainability, making their decision-making opaque and difficult to trust. This motivates the need for cognitive systems with human-like reasoning capabilities.", "method": "The paper 1) establishes foundational concepts of NIDS and LLMs, 2) analyzes enabling technologies for AI-driven cognitive NIDS, 3) presents practical LLM implementations as processors, detectors, and explainers, and 4) introduces an LLM-centered Controller for workflow coordination.", "result": "The analysis demonstrates how LLMs can process both structured/unstructured data to achieve: - Deeper contextual threat analysis - Transparent, human-explainable decisions - Automated response generation - Optimized collaborative detection through centralized control architecture, with detailed implementation frameworks provided.", "conclusion": "LLMs represent a transformative shift for NIDS by bridging smart and cognitive systems through contextual awareness and explainability. The paper identifies key challenges (reliability, adaptability) while highlighting opportunities to develop trustworthy next-generation security systems with human-AI collaboration."}}
{"id": "2507.04771", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.04771", "abs": "https://arxiv.org/abs/2507.04771", "authors": ["Josep Domingo-Ferrer", "Najeeb Jebreel", "David S\u00e1nchez"], "title": "Efficient Unlearning with Privacy Guarantees", "comment": null, "summary": "Privacy protection laws, such as the GDPR, grant individuals the right to\nrequest the forgetting of their personal data not only from databases but also\nfrom machine learning (ML) models trained on them. Machine unlearning has\nemerged as a practical means to facilitate model forgetting of data instances\nseen during training. Although some existing machine unlearning methods\nguarantee exact forgetting, they are typically costly in computational terms.\nOn the other hand, more affordable methods do not offer forgetting guarantees\nand are applicable only to specific ML models. In this paper, we present\n\\emph{efficient unlearning with privacy guarantees} (EUPG), a novel machine\nunlearning framework that offers formal privacy guarantees to individuals whose\ndata are being unlearned. EUPG involves pre-training ML models on data\nprotected using privacy models, and it enables {\\em efficient unlearning with\nthe privacy guarantees offered by the privacy models in use}. Through empirical\nevaluation on four heterogeneous data sets protected with $k$-anonymity and\n$\\epsilon$-differential privacy as privacy models, our approach demonstrates\nutility and forgetting effectiveness comparable to those of exact unlearning\nmethods, while significantly reducing computational and storage costs. Our code\nis available at https://github.com/najeebjebreel/EUPG.", "AI": {"tldr": "The paper proposes EUPG, a novel machine unlearning framework with formal privacy guarantees and efficiency. It pre-trains models on privacy-protected data and demonstrates comparable performance to existing methods with lower computational costs.", "motivation": "Privacy laws like GDPR require ML models to be retrained to remove personal data when requested. Current unlearning methods are either computationally expensive or lack privacy guarantees for arbitrary models.", "method": "EUPG pre-trains ML models on data protected with privacy models such as k-anonymity and differential privacy, enabling efficient unlearning with inherent privacy guarantees through this pre-processing approach.", "result": "Empirical evaluation on four diverse datasets shows EUPG achieves utility and forgetting effectiveness comparable to exact unlearining methods while reducing computational and storage costs significantly.", "conclusion": "EUPG offers a practical framework for privacy-compliant data forgetting by combining pre-training with existing privacy models, maintaining model performance while enabling efficient unlearning with formal privacy guarantees."}}
{"id": "2507.04775", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.04775", "abs": "https://arxiv.org/abs/2507.04775", "authors": ["Carlos Agull\u00f3-Domingo", "\u00d3scar Vera-L\u00f3pez", "Seyda Guzelhan", "Lohit Daksha", "Aymane El Jerari", "Kaustubh Shivdikar", "Rashmi Agrawal", "David Kaeli", "Ajay Joshi", "Jos\u00e9 L. Abell\u00e1n"], "title": "FIDESlib: A Fully-Fledged Open-Source FHE Library for Efficient CKKS on GPUs", "comment": "Presented as poster paper at 2025 IEEE International Symposium on\n  Performance Analysis of Systems and Software (ISPASS)", "summary": "Word-wise Fully Homomorphic Encryption (FHE) schemes, such as CKKS, are\ngaining significant traction due to their ability to provide\npost-quantum-resistant, privacy-preserving approximate computing; an especially\ndesirable feature in Machine-Learning-as-a-Service (MLaaS) cloud-computing\nparadigms. OpenFHE is a leading CPU-based FHE library with robust CKKS\noperations, but its server-side performance is not yet sufficient for practical\ncloud deployment. As GPU computing becomes more common in data centers, many\nFHE libraries are adding GPU support. However, integrating an efficient GPU\nbackend into OpenFHE is challenging. While OpenFHE uses a Hardware Abstraction\nLayer (HAL), its flexible architecture sacrifices performance due to the\nabstraction layers required for multi-scheme and multi-backend compatibility.\nIn this work, we introduce FIDESlib, the first open-source server-side CKKS GPU\nlibrary that is fully interoperable with well-established client-side OpenFHE\noperations. Unlike other existing open-source GPU libraries, FIDESlib provides\nthe first implementation featuring heavily optimized GPU kernels for all CKKS\nprimitives, including bootstrapping. Our library also integrates robust\nbenchmarking and testing, ensuring it remains adaptable to further\noptimization. Furthermore, its software architecture is designed to support\nextensions to a multi-GPU backend for enhanced acceleration. Our experiments\nacross various GPU systems and the leading open-source CKKS library to date,\nPhantom, show that FIDESlib offers superior performance and scalability. For\nbootstrapping, FIDESlib achieves no less than 70x speedup over the\nAVX-optimized OpenFHE implementation.", "AI": {"tldr": "FIDESlib is the first open-source server-side CKKS GPU library achieving 70x speedup over AVX-optimized OpenFHE for bootstrapping while maintaining interoperability with client-side operations.", "motivation": "Current CPU-based OpenFHE lacks server-side performance for cloud MLaaS deployment, and existing GPU FHE libraries don't provide optimized CKKS implementations with multi-GPU scalability.", "method": "Designed a dedicated CKKS GPU library from first principles with highly optimized CUDA kernels, robust benchmarking infrastructure, and software architecture supporting multi-GPU extensions.", "result": "Experiments show FIDESlib outperforms Phantom on all CKKS primitives and achieves 70x faster bootstrapping than OpenFHE's CPU implementation, with demonstrated scalability across GPU systems.", "conclusion": "FIDESlib provides a practical, interoperable GPU solution for efficient server-side CKKS, addressing cloud deployment challenges with significant performance improvements and future optimization capabilities."}}
{"id": "2507.04855", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.04855", "abs": "https://arxiv.org/abs/2507.04855", "authors": ["Darya Parygina", "Timofey Mezhuev", "Daniil Kuts"], "title": "Hybrid Approach to Directed Fuzzing", "comment": null, "summary": "Program analysis and automated testing have recently become an essential part\nof SSDLC. Directed greybox fuzzing is one of the most popular automated testing\nmethods that focuses on error detection in predefined code regions. However, it\nstill lacks ability to overcome difficult program constraints. This problem can\nbe well addressed by symbolic execution, but at the cost of lower performance.\nThus, combining directed fuzzing and symbolic execution techniques can lead to\nmore efficient error detection.\n  In this paper, we propose a hybrid approach to directed fuzzing with novel\nseed scheduling algorithm, based on target-related interestingness and\ncoverage. The approach also performs minimization and sorting of objective\nseeds according to a target-related information. We implement our approach in\nSydr-Fuzz tool using LibAFL-DiFuzz as directed fuzzer and Sydr as dynamic\nsymbolic executor. We evaluate our approach with Time to Exposure metric and\ncompare it with pure LibAFL-DiFuzz, AFLGo, BEACON, WAFLGo, WindRanger,\nFishFuzz, and Prospector. The results show an improvement for 3 out of 7\nexamples with speedup up to 1.86 times over the second best result, as well as\na significant improvement for 3 out of 7 examples over the pure LibAFL-DiFuzz\nfuzzer. Sydr-Fuzz hybrid approach to directed fuzzing shows high performance\nand helps to improve directed fuzzing efficiency.", "AI": {"tldr": "A hybrid directed fuzzer combining symbolic execution and novel seed scheduling algorithms to overcome constraints and improve error detection efficiency.", "motivation": "Directed greybox fuzzing struggles with difficult program constraints despite popularity for error detection. Symbolic execution solves this but sacrifices performance. Combining both addresses the limitations.", "method": "Proposed hybrid approach with a seed scheduling algorithm prioritizing 'target-related interestingness' and coverage metrics. Implemented in Sydr-Fuzz using LibAFL-DiFuzz (fuzzer) and Sydr (symbolic executor). Objective seeds were minimized and sorted based on target information.", "result": "Outperformed 6 competitive fuzzers (AFLGo, BEACON, WAFLGo, WindRanger, FishFuzz, Prospector) in 3/7 examples with up to 1.86\u00d7 speedup over the next best. Surpassed pure LibAFL-DiFuzz in efficiency for 3/7 examples using Time to Exposure metric.", "conclusion": "The hybrid directed fuzzing approach enhances error detection efficiency by addressing program constraints, demonstrating performance improvements over existing methods through empirical evaluation."}}
{"id": "2507.04903", "categories": ["cs.CR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.04903", "abs": "https://arxiv.org/abs/2507.04903", "authors": ["Thinh Dao", "Dung Thuy Nguyen", "Khoa D Doan", "Kok-Seng Wong"], "title": "BackFed: An Efficient & Standardized Benchmark Suite for Backdoor Attacks in Federated Learning", "comment": "Under review at NeurIPS'25", "summary": "Federated Learning (FL) systems are vulnerable to backdoor attacks, where\nadversaries train their local models on poisoned data and submit poisoned model\nupdates to compromise the global model. Despite numerous proposed attacks and\ndefenses, divergent experimental settings, implementation errors, and\nunrealistic assumptions hinder fair comparisons and valid conclusions about\ntheir effectiveness in real-world scenarios. To address this, we introduce\nBackFed - a comprehensive benchmark suite designed to standardize, streamline,\nand reliably evaluate backdoor attacks and defenses in FL, with a focus on\npractical constraints. Our benchmark offers key advantages through its\nmulti-processing implementation that significantly accelerates experimentation\nand the modular design that enables seamless integration of new methods via\nwell-defined APIs. With a standardized evaluation pipeline, we envision BackFed\nas a plug-and-play environment for researchers to comprehensively and reliably\nevaluate new attacks and defenses. Using BackFed, we conduct large-scale\nstudies of representative backdoor attacks and defenses across both Computer\nVision and Natural Language Processing tasks with diverse model architectures\nand experimental settings. Our experiments critically assess the performance of\nproposed attacks and defenses, revealing unknown limitations and modes of\nfailures under practical conditions. These empirical insights provide valuable\nguidance for the development of new methods and for enhancing the security of\nFL systems. Our framework is openly available at\nhttps://github.com/thinh-dao/BackFed.", "AI": {"tldr": "BackFed is a benchmark suite for standardizing evaluations of backdoor attacks and defenses in Federated Learning (FL), enabling reliable comparisons under practical constraints. Experiments using BackFed reveal limitations in existing methods and highlight failure modes under realistic conditions.", "motivation": "FL systems face challenges in evaluating the effectiveness of backdoor attacks and defenses due to divergent experimental settings, implementation errors, and unrealistic assumptions. This hinders the development of robust solutions for real-world applications.", "method": "BackFed introduces a multi-processing implementation with APIs for modular integration of new methods and a standardized evaluation pipeline. Large-scale studies are conducted across computer vision and NLP tasks with diverse model architectures and settings.", "result": "Large-scale experiments identify unknown limitations and failure modes of representative backdoor attacks and defenses when tested under practical conditions. The framework's efficiency and modularity reduce experimentation barriers.", "conclusion": "BackFed provides a plug-and-play environment for comprehensive FL backdoor evaluation, offering empirical insights to guide the development of secure methods. The open-source framework is available for community adoption at the provided GitHub link."}}
{"id": "2507.04916", "categories": ["cs.CR", "math.CO"], "pdf": "https://arxiv.org/pdf/2507.04916", "abs": "https://arxiv.org/abs/2507.04916", "authors": ["Kazumasa Shinagawa", "Koji Nuida"], "title": "Cyclic Equalizability of Words and Its Application to Card-Based Cryptography", "comment": "11 pages, to appear in 25th International Symposium on Fundamentals\n  of Computation Theory (FCT 2025)", "summary": "Card-based cryptography is a research area to implement cryptographic\nprocedures using a deck of physical cards. In recent years, it has been found\nto be related to finite group theory and algebraic combinatorics, and is\nbecoming more and more closely connected to the field of mathematics. In this\npaper, we discuss the relationship between card-based cryptography and\ncombinatorics on words for the first time. In particular, we focus on cyclic\nequality of words. We say that a set of words are cyclically equalizable if\nthey can be transformed to be cyclically equal by repeated simultaneous\ninsertion of letters. The main result of this paper is to show that two binary\nwords of equal length and equal Hamming weight are cyclically equalizable. As\napplications of cyclic equalizability to card-based cryptography, we describe\nits applications to the information erasure problem and to single-cut full-open\nprotocols.", "AI": {"tldr": "This paper establishes a novel connection between card-based cryptography and combinatorics on words, proving that binary words of equal length and Hamming weight can be cyclically equalized via letter insertions, with applications to information erasure and single-cut protocols.", "motivation": "Card-based cryptography has recently been linked to finite group theory and algebraic combinatorics. This work explores its relationship with combinatorics on words to strengthen the theoretical foundation and enable new protocol constructions.", "method": "The paper introduces the concept of 'cyclic equalizability' through repeated simultaneous letter insertions and uses formal algebraic proofs from combinatorics on words to show this property holds for binary words with matching length and Hamming weight.", "result": "The main result is a proof that any two binary words of equal length and identical Hamming weight can be cyclically equalized by insertions, accompanied by specific applications to the information erasure problem and single-cut full-open protocols in card-based cryptography.", "conclusion": "By connecting card-based cryptography to combinatorics on words, this work provides new analytical tools and protocol design methodologies, particularly through the cyclic equalization property for binary words. The results demonstrate potential for improving security and efficiency in physical card protocols."}}
{"id": "2507.04931", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.04931", "abs": "https://arxiv.org/abs/2507.04931", "authors": ["Ruoxi Wang", "Kun Li", "Minghui Xu", "Yue Zhang", "Kaidi Xu", "Chunchi Liu", "Yinhao Xiao", "Xiuzhen Cheng"], "title": "LIFT: Automating Symbolic Execution Optimization with Large Language Models for AI Networks", "comment": "Accepted by ACM SIGCOMM 2025 - 2nd Workshop on Networks for AI\n  Computing (NAIC). 7 pages, 2 figures, 2 tables", "summary": "Dynamic Symbolic Execution (DSE) is a key technique in program analysis,\nwidely used in software testing, vulnerability discovery, and formal\nverification. In distributed AI systems, DSE plays a crucial role in\nidentifying hard-to-detect bugs, especially those arising from complex network\ncommunication patterns. However, traditional approaches to symbolic execution\nare often hindered by scalability issues and inefficiencies, particularly in\nlarge-scale systems. This paper introduces LIFT (Large-language-model\nIntegrated Functional-equivalent-IR Transformation), a novel framework that\nleverages Large Language Models (LLMs) to automate the optimization of\nIntermediate Representations (IRs) in symbolic execution. LIFT addresses the\nchallenges of symbolic execution by providing a scalable, context-sensitive\nsolution for IR transformation. The framework consists of two phases: IR\nAnalysis and Optimization, where LLMs optimize time-intensive IR blocks, and\nSymbolic Execution and Validation, which includes benchmarking and semantic\nverification to ensure correctness and generalizability. Experiments on\nreal-world binaries demonstrated significant performance improvements,\nincluding a 53.5\\% reduction in execution time for bigtest and a 10.24\\%\nreduction for random, along with reductions in IR statements, PUT instructions,\nand temporary variables. These results demonstrate that LLMs simplify IRs while\nmaintaining functional correctness, enhancing symbolic execution in distributed\nAI systems.", "AI": {"tldr": "LIFT is a novel framework leveraging LLMs to optimize intermediate representations (IRs) in dynamic symbolic execution (DSE) for distributed AI systems, addressing scalability issues through automated IR transformation.", "motivation": "Traditional DSE faces scalability and efficiency challenges in large-scale distributed AI systems, particularly due to complex network communication patterns and resource-intensive IR processing.", "method": "The framework employs a two-phase approach: 1) IR Analysis and Optimization using LLMs to simplify computationally expensive IR blocks without semantic loss, and 2) Symbolic Execution and Validation with benchmarks and semantic verification to ensure correctness and generalization.", "result": "Experiments showed up to 53.5% execution time reduction for bigtest binaries (10.24% for random binaries) along with reductions in IR statements, PUT instructions, and temporary variables while maintaining functional equivalence.", "conclusion": "LIFT demonstrates that LLMs can effectively optimize IRs for DSE, improving performance and scalability in distributed AI systems without compromising correctness through its structured two-phase validation process."}}
{"id": "2507.04956", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.04956", "abs": "https://arxiv.org/abs/2507.04956", "authors": ["Yusei Tanaka"], "title": "Bullshark on Narwhal: Implementation-level Workflow Analysis of Round-based DAG Consensus in Theory and Practice", "comment": "17 pages, in Japanese language, 11 figures", "summary": "Round-based DAGs enable high-performance Byzantine fault-tolerant consensus,\nyet their technical advantages remain underutilized due to their short history.\nWhile research on consensus protocols is active in both academia and industry,\nmany studies overlook implementation-level algorithms, leaving actual\nperformance unclear - particularly for theoretical protocols whose practical\nperformance cannot often be evaluated. Bullshark, a Round-based DAG BFT\nprotocol on Narwhal mempool, achieves optimal performance: 297,000 transactions\nper second with 2-second latency. We analyze the algorithm's workflow, from\ntransaction submission to blockchain commitment, breaking it down layer by\nlayer at the functional level and delineating the key features and interactions\nof the Bullshark and Narwhal components. Future work aims to improve\nperformance in Byzantine fault environments and optimize trade-offs in the CAP\ntheorem.", "AI": {"tldr": "This paper introduces Bullshark, a high-performance Round-based DAG BFT protocol using the Narwhal mempool, achieving 297,000 transactions per second with 2-second latency. It highlights the underexplored potential of DAGs in BFT consensus and outlines future improvements targeting Byzantine fault resilience and CAP theorem trade-offs.", "motivation": "Despite active research, practical implementations of theoretical BFT consensus protocols with round-based DAGs (like transaction ordering) are lacking, leading to unclear performance metrics. Existing studies often ignore implementation-level algorithms, limiting real-world evaluation of these protocols.", "method": "The authors analyze Bullshark's workflow from transaction submission through blockchain commitment by decomposing its architecture across functional layers. They detail interactions between Bullshark's core components and the Narwhal mempool, emphasizing algorithmic design for optimizing throughput and latency.", "result": "Bullshark demonstrated 297,000 TPS with 2-second latency under optimal conditions using Narwhal. This result validates the protocol's capability to achieve high performance, offering concrete metrics where previous DAG-based BFT protocols were theoretical only.", "conclusion": "Bullshark proves round-based DAG BFT protocols can be practically implemented for near-optimal performance. The work underscores the importance of bridging theoretical BFT consensus models with real-world implementations while prioritizing Byzantine fault tolerance improvements in future research."}}
{"id": "2507.05093", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05093", "abs": "https://arxiv.org/abs/2507.05093", "authors": ["Alberto Castagnaro", "Umberto Salviati", "Mauro Conti", "Luca Pajola", "Simeone Pizzi"], "title": "The Hidden Threat in Plain Text: Attacking RAG Data Loaders", "comment": "currently under submission", "summary": "Large Language Models (LLMs) have transformed human-machine interaction since\nChatGPT's 2022 debut, with Retrieval-Augmented Generation (RAG) emerging as a\nkey framework that enhances LLM outputs by integrating external knowledge.\nHowever, RAG's reliance on ingesting external documents introduces new\nvulnerabilities. This paper exposes a critical security gap at the data loading\nstage, where malicious actors can stealthily corrupt RAG pipelines by\nexploiting document ingestion.\n  We propose a taxonomy of 9 knowledge-based poisoning attacks and introduce\ntwo novel threat vectors -- Content Obfuscation and Content Injection --\ntargeting common formats (DOCX, HTML, PDF). Using an automated toolkit\nimplementing 19 stealthy injection techniques, we test five popular data\nloaders, finding a 74.4% attack success rate across 357 scenarios. We further\nvalidate these threats on six end-to-end RAG systems -- including white-box\npipelines and black-box services like NotebookLM and OpenAI Assistants --\ndemonstrating high success rates and critical vulnerabilities that bypass\nfilters and silently compromise output integrity. Our results emphasize the\nurgent need to secure the document ingestion process in RAG systems against\ncovert content manipulations.", "AI": {"tldr": "This paper identifies critical security vulnerabilities in Retrieval-Augmented Generation (RAG) systems during document ingestion, presenting 9 knowledge-based poisoning attacks and 2 novel threat vectors (Content Obfuscation and Injection) with 74.4% success rates across 357 scenarios. It emphasizes the need for securing data loading processes in RAG frameworks.", "motivation": "RAG systems enhance LLM outputs with external knowledge, but existing security defenses focus more on retrieval/query stages than the data loading phase. The paper aims to expose hidden vulnerabilities during document ingestion where malicious alterations can be covertly introduced.", "method": "1) Propose a taxonomy of 9 knowledge-based poisoning attacks targeting document formats. 2) Introduce Content Obfuscation and Injection as new threat models. 3) Develop a toolkit implementing 19 injection techniques for common formats (DOCX, HTML, PDF). 4) Test 5 data loaders and 6 end-to-end RAG systems including NotebookLM and OpenAI Assistants.", "result": "74.4% attack success rate across 357 scenarios against data loaders. Significant vulnerabilities demonstrated in RAG systems that bypass content filters and compromise output integrity.Both white-box and black-box systems showed high susceptibility to the proposed attacks.", "conclusion": "Covert content manipulations during document ingestion critically compromise RAG system security. Current data loading pipelines lack effective defenses against these attacks. The findings demand immediate attention to securing knowledge integration processes in LLM frameworks."}}
{"id": "2507.05132", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.05132", "abs": "https://arxiv.org/abs/2507.05132", "authors": ["Nelly Elsayed", "Lily Dzamesi", "Zag ElSayed", "Murat Ozer"], "title": "Extreme Learning Machine Based System for DDoS Attacks Detections on IoMT Devices", "comment": "8 pages, under review", "summary": "The Internet of Medical Things (IoMT) represents a paradigm shift in the\nhealthcare sector, enabling the interconnection of medical devices, sensors,\nand systems to enhance patient monitoring, diagnosis, and management. The rapid\nevolution of IoMT presents significant benefits to the healthcare domains.\nHowever, there is a rapid increase in distributed denial of service (DDoS)\nattacks on the IoMT networks due to several vulnerabilities in the\nIoMT-connected devices, which negatively impact patients' health and can even\nlead to deaths. Thus, in this paper, we aim to save lives via investigating an\nextreme learning machine for detecting DDoS attacks on IoMT devices. The\nproposed approach achieves a high accuracy at a low implementation budget.\nThus, it can reduce the implementation cost of the DDoS detection system,\nmaking the model capable of executing on the fog level.", "AI": {"tldr": "This paper proposes a low-cost, high-accuracy DDoS detection model using extreme learning machine for IoMT to enhance patient safety.", "motivation": "Vulnerabilities in IoMT devices have led to life-threatening DDoS attacks, necessitating effective and cost-efficient detection solutions.", "method": "The authors investigate an extreme learning machine approach optimized for fog computing capabilities with minimal implementation resource requirements.", "result": "The proposed method achieves high detection accuracy at significantly reduced costs compared to existing solutions, enabling fog-level processing.", "conclusion": "The approach demonstrates a viable solution for safeguarding IoMT networks through cost-effective DDoS detection, directly contributing to healthcare security and patient outcomes."}}
{"id": "2507.05213", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.05213", "abs": "https://arxiv.org/abs/2507.05213", "authors": ["Max Gao", "Michael Collins", "Ricky Mok", "kc Claffy"], "title": "Hunting in the Dark: Metrics for Early Stage Traffic Discovery", "comment": "12 pages, 8 figures", "summary": "Threat hunting is an operational security process where an expert analyzes\ntraffic, applying knowledge and lightweight tools on unlabeled data in order to\nidentify and classify previously unknown phenomena. In this paper, we examine\nthreat hunting metrics and practice by studying the detection of Crackonosh, a\ncryptojacking malware package, has on various metrics for identifying its\nbehavior. Using a metric for discoverability, we model the ability of defenders\nto measure Crackonosh traffic as the malware population decreases, evaluate the\nstrength of various detection methods, and demonstrate how different darkspace\nsizes affect both the ability to track the malware, but enable emergent\nbehaviors by exploiting attacker mistakes.", "AI": {"tldr": "This paper analyzes threat hunting practices and metrics through the study of Crackonosh cryptojacking malware's detection patterns and the impact of darkspace size on tracking capabilities.", "motivation": "The paper addresses challenges in threat hunting by exploring detection effectiveness as malware populations decrease and how network environment factors (darkspace size) influence both defender tracking capabilities and attacker behavior patterns.", "method": "The authors model defender capabilities using a discoverability metric, evaluate detection methods at varying malware population levels, and conduct experiments with different darkspace sizes to assess their impact on traffic tracking and detection accuracy.", "result": "Findings show detection strength decreases with smaller malware populations, while larger darkspaces maintain tracking effectiveness while enabling emergent defender advantages through attacker errors in network navigation.", "conclusion": "Dynamic network environments significantly affect threat hunting outcomes, with larger darkspaces proving advantageous for detection by creating detectable patterns from attacker mistakes, rather than hindering tracking through evasion."}}
