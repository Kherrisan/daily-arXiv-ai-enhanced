{"id": "2509.12233", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.12233", "abs": "https://arxiv.org/abs/2509.12233", "authors": ["Meryem Malak Dif", "Mouhamed Amine Bouchiha", "Abdelaziz Amara Korba", "Yacine Ghamri-Doudane"], "title": "Towards Trustworthy Agentic IoEV: AI Agents for Explainable Cyberthreat Mitigation and State Analytics", "comment": "10 pages, 7 figures, Accepted at LCN'25", "summary": "The Internet of Electric Vehicles (IoEV) envisions a tightly coupled\necosystem of electric vehicles (EVs), charging infrastructure, and grid\nservices, yet it remains vulnerable to cyberattacks, unreliable battery-state\npredictions, and opaque decision processes that erode trust and performance. To\naddress these challenges, we introduce a novel Agentic Artificial Intelligence\n(AAI) framework tailored for IoEV, where specialized agents collaborate to\ndeliver autonomous threat mitigation, robust analytics, and interpretable\ndecision support. Specifically, we design an AAI architecture comprising\ndedicated agents for cyber-threat detection and response at charging stations,\nreal-time State of Charge (SoC) estimation, and State of Health (SoH) anomaly\ndetection, all coordinated through a shared, explainable reasoning layer;\ndevelop interpretable threat-mitigation mechanisms that proactively identify\nand neutralize attacks on both physical charging points and learning\ncomponents; propose resilient SoC and SoH models that leverage continuous and\nadversarial-aware learning to produce accurate, uncertainty-aware forecasts\nwith human-readable explanations; and implement a three-agent pipeline, where\neach agent uses LLM-driven reasoning and dynamic tool invocation to interpret\nintent, contextualize tasks, and execute formal optimizations for user-centric\nassistance. Finally, we validate our framework through comprehensive\nexperiments across diverse IoEV scenarios, demonstrating significant\nimprovements in security and prediction accuracy. All datasets, models, and\ncode will be released publicly."}
{"id": "2509.12290", "categories": ["cs.CR", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.12290", "abs": "https://arxiv.org/abs/2509.12290", "authors": ["Jonas C. Ditz", "Veronika Lazar", "Elmar Lichtme√ü", "Carola Plesch", "Matthias Heck", "Kevin Baum", "Markus Langer"], "title": "Secure Human Oversight of AI: Exploring the Attack Surface of Human Oversight", "comment": null, "summary": "Human oversight of AI is promoted as a safeguard against risks such as\ninaccurate outputs, system malfunctions, or violations of fundamental rights,\nand is mandated in regulation like the European AI Act. Yet debates on human\noversight have largely focused on its effectiveness, while overlooking a\ncritical dimension: the security of human oversight. We argue that human\noversight creates a new attack surface within the safety, security, and\naccountability architecture of AI operations. Drawing on cybersecurity\nperspectives, we analyze attack vectors that threaten the requirements of\neffective human oversight, thereby undermining the safety of AI operations.\nSuch attacks may target the AI system, its communication with oversight\npersonnel, or the personnel themselves. We then outline hardening strategies to\nmitigate these risks. Our contributions are: (1) introducing a security\nperspective on human oversight, and (2) providing an overview of attack vectors\nand hardening strategies to enable secure human oversight of AI."}
{"id": "2509.12291", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.12291", "abs": "https://arxiv.org/abs/2509.12291", "authors": ["Ouassim Karrakchou", "Alaa Zniber", "Anass Sebbar", "Mounir Ghogho"], "title": "Collaborative P4-SDN DDoS Detection and Mitigation with Early-Exit Neural Networks", "comment": "Accepted for publication in IEEE Globecom 2025", "summary": "Distributed Denial of Service (DDoS) attacks pose a persistent threat to\nnetwork security, requiring timely and scalable mitigation strategies. In this\npaper, we propose a novel collaborative architecture that integrates a\nP4-programmable data plane with an SDN control plane to enable real-time DDoS\ndetection and response. At the core of our approach is a split early-exit\nneural network that performs partial inference in the data plane using a\nquantized Convolutional Neural Network (CNN), while deferring uncertain cases\nto a Gated Recurrent Unit (GRU) module in the control plane. This design\nenables high-speed classification at line rate with the ability to escalate\nmore complex flows for deeper analysis. Experimental evaluation using\nreal-world DDoS datasets demonstrates that our approach achieves high detection\naccuracy with significantly reduced inference latency and control plane\noverhead. These results highlight the potential of tightly coupled ML-P4-SDN\nsystems for efficient, adaptive, and low-latency DDoS defense."}
{"id": "2509.12386", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12386", "abs": "https://arxiv.org/abs/2509.12386", "authors": ["Asim Waheed", "Vasisht Duddu", "Rui Zhang", "Sebastian Szyller", "N. Asokan"], "title": "Amulet: a Python Library for Assessing Interactions Among ML Defenses and Risks", "comment": "12 pages, 4 figures", "summary": "ML models are susceptible to risks to security, privacy, and fairness.\nSeveral defenses are designed to protect against their intended risks, but can\ninadvertently affect susceptibility to other unrelated risks, known as\nunintended interactions. Several jurisdictions are preparing ML regulatory\nframeworks that require ML practitioners to assess the susceptibility of ML\nmodels to different risks. A library for valuating unintended interactions that\ncan be used by (a) practitioners to evaluate unintended interactions at scale\nprior to model deployment and (b) researchers to design defenses which do not\nsuffer from an unintended increase in unrelated risks. Ideally, such a library\nshould be i) comprehensive by including representative attacks, defenses and\nmetrics for different risks, ii) extensible to new modules due to its modular\ndesign, iii) consistent with a user-friendly API template for inputs and\noutputs, iv) applicable to evaluate previously unexplored unintended\ninteractions. We present AMULET, a Python library that covers risks to\nsecurity, privacy, and fairness, which satisfies all these requirements. AMULET\ncan be used to evaluate unexplored unintended interactions, compare\neffectiveness between defenses or attacks, and include new attacks and\ndefenses."}
{"id": "2509.12395", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12395", "abs": "https://arxiv.org/abs/2509.12395", "authors": ["Yash Mundhra", "Max Valk", "Maliheh Izadi"], "title": "Evaluating Large Language Models for Functional and Maintainable Code in Industrial Settings: A Case Study at ASML", "comment": "Accepted in the 40th IEEE/ACM International Conference on Automated\n  Software Engineering, ASE 2025 (Industry track)", "summary": "Large language models have shown impressive performance in various domains,\nincluding code generation across diverse open-source domains. However, their\napplicability in proprietary industrial settings, where domain-specific\nconstraints and code interdependencies are prevalent, remains largely\nunexplored. We present a case study conducted in collaboration with the\nleveling department at ASML to investigate the performance of LLMs in\ngenerating functional, maintainable code within a closed, highly specialized\nsoftware environment.\n  We developed an evaluation framework tailored to ASML's proprietary codebase\nand introduced a new benchmark. Additionally, we proposed a new evaluation\nmetric, build@k, to assess whether LLM-generated code successfully compiles and\nintegrates within real industrial repositories. We investigate various\nprompting techniques, compare the performance of generic and code-specific\nLLMs, and examine the impact of model size on code generation capabilities,\nusing both match-based and execution-based metrics. The findings reveal that\nprompting techniques and model size have a significant impact on output\nquality, with few-shot and chain-of-thought prompting yielding the highest\nbuild success rates. The difference in performance between the code-specific\nLLMs and generic LLMs was less pronounced and varied substantially across\ndifferent model families."}
{"id": "2509.12462", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.12462", "abs": "https://arxiv.org/abs/2509.12462", "authors": ["Chuxu Song", "Dheekshith Dev Manohar Mekala", "Hao Wang", "Richard Martin"], "title": "Redefining Website Fingerprinting Attacks With Multiagent LLMs", "comment": null, "summary": "Website Fingerprinting (WFP) uses deep learning models to classify encrypted\nnetwork traffic to infer visited websites. While historically effective, prior\nmethods fail to generalize to modern web environments. Single-page applications\n(SPAs) eliminate the paradigm of websites as sets of discrete pages,\nundermining page-based classification, and traffic from scripted browsers lacks\nthe behavioral richness seen in real user sessions. Our study reveals that\nusers exhibit highly diverse behaviors even on the same website, producing\ntraffic patterns that vary significantly across individuals. This behavioral\nentropy makes WFP a harder problem than previously assumed and highlights the\nneed for larger, more diverse, and representative datasets to achieve robust\nperformance. To address this, we propose a new paradigm: we drop\nsession-boundaries in favor of contiguous traffic segments and develop a\nscalable data generation pipeline using large language models (LLM) agents.\nThese multi-agent systems coordinate decision-making and browser interaction to\nsimulate realistic, persona-driven browsing behavior at 3--5x lower cost than\nhuman collection. We evaluate nine state-of-the-art WFP models on traffic from\n20 modern websites browsed by 30 real users, and compare training performance\nacross human, scripted, and LLM-generated datasets. All models achieve under\n10\\% accuracy when trained on scripted traffic and tested on human data. In\ncontrast, LLM-generated traffic boosts accuracy into the 80\\% range,\ndemonstrating strong generalization to real-world traces. Our findings indicate\nthat for modern WFP, model performance is increasingly bottlenecked by data\nquality, and that scalable, semantically grounded synthetic traffic is\nessential for capturing the complexity of real user behavior."}
{"id": "2509.12421", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12421", "abs": "https://arxiv.org/abs/2509.12421", "authors": ["Hao Li", "Hicham Masri", "Filipe R. Cogo", "Abdul Ali Bangash", "Bram Adams", "Ahmed E. Hassan"], "title": "Understanding Prompt Management in GitHub Repositories: A Call for Best Practices", "comment": null, "summary": "The rapid adoption of foundation models (e.g., large language models) has\ngiven rise to promptware, i.e., software built using natural language prompts.\nEffective management of prompts, such as organization and quality assurance, is\nessential yet challenging. In this study, we perform an empirical analysis of\n24,800 open-source prompts from 92 GitHub repositories to investigate prompt\nmanagement practices and quality attributes. Our findings reveal critical\nchallenges such as considerable inconsistencies in prompt formatting,\nsubstantial internal and external prompt duplication, and frequent readability\nand spelling issues. Based on these findings, we provide actionable\nrecommendations for developers to enhance the usability and maintainability of\nopen-source prompts within the rapidly evolving promptware ecosystem."}
{"id": "2509.12478", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.12478", "abs": "https://arxiv.org/abs/2509.12478", "authors": ["Kathrin H√∂velmanns", "Daan Planken", "Christian Schaffner", "Sebastian R. Verschoor"], "title": "QKD Oracles for Authenticated Key Exchange", "comment": "64 pages, 17 figures", "summary": "Authenticated Key Exchange (AKE) establishes shared ('symmetric')\ncryptographic keys which are essential for secure online communication. AKE\nprotocols can be constructed from public-key cryptography like Key\nEncapsulation Mechanisms (KEMs). Another approach is to use Quantum Key\nDistribution (QKD) to establish a symmetric key, which uses quantum\ncommunication. Combining post-quantum AKE and QKD appropriately may provide\nsecurity against quantum attacks even if only one of the two approaches turns\nout to be secure.\n  We provide an extensive review of existing security analyses for combined AKE\nand their formal security models, and identify some gaps in their treatment of\nQKD key IDs. In particular, improper handling of QKD key IDs leads to\nDependent-Key attacks on AKE.\n  As our main conceptual contribution, we model QKD as an oracle that closely\nresembles the standard ETSI 014 QKD interface. We demonstrate the usability of\nour QKD oracle for cryptographic security analyses by integrating it into a\nprominent security model for AKE, called CK+ model, thereby obtaining a\nsecurity model for combined AKE that catches Dependent-Key attacks. In this\nmodel, we formally prove security of a new protocol that combines QKD with a\ntriple-KEM handshake. This is the first provably secure hybrid protocol that\nmaintains information-theoretic security of QKD."}
{"id": "2509.12443", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.12443", "abs": "https://arxiv.org/abs/2509.12443", "authors": ["Sparsh Gupta", "Kamalavasan Kamalakkannan", "Maxim Moraru", "Galen Shipman", "Patrick Diehl"], "title": "From Legacy Fortran to Portable Kokkos:An Autonomous Agentic AI Workflow", "comment": null, "summary": "Scientific applications continue to rely on legacy Fortran codebases\noriginally developed for homogeneous, CPU-based systems. As High-Performance\nComputing (HPC) shifts toward heterogeneous GPU-accelerated architectures, many\naccelerators lack native Fortran bindings, creating an urgent need to modernize\nlegacy codes for portability. Frameworks like Kokkos provide performance\nportability and a single-source C++ abstraction, but manual Fortran-to-Kokkos\nporting demands significant expertise and time. Large language models (LLMs)\nhave shown promise in source-to-source code generation, yet their use in fully\nautonomous workflows for translating and optimizing parallel code remains\nlargely unexplored, especially for performance portability across diverse\nhardware.\n  This paper presents an agentic AI workflow where specialized LLM \"agents\"\ncollaborate to translate, validate, compile, run, test, debug, and optimize\nFortran kernels into portable Kokkos C++ programs. Results show the pipeline\nmodernizes a range of benchmark kernels, producing performance-portable Kokkos\ncodes across hardware partitions. Paid OpenAI models such as GPT-5 and\no4-mini-high executed the workflow for only a few U.S. dollars, generating\noptimized codes that surpassed Fortran baselines, whereas open-source models\nlike Llama4-Maverick often failed to yield functional codes.\n  This work demonstrates the feasibility of agentic AI for Fortran-to-Kokkos\ntransformation and offers a pathway for autonomously modernizing legacy\nscientific applications to run portably and efficiently on diverse\nsupercomputers. It further highlights the potential of LLM-driven agentic\nsystems to perform structured, domain-specific reasoning tasks in scientific\nand systems-oriented applications."}
{"id": "2509.12494", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.12494", "abs": "https://arxiv.org/abs/2509.12494", "authors": ["Naifeng Zhang", "Sophia Fu", "Franz Franchetti"], "title": "Towards Closing the Performance Gap for Cryptographic Kernels Between CPUs and Specialized Hardware", "comment": "Accepted at the IEEE/ACM International Symposium on Microarchitecture\n  (MICRO), 2025", "summary": "Specialized hardware like application-specific integrated circuits (ASICs)\nremains the primary accelerator type for cryptographic kernels based on large\ninteger arithmetic. Prior work has shown that commodity and server-class GPUs\ncan achieve near-ASIC performance for these workloads. However, achieving\ncomparable performance on CPUs remains an open challenge. This work\ninvestigates the following question: How can we narrow the performance gap\nbetween CPUs and specialized hardware for key cryptographic kernels like basic\nlinear algebra subprograms (BLAS) operations and the number theoretic transform\n(NTT)?\n  To this end, we develop an optimized scalar implementation of these kernels\nfor x86 CPUs at the per-core level. We utilize SIMD instructions (specifically\nAVX2 and AVX-512) to further improve performance, achieving an average speedup\nof 38 times and 62 times over state-of-the-art CPU baselines for NTTs and BLAS\noperations, respectively. To narrow the gap further, we propose a small AVX-512\nextension, dubbed multi-word extension (MQX), which delivers substantial\nspeedup with only three new instructions and minimal proposed hardware\nmodifications. MQX cuts the slowdown relative to ASICs to as low as 35 times on\na single CPU core. Finally, we perform a roofline analysis to evaluate the peak\nperformance achievable with MQX when scaled across an entire multi-core CPU.\nOur results show that, with MQX, top-tier server-grade CPUs can approach the\nperformance of state-of-the-art ASICs for cryptographic workloads."}
{"id": "2509.12466", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.12466", "abs": "https://arxiv.org/abs/2509.12466", "authors": ["Satwik Ghanta", "Peggy Gregory", "Gul Calikli"], "title": "Perspectives, Needs and Challenges for Sustainable Software Engineering Teams: A FinServ Case Study", "comment": null, "summary": "Sustainable Software Engineering (SSE) is slowly becoming an industry need\nfor reasons including reputation enhancement, improved profits and more\nefficient practices. However, SSE has many definitions, and this is a challenge\nfor organisations trying to build a common and broadly agreed understanding of\nthe term. Although much research effort has gone into identifying general SSE\npractices, there is a gap in understanding the sustainability needs of specific\norganisational contexts, such as financial services, which are highly\ndata-driven, operate under strict regulatory requirements, and handle millions\nof transactions day to day. To address this gap, our research focuses on a\nfinancial services company (FinServCo) that invited us to investigate\nperceptions of sustainability in their IT function: how it could be put into\npractice, who is responsible for it, and what the challenges are. We conducted\nan exploratory qualitative case study using interviews and a focus group with\nsix higher management employees and 16 software engineers comprising various\nexperience levels from junior developers to team leaders. Our study found a\nclear divergence in how sustainability is perceived between organisational\nlevels. Higher management emphasised technical and economic sustainability,\nfocusing on cloud migration and business continuity through data availability.\nIn contrast, developers highlighted human-centric concerns such as workload\nmanagement and stress reduction. Scepticism toward organisational initiatives\nwas also evident, with some developers viewing them as a PR strategy. Many\nparticipants expressed a preference for a dedicated sustainability team,\ndrawing analogies to internal structures for security governance. The\ndisconnect between organisational goals and individual developer needs\nhighlights the importance of context-sensitive, co-designed interventions."}
{"id": "2509.12535", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.12535", "abs": "https://arxiv.org/abs/2509.12535", "authors": ["Ben Dong", "Hui Feng", "Qian Wang"], "title": "Exploiting Timing Side-Channels in Quantum Circuits Simulation Via ML-Based Methods", "comment": null, "summary": "As quantum computing advances, quantum circuit simulators serve as critical\ntools to bridge the current gap caused by limited quantum hardware\navailability. These simulators are typically deployed on cloud platforms, where\nusers submit proprietary circuit designs for simulation. In this work, we\ndemonstrate a novel timing side-channel attack targeting cloud-based quantum\nsimulators. A co-located malicious process can observe fine-grained execution\ntiming patterns to extract sensitive information about concurrently running\nquantum circuits. We systematically analyze simulator behavior using the\nQASMBench benchmark suite, profiling timing and memory characteristics across\nvarious circuit executions. Our experimental results show that timing profiles\nexhibit circuit-dependent patterns that can be effectively classified using\npattern recognition techniques, enabling the adversary to infer circuit\nidentities and compromise user confidentiality. We were able to achieve 88% to\n99.9% identification rate of quantum circuits based on different datasets. This\nwork highlights previously unexplored security risks in quantum simulation\nenvironments and calls for stronger isolation mechanisms to protect user\nworkloads"}
{"id": "2509.12491", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.12491", "abs": "https://arxiv.org/abs/2509.12491", "authors": ["Veronica Pimenova", "Sarah Fakhoury", "Christian Bird", "Margaret-Anne Storey", "Madeline Endres"], "title": "Good Vibrations? A Qualitative Study of Co-Creation, Communication, Flow, and Trust in Vibe Coding", "comment": "19 pages, 2 figures", "summary": "Vibe coding, a term coined by Andrej Karpathy in February 2025, has quickly\nbecome a compelling and controversial natural language programming paradigm in\nAI-assisted software development. Centered on iterative co-design with an AI\nassistant, vibe coding emphasizes flow and experimentation over strict upfront\nspecification. While initial studies have begun to explore this paradigm, most\nfocus on analyzing code artifacts or proposing theories with limited empirical\nbacking. There remains a need for a grounded understanding of vibe coding as it\nis perceived and experienced by developers. We present the first systematic\nqualitative investigation of vibe coding perceptions and practice. Drawing on\nover 190,000 words from semi-structured interviews, Reddit threads, and\nLinkedIn posts, we characterize what vibe coding is, why and how developers use\nit, where it breaks down, and which emerging practices aim to support it. We\npropose a qualitatively grounded theory of vibe coding centered on\nconversational interaction with AI, co-creation, and developer flow and joy. We\nfind that AI trust regulates movement along a continuum from delegation to\nco-creation and supports the developer experience by sustaining flow. We\nsurface recurring pain points and risks in areas including specification,\nreliability, debugging, latency, code review burden, and collaboration. We also\npresent best practices that have been discovered and shared to mitigate these\nchallenges. We conclude with implications for the future of AI dev tools and\ndirections for researchers investigating vibe coding."}
{"id": "2509.12574", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12574", "abs": "https://arxiv.org/abs/2509.12574", "authors": ["Siyuan Bao", "Ying Shi", "Zhiguang Yang", "Hanzhou Wu", "Xinpeng Zhang"], "title": "Yet Another Watermark for Large Language Models", "comment": "https://scholar.google.com/citations?hl=en&user=IdiF7M0AAAAJ", "summary": "Existing watermarking methods for large language models (LLMs) mainly embed\nwatermark by adjusting the token sampling prediction or post-processing,\nlacking intrinsic coupling with LLMs, which may significantly reduce the\nsemantic quality of the generated marked texts. Traditional watermarking\nmethods based on training or fine-tuning may be extendable to LLMs. However,\nmost of them are limited to the white-box scenario, or very time-consuming due\nto the massive parameters of LLMs. In this paper, we present a new watermarking\nframework for LLMs, where the watermark is embedded into the LLM by\nmanipulating the internal parameters of the LLM, and can be extracted from the\ngenerated text without accessing the LLM. Comparing with related methods, the\nproposed method entangles the watermark with the intrinsic parameters of the\nLLM, which better balances the robustness and imperceptibility of the\nwatermark. Moreover, the proposed method enables us to extract the watermark\nunder the black-box scenario, which is computationally efficient for use.\nExperimental results have also verified the feasibility, superiority and\npracticality. This work provides a new perspective different from mainstream\nworks, which may shed light on future research."}
{"id": "2509.12629", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.12629", "abs": "https://arxiv.org/abs/2509.12629", "authors": ["Zhihong Sun", "Jia Li", "Yao Wan", "Chuanyi Li", "Hongyu Zhang", "Zhi jin", "Ge Li", "Hong Liu", "Chen Lyu", "Songlin Hu"], "title": "Ensembling Large Language Models for Code Vulnerability Detection: An Empirical Evaluation", "comment": "24 pages", "summary": "Code vulnerability detection is crucial for ensuring the security and\nreliability of modern software systems. Recently, Large Language Models (LLMs)\nhave shown promising capabilities in this domain. However, notable\ndiscrepancies in detection results often arise when analyzing identical code\nsegments across different training stages of the same model or among\narchitecturally distinct LLMs. While such inconsistencies may compromise\ndetection stability, they also highlight a key opportunity: the latent\ncomplementarity among models can be harnessed through ensemble learning to\ncreate more robust vulnerability detection systems. In this study, we explore\nthe potential of ensemble learning to enhance the performance of LLMs in source\ncode vulnerability detection. We conduct comprehensive experiments involving\nfive LLMs (i.e., DeepSeek-Coder-6.7B, CodeLlama-7B, CodeLlama-13B,\nCodeQwen1.5-7B, and StarCoder2-15B), using three ensemble strategies (i.e.,\nBagging, Boosting, and Stacking). These experiments are carried out across\nthree widely adopted datasets (i.e., Devign, ReVeal, and BigVul). Inspired by\nMixture of Experts (MoE) techniques, we further propose Dynamic Gated Stacking\n(DGS), a Stacking variant tailored for vulnerability detection. Our results\ndemonstrate that ensemble approaches can significantly improve detection\nperformance, with Boosting excelling in scenarios involving imbalanced\ndatasets. Moreover, DGS consistently outperforms traditional Stacking,\nparticularly in handling class imbalance and multi-class classification tasks.\nThese findings offer valuable insights into building more reliable and\neffective LLM-based vulnerability detection systems through ensemble learning."}
{"id": "2509.12582", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.12582", "abs": "https://arxiv.org/abs/2509.12582", "authors": ["David Adei", "Varun Madathil", "Nithin Shyam S.", "Bradley Reaves"], "title": "Secure and Efficient Out-of-band Call Metadata Transmission", "comment": "20 pages", "summary": "The STIR/SHAKEN (S/S) attestation Framework mandated by the United States,\nCanada, and France to combat pervasive telephone abuse has not achieved its\ngoals, partly because legacy non-VoIP infrastructure could not participate. The\nindustry solution to extend S/S broadcasts sensitive metadata of every non-VoIP\ncall in plaintext to every third party required to facilitate the system. It\nhas no mechanism to determine whether a provider's request for call data is\nappropriate, nor can it ensure that every copy of that call data is unavailable\nafter its specified expiration. It threatens subscriber privacy and provider\nconfidentiality.\n  In this paper, we present Sidecar, a distributed, privacy-preserving system\nwith tunable decentralization that securely extends S/S across all telephone\nnetwork technologies. We introduce the notion of secure out-of-band signaling\nfor telephony and formalize its system and security requirements. We then\ndesign novel, scalable protocols that realize these requirements and prove\ntheir security within the Universal Composability framework. Finally, we\ndemonstrate Sidecar's efficiency with our open-sourced reference\nimplementation. Compared to the current solution, Sidecar 1) protects the\nconfidentiality of subscriber identity and provider trade secrets, 2)\nguarantees record expiration as long as a single node handling a record is\nhonest, 3) reduces resource requirements while providing virtually identical\ncall-setup times and equivalent or better uptimes, and 4) enables secure\npay-per-use billing and integrates mechanisms to mitigate and detect\nmisbehavior. Moreover, Sidecar can be extended to provide the same security\nguarantees for arbitrary call metadata. Not only is Sidecar a superior\napproach, it is also a transformative tool to retrofit fragmented global\ntelephony and enable future improvements, such as stronger call authentication\nand Branded Calling."}
{"id": "2509.12795", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.12795", "abs": "https://arxiv.org/abs/2509.12795", "authors": ["Yihua Chen", "Xingle Que", "Jiashuo Zhang", "Ting Chen", "Guangshun Li", "Jiachi Chen"], "title": "When Large Language Models Meet UAVs: How Far Are We?", "comment": null, "summary": "The integration of unmanned aerial vehicles (UAVs) and large language models\n(LLMs) has emerged as a research direction of growing interest, with the\npotential to address challenges in autonomous decision-making, human-UAV\ninteraction, and real-time adaptability. However, existing studies have\nremained largely in preliminary exploration with a limited understanding of\nreal-world practice, risking a misalignment between academic research and\npractical needs and hindering the translation of results. To examine and\naddress these potential challenges, we conducted an empirical study of 74\nselected papers and 56 public GitHub projects, identified nine task types for\nLLMs in UAV systems, and quantified their distribution. Our findings show that\nacademic research emphasizes theoretical modeling and task optimization with\ndispersed attention across tasks. In contrast, industrial projects focus on\nflight control, task planning, and human-machine interaction, prioritizing\noperability and efficiency. To further capture industry perspectives, we\ndistributed an online questionnaire. We obtained 52 valid responses: 40.4% of\npractitioners have attempted to apply LLMs to UAV tasks. We further identify\nfactors that impede real-world integration, including technological maturity,\nperformance, safety, cost, and other considerations. Finally, we highlight\nchallenges for future development and provide recommendations."}
{"id": "2509.12649", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12649", "abs": "https://arxiv.org/abs/2509.12649", "authors": ["Kiho Lee", "Jungkon Kim", "Doowon Kim", "Hyoungshick Kim"], "title": "A Systematic Evaluation of Parameter-Efficient Fine-Tuning Methods for the Security of Code LLMs", "comment": "25 pages", "summary": "Code-generating Large Language Models (LLMs) significantly accelerate\nsoftware development. However, their frequent generation of insecure code\npresents serious risks. We present a comprehensive evaluation of seven\nparameter-efficient fine-tuning (PEFT) techniques, demonstrating substantial\ngains in secure code generation without compromising functionality. Our\nresearch identifies prompt-tuning as the most effective PEFT method, achieving\nan 80.86% Overall-Secure-Rate on CodeGen2 16B, a 13.5-point improvement over\nthe 67.28% baseline. Optimizing decoding strategies through sampling\ntemperature further elevated security to 87.65%. This equates to a reduction of\napproximately 203,700 vulnerable code snippets per million generated. Moreover,\nprompt and prefix tuning increase robustness against poisoning attacks in our\nTrojanPuzzle evaluation, with strong performance against CWE-79 and CWE-502\nattack vectors. Our findings generalize across Python and Java, confirming\nprompt-tuning's consistent effectiveness. This study provides essential\ninsights and practical guidance for building more resilient software systems\nwith LLMs."}
{"id": "2509.12798", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12798", "abs": "https://arxiv.org/abs/2509.12798", "authors": ["Nenad Petrovic", "Lukasz Mazur", "Alois Knoll"], "title": "LLM-Based Approach for Enhancing Maintainability of Automotive Architectures", "comment": null, "summary": "There are many bottlenecks that decrease the flexibility of automotive\nsystems, making their long-term maintenance, as well as updates and extensions\nin later lifecycle phases increasingly difficult, mainly due to long\nre-engineering, standardization, and compliance procedures, as well as\nheterogeneity and numerosity of devices and underlying software components\ninvolved. In this paper, we explore the potential of Large Language Models\n(LLMs) when it comes to the automation of tasks and processes that aim to\nincrease the flexibility of automotive systems. Three case studies towards\nachieving this goal are considered as outcomes of early-stage research: 1)\nupdates, hardware abstraction, and compliance, 2) interface compatibility\nchecking, and 3) architecture modification suggestions. For proof-of-concept\nimplementation, we rely on OpenAI's GPT-4o model."}
{"id": "2509.12877", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.12877", "abs": "https://arxiv.org/abs/2509.12877", "authors": ["Gustavo Banegas", "Andreas Hellenbrand", "Matheus Saldanha"], "title": "Hardened CTIDH: Dummy-Free and Deterministic CTIDH", "comment": null, "summary": "Isogeny-based cryptography has emerged as a promising postquantum\nalternative, with CSIDH and its constant-time variants CTIDH and dCTIDH\noffering efficient group-action protocols. However, CTIDH and dCTIDH rely on\ndummy operations in differential addition chains (DACs) and Matryoshka, which\ncan be exploitable by fault-injection attacks. In this work, we present the\nfirst dummy-free implementation of dCTIDH. Our approach combines two recent\nideas: DACsHUND, which enforces equal-length DACs within each batch without\npadding, and a reformulated Matryoshka structure that removes dummy\nmultiplications and validates all intermediate points. Our analysis shows that\nsmall primes such as 3, 5, and 7 severely restrict feasible DACsHUND\nconfigurations, motivating new parameter sets that exclude them. We implement\ndummy-free dCTIDH-2048-194 and dCTIDH-2048-205, achieving group action costs of\nroughly 357,000-362,000 Fp-multiplications, with median evaluation times of\n1.59-1.60 (Gcyc). These results do not surpass dC-TIDH, but they outperform\nCTIDH by roughly 5% while eliminating dummy operations entirely. Compared to\ndCSIDH, our construction is more than 4x faster. To the best of our knowledge,\nthis is the first efficient implementation of a CSIDH-like protocol that is\nsimultaneously deterministic, constant-time, and fully dummy-free."}
{"id": "2509.12809", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.12809", "abs": "https://arxiv.org/abs/2509.12809", "authors": ["Jinfeng Wen", "Jianshu Zhao", "Zixi Zhu", "Xiaomin Zhang", "Qi Liang", "Ao Zhou", "Shangguang Wang"], "title": "SateLight: A Satellite Application Update Framework for Satellite Computing", "comment": "This paper has been accepted for publication in ASE 2025!", "summary": "Satellite computing is an emerging paradigm that empowers satellites to\nperform onboard processing tasks (i.e., \\textit{satellite applications}),\nthereby reducing reliance on ground-based systems and improving responsiveness.\nHowever, enabling application software updates in this context remains a\nfundamental challenge due to application heterogeneity, limited\nground-to-satellite bandwidth, and harsh space conditions. Existing software\nupdate approaches, designed primarily for terrestrial systems, fail to address\nthese constraints, as they assume abundant computational capacity and stable\nconnectivity.\n  To address this gap, we propose SateLight, a practical and effective\nsatellite application update framework tailored for satellite computing.\nSateLight leverages containerization to encapsulate heterogeneous applications,\nenabling efficient deployment and maintenance. SateLight further integrates\nthree capabilities: (1) a content-aware differential strategy that minimizes\ncommunication data volume, (2) a fine-grained onboard update design that\nreconstructs target applications, and (3) a layer-based fault-tolerant recovery\nmechanism to ensure reliability under failure-prone space conditions.\nExperimental results on a satellite simulation environment with 10\nrepresentative satellite applications demonstrate that SateLight reduces\ntransmission latency by up to 91.18% (average 56.54%) compared to the best\ncurrently available baseline. It also consistently ensures 100% update\ncorrectness across all evaluated applications. Furthermore, a case study on a\nreal-world in-orbit satellite demonstrates the practicality of our approach."}
{"id": "2509.12879", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.12879", "abs": "https://arxiv.org/abs/2509.12879", "authors": ["Gustavo Banegas", "Ricardo Villanueva-Polanco"], "title": "A Fault Analysis on SNOVA", "comment": null, "summary": "SNOVA is a post-quantum cryptographic signature scheme known for its\nefficiency and compact key sizes, making it a second-round candidate in the\nNIST post-quantum cryptography standardization process. This paper presents a\ncomprehensive fault analysis of SNOVA, focusing on both permanent and transient\nfaults during signature generation. We introduce several fault injection\nstrategies that exploit SNOVA's structure to recover partial or complete secret\nkeys with limited faulty signatures. Our analysis reveals that as few as 22 to\n68 faulty signatures, depending on the security level, can suffice for key\nrecovery. We propose a novel fault-assisted reconciliation attack,\ndemonstrating its effectiveness in extracting the secret key space via solving\na quadratic polynomial system. Simulations show transient faults in key\nsignature generation steps can significantly compromise SNOVA's security. To\naddress these vulnerabilities, we propose a lightweight countermeasure to\nreduce the success of fault attacks without adding significant overhead. Our\nresults highlight the importance of fault-resistant mechanisms in post-quantum\ncryptographic schemes like SNOVA to ensure robustness."}
{"id": "2509.12973", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.12973", "abs": "https://arxiv.org/abs/2509.12973", "authors": ["Aamer Aljagthami", "Mohammed Banabila", "Musab Alshehri", "Mohammed Kabini", "Mohammad D. Alahmadi"], "title": "Evaluating Large Language Models for Code Translation: Effects of Prompt Language and Prompt Design", "comment": null, "summary": "Large language models (LLMs) have shown promise for automated source-code\ntranslation, a capability critical to software migration, maintenance, and\ninteroperability. Yet comparative evidence on how model choice, prompt design,\nand prompt language shape translation quality across multiple programming\nlanguages remains limited. This study conducts a systematic empirical\nassessment of state-of-the-art LLMs for code translation among C++, Java,\nPython, and C#, alongside a traditional baseline (TransCoder). Using BLEU and\nCodeBLEU, we quantify syntactic fidelity and structural correctness under two\nprompt styles (concise instruction and detailed specification) and two prompt\nlanguages (English and Arabic), with direction-aware evaluation across language\npairs. Experiments show that detailed prompts deliver consistent gains across\nmodels and translation directions, and English prompts outperform Arabic by\n13-15%. The top-performing model attains the highest CodeBLEU on challenging\npairs such as Java to C# and Python to C++. Our evaluation shows that each LLM\noutperforms TransCoder across the benchmark. These results demonstrate the\nvalue of careful prompt engineering and prompt language choice, and provide\npractical guidance for software modernization and cross-language\ninteroperability."}
{"id": "2509.12899", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.12899", "abs": "https://arxiv.org/abs/2509.12899", "authors": ["Zhen Li", "Zijian Zhang", "Wenjin Yang", "Pengbo Wang", "Zhaoqi Wang", "Meng Li", "Yan Wu", "Xuyang Liu", "Jing Sun", "Liehuang Zhu"], "title": "EByFTVeS: Efficient Byzantine Fault Tolerant-based Verifiable Secret-sharing in Distributed Privacy-preserving Machine Learning", "comment": null, "summary": "Verifiable Secret Sharing (VSS) has been widespread in Distributed\nPrivacy-preserving Machine Learning (DPML), because invalid shares from\nmalicious dealers or participants can be recognized by verifying the commitment\nof the received shares for honest participants. However, the consistency and\nthe computation and communitation burden of the VSS-based DPML schemes are\nstill two serious challenges. Although Byzantine Fault Tolerance (BFT) system\nhas been brought to guarantee the consistency and improve the efficiency of the\nexisting VSS-based DPML schemes recently, we explore an Adaptive Share Delay\nProvision (ASDP) strategy, and launch an ASDP-based Customized Model Poisoning\nAttack (ACuMPA) for certain participants in this paper. We theoretically\nanalyzed why the ASDP strategy and the ACuMPA algorithm works to the existing\nschemes. Next, we propose an [E]fficient [By]zantine [F]ault [T]olerant-based\n[Ve]rifiable [S]ecret-sharing (EByFTVeS) scheme. Finally, the validity,\nliveness, consistency and privacy of the EByFTVeS scheme are theoretically\nanalyzed, while the efficiency of the EByFTVeS scheme outperforms that of\nthe-state-of-art VSS scheme according to comparative experiment results."}
{"id": "2509.13023", "categories": ["cs.SE", "cs.AI", "I.2.2;D.2.5;D.2.4;D.4.6"], "pdf": "https://arxiv.org/pdf/2509.13023", "abs": "https://arxiv.org/abs/2509.13023", "authors": ["≈ûtefan-Claudiu Susan", "Andrei Arusoaie", "Dorel Lucanu"], "title": "Validating Solidity Code Defects using Symbolic and Concrete Execution powered by Large Language Models", "comment": "In Proceedings FROM 2025, arXiv:2509.11877", "summary": "The high rate of false alarms from static analysis tools and Large Language\nModels (LLMs) complicates vulnerability detection in Solidity Smart Contracts,\ndemanding methods that can formally or empirically prove the presence of\ndefects. This paper introduces a novel detection pipeline that integrates\ncustom Slither-based detectors, LLMs, Kontrol, and Forge. Our approach is\ndesigned to reliably detect defects and generate proofs. We currently perform\nexperiments with promising results for seven types of critical defects. We\ndemonstrate the pipeline's efficacy by presenting our findings for three\nvulnerabilities -- Reentrancy, Complex Fallback, and Faulty Access Control\nPolicies -- that are challenging for current verification solutions, which\noften generate false alarms or fail to detect them entirely. We highlight the\npotential of either symbolic or concrete execution in correctly classifying\nsuch code faults. By chaining these instruments, our method effectively\nvalidates true positives, significantly reducing the manual verification\nburden. Although we identify potential limitations, such as the inconsistency\nand the cost of LLMs, our findings establish a robust framework for combining\nheuristic analysis with formal verification to achieve more reliable and\nautomated smart contract auditing."}
{"id": "2509.12923", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12923", "abs": "https://arxiv.org/abs/2509.12923", "authors": ["Magnus Wiik Eckhoff", "Peter Marius Flydal", "Siem Peters", "Martin Eian", "Jonas Halvorsen", "Vasileios Mavroeidis", "Gudmund Grov"], "title": "A Graph-Based Approach to Alert Contextualisation in Security Operations Centres", "comment": null, "summary": "Interpreting the massive volume of security alerts is a significant challenge\nin Security Operations Centres (SOCs). Effective contextualisation is\nimportant, enabling quick distinction between genuine threats and benign\nactivity to prioritise what needs further analysis.This paper proposes a\ngraph-based approach to enhance alert contextualisation in a SOC by aggregating\nalerts into graph-based alert groups, where nodes represent alerts and edges\ndenote relationships within defined time-windows. By grouping related alerts,\nwe enable analysis at a higher abstraction level, capturing attack steps more\neffectively than individual alerts. Furthermore, to show that our format is\nwell suited for downstream machine learning methods, we employ Graph Matching\nNetworks (GMNs) to correlate incoming alert groups with historical incidents,\nproviding analysts with additional insights."}
{"id": "2509.13025", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13025", "abs": "https://arxiv.org/abs/2509.13025", "authors": ["Raul Zaharia", "Drago≈ü Gavrilu≈£", "Gheorghi≈£ƒÉ Mutu"], "title": "GView: A Survey of Binary Forensics via Visual, Semantic, and AI-Enhanced Analysis", "comment": "In Proceedings FROM 2025, arXiv:2509.11877", "summary": "Cybersecurity threats continue to become more sophisticated and diverse in\ntheir artifacts, boosting both their volume and complexity. To overcome those\nchallenges, we present GView, an open-source forensic analysis framework with\nvisual and AI-enhanced reasoning. It started with focus on the practical\ncybersecurity industry. It has evolved significantly, incorporating large\nlanguage models (LLMs) to dynamically enhance reasoning and ease the forensic\nworkflows. This paper surveys both the current state of GView with its\npublished papers alongside those that are in the publishing process. It also\nincludes its innovative use of logical inference through predicates and\ninference rules for both the analyzed documents and the user's actions for\nbetter suggestions. We highlight the extensible architecture, showcasing its\npotential as a bridge between the practical forensics worlds with the academic\nresearch."}
{"id": "2509.12937", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12937", "abs": "https://arxiv.org/abs/2509.12937", "authors": ["Johan Wahr√©us", "Ahmed Hussain", "Panos Papadimitratos"], "title": "Jailbreaking Large Language Models Through Content Concretization", "comment": "Accepted for presentation in the Conference on Game Theory and AI for\n  Security (GameSec) 2025", "summary": "Large Language Models (LLMs) are increasingly deployed for task automation\nand content generation, yet their safety mechanisms remain vulnerable to\ncircumvention through different jailbreaking techniques. In this paper, we\nintroduce \\textit{Content Concretization} (CC), a novel jailbreaking technique\nthat iteratively transforms abstract malicious requests into concrete,\nexecutable implementations. CC is a two-stage process: first, generating\ninitial LLM responses using lower-tier, less constrained safety filters models,\nthen refining them through higher-tier models that process both the preliminary\noutput and original prompt. We evaluate our technique using 350\ncybersecurity-specific prompts, demonstrating substantial improvements in\njailbreak Success Rates (SRs), increasing from 7\\% (no refinements) to 62\\%\nafter three refinement iterations, while maintaining a cost of 7.5\\textcent~per\nprompt. Comparative A/B testing across nine different LLM evaluators confirms\nthat outputs from additional refinement steps are consistently rated as more\nmalicious and technically superior. Moreover, manual code analysis reveals that\ngenerated outputs execute with minimal modification, although optimal\ndeployment typically requires target-specific fine-tuning. With eventual\nimproved harmful code generation, these results highlight critical\nvulnerabilities in current LLM safety frameworks."}
{"id": "2509.13055", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13055", "abs": "https://arxiv.org/abs/2509.13055", "authors": ["Youngkyoung Kim", "Sanghyeok Park", "Misoo Kim", "Gangho Yoon", "Eunseok Lee", "Simon S. Woo"], "title": "Automating Code Generation for Semiconductor Equipment Control from Developer Utterances with LLMs", "comment": null, "summary": "Semiconductors form the backbone of modern electronics, with their\nmanufacturing and testing relying on highly specialized equipment and\ndomain-specific programming languages. Equipment languages such as the\nAlgorithmic Pattern Generator (ALPG) are critical for precise hardware control\nbut are challenging to program due to their low-level syntax and steep learning\ncurve. While large language models (LLMs) have shown promise in generating\nhigh-level code from natural language, their effectiveness on low-level\nequipment languages remains limited. To address this, we propose Progressive\nKnowledge Enhancement (PKE), a novel multi-stage prompting framework that\nprogressively extracts and activates the latent knowledge within LLMs, guiding\nthem from simple to complex examples without extensive fine-tuning. Empirical\nevaluation on an industrial ALPG dataset shows that PKE significantly\noutperforms standard prompting and surpasses state-of-the-art methods in\ngenerating correct ALPG code, achieving 11.1\\% and 15.2\\% higher exact match\nscores compared to the second-best technique. Further analysis of individual\ncomponents confirms that progressive knowledge extraction based on difficulty\nenhances accuracy. Our study offer a practical approach to boosting LLM\ncapabilities for specialized low-level programming, supporting greater\nproductivity in semiconductor software development."}
{"id": "2509.12957", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.12957", "abs": "https://arxiv.org/abs/2509.12957", "authors": ["Yihao Guo", "Haoming Zhu", "Minghui Xu", "Xiuzhen Cheng", "Bin Xiao"], "title": "xRWA: A Cross-Chain Framework for Interoperability of Real-World Assets", "comment": null, "summary": "Real-World Assets (RWAs) have recently attracted increasing attention as a\nmeans of bridging traditional financial instruments with decentralized\ninfrastructures. By representing assets such as bonds, commodities, and real\nestate on blockchains, RWAs can enhance liquidity, broaden accessibility, and\nextend the scope of decentralized finance. Industry forecasts further suggest\nrapid growth of tokenized RWAs in the coming years, underscoring their\npotential role in the evolution of digital financial markets. However, when\ndeployed across multiple blockchains, RWAs face challenges such as repeated\nauthentication on different chains and inefficiency caused by multi-step\nsettlement protocols. To address these issues, we present a cross-chain\nframework for RWAs that emphasizes identity management, authentication, and\ninteraction. The framework integrates Decentralized Identifiers and Verifiable\nCredentials with customized attributes to support decentralized identification,\nand incorporates an authentication protocol based on Simplified Payment\nVerification to avoid redundant verification across chains. Furthermore, we\ndesign a cross-chain channel that enables the settlement of RWAs without\nrequiring channel closure, thereby improving operational efficiency. We\nimplement the framework and evaluate it through simulations, which confirm its\nfeasibility and demonstrate improvements in efficiency for RWAs in cross-chain\nsettings."}
{"id": "2509.13103", "categories": ["cs.SE", "D.2"], "pdf": "https://arxiv.org/pdf/2509.13103", "abs": "https://arxiv.org/abs/2509.13103", "authors": ["Santiago Matalonga", "Domenico Amalfitano", "Jean Carlo Rossa Hauck", "Mart√≠n Solari", "Guilherme H. Travassos"], "title": "Accelerating Discovery: Rapid Literature Screening with LLMs", "comment": "This version of the manuscript has been submitted to Empirical\n  Software Engieering Journal for consideration", "summary": "Background: Conducting Multi Vocal Literature Reviews (MVLRs) is often time\nand effort-intensive. Researchers must review and filter a large number of\nunstructured sources, which frequently contain sparse information and are\nunlikely to be included in the final study. Our experience conducting an MVLR\non Context-Aware Software Systems (CASS) Testing in the avionics domain\nexemplified this challenge, with over 8,000 highly heterogeneous documents\nrequiring review. Therefore, we developed a Large Language Model (LLM)\nassistant to support the search and filtering of documents. Aims: To develop\nand validate an LLM based tool that can support researchers in performing the\nsearch and filtering of documents for an MVLR without compromising the rigor of\nthe research protocol. Method: We applied sound engineering practices to\ndevelop an on-premises LLM-based tool incorporating Retrieval Augmented\nGeneration (RAG) to process candidate sources. Progress towards the aim was\nquantified using the Positive Percent Agreement (PPA) as the primary metric to\nensure the performance of the LLM based tool. Convenience sampling, supported\nby human judgment and statistical sampling, were used to verify and validate\nthe tool's quality-in-use. Results: The tool currently demonstrates a PPA\nagreement with human researchers of 90% for sources that are not relevant to\nthe study. Development details are shared to support domain-specific adaptation\nof the tool. Conclusions: Using LLM-based tools to support academic researchers\nin rigorous MVLR is feasible. These tools can free valuable time for\nhigher-level, abstract tasks. However, researcher participation remains\nessential to ensure that the tool supports thorough research."}
{"id": "2509.12979", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.12979", "abs": "https://arxiv.org/abs/2509.12979", "authors": ["Dipak K. Rabari", "Yogesh K. Meghrajani", "Laxmi S. Desai"], "title": "Universal share based quantum multi secret image sharing scheme", "comment": null, "summary": "Image security for information has become increasingly critical as internet\nbecome more prevalent due to hacking and unauthorized access. To ensure the\nsecurity of confidential image data, image encryption using visual cryptography\nplays a crucial role. To share multiple images using visual cryptography, the\ncompany organizer utilizes the concept of a universal or common share.\nLikewise, quantum computing is an emerging technology that facilitates secure\ncommunication. The ability of quantum computers to solve certain mathematical\nproblems efficiently threatens the security of many current encryption\nalgorithms. Hence, to leverage the strengths of quantum computing and visual\ncryptography, this research introduces a novel universal share-based quantum\nmulti-secret sharing technique for secure image communication. Quantum\ncomputing enables the scheme to exhibit high resilience to different\neavesdropping threats. Consequently, the proposed method offers robust security\nsolution for sharing confidential images across a range of applications,\nincluding enterprise data access and military communications."}
{"id": "2509.13117", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.13117", "abs": "https://arxiv.org/abs/2509.13117", "authors": ["Jukka Ruohonen", "Sani Abdullahi", "Abhishek Tiwari"], "title": "Vulnerability Patching Across Software Products and Software Components: A Case Study of Red Hat's Product Portfolio", "comment": "Submitted to SecITC 2025", "summary": "Motivated by software maintenance and the more recent concept of security\ndebt, the paper presents a time series analysis of vulnerability patching of\nRed Hat's products and components between 1999 and 2024. According to the\nresults based on segmented regression analysis, the amounts of vulnerable\nproducts and components have not been stable; a linear trend describes many of\nthe series well. Nor do the amounts align well with trends characterizing\nvulnerabilities in general. There are also visible breakpoints indicating that\nthe linear trend is not universally applicable and that the growing security\ndebt may be stabilizing."}
{"id": "2509.13021", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13021", "abs": "https://arxiv.org/abs/2509.13021", "authors": ["Phung Duc Luong", "Le Tran Gia Bao", "Nguyen Vu Khai Tam", "Dong Huu Nguyen Khoa", "Nguyen Huu Quyen", "Van-Hau Pham", "Phan The Duy"], "title": "xOffense: An AI-driven autonomous penetration testing framework with offensive knowledge-enhanced LLMs and multi agent systems", "comment": "17 pages, 4 figures", "summary": "This work introduces xOffense, an AI-driven, multi-agent penetration testing\nframework that shifts the process from labor-intensive, expert-driven manual\nefforts to fully automated, machine-executable workflows capable of scaling\nseamlessly with computational infrastructure. At its core, xOffense leverages a\nfine-tuned, mid-scale open-source LLM (Qwen3-32B) to drive reasoning and\ndecision-making in penetration testing. The framework assigns specialized\nagents to reconnaissance, vulnerability scanning, and exploitation, with an\norchestration layer ensuring seamless coordination across phases. Fine-tuning\non Chain-of-Thought penetration testing data further enables the model to\ngenerate precise tool commands and perform consistent multi-step reasoning. We\nevaluate xOffense on two rigorous benchmarks: AutoPenBench and\nAI-Pentest-Benchmark. The results demonstrate that xOffense consistently\noutperforms contemporary methods, achieving a sub-task completion rate of\n79.17%, decisively surpassing leading systems such as VulnBot and PentestGPT.\nThese findings highlight the potential of domain-adapted mid-scale LLMs, when\nembedded within structured multi-agent orchestration, to deliver superior,\ncost-efficient, and reproducible solutions for autonomous penetration testing."}
{"id": "2509.13134", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13134", "abs": "https://arxiv.org/abs/2509.13134", "authors": ["Talaya Farasat", "Joachim Posegga"], "title": "Optimizing Code Embeddings and ML Classifiers for Python Source Code Vulnerability Detection", "comment": null, "summary": "In recent years, the growing complexity and scale of source code have\nrendered manual software vulnerability detection increasingly impractical. To\naddress this challenge, automated approaches leveraging machine learning and\ncode embeddings have gained substantial attention. This study investigates the\noptimal combination of code embedding techniques and machine learning\nclassifiers for vulnerability detection in Python source code. We evaluate\nthree embedding techniques, i.e., Word2Vec, CodeBERT, and GraphCodeBERT\nalongside two deep learning classifiers, i.e., Bidirectional Long Short-Term\nMemory (BiLSTM) networks and Convolutional Neural Networks (CNN). While CNN\npaired with GraphCodeBERT exhibits strong performance, the BiLSTM model using\nWord2Vec consistently achieves superior overall results. These findings suggest\nthat, despite the advanced architectures of recent models like CodeBERT and\nGraphCodeBERT, classical embeddings such as Word2Vec, when used with\nsequence-based models like BiLSTM, can offer a slight yet consistent\nperformance advantage. The study underscores the critical importance of\nselecting appropriate combinations of embeddings and classifiers to enhance the\neffectiveness of automated vulnerability detection systems, particularly for\nPython source code."}
{"id": "2509.13035", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.13035", "abs": "https://arxiv.org/abs/2509.13035", "authors": ["Dumitru-Bogdan Prelipcean", "CƒÉtƒÉlin Dima"], "title": "Bridging Threat Models and Detections: Formal Verification via CADP", "comment": "In Proceedings FROM 2025, arXiv:2509.11877", "summary": "Threat detection systems rely on rule-based logic to identify adversarial\nbehaviors, yet the conformance of these rules to high-level threat models is\nrarely verified formally. We present a formal verification framework that\nmodels both detection logic and attack trees as labeled transition systems\n(LTSs), enabling automated conformance checking via bisimulation and weak trace\ninclusion. Detection rules specified in the Generic Threat Detection Language\n(GTDL, a general-purpose detection language we formalize in this work) are\nassigned a compositional operational semantics, and threat models expressed as\nattack trees are interpreted as LTSs through a structural trace semantics. Both\nrepresentations are translated to LNT, a modeling language supported by the\nCADP toolbox. This common semantic domain enables systematic and automated\nverification of detection coverage. We evaluate our approach on real-world\nmalware scenarios such as LokiBot and Emotet and provide scalability analysis\nthrough parametric synthetic models. Results confirm that our methodology\nidentifies semantic mismatches between threat models and detection rules,\nsupports iterative refinement, and scales to realistic threat landscapes."}
{"id": "2509.13144", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13144", "abs": "https://arxiv.org/abs/2509.13144", "authors": ["Lingli Cao", "Shanshan Li", "Ying Fan", "Danyang Li", "Chenxing Zhong"], "title": "Towards the Next Generation of Software: Insights from Grey Literature on AI-Native Applications", "comment": null, "summary": "Background: The rapid advancement of large language models (LLMs) has given\nrise to AI-native applications, a new paradigm in software engineering that\nfundamentally redefines how software is designed, developed, and evolved.\nDespite their growing prominence, AI-native applications still lack a unified\nengineering definition and architectural blueprint, leaving practitioners\nwithout systematic guidance for system design, quality assurance, and\ntechnology selection.\n  Objective: This study seeks to establish a comprehensive understanding of\nAI-native applications by identifying their defining characteristics, key\nquality attributes, and typical technology stacks, as well as by clarifying the\nopportunities and challenges they present.\n  Method: We conducted a grey literature review, integrating conceptual\nperspectives retrieved from targeted Google and Bing searches with practical\ninsights derived from leading open-source projects on GitHub. A structured\nprotocol encompassing source selection, quality assessment, and thematic\nanalysis was applied to synthesize findings across heterogeneous sources.\n  Results: We finally identified 106 studies based on the selection criteria.\nThe analysis reveals that AI-native applications are distinguished by two core\npillars: the central role of AI as the system's intelligence paradigm and their\ninherently probabilistic, non-deterministic nature. Critical quality attributes\ninclude reliability, usability, performance efficiency, and AI-specific\nobservability. In addition, a typical technology stack has begun to emerge,\ncomprising LLM orchestration frameworks, vector databases, and AI-native\nobservability platforms. These systems emphasize response quality,\ncost-effectiveness, and outcome predictability, setting them apart from\nconventional software systems.\n  Conclusion: This study is the first to propose a dual-layered engineering\nblueprint..."}
{"id": "2509.13046", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13046", "abs": "https://arxiv.org/abs/2509.13046", "authors": ["Eyal German", "Daniel Samira", "Yuval Elovici", "Asaf Shabtai"], "title": "MIA-EPT: Membership Inference Attack via Error Prediction for Tabular Data", "comment": null, "summary": "Synthetic data generation plays an important role in enabling data sharing,\nparticularly in sensitive domains like healthcare and finance. Recent advances\nin diffusion models have made it possible to generate realistic, high-quality\ntabular data, but they may also memorize training records and leak sensitive\ninformation. Membership inference attacks (MIAs) exploit this vulnerability by\ndetermining whether a record was used in training. While MIAs have been studied\nin images and text, their use against tabular diffusion models remains\nunderexplored despite the unique risks of structured attributes and limited\nrecord diversity. In this paper, we introduce MIAEPT, Membership Inference\nAttack via Error Prediction for Tabular Data, a novel black-box attack\nspecifically designed to target tabular diffusion models. MIA-EPT constructs\nerrorbased feature vectors by masking and reconstructing attributes of target\nrecords, disclosing membership signals based on how well these attributes are\npredicted. MIA-EPT operates without access to the internal components of the\ngenerative model, relying only on its synthetic data output, and was shown to\ngeneralize across multiple state-of-the-art diffusion models. We validate\nMIA-EPT on three diffusion-based synthesizers, achieving AUC-ROC scores of up\nto 0.599 and TPR@10% FPR values of 22.0% in our internal tests. Under the MIDST\n2025 competition conditions, MIA-EPT achieved second place in the Black-box\nMulti-Table track (TPR@10% FPR = 20.0%). These results demonstrate that our\nmethod can uncover substantial membership leakage in synthetic tabular data,\nchallenging the assumption that synthetic data is inherently\nprivacy-preserving. Our code is publicly available at\nhttps://github.com/eyalgerman/MIA-EPT."}
{"id": "2509.13048", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.13048", "abs": "https://arxiv.org/abs/2509.13048", "authors": ["Jeremy Boy", "Antoon Purnal", "Anna P√§tschke", "Luca Wilke", "Thomas Eisenbarth"], "title": "SLasH-DSA: Breaking SLH-DSA Using an Extensible End-To-End Rowhammer Framework", "comment": null, "summary": "As quantum computing advances, PQC schemes are adopted to replace classical\nalgorithms. Among them is the SLH-DSA that was recently standardized by NIST\nand is favored for its conservative security foundations.\n  In this work, we present the first software-only universal forgery attack on\nSLH-DSA, leveraging Rowhammer-induced bit flips to corrupt the internal state\nand forge signatures. While prior work targeted embedded systems and required\nphysical access, our attack is software-only, targeting commodity desktop and\nserver hardware, significantly broadening the threat model. We demonstrate a\nfull end-to-end attack against all security levels of SLH-DSA in OpenSSL 3.5.1,\nachieving universal forgery for the highest security level after eight hours of\nhammering and 36 seconds of post-processing. Our post-processing is informed by\na novel complexity analysis that, given a concrete set of faulty signatures,\nidentifies the most promising computational path to pursue.\n  To enable the attack, we introduce Swage, a modular and extensible framework\nfor implementing end-to-end Rowhammer-based fault attacks. Swage abstracts and\nautomates key components of practical Rowhammer attacks. Unlike prior tooling,\nSwage is untangled from the attacked code, making it reusable and suitable for\nfrictionless analysis of different targets. Our findings highlight that even\ntheoretically sound PQC schemes can fail under real-world conditions,\nunderscoring the need for additional implementation hardening or hardware\ndefenses against Rowhammer."}
{"id": "2509.13072", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.13072", "abs": "https://arxiv.org/abs/2509.13072", "authors": ["Clara Maathuis", "Kasper Cools"], "title": "Digital Sovereignty Control Framework for Military AI-based Cyber Security", "comment": null, "summary": "In today's evolving threat landscape, ensuring digital sovereignty has become\nmandatory for military organizations, especially given their increased\ndevelopment and investment in AI-driven cyber security solutions. To this end,\na multi-angled framework is proposed in this article in order to define and\nassess digital sovereign control of data and AI-based models for military cyber\nsecurity. This framework focuses on aspects such as context, autonomy,\nstakeholder involvement, and mitigation of risks in this domain. Grounded on\nthe concepts of digital sovereignty and data sovereignty, the framework aims to\nprotect sensitive defence assets against threats such as unauthorized access,\nransomware, and supply-chain attacks. This approach reflects the multifaceted\nnature of digital sovereignty by preserving operational autonomy, assuring\nsecurity and safety, securing privacy, and fostering ethical compliance of both\nmilitary systems and decision-makers. At the same time, the framework addresses\ninteroperability challenges among allied forces, strategic and legal\nconsiderations, and the integration of emerging technologies by considering a\nmultidisciplinary approach that enhances the resilience and preservation of\ncontrol over (critical) digital assets. This is done by adopting a design\noriented research where systematic literature review is merged with critical\nthinking and analysis of field incidents in order to assure the effectivity and\nrealism of the framework proposed."}
{"id": "2509.13186", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.13186", "abs": "https://arxiv.org/abs/2509.13186", "authors": ["Aleksandr Nahapetyan", "Kanv Khare", "Kevin Schwarz", "Bradley Reaves", "Alexandros Kapravelos"], "title": "Characterizing Phishing Pages by JavaScript Capabilities", "comment": null, "summary": "In 2024, the Anti-Phishing Work Group identified over one million phishing\npages. Phishers achieve this scale by using phishing kits -- ready-to-deploy\nphishing websites -- to rapidly deploy phishing campaigns with specific data\nexfiltration, evasion, or mimicry techniques. In contrast, researchers and\ndefenders continue to fight phishing on a page-by-page basis and rely on manual\nanalysis to recognize static features for kit identification.\n  This paper aims to aid researchers and analysts by automatically\ndifferentiating groups of phishing pages based on the underlying kit,\nautomating a previously manual process, and enabling us to measure how popular\ndifferent client-side techniques are across these groups. For kit detection,\nour system has an accuracy of 97% on a ground-truth dataset of 548 kit families\ndeployed across 4,562 phishing URLs. On an unlabeled dataset, we leverage the\ncomplexity of 434,050 phishing pages' JavaScript logic to group them into\n11,377 clusters, annotating the clusters with what phishing techniques they\nemploy. We find that UI interactivity and basic fingerprinting are universal\ntechniques, present in 90% and 80% of the clusters, respectively. On the other\nhand, mouse detection via the browser's mouse API is among the rarest\nbehaviors, despite being used in a deployment of a 7-year-old open-source\nphishing kit. Our methods and findings provide new ways for researchers and\nanalysts to tackle the volume of phishing pages."}
{"id": "2509.13217", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.13217", "abs": "https://arxiv.org/abs/2509.13217", "authors": ["Eman Abu Ishgair", "Chinenye Okafor", "Marcela S. Melara", "Santiago Torres-Arias"], "title": "Trustworthy and Confidential SBOM Exchange", "comment": null, "summary": "Software Bills of Materials (SBOMs) have become a regulatory requirement for\nimproving software supply chain security and trust by means of transparency\nregarding components that make up software artifacts. However, enterprise and\nregulated software vendors commonly wish to restrict who can view confidential\nsoftware metadata recorded in their SBOMs due to intellectual property or\nsecurity vulnerability information. To address this tension between\ntransparency and confidentiality, we propose Petra, an SBOM exchange system\nthat empowers software vendors to interoperably compose and distribute redacted\nSBOM data using selective encryption. Petra enables software consumers to\nsearch redacted SBOMs for answers to specific security questions without\nrevealing information they are not authorized to access. Petra leverages a\nformat-agnostic, tamper-evident SBOM representation to generate efficient and\nconfidentiality-preserving integrity proofs, allowing interested parties to\ncryptographically audit and establish trust in redacted SBOMs. Exchanging\nredacted SBOMs in our Petra prototype requires less than 1 extra KB per SBOM,\nand SBOM decryption account for at most 1% of the performance overhead during\nan SBOM query."}
{"id": "2509.13117", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.13117", "abs": "https://arxiv.org/abs/2509.13117", "authors": ["Jukka Ruohonen", "Sani Abdullahi", "Abhishek Tiwari"], "title": "Vulnerability Patching Across Software Products and Software Components: A Case Study of Red Hat's Product Portfolio", "comment": "Submitted to SecITC 2025", "summary": "Motivated by software maintenance and the more recent concept of security\ndebt, the paper presents a time series analysis of vulnerability patching of\nRed Hat's products and components between 1999 and 2024. According to the\nresults based on segmented regression analysis, the amounts of vulnerable\nproducts and components have not been stable; a linear trend describes many of\nthe series well. Nor do the amounts align well with trends characterizing\nvulnerabilities in general. There are also visible breakpoints indicating that\nthe linear trend is not universally applicable and that the growing security\ndebt may be stabilizing."}
