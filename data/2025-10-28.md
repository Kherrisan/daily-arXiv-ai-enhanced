<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 37]
- [cs.SE](#cs.SE) [Total: 26]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [$Î´$-STEAL: LLM Stealing Attack with Local Differential Privacy](https://arxiv.org/abs/2510.21946)
*Kieu Dang,Phung Lai,NhatHai Phan,Yelong Shen,Ruoming Jin,Abdallah Khreishah*

Main category: cs.CR

TL;DR: The paper introduces a novel LLM model stealing attack, $\delta$-STEAL, which uses LDP-preserving noise injection during fine-tuning to bypass watermark detectors while maintaining high model utility. Experiments show a 96.95% success rate.


<details>
  <summary>Details</summary>
Motivation: Current LLM watermarking methods are insufficient against model stealing attacks that compromise intellectual property, especially for commercial models, necessitating a more effective solution.

Method: $\delta$-STEAL's method involves injecting noise into token embeddings of the adversary's model during fine-tuning to obfuscate watermark signals while maintaining model functionality through local differential privacy (LDP).

Result: The results show high success rates (up to 96.95%) with minimal impact on the adversary model's utility, demonstrating LDP's effectiveness in obscuring watermark information.

Conclusion: The $\delta$-STEAL attack demonstrates a critical weakness in watermarking as a method for IP protection, requiring the development of stronger defensive strategies.

Abstract: Large language models (LLMs) demonstrate remarkable capabilities across
various tasks. However, their deployment introduces significant risks related
to intellectual property. In this context, we focus on model stealing attacks,
where adversaries replicate the behaviors of these models to steal services.
These attacks are highly relevant to proprietary LLMs and pose serious threats
to revenue and financial stability. To mitigate these risks, the watermarking
solution embeds imperceptible patterns in LLM outputs, enabling model
traceability and intellectual property verification. In this paper, we study
the vulnerability of LLM service providers by introducing $\delta$-STEAL, a
novel model stealing attack that bypasses the service provider's watermark
detectors while preserving the adversary's model utility. $\delta$-STEAL
injects noise into the token embeddings of the adversary's model during
fine-tuning in a way that satisfies local differential privacy (LDP)
guarantees. The adversary queries the service provider's model to collect
outputs and form input-output training pairs. By applying LDP-preserving noise
to these pairs, $\delta$-STEAL obfuscates watermark signals, making it
difficult for the service provider to determine whether its outputs were used,
thereby preventing claims of model theft. Our experiments show that
$\delta$-STEAL with lightweight modifications achieves attack success rates of
up to $96.95\%$ without significantly compromising the adversary's model
utility. The noise scale in LDP controls the trade-off between attack
effectiveness and model utility. This poses a significant risk, as even robust
watermarks can be bypassed, allowing adversaries to deceive watermark detectors
and undermine current intellectual property protection methods.

</details>


### [2] [Towards Low-Latency and Adaptive Ransomware Detection Using Contrastive Learning](https://arxiv.org/abs/2510.21957)
*Zhixin Pan,Ziyu Shu,Amberbir Alemayoh*

Main category: cs.CR

TL;DR: This paper proposes a framework combining self-supervised contrastive learning and neural architecture search (NAS) to enhance ransomware detection by addressing limitations in existing methods.


<details>
  <summary>Details</summary>
Motivation: Ransomware poses significant cybersecurity challenges due to its rapid evolution, the necessity for early detection, and diversity, which traditional detection methods struggle with. Existing AI-based approaches are limited by ad-hoc feature dependencies, delayed response, and limited adaptability to new variants.

Method: The proposed framework includes (1) a contrastive learning setup using HPC for runtime behavior analysis, (2) a loss function for early-stage detection and reduced latency, and (3) a NAS framework for adaptive architecture construction.

Result: The method achieves significant improvements in detection accuracy (up to 16.1%) and response time (up to 6x) over existing approaches, with robustness under evasive attacks.

Conclusion: The framework effectively addresses key limitations in ransomware detection, offering enhanced performance and adaptability through the integration of self-supervised learning and NAS.

Abstract: Ransomware has become a critical threat to cybersecurity due to its rapid
evolution, the necessity for early detection, and growing diversity, posing
significant challenges to traditional detection methods. While AI-based
approaches had been proposed by prior works to assist ransomware detection,
existing methods suffer from three major limitations, ad-hoc feature
dependencies, delayed response, and limited adaptability to unseen variants. In
this paper, we propose a framework that integrates self-supervised contrastive
learning with neural architecture search (NAS) to address these challenges.
Specifically, this paper offers three important contributions. (1) We design a
contrastive learning framework that incorporates hardware performance counters
(HPC) to analyze the runtime behavior of target ransomware. (2) We introduce a
customized loss function that encourages early-stage detection of malicious
activity, and significantly reduces the detection latency. (3) We deploy a
neural architecture search (NAS) framework to automatically construct adaptive
model architectures, allowing the detector to flexibly align with unseen
ransomware variants. Experimental results show that our proposed method
achieves significant improvements in both detection accuracy (up to 16.1%) and
response time (up to 6x) compared to existing approaches while maintaining
robustness under evasive attacks.

</details>


### [3] [Security Analysis of LTE Connectivity in Connected Cars: A Case Study of Tesla](https://arxiv.org/abs/2510.22024)
*Evangelos Bitsikas,Jason Veara,Aanjhan Ranganathan*

Main category: cs.CR

TL;DR: This paper exposes critical LTE security vulnerabilities in Tesla vehicles (Model 3, Cybertruck) that could compromise safety systems and challenge automotive cybersecurity regulations like ISO/SAE 21434 and UN R155/R156.


<details>
  <summary>Details</summary>
Motivation: Existing mobile network vulnerabilities have not been sufficiently studied in safety-critical automotive contexts, despite connected vehicles relying on LTE for diagnostics, OTA updates, and safety services.

Method: Black-box, non-invasive security analysis of Tesla's LTE telematics stack, identifying protocol weaknesses and architectural misconfigurations through practical attack vectors.

Result: Discovery of systemic issues including IMSI catching, rogue base station hijacking, insecure fallback mechanisms, silent SMS injection, and broadcast message spoofing with potential service degradation.

Conclusion: The findings reveal flaws in core assumptions of automotive cybersecurity standards, necessitating reevaluation of regulatory frameworks for secure, traceable telematics in vehicle type approval.

Abstract: Modern connected vehicles rely on persistent LTE connectivity to enable
remote diagnostics, over-the-air (OTA) updates, and critical safety services.
While mobile network vulnerabilities are well documented in the smartphone
ecosystem, their impact in safety-critical automotive settings remains
insufficiently examined. In this work, we conduct a black-box, non-invasive
security analysis of LTE connectivity in Tesla vehicles, including the Model 3
and Cybertruck, revealing systemic protocol weaknesses and architectural
misconfigurations. We find that Tesla's telematics stack is susceptible to IMSI
catching, rogue base station hijacking, and insecure fallback mechanisms that
may silently degrade service availability. Furthermore, legacy control-plane
configurations allow for silent SMS injection and broadcast message spoofing
without driver awareness. These vulnerabilities have implications beyond a
single vendor as they challenge core assumptions in regulatory frameworks like
ISO/SAE 21434 and UN R155/R156, which require secure, traceable, and resilient
telematics for type approval of modern vehicles.

</details>


### [4] [Jailbreak Mimicry: Automated Discovery of Narrative-Based Jailbreaks for Large Language Models](https://arxiv.org/abs/2510.22085)
*Pavlos Ntais*

Main category: cs.CR

TL;DR: Jailbreak Mimicry is a method for automatically generating narrative-based adversarial prompts to expose LLM vulnerabilities in cybersecurity. It achieves an 81.0% Attack Success Rate against GPT-OSS-20B via LoRA-finetuned Mistral-7B, outperforming manual attacks by 54x. Systematic analysis reveals domain-specific vulnerabilities and defense implications for AI security systems.


<details>
  <summary>Details</summary>
Motivation: LLMs face significant risks from manual prompt engineering attacks in critical cybersecurity applications. Current vulnerability testing lacks systematic automation to assess and proactively address these threats in AI-driven security.

Method: The approach uses parameter-efficient fine-tuning (LoRA) of Mistral-7B with a curated AdvBench-derived dataset to train compact attacker models. This transforms manual adversarial prompt generation into a reproducible process for one-shot attack generation and cross-model evaluation.

Result: 81.0-79.5-33.0-66.5 ASR against GPT-OSS-20B/Llama-3/Gemini 2.5 Flash/GPT-4, achieving 54x improvement over direct prompting. Technical domains (93% ASR) and fraud attacks (87.8%) show highest vulnerability, with robustness analysis validated through automated/human evaluation.

Conclusion: Highlights systematic flaws in safety alignment while providing scalable red-teaming tools for AI security. Failure analyses and defensive strategies offer actionable insights to strengthen AI cybersecurity systems against narrative-based adversarial prompts.

Abstract: Large language models (LLMs) remain vulnerable to sophisticated prompt
engineering attacks that exploit contextual framing to bypass safety
mechanisms, posing significant risks in cybersecurity applications. We
introduce Jailbreak Mimicry, a systematic methodology for training compact
attacker models to automatically generate narrative-based jailbreak prompts in
a one-shot manner. Our approach transforms adversarial prompt discovery from
manual craftsmanship into a reproducible scientific process, enabling proactive
vulnerability assessment in AI-driven security systems. Developed for the
OpenAI GPT-OSS-20B Red-Teaming Challenge, we use parameter-efficient
fine-tuning (LoRA) on Mistral-7B with a curated dataset derived from AdvBench,
achieving an 81.0% Attack Success Rate (ASR) against GPT-OSS-20B on a held-out
test set of 200 items. Cross-model evaluation reveals significant variation in
vulnerability patterns: our attacks achieve 66.5% ASR against GPT-4, 79.5% on
Llama-3 and 33.0% against Gemini 2.5 Flash, demonstrating both broad
applicability and model-specific defensive strengths in cybersecurity contexts.
This represents a 54x improvement over direct prompting (1.5% ASR) and
demonstrates systematic vulnerabilities in current safety alignment approaches.
Our analysis reveals that technical domains (Cybersecurity: 93% ASR) and
deception-based attacks (Fraud: 87.8% ASR) are particularly vulnerable,
highlighting threats to AI-integrated threat detection, malware analysis, and
secure systems, while physical harm categories show greater resistance (55.6%
ASR). We employ automated harmfulness evaluation using Claude Sonnet 4,
cross-validated with human expert assessment, ensuring reliable and scalable
evaluation for cybersecurity red-teaming. Finally, we analyze failure
mechanisms and discuss defensive strategies to mitigate these vulnerabilities
in AI for cybersecurity.

</details>


### [5] [Lightweight and Breach-Resilient Authenticated Encryption Framework for Internet of Things](https://arxiv.org/abs/2510.22100)
*Saif E. Nouma,Attila A. Yavuz*

Main category: cs.CR

TL;DR: This paper introduces Graphene, a new framework for symmetric Forward-secure and Aggregate Authenticated Encryption (FAAE) in resource-limited Internet of Things (IoT) environments. It enhances security with key compromise resiliency, compact authentication tags, and offline-online performance, while demonstrating superior efficiency and backward compatibility with existing cryptographic standards.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of current lightweight Authenticated Encryption (AE) standards in IoT, such as lack of key compromise resiliency, compact authentication tags, and performance enhancements like offline-online cryptography. These shortcomings hinder effective security in low-energy adversarial environments and low-latency wireless channels typical in IoT.

Method: The proposed framework, Graphene, integrates key evolution strategies and offline-online cryptographic processing with Universal Message Authentication Codes (UMACs). It offers two distinct instantiations, providing extensibility for different MACs while maintaining security features like breach-resiliency and compactness.

Result: Graphene is shown to have significant performance gains over existing alternatives when evaluated on commodity hardware and 32-bit ARM Cortex-M4 microcontrollers. The framework is backward compatible with standard-compliant cryptographic implementations. The authors release their implementation as open-source.

Conclusion: Graphene effectively addresses the gaps in current AE standards for IoT by providing a secure, efficient, and flexible solution. Its forward-secure and aggregate authenticated encryption capabilities, along with performance and backward compatibility benefits, make it well-suited for low-end IoT infrastructures.

Abstract: The Internet of Things (IoT) relies heavily on resource-limited devices to
communicate critical (e.g., military data) information under low-energy
adversarial environments and low-latency wireless channels. Authenticated
Encryption (AE) guarantees confidentiality, authenticity, and integrity, making
it a vital security service for IoT. However, current deployed (lightweight) AE
standards lack essential features like key compromise resiliency and compact
authentication tags, as well as performance enhancements such as offline-online
cryptography. To address these gaps, we propose Graphene, the first (to our
knowledge) symmetric Forward-secure and Aggregate Authenticated Encryption
(FAAE) framework designed for the performance and security demands of low-end
IoT infrastructures. Graphene innovates by synergizing key evolution strategies
and offline-online cryptographic processing with Universal Message
Authentication Codes (UMACs) to guarantee breach-resiliency, near-optimal
online latency, and compactness. We demonstrate Graphene efficiency through two
distinct instantiations, each balancing unique performance trade-offs with
extensibility for diverse MACs. Our experimental evaluation on commodity
hardware and 32-bit ARM Cortex-M4 microcontroller shows Graphene significant
performance gains over existing alternatives. Graphene is also backward
compatible with standard-compliant cryptographic implementations. We release
our implementation as open source for public testing and adaptation.

</details>


### [6] [TPPR: APT Tactic / Technique Pattern Guided Attack Path Reasoning for Attack Investigation](https://arxiv.org/abs/2510.22191)
*Qi Sheng*

Main category: cs.CR

TL;DR: TLDR: This paper introduces the TPPR framework to enhance APT attack investigation by extracting anomaly subgraphs through TTP-annotation, followed by attack path reasoning and attack scenario reconstruction.


<details>
  <summary>Details</summary>
Motivation: APT attacks are challenging to detect due their high concealment and long-term persistence. Existing methods struggle to distinguish benign operations from actual attacks, often conflating when attempting to analyze provenance graphs. This necessitates a better approach to accurately characterize APT behaviors through effective context associations.

Method: The proposed method, TTP-PR, involves three steps: first, anomaly subgraphs are extracted via abnormal node detection, TTP-annotation, and graph pruning. Then, attack paths are reasoned through mined TTP sequential patterns. Finally, attack scenarios are reconstructed using path scoring and merging based on confidence levels.

Result: TPPR achieves 99.9% graph simplification with 91% of critical attack nodes preserved, demonstrating a significant improvement in attack investigation over state-of-the-art solutions. When compared to SPARSE and DepImpact, it outperforms them by 63.1% and 67.9% in reconstruction precision while maintaining the integrity of the attack scenarios.

Conclusion: This research presents the TPPR framework as a novel and effective solution for APT investigation. By leveraging TTP sequential patterns, TPPR significantly improves the accuracy of identifying real APT behaviors while simplifying the representation of provenance graphs for effective attack reconstruction.

Abstract: Provenance analysis based on system audit data has emerged as a fundamental
approach for investigating Advanced Persistent Threat (APT) attacks. Due to the
high concealment and long-term persistence of APT attacks, they are only
represented as a minimal part of the critical path in the provenance graph.
While existing techniques employ behavioral pattern matching and data flow
feature matching to uncover latent associations in attack sequences through
provenance graph path reasoning, their inability to establish effective attack
context associations often leads to the conflation of benign system operations
with real attack entities, that fail to accurately characterize real APT
behaviors. We observe that while the causality of entities in the provenance
graph exhibit substantial complexity, attackers often follow specific attack
patterns-specifically, clear combinations of tactics and techniques to achieve
their goals. Based on these insights, we propose TPPR, a novel framework that
first extracts anomaly subgraphs through abnormal node detection,
TTP-annotation and graph pruning, then performs attack path reasoning using
mined TTP sequential pattern, and finally reconstructs attack scenarios through
confidence-based path scoring and merging. Extensive evaluation on real
enterprise logs (more than 100 million events) and DARPA TC dataset
demonstrates TPPR's capability to achieve 99.9% graph simplification (700,000
to 20 edges) while preserving 91% of critical attack nodes, outperforming
state-of-the-art solutions (SPARSE, DepImpact) by 63.1% and 67.9% in
reconstruction precision while maintaining attack scenario integrity.

</details>


### [7] [SecureLearn -- An Attack-agnostic Defense for Multiclass Machine Learning Against Data Poisoning Attacks](https://arxiv.org/abs/2510.22274)
*Anum Paracha,Junaid Arshad,Mohamed Ben Farah,Khalid Ismail*

Main category: cs.CR

TL;DR: The paper proposes SecureLearn, a two-layer defense method for multiclass models against data poisoning attacks. It outperforms existing defenses with high accuracy, recall, and F1-score across multiple algorithms and datasets.


<details>
  <summary>Details</summary>
Motivation: Data poisoning attacks target machine learning models, especially multiclass classifiers, which are crucial in multi-modal applications. Current defenses are either attack-specific or limited to certain ML types like deep neural networks, leaving traditional multiclass models vulnerable.

Method: SecureLearn features two components: data sanitization and feature-oriented adversarial training. It is evaluated using a 3D matrix involving various poisoning attacks, data sanitization, and adversarial training aspects, with metrics like accuracy, recall, F1-score, and detection rates.

Result: SecureLearn achieved over 90% accuracy, 75% recall and F1-score for traditional models, and 97% recall and F1-score for neural networks across multiple poisoning attacks and ML algorithms, surpassing existing mitigation strategies.

Conclusion: SecureLearn is a robust, attack-agnostic defense for multiclass models against data poisoning, showing high effectiveness and generalization to both traditional models and neural networks.

Abstract: Data poisoning attacks are a potential threat to machine learning (ML)
models, aiming to manipulate training datasets to disrupt their performance.
Existing defenses are mostly designed to mitigate specific poisoning attacks or
are aligned with particular ML algorithms. Furthermore, most defenses are
developed to secure deep neural networks or binary classifiers. However,
traditional multiclass classifiers need attention to be secure from data
poisoning attacks, as these models are significant in developing multi-modal
applications. Therefore, this paper proposes SecureLearn, a two-layer
attack-agnostic defense to defend multiclass models from poisoning attacks. It
comprises two components of data sanitization and a new feature-oriented
adversarial training. To ascertain the effectiveness of SecureLearn, we
proposed a 3D evaluation matrix with three orthogonal dimensions: data
poisoning attack, data sanitization and adversarial training. Benchmarking
SecureLearn in a 3D matrix, a detailed analysis is conducted at different
poisoning levels (10%-20%), particularly analysing accuracy, recall, F1-score,
detection and correction rates, and false discovery rate. The experimentation
is conducted for four ML algorithms, namely Random Forest (RF), Decision Tree
(DT), Gaussian Naive Bayes (GNB) and Multilayer Perceptron (MLP), trained with
three public datasets, against three poisoning attacks and compared with two
existing mitigations. Our results highlight that SecureLearn is effective
against the provided attacks. SecureLearn has strengthened resilience and
adversarial robustness of traditional multiclass models and neural networks,
confirming its generalization beyond algorithm-specific defenses. It
consistently maintained accuracy above 90%, recall and F1-score above 75%. For
neural networks, SecureLearn achieved 97% recall and F1-score against all
selected poisoning attacks.

</details>


### [8] [Adapting Noise-Driven PUF and AI for Secure WBG ICS: A Proof-of-Concept Study](https://arxiv.org/abs/2510.22283)
*Devon A. Kelly,Christiana Chamon*

Main category: cs.CR

TL;DR: The paper proposes a novel security framework combining noise-driven PUFs and ML for WBG-based ICS, demonstrating high detection accuracy and low latency.


<details>
  <summary>Details</summary>
Motivation: WBG tech improves ICS efficiency but introduces sensor corruption and cybersecurity risks from high-frequency noise and threats. Existing solutions do not address these challenges effectively.

Method: They adapt a PUF using WBG switching noise and ML models with Bayesian filtering to detect anomalies, testing via simulations of various attack scenarios.

Result: 95% detection accuracy and sub-millisecond latency in simulated attacks, confirming the framework's robustness and scalability.

Conclusion: The approach merges hardware and AI to enhance ICS security, offering a scalable defense primitive against noise-based and cyber threats.

Abstract: Wide-bandgap (WBG) technologies offer unprecedented improvements in power
system efficiency, size, and performance, but also introduce unique sensor
corruption and cybersecurity risks in industrial control systems (ICS),
particularly due to high-frequency noise and sophisticated cyber-physical
threats. This proof-of-concept (PoC) study demonstrates the adaptation of a
noise-driven physically unclonable function (PUF) and machine learning
(ML)-assisted anomaly detection framework to the demanding environment of
WBG-based ICS sensor pathways. By extracting entropy from unavoidable WBG
switching noise (up to 100 kHz) as a PUF source, and simultaneously using this
noise as a real-time threat indicator, the proposed system unites
hardware-level authentication and anomaly detection. Our approach integrates
hybrid machine learning (ML) models with adaptive Bayesian filtering, providing
robust and low-latency detection capabilities resilient to both natural
electromagnetic interference (EMI) and active adversarial manipulation. Through
detailed simulations of WBG modules under benign and attack
scenarios--including EMI injection, signal tampering, and node
impersonation--we achieve 95% detection accuracy and sub-millisecond processing
latency. These results demonstrate the feasibility of physics-driven, dual-use
noise exploitation as a scalable ICS defense primitive. Our findings lay the
groundwork for next-generation security strategies that leverage inherent
device characteristics, bridging hardware and artificial intelligence (AI) for
enhanced protection of critical ICS infrastructure.

</details>


### [9] [T2I-RiskyPrompt: A Benchmark for Safety Evaluation, Attack, and Defense on Text-to-Image Model](https://arxiv.org/abs/2510.22300)
*Chenyu Zhang,Tairen Zhang,Lanjun Wang,Ruidong Chen,Wenhui Li,Anan Liu*

Main category: cs.CR

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: Using risky text prompts, such as pornography and violent prompts, to test
the safety of text-to-image (T2I) models is a critical task. However, existing
risky prompt datasets are limited in three key areas: 1) limited risky
categories, 2) coarse-grained annotation, and 3) low effectiveness. To address
these limitations, we introduce T2I-RiskyPrompt, a comprehensive benchmark
designed for evaluating safety-related tasks in T2I models. Specifically, we
first develop a hierarchical risk taxonomy, which consists of 6 primary
categories and 14 fine-grained subcategories. Building upon this taxonomy, we
construct a pipeline to collect and annotate risky prompts. Finally, we obtain
6,432 effective risky prompts, where each prompt is annotated with both
hierarchical category labels and detailed risk reasons. Moreover, to facilitate
the evaluation, we propose a reason-driven risky image detection method that
explicitly aligns the MLLM with safety annotations. Based on T2I-RiskyPrompt,
we conduct a comprehensive evaluation of eight T2I models, nine defense
methods, five safety filters, and five attack strategies, offering nine key
insights into the strengths and limitations of T2I model safety. Finally, we
discuss potential applications of T2I-RiskyPrompt across various research
fields. The dataset and code are provided in
https://github.com/datar001/T2I-RiskyPrompt.

</details>


### [10] [Privacy-Aware Federated nnU-Net for ECG Page Digitization](https://arxiv.org/abs/2510.22387)
*Nader Nemati*

Main category: cs.CR

TL;DR: This paper introduces a federated learning framework for digitizing ECG page images while ensuring privacy and handling cross-institutional deployment challenges.


<details>
  <summary>Details</summary>
Motivation: Centralized training of neural networks for ECG digitization conflicts with cross-institutional privacy and deployment constraints.

Method: The paper presents a cross-silo federated framework using full-model nnU-Net segmentation backbone training without image sharing. Adaptive server-side aggregators (FedAdam), secure aggregation, and central differential privacy are integrated. A calibration-aware digitization pipeline is also included.

Result: Experiments on PTB-XL ECG pages show that FedAdam achieves faster convergence and higher accuracy. The framework maintains competitive performance while ensuring privacy, preventing raw image or update exposure.

Conclusion: The proposed framework offers deployable ECG digitization with strong privacy guarantees, suitable for multi-institutional settings, and achieves centralized-level performance in ECG waveform conversion.

Abstract: Deep neural networks can convert ECG page images into analyzable waveforms,
yet centralized training often conflicts with cross-institutional privacy and
deployment constraints. A cross-silo federated digitization framework is
presented that trains a full-model nnU-Net segmentation backbone without
sharing images and aggregates updates across sites under realistic non-IID
heterogeneity (layout, grid style, scanner profile, noise).
  The protocol integrates three standard server-side aggregators--FedAvg,
FedProx, and FedAdam--and couples secure aggregation with central, user-level
differential privacy to align utility with formal guarantees. Key features
include: (i) end-to-end full-model training and synchronization across clients;
(ii) secure aggregation so the server only observes a clipped, weighted sum
once a participation threshold is met; (iii) central Gaussian DP with Renyi
accounting applied post-aggregation for auditable user-level privacy; and (iv)
a calibration-aware digitization pipeline comprising page normalization, trace
segmentation, grid-leakage suppression, and vectorization to twelve-lead
signals.
  Experiments on ECG pages rendered from PTB-XL show consistently faster
convergence and higher late-round plateaus with adaptive server updates
(FedAdam) relative to FedAvg and FedProx, while approaching centralized
performance. The privacy mechanism maintains competitive accuracy while
preventing exposure of raw images or per-client updates, yielding deployable,
auditable guarantees suitable for multi-institution settings.

</details>


### [11] [PortGPT: Towards Automated Backporting Using Large Language Models](https://arxiv.org/abs/2510.22396)
*Zhaoyang Li,Zheng Yu,Jingyi Song,Meng Xu,Yuxuan Luo,Dongliang Mu*

Main category: cs.CR

TL;DR: PORTGPT automates patch backporting via LLM agents, outperforming tools and contributing to Linux kernel security.


<details>
  <summary>Details</summary>
Motivation: Manual patch backporting is labor-intensive, and existing automated tools lack agility for complex patches in open-source projects like the Linux kernel.

Method: PORTGPT uses an LLM agent enhanced with code access, Git history summarization, and autonomous patch revision tools to simulate human-like reasoning.

Result: PORTGPT achieved 89.15% success on existing datasets and 62.33% on complex cases, with 9 patches merged into the Linux kernel community.

Conclusion: PORTGPT outperforms existing methods in patch backporting, achieving high success rates and contributing real-world patches to the Linux kernel.

Abstract: Patch backporting, the process of migrating mainline security patches to
older branches, is an essential task in maintaining popular open-source
projects (e.g., Linux kernel). However, manual backporting can be
labor-intensive, while existing automated methods, which heavily rely on
predefined syntax or semantic rules, often lack agility for complex patches.
  In this paper, we introduce PORTGPT, an LLM-agent for end-to-end automation
of patch backporting in real-world scenarios. PORTGPT enhances an LLM with
tools to access code on-demand, summarize Git history, and revise patches
autonomously based on feedback (e.g., from compilers), hence, simulating
human-like reasoning and verification. PORTGPT achieved an 89.15% success rate
on existing datasets (1815 cases), and 62.33% on our own dataset of 146 complex
cases, both outperforms state-of-the-art of backporting tools. We contributed 9
backported patches from PORTGPT to the Linux kernel community and all patches
are now merged.

</details>


### [12] [ProGQL: A Provenance Graph Query System for Cyber Attack Investigation](https://arxiv.org/abs/2510.22400)
*Fei Shao,Jia Zou,Zhichao Cao,Xusheng Xiao*

Main category: cs.CR

TL;DR: PROGQL improves cyber attack investigation by combining analyst expertise with scalable, memory-efficient provenance analysis through a novel graph query language and optimized engine, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing PA techniques are inflexible, non-extensible, and memory-inefficient, making them unsuitable for real-world deployment and analyst collaboration. These issues hinder their scalability and ability to integrate expert knowledge.

Method: PROGQL introduces a domain-specific graph search language with constructs for constrained traversal, edge weight computation, value propagation, and graph merging. The query engine supports incremental graph search across heterogeneous databases, eliminating full in-memory materialization.

Result: Evaluations on real attacks demonstrate PROGQL's superior expressiveness compared to Cypher and significant scalability improvements over DEPIMPACT, validating its design for handling complex PA tasks with reduced memory overhead.

Conclusion: The PROGQL framework addresses critical limitations in existing provenance analysis (PA) techniques by offering a scalable, memory-efficient solution through a domain-specific graph search language and optimized query engine. It enhances flexibility for incorporating analyst expertise while reducing memory overhead, enabling effective investigation of complex cyberattacks.

Abstract: Provenance analysis (PA) has recently emerged as an important solution for
cyber attack investigation. PA leverages system monitoring to monitor system
activities as a series of system audit events and organizes these events as a
provenance graph to show the dependencies among system activities, which can
reveal steps of cyber attacks. Despite their potential, existing PA techniques
face two critical challenges: (1) they are inflexible and non-extensible,
making it difficult to incorporate analyst expertise, and (2) they are memory
inefficient, often requiring>100GB of RAM to hold entire event streams, which
fundamentally limits scalability and deployment in real-world environments. To
address these limitations, we propose the PROGQL framework, which provides a
domain-specific graph search language with a well-engineered query engine,
allowing PA over system audit events and expert knowledge to be jointly
expressed as a graph search query and thereby facilitating the investigation of
complex cyberattacks. In particular, to support dependency searches from a
starting edge required in PA, PROGQL introduces new language constructs for
constrained graph traversal, edge weight computation, value propagation along
weighted edges, and graph merging to integrate multiple searches. Moreover, the
PROGQL query engine is optimized for efficient incremental graph search across
heterogeneous database backends, eliminating the need for full in-memory
materialization and reducing memory overhead. Our evaluations on real attacks
demonstrate the effectiveness of the PROGQL language in expressing a diverse
set of complex attacks compared with the state-of-the-art graph query language
Cypher, and the comparison with the SOTA PA technique DEPIMPACT further
demonstrates the significant improvement of the scalability brought by our
PROGQL framework's design.

</details>


### [13] [ZK Coprocessor Bridge: Replay-Safe Private Execution from Solana to Aztec via Wormhole](https://arxiv.org/abs/2510.22536)
*Jotaro Yano*

Main category: cs.CR

TL;DR: The study introduces a formalized framework of a cross-domain ZK coprocessor bridge which links Solana with the Aztec L2, and explains the system's components, security features, and reproducibility. *break


<details>
  <summary>Details</summary>
Motivation: This research is motivated by the desire to enable private execution of Solana programs on Aztec L2 leveraging Ethereum, thus improving scalability, privacy, and finality across different blockchain domains. *break

Method: The method involves creating a system that uses Wormhole VAAs for secure message transport, enforces replay-safety with a replay lock, provides domain-separated field commitment, frequent state machine definitions, and proof sketches for security and functional properties. *break

Result: The system promises secure, private, and scalable inter-chain compute with implications for ZK cross-domain coprocessing, including a reference implementation and specific operational strategies in situations of different VAA verification interfaces. *break

Conclusion: The system concludes with a framework that assures aligned finality and private, secure, cross-chain interaction across Ethereum-based Aztec L2 and Solana, and emphasizes reproducibility with detailed artifacts and versions. *break

Abstract: We formalize a cross-domain "ZK coprocessor bridge" that lets Solana programs
request private execution on Aztec L2 (via Ethereum) using Wormhole Verifiable
Action Approvals (VAAs) as authenticated transport. The system comprises: (i) a
Solana program that posts messages to Wormhole Core with explicit finality;
(ii) an EVM Portal that verifies VAAs, enforces a replay lock, parses a bound
payload secretHash||m from the attested VAA, derives a domain-separated field
commitment, and enqueues an L1->L2 message into the Aztec Inbox (our reference
implementation v0.1.0 currently uses consumeWithSecret(vaa, secretHash); we
provide migration guidance to the payload-bound interface); (iii) a minimal
Aztec contract that consumes the message privately; and (iv) an off-chain
relayer that ferries VAAs and can record receipts on Solana. We present state
machines, message formats, and proof sketches for replay-safety, origin
authenticity, finality alignment, parameter binding (no relayer front-running
of Aztec parameters), privacy, idempotence, and liveness. Finally, we include a
concise Reproducibility note with pinned versions and artifacts to replicate a
public testnet run.

</details>


### [14] [Cross-Paradigm Graph Backdoor Attacks with Promptable Subgraph Triggers](https://arxiv.org/abs/2510.22555)
*Dongyi Liu,Jiangtong Li,Dawei Cheng,Changjun Jiang*

Main category: cs.CR

TL;DR: The paper introduces the CP-GBA method, a transferable graph backdoor attack that employs graph prompt learning to generate universal subgraph triggers. These triggers are distilled from target graphs without being confined to a single graph learning paradigm.


<details>
  <summary>Details</summary>
Motivation: Current graph backdoor attack methods are limited to specific graph learning paradigms, possess simplistic structures, and underutilize complex structural information of graphs. This results in poor transferability and constrained attack success rates.

Method: The proposed framework, CP-GBA, distills a set of universal subgraph triggers through structuring a class-aware, feature-rich, and structurally consistent repository, then employs graph prompt learning to train them on prompt-based objectives. This promotes adaptation across paradigms.

Result: Empirical evaluation confirmed the state-of-the-art performance in attack success rates of CP-GBA on various real-world datasets and defense scenarios, demonstrating strong generalization capability across multiple graph learning paradigms.

Conclusion: The study suggests the significance of incorporating element universality and pretrained paradigms in improving the robustness of backdoor attacks across different graph learning scenarios.  The proposed approach demonstrates that designing attacks using an abstracted, paradigm-agnostic trigger structure can significantly enhance method effectiveness and adaptability.

Abstract: Graph Neural Networks(GNNs) are vulnerable to backdoor attacks, where
adversaries implant malicious triggers to manipulate model predictions.
  Existing trigger generators are often simplistic in structure and overly
reliant on specific features, confining them to a single graph learning
paradigm, such as graph supervised learning, graph contrastive learning, or
graph prompt learning.
  This specialized design, which aligns the trigger with one learning
objective, results in poor transferability when applied to other learning
paradigms.
  For instance, triggers generated for the graph supervised learning paradigm
perform poorly when tested within graph contrastive learning or graph prompt
learning environments.
  Furthermore, these simple generators often fail to utilize complex structural
information or node diversity within the graph data.
  These constraints limit the attack success rates of such methods in general
testing scenarios.
  Therefore, to address these limitations, we propose Cross-Paradigm Graph
Backdoor Attacks with Promptable Subgraph Triggers(CP-GBA), a new transferable
graph backdoor attack that employs graph prompt learning(GPL) to train a set of
universal subgraph triggers.
  First, we distill a compact yet expressive trigger set from target graphs,
which is structured as a queryable repository, by jointly enforcing
class-awareness, feature richness, and structural fidelity.
  Second, we conduct the first exploration of the theoretical transferability
of GPL to train these triggers under prompt-based objectives, enabling
effective generalization to diverse and unseen test-time paradigms.
  Extensive experiments across multiple real-world datasets and defense
scenarios show that CP-GBA achieves state-of-the-art attack success rates.

</details>


### [15] [Blockchain Signatures to Ensure Information Integrity and Non-Repudiation in the Digital Era: A comprehensive study](https://arxiv.org/abs/2510.22561)
*Kaveri Banerjee,Sajal Saha*

Main category: cs.CR

TL;DR: The paper surveys digital signature schemes in blockchain systems, analyzing their cryptographic foundations and properties to evaluate their role in non-repudiation and system security.


<details>
  <summary>Details</summary>
Motivation: Blockchain systems require strong security, particularly non-repudiation to ensure transaction integrity and authorship. The paper aims to inform the suitability of digital signatures for this purpose.

Method: Examines representative digital signature schemes, their cryptographic foundations, security assumptions, and deployment-relevant attributes like unforgeability, malleability resistance, and efficiency. These are compared against blockchain use cases.

Result: Highlights tradeoffs between throughput, storage, scalability, and security across schemes, providing a comparative analysis that connects digital signature properties to blockchain design requirements.

Conclusion: Carefully selected digital signatures are crucial for blockchain non-repudiation and security. The paper offers practical guidance for deployment and identifies open challenges in interoperability and post-quantum readiness.

Abstract: Blockchain systems rely on decentralized ledgers and strong security
guarantees. A key requirement is non-repudiation, which prevents denial of
transaction authorship and supports integrity of recorded data. This work
surveys digital signature schemes used in blockchain platforms and analyzes how
they deliver non-repudiation and contribute to overall system security. We
examine representative scheme families and their cryptographic foundations,
security assumptions, and properties relevant to deployment, including
unforgeability, resistance to malleability, support for aggregation and
multisignature or threshold settings, key and signature sizes, and verification
cost. Using these criteria, we compare the suitability of different designs for
consensus protocols, smart contract constraints, and resource limits. We
highlight practical tradeoffs that affect throughput, storage, scalability, and
attack surfaces, and summarize benefits and limitations of each scheme in
blockchain contexts. The study underscores that carefully chosen digital
signatures are central to achieving non-repudiation and preserving information
integrity, and it outlines implementation considerations and open directions
such as interoperability and post-quantum readiness.

</details>


### [16] [FAARM: Firmware Attestation and Authentication Framework for Mali GPUs](https://arxiv.org/abs/2510.22566)
*Md. Mehedi Hasan*

Main category: cs.CR

TL;DR: This paper introduces FAARM, a lightweight firmware attestation framework to secure GPU TEEs against MOLE-style attacks by enforcing cryptographic verification at boot, achieving strong security with 1.34ms overhead.


<details>
  <summary>Details</summary>
Motivation: Current GPU TEEs lack cryptographic firmware verification, leaving a critical trust gap that enables malicious firmware injections (e.g., MOLE attack) to bypass protections, exfiltrate data, and tamper with computations.

Method: FAARM verifies firmware authenticity at EL3 using vendor-signed bundles and on-device public keys during boot, enforcing version checks and memory lockdown. It is prototyped as a software-only solution on Mali GPUs with Colab-based emulation.

Result: FAARM reliably blocks firmware subversions (100% detection of tampered images), rejects post-attestation overwrites, and introduces 1.34ms verification latency, demonstrating practical security with minimal performance impact.

Conclusion: FAARM addresses fundamental vulnerabilities in GPU TEEs via firmware attestation, offering a deployable defense that enhances security for mobile and cloud GPUs without compromising efficiency.

Abstract: Recent work has revealed MOLE, the first practical attack to compromise GPU
Trusted Execution Environments (TEEs), by injecting malicious firmware into the
embedded Microcontroller Unit (MCU) of Arm Mali GPUs. By exploiting the absence
of cryptographic verification during initialization, adversaries with kernel
privileges can bypass memory protections, exfiltrate sensitive data at over 40
MB/s, and tamper with inference results, all with negligible runtime overhead.
This attack surface affects commodity mobile SoCs and cloud accelerators,
exposing a critical firmware-level trust gap in existing GPU TEE designs. To
address this gap, this paper presents FAARM, a lightweight Firmware Attestation
and Authentication framework that prevents MOLE-style firmware subversion.
FAARM integrates digital signature verification at the EL3 secure monitor using
vendor-signed firmware bundles and an on-device public key anchor. At boot, EL3
verifies firmware integrity and authenticity, enforces version checks, and
locks the firmware region, eliminating both pre-verification and
time-of-check-to-time-of-use (TOCTOU) attack vectors. We implement FAARM as a
software-only prototype on a Mali GPU testbed, using a Google Colab-based
emulation framework that models the firmware signing process, the EL1 to EL3
load path, and secure memory configuration. FAARM reliably detects and blocks
malicious firmware injections, rejecting tampered images before use and denying
overwrite attempts after attestation. Firmware verification incurs only 1.34 ms
latency on average, demonstrating that strong security can be achieved with
negligible overhead. FAARM thus closes a fundamental gap in shim-based GPU
TEEs, providing a practical, deployable defense that raises the security
baseline for both mobile and cloud GPU deployments.

</details>


### [17] [Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI Agents](https://arxiv.org/abs/2510.22620)
*Julia Bazinska,Max Mathys,Francesco Casucci,Mateo Rojas-Carulla,Xander Davies,Alexandra Souly,Niklas Pfister*

Main category: cs.CR

TL;DR: This paper introduces threat snapshots to systematically analyze how LLM vulnerabilities propagate to agent-level security, creating the bÂ³ benchmark with 194k adversarial examples and testing 31 LLMs. Results show enhanced reasoning improves security, while size doesn't correlate.


<details>
  <summary>Details</summary>
Motivation: Existing security frameworks for AI agents either focus narrowly on specific vulnerabilities or require full agent modeling, failing to address the integrated risks from LLMs and traditional software components.

Method: Developed threat snapshots to isolate vulnerable execution states in agent workflows, creating a benchmark via crowdsourced adversarial attacks and applying it to evaluate 31 LLMs for security patterns.

Result: Found that LLM reasoning capabilities correlate with improved security, model size lacks correlation, and the bÂ³ benchmark identifies exploitable vulnerabilities in agents' control flows.

Conclusion: Released benchmarks and datasets to standardize security evaluation, encouraging developers to prioritize LLM backbone security and adopt the framework for systematic risk identification in AI agents.

Abstract: AI agents powered by large language models (LLMs) are being deployed at
scale, yet we lack a systematic understanding of how the choice of backbone LLM
affects agent security. The non-deterministic sequential nature of AI agents
complicates security modeling, while the integration of traditional software
with AI components entangles novel LLM vulnerabilities with conventional
security risks. Existing frameworks only partially address these challenges as
they either capture specific vulnerabilities only or require modeling of
complete agents. To address these limitations, we introduce threat snapshots: a
framework that isolates specific states in an agent's execution flow where LLM
vulnerabilities manifest, enabling the systematic identification and
categorization of security risks that propagate from the LLM to the agent
level. We apply this framework to construct the $\operatorname{b}^3$ benchmark,
a security benchmark based on 194331 unique crowdsourced adversarial attacks.
We then evaluate 31 popular LLMs with it, revealing, among other insights, that
enhanced reasoning capabilities improve security, while model size does not
correlate with security. We release our benchmark, dataset, and evaluation code
to facilitate widespread adoption by LLM providers and practitioners, offering
guidance for agent developers and incentivizing model developers to prioritize
backbone security improvements.

</details>


### [18] [DeepfakeBench-MM: A Comprehensive Benchmark for Multimodal Deepfake Detection](https://arxiv.org/abs/2510.22622)
*Kangran Zhao,Yupeng Chen,Xiaoyu Zhang,Yize Chen,Weinan Guan,Baicheng Chen,Chengzhe Sun,Soumyya Kanti Datta,Qingshan Liu,Siwei Lyu,Baoyuan Wu*

Main category: cs.CR

TL;DR: TL;DR Summary: The misuse of advanced generative AI has led to increased falsified data, particularly AI-generated audiovisual content. This paper proposes a large-scale dataset, Mega-MMDF, and a novel benchmark, DeepfakeBench-MM, for multimodal deepfake detection research. The dataset contains 1.1 million forged and 0.1 million real samples, created through 21 pipeline combinations of audio and visual forgery techniques. The benchmark follows standardized protocols for evaluating existing and new detection methods. Key findings include issues with augmentation and stacked forgery in multimodal deepfakes. The paper asserts that these contributions will significantly advance research in detecting deepfake content.


<details>
  <summary>Details</summary>
Motivation: The paper's motivation stems from the global issue posed by misuse of advanced generative AI as it has led to the proliferation of falsified data. This falsified data, especially AI-generated audiovisual content like deepfakes, can create substantial societal harm through financial fraud and social instability. The authors suggest that due to the lack of extensive, diversified training data and absence of standardized benchmarks, advances in deepfake detection have been limited. Therefore, they aim to address these issues by constructing a new large-scale dataset and benchmark for multimodal deepfake detection.

Method: The authors first build Mega-MMDF, which they describe as a large-scale, diverse, and high-quality dataset for multimodal deepfake detection. The dataset is created using 21 different forgery pipelines, formed by combining 10 audio forgery methods, 12 visual forgery methods, and 6 audio-driven face reenactment methods. Additionally, they introduce DeepfakeBench-MM, a unified benchmark for multimodal deepfake detection, offering standardized protocols across the entire detection process and serving as a flexible platform for testing both current and novel detection methods.

Result: The Mega-MMDF dataset comprises 0.1 million real samples and 1.1 million forged samples, positioning it as one of the largest and most diverse datasets for multimodal deepfake detection. This work also leads to the establishment of the DeepfakeBench-MM benchmark, which currently supports 5 datasets and 11 multimodal deepfake detectors. Through comprehensive evaluations and analyses, the authors have uncovered several important insights, such as challenges in data augmentation and stacked forgery techniques.

Conclusion: In conclusion, the paper predicts that the proposed Mega-MMDF and DeepfakeBench-MM will serve as essential infrastructures to significantly advance the field of multimodal deepfake detection. These contributions are seen as vital for overcoming the existing limitations in deepfake detection development, which stem from insufficient training data and the lack of standardized benchmarks.

Abstract: The misuse of advanced generative AI models has resulted in the widespread
proliferation of falsified data, particularly forged human-centric audiovisual
content, which poses substantial societal risks (e.g., financial fraud and
social instability). In response to this growing threat, several works have
preliminarily explored countermeasures. However, the lack of sufficient and
diverse training data, along with the absence of a standardized benchmark,
hinder deeper exploration. To address this challenge, we first build Mega-MMDF,
a large-scale, diverse, and high-quality dataset for multimodal deepfake
detection. Specifically, we employ 21 forgery pipelines through the combination
of 10 audio forgery methods, 12 visual forgery methods, and 6 audio-driven face
reenactment methods. Mega-MMDF currently contains 0.1 million real samples and
1.1 million forged samples, making it one of the largest and most diverse
multimodal deepfake datasets, with plans for continuous expansion. Building on
it, we present DeepfakeBench-MM, the first unified benchmark for multimodal
deepfake detection. It establishes standardized protocols across the entire
detection pipeline and serves as a versatile platform for evaluating existing
methods as well as exploring novel approaches. DeepfakeBench-MM currently
supports 5 datasets and 11 multimodal deepfake detectors. Furthermore, our
comprehensive evaluations and in-depth analyses uncover several key findings
from multiple perspectives (e.g., augmentation, stacked forgery). We believe
that DeepfakeBench-MM, together with our large-scale Mega-MMDF, will serve as
foundational infrastructures for advancing multimodal deepfake detection.

</details>


### [19] [Sentra-Guard: A Multilingual Human-AI Framework for Real-Time Defense Against Adversarial LLM Jailbreaks](https://arxiv.org/abs/2510.22628)
*Md. Mehedi Hasan,Ziaur Rahman,Rafid Mostafiz,Md. Abir Hossain*

Main category: cs.CR

TL;DR: Sentra-Guard is a real-time modular defense system for large language models that detects and mitigates jailbreak and prompt injection attacks with high accuracy.


<details>
  <summary>Details</summary>
Motivation: To detect and mitigate jailbreak and prompt injection attacks on large language models effectively and in real-time, ensuring system security and reducing the impact of adversarial threats.

Method: Sentra-Guard uses a hybrid architecture combining FAISS-indexed SBERT embeddings and fine-tuned transformer classifiers. It features a classifier-retriever fusion module for context-aware risk scoring, a language-agnostic preprocessing layer for multilingual support, and a HITL feedback loop for continuous learning.

Result: Sentra-Guard achieved a 99.96% detection rate (AUC = 1.00, F1 = 1.00) and an attack success rate of 0.004%, outperforming existing methods like LlamaGuard-2 and OpenAI Moderation.

Conclusion: Sentra-Guard represents a state-of-the-art modular defense system for LLMs, offering higher detection accuracy and adaptability, and it provides transparency and compatibility with various LLM backends for scalable deployment.

Abstract: This paper presents a real-time modular defense system named Sentra-Guard.
The system detects and mitigates jailbreak and prompt injection attacks
targeting large language models (LLMs). The framework uses a hybrid
architecture with FAISS-indexed SBERT embedding representations that capture
the semantic meaning of prompts, combined with fine-tuned transformer
classifiers, which are machine learning models specialized for distinguishing
between benign and adversarial language inputs. It identifies adversarial
prompts in both direct and obfuscated attack vectors. A core innovation is the
classifier-retriever fusion module, which dynamically computes context-aware
risk scores that estimate how likely a prompt is to be adversarial based on its
content and context. The framework ensures multilingual resilience with a
language-agnostic preprocessing layer. This component automatically translates
non-English prompts into English for semantic evaluation, enabling consistent
detection across over 100 languages. The system includes a HITL feedback loop,
where decisions made by the automated system are reviewed by human experts for
continual learning and rapid adaptation under adversarial pressure.
Sentra-Guard maintains an evolving dual-labeled knowledge base of benign and
malicious prompts, enhancing detection reliability and reducing false
positives. Evaluation results show a 99.96% detection rate (AUC = 1.00, F1 =
1.00) and an attack success rate (ASR) of only 0.004%. This outperforms leading
baselines such as LlamaGuard-2 (1.3%) and OpenAI Moderation (3.7%). Unlike
black-box approaches, Sentra-Guard is transparent, fine-tunable, and compatible
with diverse LLM backends. Its modular design supports scalable deployment in
both commercial and open-source environments. The system establishes a new
state-of-the-art in adversarial LLM defense.

</details>


### [20] [RejSCore: Rejection Sampling Core for Multivariate-based Public key Cryptography](https://arxiv.org/abs/2510.22661)
*Malik Imran,Safiullah Khan,Zain Ul Abideen,Ciara Rafferty,Ayesha Khalid,Muhammad Rashid,Maire O'Neill*

Main category: cs.CR

TL;DR: This paper introduces RejSCore, a lightweight FPGA/CMOS-friendly hardware accelerator for post-quantum cryptography's rejection sampling. It achieves 8525 cycles for QR-UOV operations with favorable ADP/PDP metrics, enabling secure deployment on resource-constrained devices.


<details>
  <summary>Details</summary>
Motivation: MPKC schemes require heavy operations like rejection sampling, which challenge resource-limited devices. Prior work has overlooked rejection sampling optimization for MPKC signature generation in hardware contexts.

Method: The authors propose RejSCore, a lightweight hardware accelerator for rejection sampling leveraging an AES-CTR-128-based PRNG and an iterative method that reduces resource consumption/area overhead at the cost of modest latency increases.

Result: RejSCore achieves 2042 slices (Artix-7) and 464,866 Î¼mÂ² (65nm CMOS) with 8525 cycles for QR-UOV parameters at 222 MHz/565 MHz. ADP/PDP evaluations confirm its efficiency for constrained deployments.

Conclusion: The paper concludes that RejSCore is a suitable solution for resource-constrained, security-critical environments due to its optimized performance in terms of ADP and PDP metrics.

Abstract: Post-quantum multivariate public key cryptography (MPKC) schemes resist
quantum threats but require heavy operations, such as rejection sampling, which
challenge resource-limited devices. Prior hardware designs have addressed
various aspects of MPKC signature generation. However, rejection sampling
remains largely unexplored in such contexts. This paper presents RejSCore, a
lightweight hardware accelerator for rejection sampling in post-quantum
cryptography. It specifically targets the QR-UOV scheme, which is a prominent
candidate under the second-round of the National Institute of Standards and
Technology (NIST) additional digital signature standardization process. The
architecture includes an AES-CTR-128-based pseudorandom number generator.
Moreover, a lightweight iterative method is employed in rejection sampling,
offering reduced resource consumption and area overhead while slightly
increasing latency. The performance of RejSCore is comprehensively evaluated on
Artix-7 FPGAs and 65 nm CMOS technology using the Area-Delay Product (ADP) and
Power-Delay Product (PDP). On Artix-7 and 65 nm CMOS, RejSCore achieves an area
of 2042 slices and 464,866~$\mu m^2$, with operating frequencies of 222 MHz and
565 MHz, respectively. Using the QR-UOV parameters for security level I ($q =
127$, $v = 156$, $m = 54$, $l = 3$), the core completes its operation in 8525
clock cycles. The ADP and PDP evaluations confirm RejSCore's suitability for
deployment in resource-constrained and security-critical environments.

</details>


### [21] [SpoofTrackBench: Interpretable AI for Spoof-Aware UAV Tracking and Benchmarking](https://arxiv.org/abs/2510.22726)
*Van Le,Tan Le*

Main category: cs.CR

TL;DR: SpoofTrackBench is a benchmark for evaluating adversarial robustness in RTLS systems under radar spoofing.


<details>
  <summary>Details</summary>
Motivation: The paper aims to create a reproducible and modular benchmark (SpoofTrackBench) to evaluate the robustness of real-time localization and tracking systems against radar spoofing attacks, addressing a gap in ethical and open benchmarking for spoof-aware tracking.

Method: Leverages the Hampton University Skyler Radar Sensor dataset, simulating three types of spoofing attacks (drift, ghost, mirror), and evaluates tracker performance with JPDA and GNN architectures. Implements direct drift-from-truth metrics to quantify assignment errors and includes interpretability tools like clustering overlays and injection-aware timelines.

Result: Enables reproducible and ethical benchmarking of spoof-aware tracking pipelines for RTLS systems under various types of radar spoofing attacks.

Conclusion: SpoofTrackBench provides a new standard for open and ethical evaluation of tracking systems under spoofing, allowing for integrated cross-architecture comparisons and robust community validation.

Abstract: SpoofTrackBench is a reproducible, modular benchmark for evaluating
adversarial robustness in real-time localization and tracking (RTLS) systems
under radar spoofing. Leveraging the Hampton University Skyler Radar Sensor
dataset, we simulate drift, ghost, and mirror-type spoofing attacks and
evaluate tracker performance using both Joint Probabilistic Data Association
(JPDA) and Global Nearest Neighbor (GNN) architectures. Our framework separates
clean and spoofed detection streams, visualizes spoof-induced trajectory
divergence, and quantifies assignment errors via direct drift-from-truth
metrics. Clustering overlays, injection-aware timelines, and scenario-adaptive
visualizations enable interpretability across spoof types and configurations.
Evaluation figures and logs are auto-exported for reproducible comparison.
SpoofTrackBench sets a new standard for open, ethical benchmarking of
spoof-aware tracking pipelines, enabling rigorous cross-architecture analysis
and community validation.

</details>


### [22] [Is Your Prompt Poisoning Code? Defect Induction Rates and Security Mitigation Strategies](https://arxiv.org/abs/2510.22944)
*Bin Wang,YiLu Zhong,MiDi Wan,WenJie Yu,YuanBing Ouyang,Yenan Huang,Hui Li*

Main category: cs.CR

TL;DR: The paper introduces an evaluation framework for prompt quality that improves secure code generation from LLMs, showing advanced prompting techniques reduce security risks.


<details>
  <summary>Details</summary>
Motivation: The issue of how poorly formulated, benign prompts impact the security of generated code by Large Language Models is underexplored, despite being common use-cases, thus motivating the creation of a new evaluation framework and benchmark.

Method: The authors propose a framework with three dimensions to evaluate prompt quality (goal clarity, information completeness, logical consistency). They construct and release a Python-based benchmark dataset with four levels of prompt normativity and perform extensive experiments with multiple LLMs, testing the effects of prompting techniques.

Result: The study shows a clear correlation: lower prompt normativity results in a higher likelihood of insecure code. Advanced prompting techniques like Chain-of-Thought and Self-Correction reduce these risks effectively, significantly enhancing code safety.

Conclusion: Improving user prompt quality is a critical, effective strategy to enhance security in AI-generated code; the paper's contributions include a new evaluation framework, dataset, and insights into the effectiveness of prompting techniques for risk mitigation.

Abstract: Large language models (LLMs) have become indispensable for automated code
generation, yet the quality and security of their outputs remain a critical
concern. Existing studies predominantly concentrate on adversarial attacks or
inherent flaws within the models. However, a more prevalent yet underexplored
issue concerns how the quality of a benign but poorly formulated prompt affects
the security of the generated code. To investigate this, we first propose an
evaluation framework for prompt quality encompassing three key dimensions: goal
clarity, information completeness, and logical consistency. Based on this
framework, we construct and publicly release CWE-BENCH-PYTHON, a large-scale
benchmark dataset containing tasks with prompts categorized into four distinct
levels of normativity (L0-L3). Extensive experiments on multiple
state-of-the-art LLMs reveal a clear correlation: as prompt normativity
decreases, the likelihood of generating insecure code consistently and markedly
increases. Furthermore, we demonstrate that advanced prompting techniques, such
as Chain-of-Thought and Self-Correction, effectively mitigate the security
risks introduced by low-quality prompts, substantially improving code safety.
Our findings highlight that enhancing the quality of user prompts constitutes a
critical and effective strategy for strengthening the security of AI-generated
code.

</details>


### [23] [QuantumShield: Multilayer Fortification for Quantum Federated Learning](https://arxiv.org/abs/2510.22945)
*Dev Gurung,Shiva Raj Pokhrel*

Main category: cs.CR

TL;DR: This paper introduces a quantum-secure federated learning framework combining QKD, PQC, and other protocols to defend against quantum threats, validated through theoretical and experimental analysis.


<details>
  <summary>Details</summary>
Motivation: Classical cryptographic methods are vulnerable to quantum attacks, creating an urgent need for quantum-secure distributed learning frameworks to protect against emerging threats.

Method: Combined quantum (QKD, Quantum Teleportation) and post-quantum protocols (KEM, PQC) within a federated learning framework, validated through theoretical models and experimental assessments.

Result: Demonstrated secure and scalable interoperability of quantum/post-quantum protocols in a QFL ecosystem, with quantitative security/performance validation showing resistance to classical and quantum threats.

Conclusion: The work provides a foundation for next-generation federated learning systems that are inherently quantum-secure, demonstrating robust protection against quantum threats while maintaining scalability.

Abstract: In this paper, we propose a groundbreaking quantum-secure federated learning
(QFL) framework designed to safeguard distributed learning systems against the
emerging threat of quantum-enabled adversaries. As classical cryptographic
methods become increasingly vulnerable to quantum attacks, our framework
establishes a resilient security architecture that remains robust even in the
presence of quantum-capable attackers. We integrate and rigorously evaluate
advanced quantum and post-quantum protocols including Quantum Key Distribution
(QKD), Quantum Teleportation, Key Encapsulation Mechanisms (KEM) and
Post-Quantum Cryptography (PQC) to fortify the QFL process against both
classical and quantum threats. These mechanisms are systematically analyzed and
implemented to demonstrate their seamless interoperability within a secure and
scalable QFL ecosystem. Through comprehensive theoretical modeling and
experimental validation, this work provides a detailed security and performance
assessment of the proposed framework. Our findings lay a strong foundation for
next-generation federated learning systems that are inherently secure in the
quantum era.

</details>


### [24] [CompressionAttack: Exploiting Prompt Compression as a New Attack Surface in LLM-Powered Agents](https://arxiv.org/abs/2510.22963)
*Zesen Liu,Zhixiang Zhang,Yuchong Xie,Dongdong She*

Main category: cs.CR

TL;DR: The paper introduces CompressionAttack, a framework exploiting prompt compression in LLM agents as a security risk, achieving high success rates with stealth and transferability.


<details>
  <summary>Details</summary>
Motivation: Prompt compression reduces inference costs but introduces new security vulnerabilities that require investigation.

Method: CompressionAttack employs two strategies: HardCom with discrete adversarial edits for hard compression and SoftCom with latent-space perturbations for soft compression.

Result: The framework achieves up to 80% attack success and 98% preference flips across multiple LLMs, while remaining undetected by current defenses.

Conclusion: CompressionAttack demonstrates a novel attack surface in prompt compression, proving its real-world impact and underscoring the need for improved defensive mechanisms.

Abstract: LLM-powered agents often use prompt compression to reduce inference costs,
but this introduces a new security risk. Compression modules, which are
optimized for efficiency rather than safety, can be manipulated by adversarial
inputs, causing semantic drift and altering LLM behavior. This work identifies
prompt compression as a novel attack surface and presents CompressionAttack,
the first framework to exploit it. CompressionAttack includes two strategies:
HardCom, which uses discrete adversarial edits for hard compression, and
SoftCom, which performs latent-space perturbations for soft compression.
Experiments on multiple LLMs show up to 80% attack success and 98% preference
flips, while remaining highly stealthy and transferable. Case studies in VSCode
Cline and Ollama confirm real-world impact, and current defenses prove
ineffective, highlighting the need for stronger protections.

</details>


### [25] [Advancing Honeywords for Real-World Authentication Security](https://arxiv.org/abs/2510.22971)
*Sudiksha Das,Ashish Kundu*

Main category: cs.CR

TL;DR: Honeywords, a proactive security method using decoy passwords, have potential but require more research on practical issues for widespread adoption.


<details>
  <summary>Details</summary>
Motivation: Honeywords, introduced in 2013, aim to detect password misuse by storing decoy passwords next to real ones. Despite this potential, they haven't been implemented by major platforms, possibly due to unresolved technical and architectural issues.

Method: The paper reviews existing literature on honeychecker architecture, attacker modeling, and Honeyword generation. It then proposes a new framework based on combining attacker-resistant decoy creation with easy system integration.

Result: Analysis identifies unresolved subproblems in Honeyword systems, including flatness, integration challenges, and reliability concerns. The paper outlines a framework to address these issues.

Conclusion: To transition Honeywords from an academic concept to a practical security tool, technical improvements must be combined with secure, user-friendly architectures, response handling, and configuration assurance.

Abstract: Introduced by Juels and Rivest in 2013, Honeywords, which are decoy passwords
stored alongside a real password, appear to be a proactive method to help
detect password credentials misuse. However, despite over a decade of research,
this technique has not been adopted by major authentication platforms. This
position paper argues that the core concept of Honeywords has potential but
requires more research on issues such as flatness, integration, and
reliability, in order to be a practical deployable solution. This paper
examines the current work on Honeyword generation, attacker modeling, and
honeychecker architecture, analyzing the subproblems that have been addressed
and ongoing issues that prevent this system from being more widely used. The
paper then suggests a deployable framework that combines the
attacker-resilient, context-aware decoy creation that Honeywords provide with
easy integration into existing systems. Honeywords will only move from an
academic idea to a practical security tool if technical advances are paired
with secure and straightforward architectures, along with adaptive response
handling and detailed configuration checks.

</details>


### [26] [A Multi-Store Privacy Measurement of Virtual Reality App Ecosystem](https://arxiv.org/abs/2510.23024)
*Chuan Yan,Zeng Li,Kunlin Cai,Liuhuo Wan,Ruomai Ren,Yiran Shen,Guangdong Bai*

Main category: cs.CR

TL;DR: This paper analyzes privacy practices in VR apps across five app stores using 6,565 samples, revealing systemic compliance issues like missing data declarations and privacy policies.


<details>
  <summary>Details</summary>
Motivation: VR apps collect sensitive data without-domain-specific regulations, leading to inconsistent privacy practices that risk user security in a rapidly growing ecosystem.

Method: Large-scale multi-store study combining natural language processing, reverse engineering, and static analysis to assess both declared and actual privacy behaviors of VR apps.

Result: 33% of apps don't declare sensitive data usage, 21.5%' lack valid privacy policies, and compliance issues exist across all stores, demonstrating premature privacy protection measures.

Conclusion: The findings highlight urgent need for standardized privacy regulations in VR app stores to protect users, prompting action from developers, users, and platform operators.

Abstract: Virtual Reality (VR) has gained increasing traction among various domains in
recent years, with major companies such as Meta, Pico, and Microsoft launching
their application stores to support third-party developers in releasing their
applications (or simply apps). These apps offer rich functionality but
inherently collect privacy-sensitive data, such as user biometrics, behaviors,
and the surrounding environment. Nevertheless, there is still a lack of
domain-specific regulations to govern the data handling of VR apps, resulting
in significant variations in their privacy practices among app stores.
  In this work, we present the first comprehensive multi-store study of privacy
practices in the current VR app ecosystem, covering a large-scale dataset
involving 6,565 apps collected from five major app stores. We assess both
declarative and behavioral privacy practices of VR apps, using a multi-faceted
approach based on natural language processing, reverse engineering, and static
analysis. Our assessment reveals significant privacy compliance issues across
all stores, underscoring the premature status of privacy protection in this
rapidly growing ecosystem. For instance, one third of apps fail to declare
their use of sensitive data, and 21.5\% of apps neglect to provide valid
privacy policies. Our work sheds light on the status quo of privacy protection
within the VR app ecosystem for the first time. Our findings should raise an
alert to VR app developers and users, and encourage store operators to
implement stringent regulations on privacy compliance among VR apps.

</details>


### [27] [Efficient and Encrypted Inference using Binarized Neural Networks within In-Memory Computing Architectures](https://arxiv.org/abs/2510.23034)
*Gokulnath Rajendran,Suman Deb,Anupam Chattopadhyay*

Main category: cs.CR

TL;DR: This paper proposes a security method for Binarized Neural Networks (BNNs) in in-memory computing, using a key from a physical unclonable function to encrypt weights and maintain performance.


<details>
  <summary>Details</summary>
Motivation: BNNs' efficiency is compromised by encryption overhead in in-memory computing. The need arises for a secure, low-overhead method to protect model parameters during inference.

Method: A unique secret key, generated via a physical unclonable function (PUF), is used to encrypt BNN parameters before storage in the crossbar, enabling inference on encrypted data with minimal overhead.

Result: The encryption scheme significantly reduces the accuracy of BNNs (under 15%) when the secret key isn't used, confirming the method's efficacy in defense against theft without hampering computational efficiency.

Conclusion: The proposed encryption strategy successfully secures BNN parameters in in-memory computing, leveraging PUFs for a practical and efficient solution to a pressing security issue.

Abstract: Binarized Neural Networks (BNNs) are a class of deep neural networks designed
to utilize minimal computational resources, which drives their popularity
across various applications. Recent studies highlight the potential of mapping
BNN model parameters onto emerging non-volatile memory technologies,
specifically using crossbar architectures, resulting in improved inference
performance compared to traditional CMOS implementations. However, the common
practice of protecting model parameters from theft attacks by storing them in
an encrypted format and decrypting them at runtime introduces significant
computational overhead, thus undermining the core principles of in-memory
computing, which aim to integrate computation and storage. This paper presents
a robust strategy for protecting BNN model parameters, particularly within
in-memory computing frameworks. Our method utilizes a secret key derived from a
physical unclonable function to transform model parameters prior to storage in
the crossbar. Subsequently, the inference operations are performed on the
encrypted weights, achieving a very special case of Fully Homomorphic
Encryption (FHE) with minimal runtime overhead. Our analysis reveals that
inference conducted without the secret key results in drastically diminished
performance, with accuracy falling below 15%. These results validate the
effectiveness of our protection strategy in securing BNNs within in-memory
computing architectures while preserving computational efficiency.

</details>


### [28] [A high-capacity linguistic steganography based on entropy-driven rank-token mapping](https://arxiv.org/abs/2510.23035)
*Jun Jiang,Weiming Zhang,Nenghai Yu,Kejiang Chen*

Main category: cs.CR

TL;DR: RTMStega is an entropy-driven linguistic steganography framework that triples payload capacity, reduces processing time by 50%, and maintains text quality through rank-based coding and entropy-adjusted sampling.


<details>
  <summary>Details</summary>
Motivation: Current steganography methods face payload limitations (modification-based approaches introduce detectable anomalies; retrieval-based strategies have low capacity; generative models suffer from low token entropy).

Method: Proposes RTMStega framework combining rank-based adaptive coding, context-aware decompression with normalized entropy, and dynamic sampling guided by token probability ranks and entropy adjustments.

Result: Achieves 3Ã payload capacity increase, >50% processing speed improvement, and maintains high text quality across diverse datasets/models compared to mainstream generative steganography.

Conclusion: RTMStega offers a secure, efficient covert communication solution by balancing payload capacity and imperceptibility through entropy-driven innovations, outperforming existing methods.

Abstract: Linguistic steganography enables covert communication through embedding
secret messages into innocuous texts; however, current methods face critical
limitations in payload capacity and security. Traditional modification-based
methods introduce detectable anomalies, while retrieval-based strategies suffer
from low embedding capacity. Modern generative steganography leverages language
models to generate natural stego text but struggles with limited entropy in
token predictions, further constraining capacity. To address these issues, we
propose an entropy-driven framework called RTMStega that integrates rank-based
adaptive coding and context-aware decompression with normalized entropy. By
mapping secret messages to token probability ranks and dynamically adjusting
sampling via context-aware entropy-based adjustments, RTMStega achieves a
balance between payload capacity and imperceptibility. Experiments across
diverse datasets and models demonstrate that RTMStega triples the payload
capacity of mainstream generative steganography, reduces processing time by
over 50%, and maintains high text quality, offering a trustworthy solution for
secure and efficient covert communication.

</details>


### [29] [KAPG: Adaptive Password Guessing via Knowledge-Augmented Generation](https://arxiv.org/abs/2510.23036)
*Xudong Yang,Jincheng Li,Kaiwen Xing,Zhenjia Xiao,Mingjian Duan,Weili Han,Hu Xiong*

Main category: cs.CR

TL;DR: This paper introduces KAPG, a password guessing framework that integrates external lexical knowledge with leaked password data, improving guessing effectiveness by 36.5â74.7x. It also proposes KAPSM, a trend-aware password strength meter that outperforms existing tools.


<details>
  <summary>Details</summary>
Motivation: Traditional password guessing models suffer from over-reliance on internal data from leaked passwords while ignoring external influences like cultural trends, leading to diminishing effectiveness as password behaviors evolve.

Method: KAPG combines internal statistical knowledge from leaked passwords with external lexical knowledge via password prefixes, dynamically injecting contextual cues during generation. KAPSM evaluates password strength using site-specific and cultural trend data.

Result: KAPG achieves 36.5x and 74.7x improvements over state-of-the-art models in intra-site and cross-site password recovery. KAPSM demonstrates superior accuracy across diverse datasets, validated through overlap analysis and efficiency benchmarks.

Conclusion: Integrating external knowledge addresses the limitations of static password guessing models. KAPG and KAPSM collectively provide robust tools for understanding password trends and strengthening security defenses.

Abstract: As the primary mechanism of digital authentication, user-created passwords
exhibit common patterns and regularities that can be learned from leaked
datasets. Password choices are profoundly shaped by external factors, including
social contexts, cultural trends, and popular vocabulary. Prevailing password
guessing models primarily emphasize patterns derived from leaked passwords,
while neglecting these external influences -- a limitation that hampers their
adaptability to emerging password trends and erodes their effectiveness over
time.
  To address these challenges, we propose KAPG, a knowledge-augmented password
guessing framework that adaptively integrates external lexical knowledge into
the guessing process. KAPG couples internal statistical knowledge learned from
leaked passwords with external information that reflects real-world trends. By
using password prefixes as anchors for knowledge lookup, it dynamically injects
relevant external cues during generation while preserving the structural
regularities of authentic passwords. Experiments on twelve leaked datasets show
that KnowGuess achieves average improvements of 36.5\% and 74.7\% over
state-of-the-art models in intra-site and cross-site scenarios, respectively.
Further analyses of password overlap and model efficiency highlight its
robustness and computational efficiency. To counter these attacks, we further
develop KAPSM, a trend-aware and site-specific password strength meter.
Experiments demonstrate that KAPSM significantly outperforms existing tools in
accuracy across diverse evaluation settings.

</details>


### [30] [zkSTAR: A zero knowledge system for time series attack detection enforcing regulatory compliance in critical infrastructure networks](https://arxiv.org/abs/2510.23060)
*Paritosh Ramanan,H. M. Mohaimanul Islam,Abhiram Reddy Alugula*

Main category: cs.CR

TL;DR: zkSTAR is a framework using zk-SNARKs for cyberattack detection in ICS, enabling regulators to verify detection effectiveness without accessing sensitive data.


<details>
  <summary>Details</summary>
Motivation: Regulators need to ensure security and compliance of ICS without access to sensitive operational data from utilities.

Method: The framework uses zk-SNARKs to create a two-pronged architecture that enforces temporal and statistical consistency in state-space detection models.

Result: zkSTAR provides provable detection guarantees while preserving data confidentiality, validated through experiments on real-world datasets.

Conclusion: zkSTAR offers a scalable, privacy-preserving solution for regulatory compliance in ICS-driven critical infrastructure.

Abstract: Industrial control systems (ICS) form the operational backbone of critical
infrastructure networks (CIN) such as power grids, water supply systems, and
gas pipelines. As cyber threats to these systems escalate, regulatory agencies
are imposing stricter compliance requirements to ensure system-wide security
and reliability. A central challenge, however, is enabling regulators to verify
the effectiveness of detection mechanisms without requiring utilities to
disclose sensitive operational data. In this paper, we introduce zkSTAR, a
cyberattack detection framework that leverages zk-SNARKs to reconcile these
requirements and enable provable detection guarantees while preserving data
confidentiality. Our approach builds on established residual-based statistical
hypothesis testing methods applied to state-space detection models.
Specifically, we design a two-pronged zk-SNARK architecture that enforces
temporal consistency of the state-space dynamics and statistical consistency of
the detection tests, allowing regulators to temporally verify alarm correctness
without visibility into utility-level data. We formally analyze the soundness
and zero knowledge properties of our framework and validate its practical
feasibility through computational experiments on real-world ICS datasets. As a
result, our work demonstrates a scalable, privacy-preserving alternative for
regulatory compliance for ICS driven critical infrastructure networks.

</details>


### [31] [Fast-MIA: Efficient and Scalable Membership Inference for LLMs](https://arxiv.org/abs/2510.23074)
*Hiromu Takahashi,Shotaro Ishihara*

Main category: cs.CR

TL;DR: The paper presents Fast-MIA, an open-source Python library that enables efficient evaluation of membership inference attacks (MIA) on Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Membership inference attacks (MIA) against LLMs are gaining attention due to concerns about copyright, security, and data privacy. However, research progress is limited by high computational costs and a lack of standardized, maintained MIA implementations, making it difficult to perform large-scale empirical comparisons.

Method: Fast-MIA addresses these challenges by offering fast batch inference and implementing representative MIA methods within a unified evaluation framework. The library supports reproducible benchmarks through simple configuration and is designed to be easily extendable.

Result: The release of Fast-MIA as an open-source tool under the Apache License 2.0 provides researchers with a scalable and transparent platform for conducting MIA research on LLMs.

Conclusion: Fast-MIA facilitates efficient and reproducible research on membership inference attacks against LLMs by overcoming the limitations of computational cost and lack of standardization in existing methods.

Abstract: We propose Fast-MIA (https://github.com/Nikkei/fast-mia), a Python library
for efficiently evaluating membership inference attacks (MIA) against Large
Language Models (LLMs). MIA against LLMs has emerged as a crucial challenge due
to growing concerns over copyright, security, and data privacy, and has
attracted increasing research attention. However, the progress of this research
is significantly hindered by two main obstacles: (1) the high computational
cost of inference in LLMs, and (2) the lack of standardized and maintained
implementations of MIA methods, which makes large-scale empirical comparison
difficult. To address these challenges, our library provides fast batch
inference and includes implementations of representative MIA methods under a
unified evaluation framework. This library supports easy implementation of
reproducible benchmarks with simple configuration and extensibility. We release
Fast-MIA as an open-source (Apache License 2.0) tool to support scalable and
transparent research on LLMs.

</details>


### [32] [Beyond Imprecise Distance Metrics: LLM-Predicted Target Call Stacks for Directed Greybox Fuzzing](https://arxiv.org/abs/2510.23101)
*Yifan Zhang,Xin Zhang*

Main category: cs.CR

TL;DR: This paper introduces a novel directed greybox fuzzing (DGF) approach that replaces static analysis-based distance metrics with precise call stack representations generated by large language models (LLMs). By predicting vulnerability-triggering call stacks, the method prioritizes seeds with higher overlap to accelerate bug triggering. It achieves 1.86Ã-3.09Ã speedup over baselines, discovers 10 new vulnerabilities, and identifies 2 incomplete fixes with 10 CVEs.


<details>
  <summary>Details</summary>
Motivation: Existing DGF approaches use static analysis to estimate bug-triggering probabilities but suffer from imprecise metrics due to over-approximations. This leads to irrelevant execution paths being prioritized, reducing fuzzing efficiency and effectiveness in locating real-world vulnerabilities.

Method: The method (GLLM-DGF) combines static analysis and LLMs: (1)âStatic analysis generates call graphs to identify methods reachable to targets. (2)âLLMs predict the most probable call stack sequences for exploitation. (3)âSeeds are prioritized based on overlap between their execution paths and predicted call stacks, ensuring precise path prioritization without over-approximations.

Result: Evaluations on real-world programs show GLLM-DGF triggers vulnerabilities 1.86â3.09Ã faster than existing fuzzer baselines. It discovers 10 new bugs and 2 incomplete fixes through directed patch testing, with all findings receiving CVE identifiers in the latest software versions tested.

Conclusion: This work establishes the first integration of LLMs into DGF's core seed prioritization mechanism. The combination of static analysis and LLMs eliminates over-approximations, achieving both faster bug detection and novel vulnerability discovery, demonstrating the viability of AI-enhanced fuzzing for real-world security testing.

Abstract: Directed greybox fuzzing (DGF) aims to efficiently trigger bugs at specific
target locations by prioritizing seeds whose execution paths are more likely to
mutate into triggering target bugs. However, existing DGF approaches suffer
from imprecise probability calculations due to their reliance on complex
distance metrics derived from static analysis. The over-approximations inherent
in static analysis cause a large number of irrelevant execution paths to be
mistakenly considered to potentially mutate into triggering target bugs,
significantly reducing fuzzing efficiency. We propose to replace static
analysis-based distance metrics with precise call stack representations. Call
stacks represent precise control flows, thereby avoiding false information in
static analysis. We leverage large language models (LLMs) to predict
vulnerability-triggering call stacks for guiding seed prioritization. Our
approach constructs call graphs through static analysis to identify methods
that can potentially reach target locations, then utilizes LLMs to predict the
most likely call stack sequence that triggers the vulnerability. Seeds whose
execution paths have higher overlap with the predicted call stack are
prioritized for mutation. This is the first work to integrate LLMs into the
core seed prioritization mechanism of DGF. We implement our approach and
evaluate it against several state-of-the-art fuzzers. On a suite of real-world
programs, our approach triggers vulnerabilities $1.86\times$ to $3.09\times$
faster compared to baselines. In addition, our approach identifies 10 new
vulnerabilities and 2 incomplete fixes in the latest versions of programs used
in our controlled experiments through directed patch testing, with 10 assigned
CVE IDs.

</details>


### [33] [Optimizing Optimism: Up to 6.5x Faster zkVM Validty Proofs via Sparse Derivation](https://arxiv.org/abs/2510.23172)
*Mohsen Ahmadvand,Pedro Souto*

Main category: cs.CR

TL;DR: The paper optimizes validity proofs within the Optimism derivation pipeline by addressing inefficiencies, resulting in a significant speedup while preserving correctness and liveness.


<details>
  <summary>Details</summary>
Motivation: Porting the Optimism derivation pipeline to a zkVM entails high overheads and expensive validity proofs, necessitating a more efficient approach.

Method: The authors identify inefficiencies in the existing pipeline, quantify their impact on proving costs, and propose a redesigned pipeline optimized for zk proving.

Result: The redesigned derivation pipeline attains up to 6.5x faster derivation and 3.5x overall speedup in zkVMs, maintaining the same safety guarantees.

Conclusion: Systematic analysis of inefficiencies and soundness-preserving redesign in Optimism's zkVM compatibility led to a substantial speed improvement.

Abstract: The Optimism derivation pipeline is engineered for correctness and liveness,
not for succinct validity proofs. A straightforward port to a zkVM imposes
significant overheads, making validity proofs significantly more costly than
necessary. We systematically identify inefficiencies in the current design,
analyze their impact on proving costs, and provide a soundness-preserving
redesign tailored to zk proving. Our redesign achieves up to 6.5x faster
derivation inside zkVMs (3.5x overall speedup) while maintaining identical
safety guarantees.

</details>


### [34] [Privacy-Preserving Semantic Communication over Wiretap Channels with Learnable Differential Privacy](https://arxiv.org/abs/2510.23274)
*Weixuan Chen,Qianqian Yang,Shuo Shao,Shunpu Tang,Zhiguo Shi,Shui Yu*

Main category: cs.CR

TL;DR: A secure SemCom framework for image transmission using differential privacy with learnable noise patterns via adversarial training to balance privacy protection and task performance.


<details>
  <summary>Details</summary>
Motivation: Semantic communication (SemCom) improves transmission efficiency but must address privacy risks, particularly in wiretap channels where adversaries might infer sensitive data. Existing secure SemCom solutions often assume impractical safety conditions or knowledge about eavesdroppers, which limits their real-world applicability. The paper highlights the need for a more robust method that ensures privacy without such assumptions.

Method: The proposed framework involves two key stages: (1) Disentangled semantic representation extraction from source images using GAN inversion to separate private and task-relevant features, and (2) Learning a neural network to generate task-specific differential privacy (DP) noise through adversarial training. This trained noise avoids using traditional random DP noise (Gaussian or Laplace) and enables precise privacy control by adjusting the privacy budget.

Result: Experimental results show that the method causes minimal degradation in the legitimate image task performance while significantly reducing reconstruction quality for eavesdroppers. It outperforms the state-of-the-art DP-based method by 0.06-0.29 in LPIPS and 0.10-0.86 in FPPSR metrics.

Conclusion: This approach offers a novel and effective secure SemCom framework by using learnable DP noise patterns trained with adversarial techniques. It overcomes limitations of current methods by improving privacy guarantees and providing intuitive privacy controls without compromising substantial task performance, addressing real-world security constraints.

Abstract: While semantic communication (SemCom) improves transmission efficiency by
focusing on task-relevant information, it also raises critical privacy
concerns. Many existing secure SemCom approaches rely on restrictive or
impractical assumptions, such as favorable channel conditions for the
legitimate user or prior knowledge of the eavesdropper's model. To address
these limitations, this paper proposes a novel secure SemCom framework for
image transmission over wiretap channels, leveraging differential privacy (DP)
to provide approximate privacy guarantees. Specifically, our approach first
extracts disentangled semantic representations from source images using
generative adversarial network (GAN) inversion method, and then selectively
perturbs private semantic representations with approximate DP noise. Distinct
from conventional DP-based protection methods, we introduce DP noise with
learnable pattern, instead of traditional white Gaussian or Laplace noise,
achieved through adversarial training of neural networks (NNs). This design
mitigates the inherent non-invertibility of DP while effectively protecting
private information. Moreover, it enables explicitly controllable security
levels by adjusting the privacy budget according to specific security
requirements, which is not achieved in most existing secure SemCom approaches.
Experimental results demonstrate that, compared with the previous DP-based
method and direct transmission, the proposed method significantly degrades the
reconstruction quality for the eavesdropper, while introducing only slight
degradation in task performance. Under comparable security levels, our approach
achieves an LPIPS advantage of 0.06-0.29 and an FPPSR advantage of 0.10-0.86
for the legitimate user compared with the previous DP-based method.

</details>


### [35] [Network Intrusion Detection: Evolution from Conventional Approaches to LLM Collaboration and Emerging Risks](https://arxiv.org/abs/2510.23313)
*Yaokai Feng,Kouichi Sakurai*

Main category: cs.CR

TL;DR: The paper provides a comprehensive survey of network intrusion detection systems (NIDS), tracing their evolution from traditional methods to recent integrations with large language models (LLMs), and discussing the challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: Understanding the historical progression and current state of NIDS is crucial to identify strengths, limitations, and the potential of emerging LLM-based techniques in this field.

Method: The paper conducts an extensive literature review, categorizing NIDS approaches into conventional methods (signature-based and neural networks) and modern LLM integrations, with a focus on summarizing existing work and identifying emerging challenges.

Result: The survey reveals that while signature-based IDS persists despite limitations, NN-based NIDS still struggles with deployment. LLM integration shows promise but faces practical challenges, with some LLMs being susceptible to misuse. Strategies for domain-specific LLMs are explored.

Conclusion: This survey highlights the progression of NIDS technologies and illustrates the transformative yet challenging role of LLMs. Constructing domain-specific LLMs is seen as a viable path forward, given the impracticality of training NIDS-specific LLMs from scratch.

Abstract: This survey systematizes the evolution of network intrusion detection systems
(NIDS), from conventional methods such as signature-based and neural network
(NN)-based approaches to recent integrations with large language models (LLMs).
It clearly and concisely summarizes the current status, strengths, and
limitations of conventional techniques, and explores the practical benefits of
integrating LLMs into NIDS. Recent research on the application of LLMs to NIDS
in diverse environments is reviewed, including conventional network
infrastructures, autonomous vehicle environments and IoT environments.
  From this survey, readers will learn that: 1) the earliest methods,
signature-based IDSs, continue to make significant contributions to modern
systems, despite their well-known weaknesses; 2) NN-based detection, although
considered promising and under development for more than two decades, and
despite numerous related approaches, still faces significant challenges in
practical deployment; 3) LLMs are useful for NIDS in many cases, and a number
of related approaches have been proposed; however, they still face significant
challenges in practical applications. Moreover, they can even be exploited as
offensive tools, such as for generating malware, crafting phishing messages, or
launching cyberattacks. Recently, several studies have been proposed to address
these challenges, which are also reviewed in this survey; and 4) strategies for
constructing domain-specific LLMs have been proposed and are outlined in this
survey, as it is nearly impossible to train a NIDS-specific LLM from scratch.

</details>


### [36] [Authentication Against Insecure Bootstrapping for 5G Networks: Feasibility, Resiliency, and Transitional Solutions in Post-Quantum Era](https://arxiv.org/abs/2510.23457)
*Saleh Darzi,Mirza Masfiqur Rahman,Imtiaz Karim,Rouzbeh Behnia,Attila A Yavuz,Elisa Bertino*

Main category: cs.CR

TL;DR: This paper addresses the vulnerability of 5G base stations during bootstrapping to fake attacks and lack of quantum-resistant solutions, proposing BORG as a transitional authentication method.


<details>
  <summary>Details</summary>
Motivation: The current 5G protocol lacks a robust authentication mechanism against fake base stations, and existing solutions are not quantum-resistant. Integration of NIST-PQC standards into 5G has not been evaluated for feasibility.

Method: The paper conducts a network-level performance characterization of integrating NIST-PQC standards and conventional digital signatures (threshold and identity-based schemes) into 5G base station authentication. It proposes BORG, a transitional solution based on a Hierarchical Identity-Based Threshold Signature scheme with a Fail-Stop property.

Result: The study reveals significant feasibility concerns with direct PQC adoption due to protocol constraints and large signature sizes. Conventional methods also face performance issues from certificate chain overhead. BORG is shown to provide post-quantum forgery detection, distributed trust, and is suitable for 5G's requirements.

Conclusion: Direct integration of NIST-PQC standards into 5G for authentication is infeasible due to identified performance issues. BORG is positioned as an effective transitional solution toward future quantum-resilient 5G authentication, offering distributed and compact signatures.

Abstract: The 5G protocol lacks a robust base station authentication mechanism during
the initial bootstrapping phase, leaving it susceptible to threats such as fake
base station attacks. Conventional solutions, including digital signatures
based on Public Key Infrastructures (PKIs) and identity-based signatures, are
inadequate against quantum-capable adversaries. While integrating NIST's
Post-Quantum Cryptography (PQC) standards is a leading approach for quantum
resistance, their suitability for 5G base station authentication remains
unexplored. Moreover, current solutions are predominantly centralized and lack
security features such as distributed authentication. This work presents, to
our knowledge, the first comprehensive network-level performance
characterization of integrating NIST-PQC standards and conventional digital
signatures (including threshold and identity-based schemes) into 5G base
station authentication. Our findings reveal significant feasibility concerns,
with direct PQC adoption hindered by protocol constraints and large signature
sizes. We also highlight the performance limitations of conventional methods
due to the overhead of certificate chains. To mitigate these challenges, we
propose BORG, a transitional authentication solution based on a Hierarchical
Identity-Based Threshold Signature scheme with a Fail-Stop property. BORG
offers post-mortem post-quantum forgery detection and distributed trust via
threshold and compact signatures, well-suited for 5G's stringent requirements.
Our performance analysis underscores an important warning on the infeasibility
of direct PQC integration and positions BORG as an effective transitional
solution toward future quantum-resilient 5G authentication.

</details>


### [37] [Towards a Functionally Complete and Parameterizable TFHE Processor](https://arxiv.org/abs/2510.23483)
*Valentin Reyes HÃ¤usler,Gabriel Ott,Aruna Jayasena,Andreas Peter*

Main category: cs.CR

TL;DR: This paper presents an FPGA-based hardware accelerator for the Torus-based Fully Homomorphic Encryption (TFHE) scheme, aiming to address the high computational overhead and memory bandwidth costs faced by existing implementations. The design is fully integrated on the FPGA and features an improved bootstrapping module that significantly increases throughput.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is driven by the computational inefficiency of current TFHE implementations. Although TFHE offers fast bootstrapping, it incurs high memory bandwidth costs and suffers from a bottleneck during the evaluation of encrypted circuits. As a result, the performance is orders of magnitude slower than its unencrypted counterpart, which limits its adoption for practical applications involving sensitive data. The authors aim to overcome these limitations by designing a dedicated FPGA-based solution.

Method: The authors propose a functionally complete TFHE processor implemented on FPGA hardware. This design integrates the entire homomorphic computation process into the FPGA, including the evaluation of encrypted data and the execution of operations. They introduce an improved programmable bootstrapping module, a critical component that accelerates the reinitialization of cryptographic parameters during computation. Additionally, the design is optimized for efficiency, compactness, and scalability to support a broader range of homomorphic circuit evaluations.

Result: The proposed FPGA-based TFHE processor achieves a 240% to 480% improvement in the number of bootstrappings per second compared to current state-of-the-art implementations of the TFHE scheme.

Conclusion: The paper concludes that the introduced hardware accelerator provides a significant advancement in practical TFHE performance, offering a scalable and efficient foundation for fully homomorphic encryption on FPGAs. This advancement could lead to a wider adoption of (T)FHE in scenarios requiring secure outsourced computation, as the authors' design effectively mitigates key limitations of existing software-based solutions.

Abstract: Fully homomorphic encryption allows the evaluation of arbitrary functions on
encrypted data. It can be leveraged to secure outsourced and multiparty
computation. TFHE is a fast torus-based fully homomorphic encryption scheme
that allows both linear operations, as well as the evaluation of arbitrary
non-linear functions. It currently provides the fastest bootstrapping operation
performance of any other FHE scheme. Despite its fast performance, TFHE suffers
from a considerably higher computational overhead for the evaluation of
homomorphic circuits. Computations in the encrypted domain are orders of
magnitude slower than their unencrypted equivalents. This bottleneck hinders
the widespread adoption of (T)FHE for the protection of sensitive data. While
state-of-the-art implementations focused on accelerating and outsourcing single
operations, their scalability and practicality are constrained by high memory
bandwidth costs. In order to overcome this, we propose an FPGA-based hardware
accelerator for the evaluation of homomorphic circuits. Specifically, we design
a functionally complete TFHE processor for FPGA hardware capable of processing
instructions on the data completely on the FPGA. In order to achieve a higher
throughput from our TFHE processor, we implement an improved programmable
bootstrapping module which outperforms the current state-of-the-art by 240\% to
480\% more bootstrappings per second. Our efficient, compact, and scalable
design lays the foundation for implementing complete FPGA-based TFHE processor
architectures.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [38] [Software Engineering Agents for Embodied Controller Generation : A Study in Minigrid Environments](https://arxiv.org/abs/2510.21902)
*TimothÃ© Boulet,Xavier Hinaut,ClÃ©ment Moulin-Frier*

Main category: cs.SE

TL;DR: This paper evaluates SWE-Agents on controller generation for embodied tasks, adapting MSWEA to solve tasks in the Minigrid environment under different information access conditions.


<details>
  <summary>Details</summary>
Motivation: While SWE-Agents are effective for traditional software engineering tasks, their performance on embodied tasks requiring extensive information discovery has not been studied. The paper aims to address this gap and establish a new evaluation domain for SWE-Agents.

Method: The authors adapted the Mini-SWE-Agent (MSWEA) to solve 20 diverse embodied tasks in the Minigrid environment. They conducted experiments comparing agent performance under different information access conditions: with and without environment source code access, and varying interactive exploration capabilities.

Result: The experiments quantified the impact of different information access levels on SWE-Agent performance for embodied tasks, and analyzed the relative importance of static code analysis versus dynamic exploration.

Conclusion: This work highlights the importance of controller generation for embodied tasks as an evaluation domain for SWE-Agents, providing baseline results for future research in efficient reasoning systems.

Abstract: Software Engineering Agents (SWE-Agents) have proven effective for
traditional software engineering tasks with accessible codebases, but their
performance for embodied tasks requiring well-designed information discovery
remains unexplored. We present the first extended evaluation of SWE-Agents on
controller generation for embodied tasks, adapting Mini-SWE-Agent (MSWEA) to
solve 20 diverse embodied tasks from the Minigrid environment. Our experiments
compare agent performance across different information access conditions: with
and without environment source code access, and with varying capabilities for
interactive exploration. We quantify how different information access levels
affect SWE-Agent performance for embodied tasks and analyze the relative
importance of static code analysis versus dynamic exploration for task solving.
This work establishes controller generation for embodied tasks as a crucial
evaluation domain for SWE-Agents and provides baseline results for future
research in efficient reasoning systems.

</details>


### [39] [TOM-SWE: User Mental Modeling For Software Engineering Agents](https://arxiv.org/abs/2510.21903)
*Xuhui Zhou,Valerie Chen,Zora Zhiruo Wang,Graham Neubig,Maarten Sap,Xingyao Wang*

Main category: cs.SE

TL;DR: ToM-SWE pairs a coding agent with a theory-of-mind partner that tracks user intent, achieving state-of-the-art results (59.7% success) on software engineering benchmarks and high practical usability (86% usefulness) through persistent user modeling.


<details>
  <summary>Details</summary>
Motivation: Existing coding agents struggle to infer user intent in underspecified or context-dependent scenarios, limiting their practical effectiveness despite technical capabilities.

Method: The authors propose ToM-SWE, a dual-agent system combining a primary SWE agent with a ToM partner agent. The ToM agent maintains persistent memory of user goals and preferences from interaction history, providing contextual guidance to the SWE agent.

Result: ToM-SWE achieves 59.7% task success on the stateful SWE-bench (vs. 18.1% for OpenHands) and demonstrates 86% usefulness in a three-week study with professional developers. It shows improved performance on ambiguous and stateful tasks.

Conclusion: The study demonstrates that stateful user modeling significantly enhances the performance of coding agents, particularly in handling context-dependent tasks and improving real-world utility for developers.

Abstract: Recent advances in coding agents have made them capable of planning, editing,
running, and testing complex code bases. Despite their growing ability in
coding tasks, these systems still struggle to infer and track user intent,
especially when instructions are underspecified or context-dependent. To bridge
this gap, we introduce ToM-SWE, a dual-agent architecture that pairs a primary
software-engineering (SWE) agent with a lightweight theory-of-mind (ToM)
partner agent dedicated to modeling the user's mental state. The ToM agent
infers user goals, constraints, and preferences from instructions and
interaction history, maintains a \textbf{persistent memory} of the user, and
provides user-related suggestions to the SWE agent. In two software engineering
benchmarks (ambiguous SWE-bench and stateful SWE-bench), ToM-SWE improves task
success rates and user satisfaction. Notably, on the stateful SWE benchmark, a
newly introduced evaluation that provides agents with a user simulator along
with previous interaction histories, ToM-SWE achieves a substantially higher
task success rate of 59.7\% compared to 18.1\% for OpenHands, a
state-of-the-art SWE agent. Furthermore, in a three-week study with
professional developers using ToM-SWE in their daily work, participants found
it useful 86\% of the time, underscoring the value of stateful user modeling
for practical coding agents.

</details>


### [40] [A Comparison of Conversational Models and Humans in Answering Technical Questions: the Firefox Case](https://arxiv.org/abs/2510.21933)
*Joao Correia,Daniel Coutinho,Marco Castelluccio,Caio Barbosa,Rafael de Mello,Anita Sarma,Alessandro Garcia,Marco Gerosa,Igor Steinmacher*

Main category: cs.SE

TL;DR: RAG improves comprehensive developer assistance in OSS projects but requires optimization for conciseness to maximize effectiveness in large projects like Mozilla Firefox.


<details>
  <summary>Details</summary>
Motivation: The paper aims to assess whether Retrieval-Augmented Generation (RAG) can improve developer support in large OSS projects. It addresses the challenge of balancing assistance quality with reduced workload for core maintainers in environments requiring scalable, high-quality responses.

Method: The researchers conducted an empirical analysis comparing responses from human developers, a standard GPT model, and a GPT model augmented with RAG. Real-world queries from Mozilla's developer chat rooms were evaluated by experts using metrics for helpfulness, comprehensiveness, and conciseness.

Result: RAG-enhanced responses outperformed human developers in comprehensiveness (62.50% vs. 54.17%) and were nearly as helpful (75.00% vs. 79.17%). However, RAG responses were frequently verbose and less concise compared to human answers.

Conclusion: The study concludes that Retrieval-Augmented Generation (RAG) shows potential to enhance developer assistance in Open Source Software (OSS) projects like Mozilla Firefox, reducing the burden on maintainers while maintaining answer quality. However, improvements in conciseness and retrieval mechanisms are recommended for future applications.

Abstract: The use of Large Language Models (LLMs) to support tasks in software
development has steadily increased over recent years. From assisting developers
in coding activities to providing conversational agents that answer newcomers'
questions. In collaboration with the Mozilla Foundation, this study evaluates
the effectiveness of Retrieval-Augmented Generation (RAG) in assisting
developers within the Mozilla Firefox project. We conducted an empirical
analysis comparing responses from human developers, a standard GPT model, and a
GPT model enhanced with RAG, using real queries from Mozilla's developer chat
rooms. To ensure a rigorous evaluation, Mozilla experts assessed the responses
based on helpfulness, comprehensiveness, and conciseness. The results show that
RAG-assisted responses were more comprehensive than human developers (62.50% to
54.17%) and almost as helpful (75.00% to 79.17%), suggesting RAG's potential to
enhance developer assistance. However, the RAG responses were not as concise
and often verbose. The results show the potential to apply RAG-based tools to
Open Source Software (OSS) to minimize the load to core maintainers without
losing answer quality. Toning down retrieval mechanisms and making responses
even shorter in the future would enhance developer assistance in massive
projects like Mozilla Firefox.

</details>


### [41] [ArchISMiner: A Framework for Automatic Mining of Architectural Issue-Solution Pairs from Online Developer Communities](https://arxiv.org/abs/2510.21966)
*Musengamana Jean de Dieu,Ruiyin Li,Peng Liang,Mojtaba Shahin,Muhammad Waseem,Arif Ali Khan,Bangchao Wang,Mst Shamima Aktar*

Main category: cs.SE

TL;DR: ArchISMiner is a framework that mines architectural knowledge from Stack Overflow using components ArchPI and ArchISPE to detect ARPs and extract issue-solution pairs with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Developers struggle to find architectural knowledge in SO due to unstructured content, requiring manual sorting through posts, which is time-consuming and error-prone.

Method: The framework uses ArchPI for training and evaluating ML, DL, and LLM models to identify ARPs and ArchISPE for feature-based extraction of architectural issues and solutions using BERT and TextCNN.

Result: ArchISMiner achieves an F1-score of 0.960 for ARP detection and 0.883-0.894 for issue and solution pairs. It was validated through user studies and additional forum applications.

Conclusion: ArchISMiner efficiently assists architects and developers in extracting and identifying architectural knowledge from online developer communities with accuracy.

Abstract: Stack Overflow (SO), a leading online community forum, is a rich source of
software development knowledge. However, locating architectural knowledge, such
as architectural solutions remains challenging due to the overwhelming volume
of unstructured content and fragmented discussions. Developers must manually
sift through posts to find relevant architectural insights, which is
time-consuming and error-prone. This study introduces ArchISMiner, a framework
for mining architectural knowledge from SO. The framework comprises two
complementary components: ArchPI and ArchISPE. ArchPI trains and evaluates
multiple models, including conventional ML/DL models, Pre-trained Language
Models (PLMs), and Large Language Models (LLMs), and selects the
best-performing model to automatically identify Architecture-Related Posts
(ARPs) among programming-related discussions. ArchISPE employs an indirect
supervised approach that leverages diverse features, including BERT embeddings
and local TextCNN features, to extract architectural issue-solution pairs. Our
evaluation shows that the best model in ArchPI achieves an F1-score of 0.960 in
ARP detection, and ArchISPE outperforms baselines in both SE and NLP fields,
achieving F1-scores of 0.883 for architectural issues and 0.894 for solutions.
A user study further validated the quality (e.g., relevance and usefulness) of
the identified ARPs and the extracted issue-solution pairs. Moreover, we
applied ArchISMiner to three additional forums, releasing a dataset of over 18K
architectural issue-solution pairs. Overall, ArchISMiner can help architects
and developers identify ARPs and extract succinct, relevant, and useful
architectural knowledge from developer communities more accurately and
efficiently. The replication package of this study has been provided at
https://github.com/JeanMusenga/ArchISPE

</details>


### [42] [FeaGPT: an End-to-End agentic-AI for Finite Element Analysis](https://arxiv.org/abs/2510.21993)
*Yupeng Qi,Ran Xu,Xu Chu*

Main category: cs.SE

TL;DR: FeaGPT is a novel framework that automates the entire geometry-mesh-simulation workflow for engineering applications using natural language. It successfully transforms engineering specs into validated simulations, confirmed through industrial and parametric studies.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between complex FEA processes and user accessibility by enabling fully automated workflows through natural language interaction.

Method: FeaGPT implements an integrated GMSA pipeline with modules for intent interpretation, mesh generation, simulation setup including boundary condition inference, and multi-objective analysis through closed-loop iteration.

Result: Validated CalculiX simulations for turbocharger cases and successful processing of 432 NACA airfoil configurations, confirming end-to-end automation and scalability.

Conclusion: Natural language interfaces can democratize advanced FEA tools while maintaining analytical accuracy, marking a significant step in computational engineering accessibility.

Abstract: Large language models (LLMs) are establishing new paradigms for engineering
applications by enabling natural language control of complex computational
workflows. This paper introduces FeaGPT, the first framework to achieve
complete geometry-mesh-simulation workflows through conversational interfaces.
Unlike existing tools that automate individual FEA components, FeaGPT
implements a fully integrated Geometry-Mesh-Simulation-Analysis (GMSA) pipeline
that transforms engineering specifications into validated computational results
without manual intervention. The system interprets engineering intent,
automatically generates physics-aware adaptive meshes, configures complete FEA
simulations with proper boundary condition inference, and performs
multi-objective analysis through closed-loop iteration.
  Experimental validation confirms complete end-to-end automation capability.
Industrial turbocharger cases (7-blade compressor and 12-blade turbine at
\SI{110000}{rpm}) demonstrate the system successfully transforms natural
language specifications into validated CalculiX simulations, producing
physically realistic results for rotating machinery analysis. Additional
validation through 432 NACA airfoil configurations confirms scalability for
parametric design exploration. These results demonstrate that natural language
interfaces can effectively democratize access to advanced computational
engineering tools while preserving analytical rigor.

</details>


### [43] [Impact and Implications of Generative AI for Enterprise Architects in Agile Environments: A Systematic Literature Review](https://arxiv.org/abs/2510.22003)
*Stefan Julian Kooy,Jean Paul Sebastian Piest,Rob Henk Bemthuis*

Main category: cs.SE

TL;DR: This paper provides a systematic literature review of 33 studies on Generative AI in enterprise architecture, highlighting key use cases, risks, required skills, and implications for governance and research.


<details>
  <summary>Details</summary>
Motivation: Generative AI is changing enterprise architecture, but there is a lack of comprehensive evidence. The study aims to gather and synthesize existing knowledge to guide responsible adoption.

Method: The authors conducted a systematic literature review following established protocols (Kitchenham and PRISMA), starting with 1,697 records and selecting 33 relevant studies.

Result: Findings include GenAI's support for design, artifact creation, and decision-making; risks like opacity and bias; and insights on required skills and organizational enablers for adoption.

Conclusion: The study maps GenAI use cases and risks in agile architecting, offers guidance on capability building and governance, and outlines a research agenda for human-AI collaboration in architecture.

Abstract: Generative AI (GenAI) is reshaping enterprise architecture work in agile
software organizations, yet evidence on its effects remains scattered. We
report a systematic literature review (SLR), following established SLR
protocols of Kitchenham and PRISMA, of 1,697 records, yielding 33 studies
across enterprise, solution, domain, business, and IT architect roles. GenAI
most consistently supports (i) design ideation and trade-off exploration; (ii)
rapid creation and refinement of artifacts (e.g., code, models, documentation);
and (iii) architectural decision support and knowledge retrieval. Reported
risks include opacity and bias, contextually incorrect outputs leading to
rework, privacy and compliance concerns, and social loafing. We also identify
emerging skills and competencies, including prompt engineering, model
evaluation, and professional oversight, and organizational enablers around
readiness and adaptive governance. The review contributes with (1) a mapping of
GenAI use cases and risks in agile architecting, (2) implications for
capability building and governance, and (3) an initial research agenda on
human-AI collaboration in architecture. Overall, the findings inform
responsible adoption of GenAI that accelerates digital transformation while
safeguarding architectural integrity.

</details>


### [44] [LSPRAG: LSP-Guided RAG for Language-Agnostic Real-Time Unit Test Generation](https://arxiv.org/abs/2510.22210)
*Gwihwan Go,Quan Zhang,Chijin Zhou,Zhao Wei,Yu Jiang*

Main category: cs.SE

TL;DR: LSPRAG is a framework for language-agnostic, real-time unit test generation using Language Server Protocol (LSP), improving test coverage by 31.57%-213.31% across Java/Go/Python without language-specific engineering.


<details>
  <summary>Details</summary>
Motivation: Existing test generation approaches suffer from poor language generalization, imprecise context retrieval via similarity searches, and high infrastructure costs for language pipelines.

Method: Leverages existing LSP backends to provide LLMs with precise symbol definitions/references in real-time, reusing mature language servers for context retrieval with minimal language-specific effort.

Result: For Java/Go/Python projects, LSPRAG outperformed baselines by 174.55%, 213.31%, and 31.57%-point line coverage improvements respectively in open-source evaluations.

Conclusion: LSPRAG demonstrates that language server integration is a scalable solution for high-coverage test generation across programming languages without custom static analysis pipelines.

Abstract: Automated unit test generation is essential for robust software development,
yet existing approaches struggle to generalize across multiple programming
languages and operate within real-time development. While Large Language Models
(LLMs) offer a promising solution, their ability to generate high coverage test
code depends on prompting a concise context of the focal method. Current
solutions, such as Retrieval-Augmented Generation, either rely on imprecise
similarity-based searches or demand the creation of costly, language-specific
static analysis pipelines. To address this gap, we present LSPRAG, a framework
for concise-context retrieval tailored for real-time, language-agnostic unit
test generation. LSPRAG leverages off-the-shelf Language Server Protocol (LSP)
back-ends to supply LLMs with precise symbol definitions and references in real
time. By reusing mature LSP servers, LSPRAG provides an LLM with language-aware
context retrieval, requiring minimal per-language engineering effort. We
evaluated LSPRAG on open-source projects spanning Java, Go, and Python.
Compared to the best performance of baselines, LSPRAG increased line coverage
by up to 174.55% for Golang, 213.31% for Java, and 31.57% for Python.

</details>


### [45] [Taming Silent Failures: A Framework for Verifiable AI Reliability](https://arxiv.org/abs/2510.22224)
*Guan-Yan Yang,Farn Wang*

Main category: cs.SE

TL;DR: FAME addresses AI silent failures in safety-critical systems via formal synthesis and runtime monitoring, enabling certifiable safety for autonomous vehicles.


<details>
  <summary>Details</summary>
Motivation: AI's silent failures (confidently incorrect outputs) pose safety risks in critical systems, requiring verifiable assurance frameworks beyond traditional reliability approaches.

Method: FAME combines offline formal synthesis (mathematical rigor for safety preconditions) with online runtime monitoring (dynamic violation detection), validated on autonomous vehicle perception systems.

Result: Achieved 93.56% detection of critical safety violations in autonomous vehicle tests, with alignment to ISO 26262/PAS 8800 standards enabling certifiable AI deployment.

Conclusion: FAME shifts AI system design from probabilistic performance to provable safety, providing a certified pathway for trustworthy implementation in compliance with industry standards.

Abstract: The integration of Artificial Intelligence (AI) into safety-critical systems
introduces a new reliability paradigm: silent failures, where AI produces
confident but incorrect outputs that can be dangerous. This paper introduces
the Formal Assurance and Monitoring Environment (FAME), a novel framework that
confronts this challenge. FAME synergizes the mathematical rigor of offline
formal synthesis with the vigilance of online runtime monitoring to create a
verifiable safety net around opaque AI components. We demonstrate its efficacy
in an autonomous vehicle perception system, where FAME successfully detected
93.5% of critical safety violations that were otherwise silent. By
contextualizing our framework within the ISO 26262 and ISO/PAS 8800 standards,
we provide reliability engineers with a practical, certifiable pathway for
deploying trustworthy AI. FAME represents a crucial shift from accepting
probabilistic performance to enforcing provable safety in next-generation
systems.

</details>


### [46] [Understanding Self-Admitted Technical Debt in Test Code: An Empirical Study](https://arxiv.org/abs/2510.22249)
*Ibuki Nakamura,Yutaro Kashiwa,Bin Lin,Hajimu Iida*

Main category: cs.SE

TL;DR: The paper discusses the prevalence of Self-Admitted Technical Debt (SATD) in test code, its different types, and its impact on test quality. It also introduces a machine learning model for categorizing SATD in test code.


<details>
  <summary>Details</summary>
Motivation: Existing studies on SATD often focus on production code and assume similar characteristics in test code, ignoring potential differences. The authors aim to address this gap by analyzing test code SATD and its implications.

Method: The authors collected 17,766 SATD comments from 50 repositories, analyzed the distribution and types of SATD in test code, and used machine learning models for classification, including a CodeBERT-based model.

Result: The study found that SATD is common in test code but not directly linked to test smells. The CodeBERT model achieved the best performance in classifying SATD comments according to their types.

Conclusion: The paper concludes that SATD in test code requires specific attention, as its characteristics differ from production code. Future approaches should consider these differences for better management and analysis of technical debt.

Abstract: Developers often opt for easier but non-optimal implementation to meet
deadlines or create rapid prototypes, leading to additional effort known as
technical debt to improve the code later. Oftentimes, developers explicitly
document the technical debt in code comments, referred to as Self-Admitted
Technical Debt (SATD). Numerous researchers have investigated the impact of
SATD on different aspects of software quality and development processes.
However, most of these studies focus on SATD in production code, often
overlooking SATD in the test code or assuming that it shares similar
characteristics with SATD in production code. In fact, a significant amount of
SATD is also present in the test code, with many instances not fitting into
existing categories for the production code. This study aims to fill this gap
and disclose the nature of SATD in the test code by examining its distribution
and types. Moreover, the relation between its presence and test quality is also
analyzed. Our empirical study, involving 17,766 SATD comments (14,987 from
production code, 2,779 from test code) collected from 50 repositories,
demonstrates that while SATD widely exists in test code, it is not directly
associated with test smells. Our study also presents comprehensive categories
of SATD types in the test code, and machine learning models are developed to
automatically classify SATD comments based on their types for easier
management. Our results show that the CodeBERT-based model outperforms other
machine learning models in terms of recall and F1-score. However, the
performance varies on different types of SATD.

</details>


### [47] [Ten Simple Rules for AI-Assisted Coding in Science](https://arxiv.org/abs/2510.22254)
*Eric W. Bridgeford,Iain Campbell,Zijao Chen,Zhicheng Lin,Harrison Ritz,Joachim Vandekerckhove,Russell A. Poldrack*

Main category: cs.SE

TL;DR: The paper offers ten practical guidelines for using AI coding tools in scientific research, emphasizing the balance between AI's efficiency and scientific rigor through themes like problem understanding, context management, testing, and quality assurance.


<details>
  <summary>Details</summary>
Motivation: AI coding tools may accelerate software development, but their integration into scientific computing necessitates ensuring code quality and scientific validity, which are critical for research integrity.

Method: We present four key themes and ten practical rules for AI-assisted coding to help researchers maintain methodological rigor while leveraging AI capabilities.

Result: The ten rules facilitate effective integration of AI into the scientific development process, ensuring code reliability and reproducibility.

Conclusion: Researchers can effectively use AI coding tools without compromising the quality and validity of their scientific work by following these ten rules, which ensure human oversight and rigorous validation.

Abstract: While AI coding tools have demonstrated potential to accelerate software
development, their use in scientific computing raises critical questions about
code quality and scientific validity. In this paper, we provide ten practical
rules for AI-assisted coding that balance leveraging capabilities of AI with
maintaining scientific and methodological rigor. We address how AI can be
leveraged strategically throughout the development cycle with four key themes:
problem preparation and understanding, managing context and interaction,
testing and validation, and code quality assurance and iterative improvement.
These principles serve to emphasize maintaining human agency in coding
decisions, establishing robust validation procedures, and preserving the domain
expertise essential for methodologically sound research. These rules are
intended to help researchers harness AI's transformative potential for faster
software development while ensuring that their code meets the standards of
reliability, reproducibility, and scientific validity that research integrity
demands.

</details>


### [48] [Harnessing the Power of Large Language Models for Software Testing Education: A Focus on ISTQB Syllabus](https://arxiv.org/abs/2510.22318)
*Tuan-Phong Ngo,Bao-Ngoc Duong,Tuan-Anh Hoang,Joshua Dwight,Ushik Shrestha Khwakhali*

Main category: cs.SE

TL;DR: This paper explores integrating generative AI with ISTQB-based learning in higher education, creating a dataset, optimizing prompts, evaluating LLMs, and providing actionable insights.


<details>
  <summary>Details</summary>
Motivation: Academia needs to update educational methods to keep pace with advancements in software testing and AI. While ISTQB is widely recognized, its integration with recent generative AI advancements like LLMs has not been thoroughly explored, especially in higher education contexts.

Method: The research involves four main contributions: building a comprehensive ISTQB-aligned dataset with historical exams and questions; developing domain-optimized prompts for LLM performance in testing tasks; systematically evaluating SotA LLMs on the dataset; and generating educational recommendations based on these findings.

Result: The study created a decade-spanning dataset (28 exams, 1,145 questions), developed optimized prompts significantly improving LLM performance, and systematically validated these on various LLMs. The implemented evaluation framework yielded actionable educational integration strategies.

Conclusion: LLMs show promise in supporting ISTQB certification training through enhanced performance on testing tasks when given appropriate domain knowledge. This demonstrates a viable path to integrate advanced AI tools into software engineering education as a scalable teaching resource.

Abstract: Software testing is a critical component in the software engineering field
and is important for software engineering education. Thus, it is vital for
academia to continuously improve and update educational methods to reflect the
current state of the field. The International Software Testing Qualifications
Board (ISTQB) certification framework is globally recognized and widely adopted
in industry and academia. However, ISTQB-based learning has been rarely applied
with recent generative artificial intelligence advances. Despite the growing
capabilities of large language models (LLMs), ISTQB-based learning and
instruction with LLMs have not been thoroughly explored. This paper explores
and evaluates how LLMs can complement the ISTQB framework for higher education.
The findings present four key contributions: (i) the creation of a
comprehensive ISTQB-aligned dataset spanning over a decade, consisting of 28
sample exams and 1,145 questions; (ii) the development of a domain-optimized
prompt that enhances LLM precision and explanation quality on ISTQB tasks;
(iii) a systematic evaluation of state-of-the-art LLMs on this dataset; and
(iv) actionable insights and recommendations for integrating LLMs into software
testing education. These findings highlight the promise of LLMs in supporting
ISTQB certification preparation and offer a foundation for their broader use in
software engineering at higher education.

</details>


### [49] [Operationalizing Large Language Models with Design-Aware Contexts for Code Comment Generation](https://arxiv.org/abs/2510.22338)
*Aritra Mitra,Srijoni Majumdar,Anamitra Mukhopadhyay,Partha Pratim Das,Paul D Clough,Partha Pratim Chakrabarti*

Main category: cs.SE

TL;DR: This study shows that using design documents as context enables large language models to generate more useful code comments, offering a promising solution to the persistent problem of ineffective commenting in novice-developed codebases.


<details>
  <summary>Details</summary>
Motivation: Poorly written code comments by novice developers waste maintenance time; standardization of commenting practices has proven challenging despite the growing importance of code reuse.

Method: The study evaluates the feasibility of leveraging design documents as contextual input for LLMs to generate more useful code comments compared to traditional approaches.

Result: Preliminary findings demonstrate that design documents provide sufficient contextual guidance for LLMs to generate semantically meaningful comments that better explain code intent.

Conclusion: Using large language models (LLMs) with design documents as context can significantly improve code comment quality, addressing common inefficiencies in novice-coded projects.

Abstract: Comments are very useful to the flow of code development. With the increasing
commonality of code, novice coders have been creating a significant amount of
codebases. Due to lack of commenting standards, their comments are often
useless, and increase the time taken to further maintain codes. This study
intends to find the usefulness of large language models (LLMs) in these cases
to generate potentially better comments. This study focuses on the feasibility
of design documents as a context for the LLMs to generate more useful comments,
as design documents are often used by maintainers to understand code when
comments do not suffice.

</details>


### [50] [A First Look at the Self-Admitted Technical Debt in Test Code: Taxonomy and Detection](https://arxiv.org/abs/2510.22409)
*Shahidul Islam,Md Nahidul Islam Opu,Shaowei Wang,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: First study on SATD in test code: found 15 SATD categories, existing detection tools have limited reliability, and LLMs perform poorly. Highlights need for test-specific SATD detection methods.


<details>
  <summary>Details</summary>
Motivation: Existing research on SATD focuses on source code, leaving test code SATD unexplored despite its potential to increase maintenance effort. This work addresses this critical gap.

Method: Manual analysis of 50,000 comments from 1.6 million test code comments across 1,000 Java projects, combined with evaluation of SATD detection tools (e.g., MAT) and both open-source and proprietary LLMs for automated detection.

Result: Identified 615 test code SATD comments categorized into 15 types; MAT showed best detection performance (moderate recall), while LLMs had poor accuracy due to low precision.

Conclusion: This paper provides the first large-scale analysis of SATD in test code, offering a taxonomy of 15 categories, highlighting limitations in existing detection tools and LLMs, and laying groundwork for future test code-specific SATD research.

Abstract: Self-admitted technical debt (SATD) refers to comments in which developers
explicitly acknowledge code issues, workarounds, or suboptimal solutions. SATD
is known to significantly increase software maintenance effort. While extensive
research has examined SATD in source code, its presence and impact in test code
have received no focused attention, leaving a significant gap in our
understanding of how SATD manifests in testing contexts.
  This study, the first of its kind, investigates SATD in test code by manually
analyzing 50,000 comments randomly sampled from 1.6 million comments across
1,000 open-source Java projects. From this sample, after manual analysis and
filtering, we identified 615 SATD comments and classified them into 15 distinct
categories, building a taxonomy of test code SATD. To investigate whether test
code SATD can be detected automatically, we evaluated existing SATD detection
tools, as well as both open-source and proprietary LLMs. Among the existing
tools, MAT performed the best, albeit with moderate recall. To our surprise,
both open-source and proprietary LLMs exhibited poor detection accuracy,
primarily due to low precision. These results indicate that neither existing
approaches nor current LLMs can reliably detect SATD in test code.
  Overall, this work provides the first large-scale analysis of SATD in test
code, a nuanced understanding of its types, and the limitations of current SATD
detection methods. Our findings lay the groundwork for future research on test
code-specific SATD.

</details>


### [51] [A Multifaceted View on Discrimination in Software Development Careers](https://arxiv.org/abs/2510.22457)
*Shalini Chakraborty,Sebastian Baltes*

Main category: cs.SE

TL;DR: This paper highlights the often overlooked forms of discrimination in the software engineering field, specifically age, political perspective, disabilities, and cognitive differences, revealing their prevalence and impact on the workforce. The findings aim to broaden the scope of diversity and inclusion research in this area.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this study stems from the recognition that while gender and racial disparities in software engineering are commonly discussed, other significant yet under-recognized forms of discrimination, such as those based on age, political views, and disabilities, are affecting the community disproportionately. The paper seeks to bring attention to these issues and to encourage a more comprehensive approach to diversity and inclusion.

Method: The research employed a secondary analysis of 800 qualitative responses from a survey of 8,717 participants, examining patterns and impacts of perceived discrimination across multiple facets of identity, including age, gender, race, and disability.

Result: Results indicated that while age- and gender-related discrimination are the most frequently reported, discrimination based on political and religious views is also a notable issue. Discrimination related to caregiving responsibilities was prevalent across all gender identities. Women and non-binary respondents experienced a higher frequency of workplace issues, particularly discrimination and mental health challenges.

Conclusion: The study concludes that discrimination in software engineering is multifaceted and calls for researchers to consider a broader range of identity facets beyond just age and gender when conducting software engineering studies to promote a more inclusive and comprehensive understanding of diversity in the field.

Abstract: Conversations around diversity and inclusion in software engineering often
focus on gender and racial disparities. However, the State of the Developer
Nation 2025 survey with 8,717 participants revealed that other forms of
discrimination are similarly prevalent but receive considerably less attention.
This includes discrimination based on age, political perspective, disabilities,
or cognitive differences such as neurodivergence. We conducted a secondary
analysis of 800 open-ended survey responses to examine patterns of perceived
discrimination, as well as related challenges and negative impacts. Our study
covers multiple identity facets, including age, gender, race, and disability.
We found that age- and gender-related discrimination was the most frequently
reported workplace issue, but discrimination based on political and religious
views emerged as further notable concerns. Most of the participants who
identified as female cited gender as the primary source of discrimination,
often accompanied by intersectional factors such as race, political views, age,
or sexual orientation. Discrimination related to caregiving responsibilities
was reported by all gender identities. Regarding the negative impacts of
workplace issues, many participants described modifying their appearance or
behavior in response to gender biases. Gender also appeared to influence
broader career challenges, as women and non-binary respondents reported
experiencing almost all workplace issues at higher rates, particularly
discrimination (35%) and mental health challenges (62%). Our goal is to raise
awareness in the research community that discrimination in software development
is multifaceted, and to encourage researchers to select and assess relevant
facets beyond age and gender when designing software engineering studies.

</details>


### [52] [Finding the Needle in the Crash Stack: Industrial-Scale Crash Root Cause Localization with AutoCrashFL](https://arxiv.org/abs/2510.22530)
*Sungmin Kang,Sumi Yun,Jingun Hong,Shin Yoo,Gabin An*

Main category: cs.SE

TL;DR: This paper proposes AutoCrashFL, an LLM agent for crash fault localization that only requires crashdump and source code access. Evaluated on SAP HANA, it achieves 30% top localization compared to 17% for baselines. It is effective for complex bugs and provides result confidence.


<details>
  <summary>Details</summary>
Motivation: Traditional FL methods using coverage profiling or mutation testing are costly for large industrial software. Need for an efficient, effective fault localization method that does not require coverage data for every execution.

Method: AutoCrashFL is an LLM agent that uses crashdump from the Program Under Test (PUT) and access to the corresponding source code repository. It localizes crashes without requiring test execution coverage data.

Result: On SAP HANA, AutoCrashFL identified 30% of crashes at the top ranking, compared to 17% by baselines. It shows higher effectiveness for complex bugs and provides confidence estimates for its results.

Conclusion: AutoCrashFL demonstrates the practicality of LLM agents for industrial-scale fault localization. It overcomes limitations of traditional methods by not requiring coverage data and achieves better results on complex bugs.

Abstract: Fault Localization (FL) aims to identify root causes of program failures. FL
typically targets failures observed from test executions, and as such, often
involves dynamic analyses to improve accuracy, such as coverage profiling or
mutation testing. However, for large industrial software, measuring coverage
for every execution is prohibitively expensive, making the use of such
techniques difficult. To address these issues and apply FL in an industrial
setting, this paper proposes AutoCrashFL, an LLM agent for the localization of
crashes that only requires the crashdump from the Program Under Test (PUT) and
access to the repository of the corresponding source code. We evaluate
AutoCrashFL against real-world crashes of SAP HANA, an industrial software
project consisting of more than 35 million lines of code. Experiments reveal
that AutoCrashFL is more effective in localization, as it identified 30%
crashes at the top, compared to 17% achieved by the baseline. Through thorough
analysis, we find that AutoCrashFL has attractive practical properties: it is
relatively more effective for complex bugs, and it can indicate confidence in
its results. Overall, these results show the practicality of LLM agent
deployment on an industrial scale.

</details>


### [53] [DynaCausal: Dynamic Causality-Aware Root Cause Analysis for Distributed Microservices](https://arxiv.org/abs/2510.22613)
*Songhan Zhang,Aoyang Fang,Yifan Yang,Ruiyi Cheng,Xiaoying Tang,Pinjia He*

Main category: cs.SE

TL;DR: DynaCausal is a dynamic causality-aware framework for root cause analysis in cloud-native microservices that addresses limitations in capturing dynamic behaviors and service relationships, outperforming existing methods in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Cloud-native microservices create complex, fast-evolving dependencies that challenge reliable diagnosis, with existing RCA approaches limited in capturing dynamic behaviors, vulnerable to noise, and over-reliant on deviation intensity.

Method: DynaCausal uses multi-modal dynamic signal unification, interaction-aware representation learning for spatio-temporal dependencies, dynamic contrastive mechanisms to disentangle fault indicators from noise, and causal-prioritized pairwise ranking for causal attribution optimization.

Result: DynaCausal shows average AC@1 of 0.63 with gains from 0.25 to 0.46 over state-of-the-art methods in public benchmarks, achieving accurate and interpretable diagnoses.

Conclusion: DynaCausal effectively handles dynamic microservice environments through its causality-aware design, addressing key challenges in RCA for cloud-native systems while delivering superior performance.

Abstract: Cloud-native microservices enable rapid iteration and scalable deployment but
also create complex, fast-evolving dependencies that challenge reliable
diagnosis. Existing root cause analysis (RCA) approaches, even with multi-modal
fusion of logs, traces, and metrics, remain limited in capturing dynamic
behaviors and shifting service relationships. Three critical challenges
persist: (i) inadequate modeling of cascading fault propagation, (ii)
vulnerability to noise interference and concept drift in normal service
behavior, and (iii) over-reliance on service deviation intensity that obscures
true root causes. To address these challenges, we propose DynaCausal, a dynamic
causality-aware framework for RCA in distributed microservice systems.
DynaCausal unifies multi-modal dynamic signals to capture time-varying
spatio-temporal dependencies through interaction-aware representation learning.
It further introduces a dynamic contrastive mechanism to disentangle true fault
indicators from contextual noise and adopts a causal-prioritized pairwise
ranking objective to explicitly optimize causal attribution. Comprehensive
evaluations on public benchmarks demonstrate that DynaCausal consistently
surpasses state-of-the-art methods, attaining an average AC@1 of 0.63 with
absolute gains from 0.25 to 0.46, and delivering both accurate and
interpretable diagnoses in highly dynamic microservice environments.

</details>


### [54] [Does In-IDE Calibration of Large Language Models work at Scale?](https://arxiv.org/abs/2510.22614)
*Roham Koohestani,Agnia Sergeyuk,David Gros,Claudio Spiess,Sergey Titov,Prem Devanbu,Maliheh Izadi*

Main category: cs.SE

TL;DR: This paper explores the challenges of applying calibration to code generation models in IDEs and finds that general post-hoc calibration (using Platt-scaling) does not consistently improve reliability. Personalized calibration can work but needs substantial user data. The study also suggests presenting reliability via color-coded indicators rather than numerical scores.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the issue of unreliable code generated by AI, particularly within IDEs, by investigating the effectiveness of confidence calibration techniques and how to communicate reliability to developers.

Method: The paper uses two main methods: (1) a scalable calibration framework to assess the impact of calibration on code reliability, using 24 million real-world interactions, and (2) a multi-phase design study involving expert designers and professional developers to understand the optimal design for reliability signals.

Result: General post-hoc calibration using Platt-scaling did not improve model confidence reliability. Personalized calibration shows promise but requires sufficient user interaction data. Design study reveals a preference for non-numerical color-coded reliability indicators in the code generation workflow.

Conclusion: The paper concludes that general calibration is ineffective for improving confidence signals in code generation, but personalized calibration could help if enough user data is available. It also recommends using color-coded indicators to communicate reliability to users.

Abstract: The introduction of large language models into integrated development
environments (IDEs) is revolutionizing software engineering, yet it poses
challenges to the usefulness and reliability of Artificial
Intelligence-generated code. Post-hoc calibration of internal model confidences
aims to align probabilities with an acceptability measure. Prior work suggests
calibration can improve alignment, but at-scale evidence is limited. In this
work, we investigate the feasibility of applying calibration of code models to
an in-IDE context. We study two aspects of the problem: (1) the technical
method for implementing confidence calibration and improving the reliability of
code generation models, and (2) the human-centered design principles for
effectively communicating reliability signal to developers. First, we develop a
scalable and flexible calibration framework which can be used to obtain
calibration weights for open-source models using any dataset, and evaluate
whether calibrators improve the alignment between model confidence and
developer acceptance behavior. Through a large-scale analysis of over 24
million real-world developer interactions across multiple programming
languages, we find that a general, post-hoc calibration model based on
Platt-scaling does not, on average, improve the reliability of model confidence
signals. We also find that while dynamically personalizing calibration to
individual users can be effective, its effectiveness is highly dependent on the
volume of user interaction data. Second, we conduct a multi-phase design study
with 3 expert designers and 153 professional developers, combining
scenario-based design, semi-structured interviews, and survey validation,
revealing a clear preference for presenting reliability signals via
non-numerical, color-coded indicators within the in-editor code generation
workflow.

</details>


### [55] [Collaborative LLM Agents for C4 Software Architecture Design Automation](https://arxiv.org/abs/2510.22787)
*Kamil Szczepanik,JarosÅaw A. Chudziak*

Main category: cs.SE

TL;DR: Researchers developed an AI system that automatically creates software architecture diagrams using collaborative LLM agents. The system generates accurate C4 models 5x faster than manual methods while maintaining quality, with different AI models showing unique strengths in architectural design tasks.


<details>
  <summary>Details</summary>
Motivation: Software architecture design is critical but manual C4 model creation is time-consuming. Existing methods lack automation and systematic quality assessment for architectural design tasks.

Method: The approach uses an LLM-based multi-agent system simulating role-specific experts to analyze requirements and generate C4 models (Context, Container, Component views). Quality is evaluated through deterministic structural/syntactic checks, C4 rule consistency, and semantic scoring via LLM-as-a-Judge.

Result: The workflow achieved fast C4 model creation with high compilation success (tested on five canonical systems) and maintained semantic fidelity. Comparative analysis of four LLMs revealed distinct strengths relevant to architectural design tasks.

Conclusion: This study contributes to automated software architecture design by introducing an LLM-based multi-agent system for generating C4 models and proposing a hybrid evaluation framework, advancing both automation and evaluation methodologies in the field.

Abstract: Software architecture design is a fundamental part of creating every software
system. Despite its importance, producing a C4 software architecture model, the
preferred notation for such architecture, remains manual and time-consuming. We
introduce an LLM-based multi-agent system that automates this task by
simulating a dialogue between role-specific experts who analyze requirements
and generate the Context, Container, and Component views of the C4 model.
Quality is assessed with a hybrid evaluation framework: deterministic checks
for structural and syntactic integrity and C4 rule consistency, plus semantic
and qualitative scoring via an LLM-as-a-Judge approach. Tested on five
canonical system briefs, the workflow demonstrates fast C4 model creation,
sustains high compilation success, and delivers semantic fidelity. A comparison
of four state-of-the-art LLMs shows different strengths relevant to
architectural design. This study contributes to automated software architecture
design and its evaluation methods.

</details>


### [56] [On the Freshness of Pinned Dependencies in Maven](https://arxiv.org/abs/2510.22815)
*Vasudev Vikram,Yuvraj Agarwal,Rohan Padhye*

Main category: cs.SE

TL;DR: The paper introduces Pin-Freshener, a tool that encourages developers to update outdated library dependencies to reduce vulnerabilities. It finds that over 60% of Maven consumers use stale pins, and shows that additional test coverage from other projects can help validate safe upgrades.


<details>
  <summary>Details</summary>
Motivation: Outdated library dependencies in software ecosystems pose security risks. Developers often pin dependencies to specific versions, but this can lead to stale pins that miss security fixes. The paper seeks to understand the prevalence of stale pins and propose a solution (Pin-Freshener) to guide developers towards safer upgrades by leveraging community test data.

Method: The study categorizes dependency pins as 'stale' or 'fresh' based on their age relative to the project's last release. They analyze a dataset of popular Maven projects to quantify the issue. Pin-Freshener's approach is tested through an empirical evaluation that measures the test coverage provided by crowdsourced test suites from other projects, demonstrating its effectiveness in detecting safe upgrades.

Result: Over 60% of Maven project consumers use stale pins, some over a year old. 10% of their dependency upgrades to the latest minor or patch versions could reduce security vulnerabilities. Pin-Freshener increases test coverage for upgrades by 35-100% using 1-5 crowdsourced test suites. Real-world evaluations find it can provide at least 5 passing test signals for over 3,000 projects.

Conclusion: Pin-Freshener demonstrates the feasibility of using crowdsourced test data to assist with safe dependency upgrades that reduce security risks, outperforming current practices by providing significant additional test coverage without requiring full suite execution.

Abstract: Library dependencies in software ecosystems play a crucial role in the
development of software. As newer releases of these libraries are published,
developers may opt to pin their dependencies to a particular version. While
pinning may have benefits in ensuring reproducible builds and avoiding breaking
changes, it bears larger risks in using outdated dependencies that may contain
bugs and security vulnerabilities. To understand the frequency and consequences
of dependency pinning, we first define the concepts of stale and fresh pins,
which are distinguished based on how outdated the dependency is relative to the
release date of the project. We conduct an empirical study to show that over
60% of consumers of popular Maven libraries contain stale pins to their
dependencies, with some outdated versions over a year old. These pinned
versions often miss out on security fixes; we find that 10% of all dependency
upgrades in our dataset to the latest minor or patch version would reduce
security vulnerabilities.
  We prototype an approach called Pin-Freshener that can encourage developers
to freshen their pins by leveraging the insight that crowdsourced tests of peer
projects can provide additional signal for the safety of an upgrade. Running
Pin-Freshener on dependency upgrades shows that just 1-5 additional test suites
can provide 35-100% more coverage of a dependency, compared to that of a single
consumer test suite. Our evaluation on real-world pins to the top 500 popular
libraries in Maven shows that Pin-Freshener can provide an additional signal of
at least 5 passing crowdsourced test suites to over 3,000 consumers to safely
perform an upgrade that reduces security vulnerabilities. Pin-Freshener can
provide practical confidence to developers by offering additional signal beyond
their own test suites, representing an improvement over current practices.

</details>


### [57] [CodeAD: Synthesize Code of Rules for Log-based Anomaly Detection with LLMs](https://arxiv.org/abs/2510.22986)
*Junjie Huang,Minghua He,Jinyang Liu,Yintong Huo,Domenico Bianculli,Michael R. Lyu*

Main category: cs.SE

TL;DR: CodeAD is a framework for log-based anomaly detection that uses LLMs to generate Python rule functions, achieving better performance and efficiency compared to existing methods while being cost-effective and interpretable.


<details>
  <summary>Details</summary>
Motivation: Machine learning and LLM-based log anomaly detection methods often face challenges like limited interpretability, high inference costs, and preprocessing needs, while rule-based systems require manual effort and lack scalability. This creates a need for a solution that combines automation, efficiency, and interpretability.

Method: CodeAD uses hierarchical clustering and anchor-grounded sampling to create contrastive log windows for training LLMs. It then employs an agentic workflow to iteratively generate, test, repair, and refine Python rule functions, making them robust and generalizable. These rules are applied directly to raw logs for efficient and transparent anomaly detection.

Result: CodeAD outperformed state-of-the-art log anomaly detection baselines by an average of 3.6% in F1 score. It processed large datasets 4 times faster with significantly lower costs (under $4 per dataset in LLM invocation costs) while maintaining high accuracy and efficiency.

Conclusion: CodeAD provides an effective, interpretable, and scalable solution for log-based anomaly detection. Its agentic workflow and rule synthesis methodology reduce manual effort and deliver high performance, making it suitable for real-time and real-world application monitoring.

Abstract: Log-based anomaly detection (LogAD) is critical for maintaining the
reliability and availability of large-scale online service systems. While
machine learning, deep learning, and large language models (LLMs)-based methods
have advanced the LogAD, they often suffer from limited interpretability, high
inference costs, and extensive preprocessing requirements, limiting their
practicality for real-time, high-volume log analysis. In contrast, rule-based
systems offer efficiency and transparency, but require significant manual
effort and are difficult to scale across diverse and evolving environments. In
this paper, We present CodeAD, a novel framework that automatically synthesizes
lightweight Python rule functions for LogAD using LLMs. CodeAD introduces a
hierarchical clustering and anchor-grounded sampling strategy to construct
representative contrastive log windows, enabling LLMs to discern discriminative
anomaly patterns. To ensure robustness and generalizability, CodeAD employs an
agentic workflow that iteratively generates, tests, repairs, and refines the
rules until it meets correctness and abstraction requirements. The synthesized
rules are interpretable, lightweight, and directly executable on raw logs,
supporting efficient and transparent online anomaly detection. Our
comprehensive experiments on three public datasets (BGL, Hadoop, Thunderbird)
demonstrate that CodeAD achieves an average absolute improvement of 3.6% F1
score over the state-of-the-art baselines, while processing large datasets up
to 4x faster and at a fraction of the cost (total LLM invocation cost under 4
USD per dataset). These results highlight CodeAD as a practical and scalable
solution for online monitoring systems, enabling interpretable, efficient, and
automated LogAD in real-world environment.

</details>


### [58] [TALM: Dynamic Tree-Structured Multi-Agent Framework with Long-Term Memory for Scalable Code Generation](https://arxiv.org/abs/2510.23010)
*Ming-Tung Shen,Yuh-Jzer Joung*

Main category: cs.SE

TL;DR: This paper introduces TALM, a dynamic tree-structured multi-agent framework for code generation...


<details>
  <summary>Details</summary>
Motivation: Current code generation systems exhibit limitations in...

Method: The tree-based architecture features parent-agent coordination with child-agent...

Result: TALM sees 30% improvements on ClassEval benchmarks compared to CoeBorg+Plus while consuming 20% fewer tokens.

Conclusion: Empirical evidence suggests TALM represents a promising new paradigm for agent-based...

Abstract: Agentic code generation requires large language models (LLMs) capable of
complex context management and multi-step reasoning. Prior multi-agent
frameworks attempt to address these challenges through collaboration, yet they
often suffer from rigid workflows and high reasoning recovery costs. To
overcome these limitations, we propose TALM (Tree-Structured Multi-Agent
Framework with Long-Term Memory), a dynamic framework that integrates
structured task decomposition, localized re-reasoning, and long-term memory
mechanisms. TALM employs an extensible tree-based collaboration structure. The
parent-child relationships, when combined with a divide-and-conquer strategy,
enhance reasoning flexibility and enable efficient error correction across
diverse task scopes. Furthermore, a long-term memory module enables semantic
querying and integration of prior knowledge, supporting implicit
self-improvement through experience reuse. Experimental results on HumanEval,
BigCodeBench, and ClassEval benchmarks demonstrate that TALM consistently
delivers strong reasoning performance and high token efficiency, highlighting
its robustness and practical utility in complex code generation tasks.

</details>


### [59] [From Online User Feedback to Requirements: Evaluating Large Language Models for Classification and Specification Tasks](https://arxiv.org/abs/2510.23055)
*Manjeshwar Aniruddh Mallya,Alessio Ferrari,Mohammad Amin Zadenoori,Jacek DÄbrowski*

Main category: cs.SE

TL;DR: This paper evaluates five lightweight open-source LLMs for three RE tasks (user request/NFR classification and requirements generation), finding moderate-to-high performance (F1 ~ 0.47-0.68; quality ~3/5), while providing a replication package and insights on LLMs in feedback-driven RE.


<details>
  <summary>Details</summary>
Motivation: Online user feedback contains valuable requirements information but is difficult to analyze at scale. While LLMs show promise for automating this process, their use in RE lacks empirical validation and reproducibility. This study addresses this gap by evaluating LLMs for core RE tasks.

Method: Evaluated five open-source LLMs on three RE tasks using two feedback datasets for classification (F1 metric) and human evaluation for specification quality (scale 1-5). Tasks included: user request classification, NFR classification, and requirements specification generation.

Result: LLMs achieved moderate-to-high classification accuracy (F1 scores of 0.47-0.68 across tasks) and moderately high specification quality (mean human rating of ~3/5). Performance varied across models and tasks, with no single model dominating all objectives.

Conclusion: Lightweight LLMs demonstrate viability for feedback-driven requirements development, but their performance highlights both capabilities and limitations. The study provides empirical evidence supporting their use while emphasizing the need for further research on model selection, evaluation rigor, and task-specific optimizations in RE contexts.

Abstract: [Context and Motivation] Online user feedback provides valuable information
to support requirements engineering (RE). However, analyzing online user
feedback is challenging due to its large volume and noise. Large language
models (LLMs) show strong potential to automate this process and outperform
previous techniques. They can also enable new tasks, such as generating
requirements specifications.
  [Question-Problem] Despite their potential, the use of LLMs to analyze user
feedback for RE remains underexplored. Existing studies offer limited empirical
evidence, lack thorough evaluation, and rarely provide replication packages,
undermining validity and reproducibility.
  [Principal Idea-Results] We evaluate five lightweight open-source LLMs on
three RE tasks: user request classification, NFR classification, and
requirements specification generation. Classification performance was measured
on two feedback datasets, and specification quality via human evaluation. LLMs
achieved moderate-to-high classification accuracy (F1 ~ 0.47-0.68) and
moderately high specification quality (mean ~ 3/5).
  [Contributions] We newly explore lightweight LLMs for feedback-driven
requirements development. Our contributions are: (i) an empirical evaluation of
lightweight LLMs on three RE tasks, (ii) a replication package, and (iii)
insights into their capabilities and limitations for RE.

</details>


### [60] [Checkstyle+: Reducing Technical Debt Through The Use of Linters with LLMs](https://arxiv.org/abs/2510.23068)
*Ella Dodor,Cristina V. Lopes*

Main category: cs.SE

TL;DR: This paper presents Checkstyle+, which integrates LLMs with traditional linters like Checkstyle to better detect nuanced style violations in Java code. Evaluated on 380 files from 30,800 real-world submissions, it outperforms conventional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional linters like Checkstyle use rigid rules for code style checking but struggle with semantically complex style violations, necessitating a more nuanced approach.

Method: Checkstyle+ combines Checkstyle with large language model (LLM) capabilities to address the limitations of rule-based analysis in detecting nuanced style violations.

Result: Checkstyle+ outperforms standard Checkstyle in detecting semantically nuanced style violations, evaluated across 380 Java code files from real-world projects.

Conclusion: The integration of LLMs with traditional linters offers a promising approach to improving the detection of complex code style violations, enhancing software quality.

Abstract: Good code style improves program readability, maintainability, and
collaboration, and is an integral component of software quality. Developers,
however, often cut corners when following style rules, leading to the wide
adoption of tools such as linters in professional software development
projects. Traditional linters like Checkstyle operate using rigid, rule-based
mechanisms that effectively detect many surface-level violations. However, in
most programming languages, there is a subset of style rules that require a
more nuanced understanding of code, and fall outside the scope of such static
analysis. In this paper, we propose Checkstyle+, a hybrid approach that
augments Checkstyle with large language model (LLM) capabilities, to identify
style violations that elude the conventional rule-based analysis. Checkstyle+
is evaluated on a sample of 380 Java code files, drawn from a broader dataset
of 30,800 real-world Java programs sourced from accepted Codeforces
submissions. The results show that Checkstyle+ achieves superior performance
over standard Checkstyle in detecting violations of the semantically nuanced
rules.

</details>


### [61] [Validating Formal Specifications with LLM-generated Test Cases](https://arxiv.org/abs/2510.23350)
*Alcino Cunha,Nuno Macedo*

Main category: cs.SE

TL;DR: The paper evaluates the use of pre-trained LLMs like GPT-5 to automate generating test cases for Alloy specification language models, demonstrating GPT-5's effectiveness in creating syntactically correct and semantically valid test scenarios.


<details>
  <summary>Details</summary>
Motivation: Manual test-case specification for formal validation is labor-intensive and error-prone, potentially leading to skipped validation. This study explores LLMs as a solution to automate this critical process.

Method: Empirical evaluation of multiple LLMs, primarily GPT-5, to generate test cases from natural language requirements for Alloy domain models. Assessment focused on syntactic validity, requirement satisfaction, and ability to detect flawed specifications.

Result: GPT-5 produced 85%-95% syntactically valid test cases that correctly satisfied or violated requirements. The model detected 65-75% of human-written specification errors while matching human test cases in quality assessments.

Conclusion: LLMs like GPT-5 can effectively automate Alloy test case generation, improving validation efficiency while maintaining technical accuracy. This suggests significant potential to reduce human effort in formal specification development.

Abstract: Validation is a central activity when developing formal specifications.
Similarly to coding, a possible validation technique is to define upfront test
cases or scenarios that a future specification should satisfy or not.
Unfortunately, specifying such test cases is burdensome and error prone, which
could cause users to skip this validation task. This paper reports the results
of an empirical evaluation of using pre-trained large language models (LLMs) to
automate the generation of test cases from natural language requirements. In
particular, we focus on test cases for structural requirements of simple domain
models formalized in the Alloy specification language. Our evaluation focuses
on the state-of-art GPT-5 model, but results from other closed- and open-source
LLMs are also reported. The results show that, in this context, GPT-5 is
already quite effective at generating positive (and negative) test cases that
are syntactically correct and that satisfy (or not) the given requirement, and
that can detect many wrong specifications written by humans.

</details>


### [62] [Floating-Point Neural Network Verification at the Software Level](https://arxiv.org/abs/2510.23389)
*Edoardo Manino,Bruno Farias,Rafael SÃ¡ Menezes,Fedor Shmarov,Lucas C. Cordeiro*

Main category: cs.SE

TL;DR: This paper introduces NeuroCodeBench 2.0, a benchmark for verifying neural network software correctness in safety-critical systems, demonstrating significant gaps in current verification tools while showing the benchmark's positive impact on their performance.


<details>
  <summary>Details</summary>
Motivation: Existing neural network verification techniques cannot certify the absence of software-level faults due to unaccounted numerical errors in floating-point implementations.

Method: Constructed NeuroCodeBench 2.0 with 912 C-based verification examples covering neural network components, evaluated eight state-of-the-art software verifiers using the SV-COMP format benchmark.

Result: Tools solved 11.3% of benchmarks on average with 3.5 Ð½ÐµÐºÐ¾ÑÑÐµÐºÑÐ½ÑÑ verdicts; historical analysis showed 36.2% improvement in tool performance post-benchmark release.

Conclusion: Dedicated neural network benchmarks are essential for evaluating verification tools, and NeuroCodeBench 2.0 has already catalyzed improvements in verification effectiveness.

Abstract: The behaviour of neural network components must be proven correct before
deployment in safety-critical systems. Unfortunately, existing neural network
verification techniques cannot certify the absence of faults at the software
level. In this paper, we show how to specify and verify that neural networks
are safe, by explicitly reasoning about their floating-point implementation. In
doing so, we construct NeuroCodeBench 2.0, a benchmark comprising 912 neural
network verification examples that cover activation functions, common layers,
and full neural networks of up to 170K parameters. Our verification suite is
written in plain C and is compatible with the format of the International
Competition on Software Verification (SV-COMP). Thanks to it, we can conduct
the first rigorous evaluation of eight state-of-the-art software verifiers on
neural network code. The results show that existing automated verification
tools can correctly solve an average of 11% of our benchmark, while producing
around 3% incorrect verdicts. At the same time, a historical analysis reveals
that the release of our benchmark has already had a significantly positive
impact on the latter.

</details>


### [63] [Tracing Distribution Shifts with Causal System Maps](https://arxiv.org/abs/2510.23528)
*Joran Leest,Ilias Gerostathopoulos,Patricia Lago,Claudia Raibulet*

Main category: cs.SE

TL;DR: The paper introduces ML System Maps, a novel approach using causal maps to track the root causes of distribution shifts in ML systems, shifting focus from detection to systematic analysis of causation.


<details>
  <summary>Details</summary>
Motivation: Current ML system monitoring practices primarily detect distribution shifts but lack systematic methods to identify their root causes, such as distinguishing between data quality issues and environmental changes.

Method: The authors propose ML System Maps, a framework that uses layered views of causal relationships to visualize and analyze the propagation paths from the environment to system internals, enabling structured root-cause identification.

Result: The paper presents a detailed approach for developing ML System Maps and suggests a research agenda to further explore their implementation and effectiveness.

Conclusion: ML System Maps offer a structured way to attribute distribution shifts to specific causes, improving the monitoring and development of ML systems by focusing on understanding internal and environmental interactions.

Abstract: Monitoring machine learning (ML) systems is hard, with standard practice
focusing on detecting distribution shifts rather than their causes. Root-cause
analysis often relies on manual tracing to determine whether a shift is caused
by software faults, data-quality issues, or natural change. We propose ML
System Maps -- causal maps that, through layered views, make explicit the
propagation paths between the environment and the ML system's internals,
enabling systematic attribution of distribution shifts. We outline the approach
and a research agenda for its development and evaluation.

</details>
