{"id": "2509.22908", "categories": ["cs.SE", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.22908", "abs": "https://arxiv.org/abs/2509.22908", "authors": ["Sergiu Bursuc", "Theodore Ehrenborg", "Shaowei Lin", "Lacramioara Astefanoaei", "Ionel Emilian Chiosa", "Jure Kukovec", "Alok Singh", "Oliver Butterley", "Adem Bizid", "Quinn Dougherty", "Miranda Zhao", "Max Tan", "Max Tegmark"], "title": "A benchmark for vericoding: formally verified program synthesis", "comment": "25 pages, 1 figure; data available at\n  https://github.com/Beneficial-AI-Foundation/vericoding-benchmark", "summary": "We present and test the largest benchmark for vericoding, LLM-generation of\nformally verified code from formal specifications - in contrast to vibe coding,\nwhich generates potentially buggy code from a natural language description. Our\nbenchmark contains 12,504 formal specifications, with 3,029 in Dafny, 2,334 in\nVerus/Rust and 7,141 in Lean. Of these, 6,174 are new unseen problems. We find\nvericoding success rates of 27% in Lean, 44% in Verus/Rust and 82% in Dafny\nusing off-the-shelf LLMs. Adding natural-language descriptions does not\nsignificantly improve performance. We also find that LLM progress has improved\nprogress on pure Dafny verification from 68% to 96% over the past year. The\nbenchmark and vericoding results are shared at\nhttps://github.com/Beneficial-AI-Foundation/vericoding-benchmark"}
{"id": "2509.22978", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22978", "abs": "https://arxiv.org/abs/2509.22978", "authors": ["Teeradaj Racharak", "Chaiyong Ragkhitwetsagul", "Chayanee Junplong", "Akara Supratak"], "title": "Towards Human-interpretable Explanation in Code Clone Detection using LLM-based Post Hoc Explainer", "comment": null, "summary": "Recent studies highlight various machine learning (ML)-based techniques for\ncode clone detection, which can be integrated into developer tools such as\nstatic code analysis. With the advancements brought by ML in code\nunderstanding, ML-based code clone detectors could accurately identify and\nclassify cloned pairs, especially semantic clones, but often operate as black\nboxes, providing little insight into the decision-making process. Post hoc\nexplainers, on the other hand, aim to interpret and explain the predictions of\nthese ML models after they are made, offering a way to understand the\nunderlying mechanisms driving the model's decisions. However, current post hoc\ntechniques require white-box access to the ML model or are computationally\nexpensive, indicating a need for advanced post hoc explainers. In this paper,\nwe propose a novel approach that leverages the in-context learning capabilities\nof large language models to elucidate the predictions made by the ML-based code\nclone detectors. We perform a study using ChatGPT-4 to explain the code clone\nresults inferred by GraphCodeBERT. We found that our approach is promising as a\npost hoc explainer by giving the correct explanations up to 98% and offering\ngood explanations 95% of the time. However, the explanations and the code line\nexamples given by the LLM are useful in some cases. We also found that lowering\nthe temperature to zero helps increase the accuracy of the explanation. Lastly,\nwe list the insights that can lead to further improvements in future work. This\nstudy paves the way for future studies in using LLMs as a post hoc explainer\nfor various software engineering tasks."}
{"id": "2509.23261", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23261", "abs": "https://arxiv.org/abs/2509.23261", "authors": ["Fei Gu", "Zi Liang", "Hongzong LI", "Jiahao MA"], "title": "The Matthew Effect of AI Programming Assistants: A Hidden Bias in Software Evolution", "comment": null, "summary": "AI-assisted programming is rapidly reshaping software development, with large\nlanguage models (LLMs) enabling new paradigms such as vibe coding and agentic\ncoding. While prior works have focused on prompt design and code generation\nquality, the broader impact of LLM-driven development on the iterative dynamics\nof software engineering remains underexplored. In this paper, we conduct\nlarge-scale experiments on thousands of algorithmic programming tasks and\nhundreds of framework selection tasks to systematically investigate how\nAI-assisted programming interacts with the software ecosystem. Our analysis\nreveals \\textbf{a striking Matthew effect: the more popular a programming\nlanguage or framework, the higher the success rate of LLM-generated code}. The\nphenomenon suggests that AI systems may reinforce existing popularity\nhierarchies, accelerating convergence around dominant tools while hindering\ndiversity and innovation. We provide a quantitative characterization of this\neffect and discuss its implications for the future evolution of programming\necosystems."}
{"id": "2509.23297", "categories": ["cs.SE", "cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.23297", "abs": "https://arxiv.org/abs/2509.23297", "authors": ["Anthony Savidis", "Christos Vasilopoulos"], "title": "Code Arcades: 3d Visualization of Classes, Dependencies and Software Metrics", "comment": null, "summary": "Software visualization seeks to represent software artifacts graphical-ly in\ntwo or three dimensions, with the goal of enhancing comprehension, anal-ysis,\nmaintenance, and evolution of the source code. In this context, visualiza-tions\nemploy graphical forms such as dependency structures, treemaps, or time-lines\nthat incorporate repository histories. These visualizations allow software\nengineers to identify structural patterns, detect complexity hotspots, and\ninfer system behaviors that are difficult to perceive directly from source\ntext. By adopting metaphor-based approaches, visualization tools provide\nmacroscopic overviews while enabling focused inspection of specific program\nelements, thus offering an accessible means of understanding large-scale\nsystems. The contri-bution of our work lies in three areas. First, we introduce\na configurable group-ing mechanism that supports flexible organization of code\nelements based on arbitrary relationships. Second, we combine fine-grained and\ncoarse-grained software metrics to provide a multi-level perspective on system\nproperties. Third, we present an interactive visualization engine that allows\ndevelopers to dynamically adjust rendering attributes. Collectively, these\nadvances provide a more adaptable and insightful approach to source code\ncomprehension."}
{"id": "2509.22662", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22662", "abs": "https://arxiv.org/abs/2509.22662", "authors": ["Mathilde Durieux", "Kayla D. Taylor", "Laxima Niure Kandel", "Deepti Gupta"], "title": "GPS Spoofing Attacks and Pilot Responses Using a Flight Simulator Environment", "comment": null, "summary": "Global Positioning System (GPS) spoofing involves transmitting fake signals\nthat mimic those from GPS satellites, causing the GPS receivers to calculate\nincorrect Positioning, Navigation, and Timing (PNT) information. Recently,\nthere has been a surge in GPS spoofing attacks targeting aircraft. Since GPS\nsatellite signals are weak, the spoofed high-power signal can easily overpower\nthem. These spoofed signals are often interpreted as valid by the GPS receiver,\nwhich can cause severe and cascading effects on air navigation. While much of\nthe existing research on GPS spoofing focuses on technical aspects of detection\nand mitigation, human factors are often neglected, even though pilots are an\nintegral part of aircraft operation and potentially vulnerable to deception.\nThis research addresses this gap by conducting a detailed analysis of the\nbehavior of student pilots when subjected to GPS spoofing using the Force\nDynamics 401CR flight simulator with X-Plane 11 and a Cessna 172 equipped with\nGarmin G1000. Spoofing scenarios were implemented via custom scripts that\naltered navigational data without modifying the external visual environment.\nThirty student pilots from the Embry-Riddle Aeronautical University Daytona\nBeach campus with diverse flying experience levels were recruited to\nparticipate in three spoofing scenarios. A pre-simulation questionnaire was\ndistributed to measure pilot experience and confidence in GPS.Inflight\ndecision-making during the spoofing attacks was observed, including reaction\ntime to anomalies, visual attention to interface elements, and cognitive\nbiases. A post-flight evaluation of workload was obtained using a modified NASA\nTask Load Index (TLX) method. This study provides a first step toward\nidentifying human vulnerabilities to GPS spoofing amid the ongoing debate over\nGPS reliance."}
{"id": "2509.23469", "categories": ["cs.SE", "68N30", "D.2.8"], "pdf": "https://arxiv.org/pdf/2509.23469", "abs": "https://arxiv.org/abs/2509.23469", "authors": ["Mykola Kuz", "Ivan Yaremiy", "Hanna Yaremii", "Mykola Pikuliak", "Ihor Lazarovych", "Mykola Kozlenko", "Denys Vekeryk"], "title": "Methods for evaluating software accessibility", "comment": "10 pages, 4 figures, 1 table", "summary": "The development and enhancement of methods for evaluating software\naccessibility is a relevant challenge in modern software engineering, as\nensuring equal access to digital services is a key factor in improving their\nefficiency and inclusivity. The increasing digitalization of society\nnecessitates the creation of software that complies with international\naccessibility standards such as ISO/IEC 25023 and WCAG. Adhering to these\nstandards helps eliminate barriers to software use for individuals with diverse\nphysical, sensory, and cognitive needs. Despite advancements in regulatory\nframeworks, existing accessibility evaluation methodologies are often\ngeneralized and fail to account for the specific needs of different user\ncategories or the unique ways they interact with digital systems. This\nhighlights the need for the development of new, more detailed methods for\ndefining metrics that influence the quality of user interaction with software\nproducts. Building a classification and mathematical model and developing\naccessibility assessment methods for software based on it. A method for\nassessing the quality subcharacteristic \"Accessibility\", which is part of the\n\"Usability\" quality characteristic, has been developed. This enabled the\nanalysis of a website's inclusivity for individuals with visual impairments,\nand the formulation of specific recommendations for further improvements, which\nis a crucial step toward creating an inclusive digital environment. Comparing\nto standardized approaches, a more detailed and practically oriented\naccessibility assessment methodology has been proposed. Using this methodology,\nan analysis of the accessibility of the main pages of Vasyl Stefanyk\nPrecarpathian National University's website was conducted, and improvements\nwere suggested to enhance its inclusivity."}
{"id": "2509.22663", "categories": ["cs.CR", "cs.HC", "cs.CR, cs.HC"], "pdf": "https://arxiv.org/pdf/2509.22663", "abs": "https://arxiv.org/abs/2509.22663", "authors": ["Michel Youssef"], "title": "Security Friction Quotient for Zero Trust Identity Policy with Empirical Validation", "comment": "10 pages, 3 figures", "summary": "We define a practical method to quantify the trade-off between security and\noperational friction in modern identity-centric programs. We introduce the\nSecurity Friction Quotient (SFQ), a bounded composite index that combines a\nresidual-risk estimator with empirically grounded friction terms (latency,\nfailure rate, and helpdesk impact). We establish clarity properties\n(boundedness, monotonic response, and weight identifiability) with short\nproofs, then evaluate widely used Conditional Access policies over a 12-week\nhorizon using Monte Carlo simulation (n = 2,000 runs per policy/scenario) with\neffect sizes and 95% confidence intervals. We further assess rank stability\nunder 10,000 random weight draws, finding 95.5% preservation of policy\nordering. Finally, we provide a 12-week passkey field observation from an\nenterprise-scale cohort (N = 1,200) that directionally aligns with the\nsimulation's phishing-resistant MFA gains. The SFQ framework is designed to be\nreproducible, interpretable, and directly actionable for Zero Trust identity\npolicy decisions, with artifacts and parameter ranges provided to support\npolicy design, review, and continuous improvement."}
{"id": "2509.23586", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23586", "abs": "https://arxiv.org/abs/2509.23586", "authors": ["Yuan-An Xiao", "Pengfei Gao", "Chao Peng", "Yingfei Xiong"], "title": "Improving the Efficiency of LLM Agent Systems through Trajectory Reduction", "comment": "20 pages, 4 figures", "summary": "Multi-turn agent systems based on Large Language Models (LLMs) have been\nincreasingly popular for software engineering tasks. While LLM agents show\ndecent effectiveness, the high computational cost of input tokens due to the\never-growing trajectory remains an efficiency concern for their applications.\nEfficiency is largely neglected in existing studies and agent products, and\nthis paper fills the gap by introducing an inference-time trajectory reduction\napproach to reduce the cost of agents.\n  Through analyzing existing agent trajectories, we demonstrate that useless,\nredundant, and expired information is widespread in all trajectories, which can\nbe identified and reduced without harming the agent's performance. We then\ndesign a simple yet effective trajectory reduction approach, AgentDiet, which\nautomatically removes such waste information. We implement AgentDiet on a\ntop-performing coding agent, and the evaluation on two LLMs and two benchmarks\nshows that AgentDiet can reduce input tokens by 39.9% ~ 59.7%, or the final\ncomputational cost by 21.1% ~ 35.9%, while maintaining the same agent\nperformance. This indicates that trajectory reduction is a promising direction\nfor agent systems."}
{"id": "2509.22664", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22664", "abs": "https://arxiv.org/abs/2509.22664", "authors": ["Chaerin Kim"], "title": "Security Issues on the OpenPLC project and corresponding solutions", "comment": "Master's thesis", "summary": "As Programmable Logic Controller (PLC) became a useful device and rose as an\ninteresting research topic but remained expensive, multiple PLC\nsimulators/emulators were introduced for various purposes. Open-source\nProgrammable Logic Controller (OpenPLC) software, one of the most popular PLC\nsimulators, is designed to be vendor-neutral and run on almost any computer or\nlow-cost embedded devices, e.g., Raspberry Pi, Arduino, and other controllers.\nThe project succeeded in introducing itself as an affordable and practical\nsolution for the high cost of real hardware PLCs. However, it still lacks\nappropriate securing methods, resulting in several vulnerabilities. Through a\ncombination of threat modeling, vulnerability analysis, and practical\nexperiments, this thesis provides valuable insights for developers,\nresearchers, and engineers aiming to deploy OpenPLC securely in industrial\nenvironments. To this end, this work first conducts an in-depth analysis aimed\nto shed light on va! rious security challenges and vulnerabilities within the\nOpenPLC project. After that, an advanced control logic injection attack was\nperformed. This attack modifies the user program maliciously, exploiting\npresented vulnerabilities. Finally, the work introduces a security-enhanced\nOpenPLC software called OpenPLC Aqua. The new software is equipped with a set\nof security solutions designed specifically to address the vulnerabilities to\nwhich current OpenPLC versions are prone."}
{"id": "2509.23645", "categories": ["cs.SE", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.23645", "abs": "https://arxiv.org/abs/2509.23645", "authors": ["A S M Shahadat Hossain", "Colin Brown", "David Koop", "Tanu Malik"], "title": "Similarity-Based Assessment of Computational Reproducibility in Jupyter Notebooks", "comment": "10 pages", "summary": "Computational reproducibility refers to obtaining consistent results when\nrerunning an experiment. Jupyter Notebook, a web-based computational notebook\napplication, facilitates running, publishing, and sharing computational\nexperiments along with their results. However, rerunning a Jupyter Notebook may\nnot always generate identical results due to various factors, such as\nrandomness, changes in library versions, or variations in the computational\nenvironment. This paper introduces the Similarity-based Reproducibility Index\n(SRI) -- a metric for assessing the reproducibility of results in Jupyter\nNotebooks. SRI employs novel methods developed based on similarity metrics\nspecific to different types of Python objects to compare rerun outputs against\noriginal outputs. For every cell generating an output in a rerun notebook, SRI\nreports a quantitative score in the range [0, 1] as well as some qualitative\ninsights to assess reproducibility. The paper also includes a case study in\nwhich the proposed metric is applied to a set of Jupyter Notebooks,\ndemonstrating how various similarity metrics can be leveraged to quantify\ncomputational reproducibility."}
{"id": "2509.22723", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22723", "abs": "https://arxiv.org/abs/2509.22723", "authors": ["Kang Wei", "Xin Yuan", "Fushuo Huo", "Chuan Ma", "Long Yuan", "Songze Li", "Ming Ding", "Dacheng Tao"], "title": "Responsible Diffusion: A Comprehensive Survey on Safety, Ethics, and Trust in Diffusion Models", "comment": null, "summary": "Diffusion models (DMs) have been investigated in various domains due to their\nability to generate high-quality data, thereby attracting significant\nattention. However, similar to traditional deep learning systems, there also\nexist potential threats to DMs. To provide advanced and comprehensive insights\ninto safety, ethics, and trust in DMs, this survey comprehensively elucidates\nits framework, threats, and countermeasures. Each threat and its\ncountermeasures are systematically examined and categorized to facilitate\nthorough analysis. Furthermore, we introduce specific examples of how DMs are\nused, what dangers they might bring, and ways to protect against these dangers.\nFinally, we discuss key lessons learned, highlight open challenges related to\nDM security, and outline prospective research directions in this critical\nfield. This work aims to accelerate progress not only in the technical\ncapabilities of generative artificial intelligence but also in the maturity and\nwisdom of its application."}
{"id": "2509.23675", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23675", "abs": "https://arxiv.org/abs/2509.23675", "authors": ["Xinyue Zuo", "Yifan Zhang", "Hongshu Wang", "Yufan Cai", "Zhe Hou", "Jing Sun", "Jin Song Dong"], "title": "PAT-Agent: Autoformalization for Model Checking", "comment": "Accepted in ASE 2025 (International Conference on Automated Software\n  Engineering)", "summary": "Recent advances in large language models (LLMs) offer promising potential for\nautomating formal methods. However, applying them to formal verification\nremains challenging due to the complexity of specification languages, the risk\nof hallucinated output, and the semantic gap between natural language and\nformal logic. We introduce PAT-Agent, an end-to-end framework for natural\nlanguage autoformalization and formal model repair that combines the generative\ncapabilities of LLMs with the rigor of formal verification to automate the\nconstruction of verifiable formal models. In PAT-Agent, a Planning LLM first\nextracts key modeling elements and generates a detailed plan using semantic\nprompts, which then guides a Code Generation LLM to synthesize syntactically\ncorrect and semantically faithful formal models. The resulting code is verified\nusing the Process Analysis Toolkit (PAT) model checker against user-specified\nproperties, and when discrepancies occur, a Repair Loop is triggered to\niteratively correct the model using counterexamples. To improve flexibility, we\nbuilt a web-based interface that enables users, particularly non-FM-experts, to\ndescribe, customize, and verify system behaviors through user-LLM interactions.\nExperimental results on 40 systems show that PAT-Agent consistently outperforms\nbaselines, achieving high verification success with superior efficiency. The\nablation studies confirm the importance of both planning and repair components,\nand the user study demonstrates that our interface is accessible and supports\neffective formal modeling, even for users with limited formal methods\nexperience."}
{"id": "2509.22732", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22732", "abs": "https://arxiv.org/abs/2509.22732", "authors": ["Haibo Tong", "Dongcheng Zhao", "Guobin Shen", "Xiang He", "Dachuan Lin", "Feifei Zhao", "Yi Zeng"], "title": "Bidirectional Intention Inference Enhances LLMs' Defense Against Multi-Turn Jailbreak Attacks", "comment": null, "summary": "The remarkable capabilities of Large Language Models (LLMs) have raised\nsignificant safety concerns, particularly regarding \"jailbreak\" attacks that\nexploit adversarial prompts to bypass safety alignment mechanisms. Existing\ndefense research primarily focuses on single-turn attacks, whereas multi-turn\njailbreak attacks progressively break through safeguards through by concealing\nmalicious intent and tactical manipulation, ultimately rendering conventional\nsingle-turn defenses ineffective. To address this critical challenge, we\npropose the Bidirectional Intention Inference Defense (BIID). The method\nintegrates forward request-based intention inference with backward\nresponse-based intention retrospection, establishing a bidirectional synergy\nmechanism to detect risks concealed within seemingly benign inputs, thereby\nconstructing a more robust guardrails that effectively prevents harmful content\ngeneration. The proposed method undergoes systematic evaluation compared with a\nno-defense baseline and seven representative defense methods across three LLMs\nand two safety benchmarks under 10 different attack methods. Experimental\nresults demonstrate that the proposed method significantly reduces the Attack\nSuccess Rate (ASR) across both single-turn and multi-turn jailbreak attempts,\noutperforming all existing baseline methods while effectively maintaining\npractical utility. Notably, comparative experiments across three multi-turn\nsafety datasets further validate the proposed model's significant advantages\nover other defense approaches."}
{"id": "2509.23679", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23679", "abs": "https://arxiv.org/abs/2509.23679", "authors": ["Zeqin Liao", "Yuhong Nan", "Zixu Gao", "Henglong Liang", "Sicheng Hao", "Jiajing Wu", "Zibin Zheng"], "title": "Satellite: Detecting and Analyzing Smart Contract Vulnerabilities caused by Subcontract Misuse", "comment": "This is the author version of the article accepted for publication in\n  IEEE Transactions on Software Engineering. The final version is available at\n  10.1109/TSE.2025.3613470", "summary": "Developers of smart contracts pervasively reuse subcontracts to improve\ndevelopment efficiency. Like any program language, such subcontract reuse may\nunexpectedly include, or introduce vulnerabilities to the end-point smart\ncontract. Unfortunately, automatically detecting such issues poses several\nunique challenges. Particularly, in most cases, smart contracts are compiled as\nbytecode, whose class-level information (e.g., inheritance, virtual function\ntable), and even semantics (e.g., control flow and data flow) are fully\nobscured as a single smart contract after compilation.\n  In this paper, we propose Satellite, a new bytecode-level static analysis\nframework for subcontract misuse vulnerability (SMV) detection in smart\ncontracts. Satellite incorporates a series of novel designs to enhance its\noverall effectiveness.. Particularly, Satellite utilizes a transfer learning\nmethod to recover the inherited methods, which are critical for identifying\nsubcontract reuse in smart contracts. Further, Satellite extracts a set of\nfine-grained method-level features and performs a method-level comparison, for\nidentifying the reuse part of subcontract in smart contracts. Finally,\nSatellite summarizes a set of SMV indicators according to their types, and\nhence effectively identifies SMVs. To evaluate Satellite, we construct a\ndataset consisting of 58 SMVs derived from real-world attacks and collect\nadditional 56 SMV patterns from SOTA studies. Experiment results indicate that\nSatellite exhibits good performance in identifying SMV, with a precision rate\nof 84.68% and a recall rate of 92.11%. In addition, Satellite successfully\nidentifies 14 new/unknown SMV over 10,011 real-world smart contracts, affecting\na total amount of digital assets worth 201,358 USD."}
{"id": "2509.22745", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22745", "abs": "https://arxiv.org/abs/2509.22745", "authors": ["Jaehan Kim", "Minkyoo Song", "Seungwon Shin", "Sooel Son"], "title": "Defending MoE LLMs against Harmful Fine-Tuning via Safety Routing Alignment", "comment": "Under review", "summary": "Recent large language models (LLMs) have increasingly adopted the\nMixture-of-Experts (MoE) architecture for efficiency. MoE-based LLMs heavily\ndepend on a superficial safety mechanism in which harmful inputs are routed\nsafety-critical experts. However, our analysis reveals that routing decisions\nfor harmful inputs drift significantly after fine-tuning, exposing a critical\nvulnerability to harmful fine-tuning (HFT) attacks. Existing defenses,\nprimarily designed for monolithic LLMs, are less effective for MoE LLMs as they\nfail to prevent drift in harmful input routing. To address this limitation, we\npropose SafeMoE, a safe fine-tuning method tailored to MoE LLMs. SafeMoE\ndirectly mitigates routing drift by penalizing the gap between the routing\nweights of a fine-tuned model and those of the initial safety-aligned model,\nthereby preserving the safety-aligned routing of harmful inputs to\nsafety-critical experts. Experiments on open-source MoE LLMs ranging from 7B to\n141B parameters demonstrate that SafeMoE effectively mitigates HFT attacks,\nreducing the harmfulness score of OLMoE from 62.0 to 5.0, for example, while\nmaintaining task utility within 1% degradation and incurring only 2% overhead.\nIt significantly outperforms state-of-the-art defense methods for safeguarding\nLLM fine-tuning and remains effective in recent large-scale MoE LLMs such as\ngpt-oss and Llama 4. Our implementation is available at\nhttps://anonymous.4open.science/r/SafeMoE."}
{"id": "2509.23806", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23806", "abs": "https://arxiv.org/abs/2509.23806", "authors": ["Chih-Duo Hong", "Yu Wang", "Yao-Chen Chang", "Fang Yu"], "title": "Influence-Guided Concolic Testing of Transformer Robustness", "comment": null, "summary": "Concolic testing for deep neural networks alternates concrete execution with\nconstraint solving to search for inputs that flip decisions. We present an\n{influence-guided} concolic tester for Transformer classifiers that ranks path\npredicates by SHAP-based estimates of their impact on the model output. To\nenable SMT solving on modern architectures, we prototype a solver-compatible,\npure-Python semantics for multi-head self-attention and introduce practical\nscheduling heuristics that temper constraint growth on deeper models. In a\nwhite-box study on compact Transformers under small $L_0$ budgets, influence\nguidance finds label-flip inputs more efficiently than a FIFO baseline and\nmaintains steady progress on deeper networks. Aggregating successful attack\ninstances with a SHAP-based critical decision path analysis reveals recurring,\ncompact decision logic shared across attacks. These observations suggest that\n(i) influence signals provide a useful search bias for symbolic exploration,\nand (ii) solver-friendly attention semantics paired with lightweight scheduling\nmake concolic testing feasible for contemporary Transformer models, offering\npotential utility for debugging and model auditing."}
{"id": "2509.22757", "categories": ["cs.CR", "cs.AI", "cs.NI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.22757", "abs": "https://arxiv.org/abs/2509.22757", "authors": ["Petar Radanliev"], "title": "Red Teaming Quantum-Resistant Cryptographic Standards: A Penetration Testing Framework Integrating AI and Quantum Security", "comment": null, "summary": "This study presents a structured approach to evaluating vulnerabilities\nwithin quantum cryptographic protocols, focusing on the BB84 quantum key\ndistribution method and National Institute of Standards and Technology (NIST)\napproved quantum-resistant algorithms. By integrating AI-driven red teaming,\nautomated penetration testing, and real-time anomaly detection, the research\ndevelops a framework for assessing and mitigating security risks in quantum\nnetworks. The findings demonstrate that AI can be effectively used to simulate\nadversarial attacks, probe weaknesses in cryptographic implementations, and\nrefine security mechanisms through iterative feedback. The use of automated\nexploit simulations and protocol fuzzing provides a scalable means of\nidentifying latent vulnerabilities, while adversarial machine learning\ntechniques highlight novel attack surfaces within AI-enhanced cryptographic\nprocesses. This study offers a comprehensive methodology for strengthening\nquantum security and provides a foundation for integrating AI-driven\ncybersecurity practices into the evolving quantum landscape."}
{"id": "2509.23812", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23812", "abs": "https://arxiv.org/abs/2509.23812", "authors": ["Dianshu Liao", "Xin Yin", "Shidong Pan", "Chao Ni", "Zhenchang Xing", "Xiaoyu Sun"], "title": "Navigating the Labyrinth: Path-Sensitive Unit Test Generation with Large Language Models", "comment": null, "summary": "Unit testing is essential for software quality assurance, yet writing and\nmaintaining tests remains time-consuming and error-prone. To address this\nchallenge, researchers have proposed various techniques for automating unit\ntest generation, including traditional heuristic-based methods and more recent\napproaches that leverage large language models (LLMs). However, these existing\napproaches are inherently path-insensitive because they rely on fixed\nheuristics or limited contextual information and fail to reason about deep\ncontrol-flow structures. As a result, they often struggle to achieve adequate\ncoverage, particularly for deep or complex execution paths. In this work, we\npresent a path-sensitive framework, JUnitGenie, to fill this gap by combining\ncode knowledge with the semantic capabilities of LLMs in guiding context-aware\nunit test generation. After extracting code knowledge from Java projects,\nJUnitGenie distills this knowledge into structured prompts to guide the\ngeneration of high-coverage unit tests. We evaluate JUnitGenie on 2,258 complex\nfocal methods from ten real-world Java projects. The results show that\nJUnitGenie generates valid tests and improves branch and line coverage by\n29.60% and 31.00% on average over both heuristic and LLM-based baselines. We\nfurther demonstrate that the generated test cases can uncover real-world bugs,\nwhich were later confirmed and fixed by developers."}
{"id": "2509.22762", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22762", "abs": "https://arxiv.org/abs/2509.22762", "authors": ["Friedrich Doku", "Peter Dinda"], "title": "TRUSTCHECKPOINTS: Time Betrays Malware for Unconditional Software Root of Trust", "comment": null, "summary": "Modern IoT and embedded platforms must start execution from a known trusted\nstate to thwart malware, ensure secure firmware updates, and protect critical\ninfrastructure. Current approaches to establish a root of trust depend on\nsecret keys and/or specialized secure hardware, which drives up costs, may\ninvolve third parties, adds operational complexity, and relies on assumptions\nabout an attacker's computational power. In contrast, TRUSTCHECKPOINTS is the\nfirst system to establish an unconditional software root of trust based on a\nformal model without relying on secrets or trusted hardware. Developers capture\na full-system checkpoint and later roll back to it and prove this to an\nexternal verifier. The verifier issues timing-constrained, randomized\nk-independent polynomial challenges (via Horner's rule) that repeatedly scan\nthe fast on-chip memory in randomized passes. When malicious code attempts to\npersist, it must swap into slower, unchecked off-chip storage, causing a\ndetectable timing delay.\n  Our prototype for a commodity ARM Cortex-A53-based platform validates 192 KB\nof SRAM in approximately 10 s using 500 passes, sufficient to detect\nsingle-instruction persistent malware. The prototype then seamlessly extends\ntrust to DRAM. Two modes (fast SRAM-bootstrap and comprehensive full-memory\nscan) allow trade-offs between speed and coverage, demonstrating reliable\nmalware detection on unmodified hardware."}
{"id": "2509.23824", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23824", "abs": "https://arxiv.org/abs/2509.23824", "authors": ["Zhifan Ye", "Jiachi Chen", "Zhenzhe Shao", "Lingfeng Bao", "Xiaohu Yang", "Zhongxin Liu"], "title": "SolContractEval: A Benchmark for Evaluating Contract-Level Solidity Code Generation", "comment": null, "summary": "The rise of blockchain has brought smart contracts into mainstream use,\ncreating a demand for smart contract generation tools. While large language\nmodels (LLMs) excel at generating code in general-purpose languages, their\neffectiveness on Solidity, the primary language for smart contracts, remains\nunderexplored. Solidity constitutes only a small portion of typical LLM\ntraining data and differs from general-purpose languages in its\nversion-sensitive syntax and limited flexibility. These factors raise concerns\nabout the reliability of existing LLMs for Solidity code generation.\nCritically, existing evaluations, focused on isolated functions and synthetic\ninputs, fall short of assessing models' capabilities in real-world contract\ndevelopment.\n  To bridge this gap, we introduce SolContractEval, the first contract-level\nbenchmark for Solidity code generation. It comprises 124 tasks drawn from real\non-chain contracts across nine major domains. Each task input, consisting of\ncomplete context dependencies, a structured contract framework, and a concise\ntask prompt, is independently annotated and cross-validated by experienced\ndevelopers. To enable precise and automated evaluation of functional\ncorrectness, we also develop a dynamic evaluation framework based on historical\ntransaction replay. Building on SolContractEval, we perform a systematic\nevaluation of six mainstream LLMs. We find that Claude-3.7-Sonnet achieves the\nhighest overall performance, though evaluated models underperform relative to\ntheir capabilities on class-level generation tasks in general-purpose\nprogramming languages. Second, current models perform better on tasks that\nfollow standard patterns but struggle with complex logic and inter-contract\ndependencies. Finally, they exhibit limited understanding of Solidity-specific\nfeatures and contextual dependencies."}
{"id": "2509.22796", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22796", "abs": "https://arxiv.org/abs/2509.22796", "authors": ["Xingyu Li", "Juefei Pu", "Yifan Wu", "Xiaochen Zou", "Shitong Zhu", "Xiaochen Zou", "Shitong Zhu", "Qiushi Wu", "Zheng Zhang", "Joshua Hsu", "Yue Dong", "Zhiyun Qian", "Kangjie Lu", "Trent Jaeger", "Michael De Lucia", "Srikanth V. Krishnamurthy"], "title": "What Do They Fix? LLM-Aided Categorization of Security Patches for Critical Memory Bugs", "comment": null, "summary": "Open-source software projects are foundational to modern software ecosystems,\nwith the Linux kernel standing out as a critical exemplar due to its ubiquity\nand complexity. Although security patches are continuously integrated into the\nLinux mainline kernel, downstream maintainers often delay their adoption,\ncreating windows of vulnerability. A key reason for this lag is the difficulty\nin identifying security-critical patches, particularly those addressing\nexploitable vulnerabilities such as out-of-bounds (OOB) accesses and\nuse-after-free (UAF) bugs. This challenge is exacerbated by intentionally\nsilent bug fixes, incomplete or missing CVE assignments, delays in CVE\nissuance, and recent changes to the CVE assignment criteria for the Linux\nkernel. While fine-grained patch classification approaches exist, they exhibit\nlimitations in both coverage and accuracy. In this work, we identify previously\nunexplored opportunities to significantly improve fine-grained patch\nclassification. Specifically, by leveraging cues from commit titles/messages\nand diffs alongside appropriate code context, we develop DUALLM, a dual-method\npipeline that integrates two approaches based on a Large Language Model (LLM)\nand a fine-tuned small language model. DUALLM achieves 87.4% accuracy and an\nF1-score of 0.875, significantly outperforming prior solutions. Notably, DUALLM\nsuccessfully identified 111 of 5,140 recent Linux kernel patches as addressing\nOOB or UAF vulnerabilities, with 90 true positives confirmed by manual\nverification (many do not have clear indications in patch descriptions).\nMoreover, we constructed proof-of-concepts for two identified bugs (one UAF and\none OOB), including one developed to conduct a previously unknown control-flow\nhijack as further evidence of the correctness of the classification."}
{"id": "2509.23835", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23835", "abs": "https://arxiv.org/abs/2509.23835", "authors": ["Yukai Zhao", "Menghan Wu", "Xing Hu", "Xin Xia"], "title": "HFuzzer: Testing Large Language Models for Package Hallucinations via Phrase-based Fuzzing", "comment": null, "summary": "Large Language Models (LLMs) are widely used for code generation, but they\nface critical security risks when applied to practical production due to\npackage hallucinations, in which LLMs recommend non-existent packages. These\nhallucinations can be exploited in software supply chain attacks, where\nmalicious attackers exploit them to register harmful packages. It is critical\nto test LLMs for package hallucinations to mitigate package hallucinations and\ndefend against potential attacks. Although researchers have proposed testing\nframeworks for fact-conflicting hallucinations in natural language generation,\nthere is a lack of research on package hallucinations. To fill this gap, we\npropose HFUZZER, a novel phrase-based fuzzing framework to test LLMs for\npackage hallucinations. HFUZZER adopts fuzzing technology and guides the model\nto infer a wider range of reasonable information based on phrases, thereby\ngenerating enough and diverse coding tasks. Furthermore, HFUZZER extracts\nphrases from package information or coding tasks to ensure the relevance of\nphrases and code, thereby improving the relevance of generated tasks and code.\nWe evaluate HFUZZER on multiple LLMs and find that it triggers package\nhallucinations across all selected models. Compared to the mutational fuzzing\nframework, HFUZZER identifies 2.60x more unique hallucinated packages and\ngenerates more diverse tasks. Additionally, when testing the model GPT-4o,\nHFUZZER finds 46 unique hallucinated packages. Further analysis reveals that\nfor GPT-4o, LLMs exhibit package hallucinations not only during code generation\nbut also when assisting with environment configuration."}
{"id": "2509.22814", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22814", "abs": "https://arxiv.org/abs/2509.22814", "authors": ["Aditi Tiwari", "Akshit Bhalla", "Darshan Prasad"], "title": "Model Context Protocol for Vision Systems: Audit, Security, and Protocol Extensions", "comment": "Accepted to NeurIPS 2025 Workshop on Bridging Language, Agent, and\n  World Models for Reasoning and Planning (LAW 2025)", "summary": "The Model Context Protocol (MCP) defines a schema bound execution model for\nagent-tool interaction, enabling modular computer vision workflows without\nretraining. To our knowledge, this is the first protocol level, deployment\nscale audit of MCP in vision systems, identifying systemic weaknesses in schema\nsemantics, interoperability, and runtime coordination. We analyze 91 publicly\nregistered vision centric MCP servers, annotated along nine dimensions of\ncompositional fidelity, and develop an executable benchmark with validators to\ndetect and categorize protocol violations. The audit reveals high prevalence of\nschema format divergence, missing runtime schema validation, undeclared\ncoordinate conventions, and reliance on untracked bridging scripts. Validator\nbased testing quantifies these failures, with schema format checks flagging\nmisalignments in 78.0 percent of systems, coordinate convention checks\ndetecting spatial reference errors in 24.6 percent, and memory scope checks\nissuing an average of 33.8 warnings per 100 executions. Security probes show\nthat dynamic and multi agent workflows exhibit elevated risks of privilege\nescalation and untyped tool connections. The proposed benchmark and validator\nsuite, implemented in a controlled testbed and to be released on GitHub,\nestablishes a reproducible framework for measuring and improving the\nreliability and security of compositional vision workflows."}
{"id": "2509.23961", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23961", "abs": "https://arxiv.org/abs/2509.23961", "authors": ["Sheikh Md Mushfiqur Rahman", "Nasir Eisty"], "title": "Learning-Based Testing for Deep Learning: Enhancing Model Robustness with Adversarial Input Prioritization", "comment": null, "summary": "Context: Deep Neural Networks (DNNs) are increasingly deployed in critical\napplications, where resilience against adversarial inputs is paramount.\nHowever, whether coverage-based or confidence-based, existing test\nprioritization methods often fail to efficiently identify the most\nfault-revealing inputs, limiting their practical effectiveness. Aims: This\nproject aims to enhance fault detection and model robustness in DNNs by\nintegrating Learning-Based Testing (LBT) with hypothesis and mutation testing\nto efficiently prioritize adversarial test cases. Methods: Our method selects a\nsubset of adversarial inputs with a high likelihood of exposing model faults,\nwithout relying on architecture-specific characteristics or formal\nverification, making it adaptable across diverse DNNs. Results: Our results\ndemonstrate that the proposed LBT method consistently surpasses baseline\napproaches in prioritizing fault-revealing inputs and accelerating fault\ndetection. By efficiently organizing test permutations, it uncovers all\npotential faults significantly faster across various datasets, model\narchitectures, and adversarial attack techniques. Conclusion: Beyond improving\nfault detection, our method preserves input diversity and provides effective\nguidance for model retraining, further enhancing robustness. These advantages\nestablish our approach as a powerful and practical solution for adversarial\ntest prioritization in real-world DNN applications."}
{"id": "2509.22857", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22857", "abs": "https://arxiv.org/abs/2509.22857", "authors": ["Eduardo Chielle", "Manaar Alam", "Jinting Liu", "Jovan Kascelan", "Michail Maniatakos"], "title": "PAPER: Privacy-Preserving ResNet Models using Low-Degree Polynomial Approximations and Structural Optimizations on Leveled FHE", "comment": null, "summary": "Recent work has made non-interactive privacy-preserving inference more\npractical by running deep Convolution Neural Network (CNN) with Fully\nHomomorphic Encryption (FHE). However, these methods remain limited by their\nreliance on bootstrapping, a costly FHE operation applied across multiple\nlayers, severely slowing inference. They also depend on high-degree polynomial\napproximations of non-linear activations, which increase multiplicative depth\nand reduce accuracy by 2-5% compared to plaintext ReLU models. In this work, we\nfocus on ResNets, a widely adopted benchmark architecture in privacy-preserving\ninference, and close the accuracy gap between their FHE-based non-interactive\nmodels and plaintext counterparts, while also achieving faster inference than\nexisting methods. We use a quadratic polynomial approximation of ReLU, which\nachieves the theoretical minimum multiplicative depth for non-linear\nactivations, along with a penalty-based training strategy. We further introduce\nstructural optimizations such as node fusing, weight redistribution, and tower\nreuse. These optimizations reduce the required FHE levels in CNNs by nearly a\nfactor of five compared to prior work, allowing us to run ResNet models under\nleveled FHE without bootstrapping. To further accelerate inference and recover\naccuracy typically lost with polynomial approximations, we introduce parameter\nclustering along with a joint strategy of data encoding layout and ensemble\ntechniques. Experiments with ResNet-18, ResNet-20, and ResNet-32 on CIFAR-10\nand CIFAR-100 show that our approach achieves up to 4x faster private inference\nthan prior work with comparable accuracy to plaintext ReLU models."}
{"id": "2509.24032", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24032", "abs": "https://arxiv.org/abs/2509.24032", "authors": ["Jialun Zhang", "Merve Gülmez", "Thomas Nyman", "Gang Tan"], "title": "SandCell: Sandboxing Rust Beyond Unsafe Code", "comment": null, "summary": "Rust is a modern systems programming language that ensures memory safety by\nenforcing ownership and borrowing rules at compile time. While the unsafe\nkeyword allows programmers to bypass these restrictions, it introduces\nsignificant risks. Various approaches for isolating unsafe code to protect safe\nRust from vulnerabilities have been proposed, yet these methods provide only\nfixed isolation boundaries and do not accommodate expressive policies that\nrequire sandboxing both safe and unsafe code. This paper presents SandCell for\nflexible and lightweight isolation in Rust by leveraging existing syntactic\nboundaries. SandCell allows programmers to specify which components to sandbox\nwith minimal annotation effort, enabling fine-grained control over isolation.\nThe system also introduces novel techniques to minimize overhead when\ntransferring data between sandboxes. Our evaluation demonstrates SandCell's\neffectiveness in preventing vulnerabilities across various Rust applications\nwhile maintaining reasonable performance overheads."}
{"id": "2509.22873", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22873", "abs": "https://arxiv.org/abs/2509.22873", "authors": ["Aashnan Rahman", "Abid Hasan", "Sherajul Arifin", "Faisal Haque Bappy", "Tahrim Hossain", "Tariqul Islam", "Abu Raihan Mostofa Kamal", "Md. Azam Hossain"], "title": "AntiFLipper: A Secure and Efficient Defense Against Label-Flipping Attacks in Federated Learning", "comment": "Submitted to IEEE International Conference on Communications (ICC)\n  2026", "summary": "Federated learning (FL) enables privacy-preserving model training by keeping\ndata decentralized. However, it remains vulnerable to label-flipping attacks,\nwhere malicious clients manipulate labels to poison the global model. Despite\ntheir simplicity, these attacks can severely degrade model performance, and\ndefending against them remains challenging. We introduce AntiFLipper, a novel\nand computationally efficient defense against multi-class label-flipping\nattacks in FL. Unlike existing methods that ensure security at the cost of high\ncomputational overhead, AntiFLipper employs a novel client-side detection\nstrategy, significantly reducing the central server's burden during\naggregation. Comprehensive empirical evaluations across multiple datasets under\ndifferent distributions demonstrate that AntiFLipper achieves accuracy\ncomparable to state-of-the-art defenses while requiring substantially fewer\ncomputational resources in server side. By balancing security and efficiency,\nAntiFLipper addresses a critical gap in existing defenses, making it\nparticularly suitable for resource-constrained FL deployments where both model\nintegrity and operational efficiency are essential."}
{"id": "2509.24091", "categories": ["cs.SE", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.24091", "abs": "https://arxiv.org/abs/2509.24091", "authors": ["Spandan Garg", "Roshanak Zilouchian Moghaddam"], "title": "PerfBench: Can Agents Resolve Real-World Performance Bugs?", "comment": null, "summary": "Performance bugs are inefficiencies in software that waste computational\nresources without causing functional failures, making them particularly\nchallenging to detect and fix. While recent advances in Software Engineering\nagents have shown promise in automated bug fixing, existing benchmarks\nprimarily focus on functional correctness and fail to evaluate agents'\nabilities to identify and resolve non-functional issues like performance bugs.\nWe introduce PerfBench, a benchmark comprising 81 real-world performance\nbug-fixing tasks from popular .NET repositories on GitHub. Unlike existing\nbenchmarks that rely on pre-existing test suites, PerfBench features a novel\nevaluation harness that allows agents to generate their own performance\nbenchmarks and validates fixes by comparing execution metrics collected for\ndeveloper fix and agent fix. Each task in PerfBench is derived from actual\ndeveloper fixes linked to performance-related issues, which are then verified\nby human experts, ensuring real-world relevance. Our evaluation reveals that\ncurrent state-of-the-art coding agents struggle with performance optimization\ntasks, with baseline OpenHands agent achieving only a ~3% success rate on our\nbenchmark. We develop OpenHands-Perf-Agent, which incorporates\nperformance-aware tooling and instructions and achieves a ~20% success rate on\nthe benchmark. We show that by ensuring the agent has proper instructions to\nbenchmark its changes and tooling for benchmark output processing, we can\nimprove the agent performance significantly, but room for improvement still\nremains. PerfBench provides a challenging test set for furthering the\ncapabilities of agents in fixing performance issues."}
{"id": "2509.22900", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22900", "abs": "https://arxiv.org/abs/2509.22900", "authors": ["Haochen Gong", "Zhen Tao", "Shidong Pan", "Zhenchang Xing", "Xiaoyu Sun"], "title": "Towards Context-aware Mobile Privacy Notice: Implementation of A Deployable Contextual Privacy Policies Generator", "comment": "Accepted by ASE 2025, Tool Demonstration Track", "summary": "Lengthy and legally phrased privacy policies impede users' understanding of\nhow mobile applications collect and process personal data. Prior work proposed\nContextual Privacy Policies (CPPs) for mobile apps to display shorter policy\nsnippets only in the corresponding user interface contexts, but the pipeline\ncould not be deployable in real-world mobile environments. In this paper, we\npresent PrivScan, the first deployable CPP Software Development Kit (SDK) for\nAndroid. It captures live app screenshots to identify GUI elements associated\nwith types of personal data and displays CPPs in a concise, user-facing format.\nWe provide a lightweight floating button that offers low-friction, on-demand\ncontrol. The architecture leverages remote deployment to decouple the\nmultimodal backend pipeline from a mobile client comprising five modular\ncomponents, thereby reducing on-device resource demands and easing\ncross-platform portability. A feasibility-oriented evaluation shows an average\nexecution time of 9.15\\,s, demonstrating the practicality of our approach. The\nsource code of PrivScan is available at https://github.com/buyanghc/PrivScan\nand the demo video can be found at https://www.youtube.com/watch?v=ck-25otfyHc."}
{"id": "2509.24148", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24148", "abs": "https://arxiv.org/abs/2509.24148", "authors": ["Yiran Hu", "Nan Jiang", "Shanchao Liang", "Yi Wu", "Lin Tan"], "title": "TENET: Leveraging Tests Beyond Validation for Code Generation", "comment": null, "summary": "Test-Driven Development (TDD) is a widely adopted software engineering\npractice that requires developers to create and execute tests alongside code\nimplementation, ensuring that software behavior is continuously validated and\nrefined. In the era of vibe coding, where developers increasingly delegate code\nwriting to large language models (LLMs) by specifying high-level intentions,\nTDD becomes even more crucial, as test cases serve as executable specifications\nthat explicitly define and verify intended functionality beyond what\nnatural-language descriptions and code context can convey. While vibe coding\nunder TDD is promising, there are three main challenges: (1) selecting a small\nyet effective test suite to improve the generation accuracy and control the\nexecution workload, (2) retrieving context such as relevant code effectively,\nand (3) systematically using test feedback for effective code refinement. To\naddress these challenges, we introduce TENET, an LLM agent for generating\nfunctions in complex real-world repositories under the TDD setting. TENET\nfeatures three components: (1) a novel test harness mechanism that selects a\nconcise test suite to maximize diversity of target usage scenarios; (2) a\ntailored agent toolset that performs efficient retrieval of relevant code with\ninteractive debugging; and (3) a reflection-based refinement workflow that\niteratively analyzes failures, replenishes context, and applies code\nrefinement. TENET achieves 69.08% and 81.77% Pass@1 on RepoCod and RepoEval\nbenchmarks, outperforming the best agentic baselines by 9.49 and 2.17\npercentage points, respectively. In addition, this is the first study of\ntest-driven code generation with repository-level context, examining how\ndifferent aspects of test suites affect the performance of LLM agents under the\nTDD setting."}
{"id": "2509.22965", "categories": ["cs.CR", "68M14, 94A60", "K.6.5; D.4.6; J.1"], "pdf": "https://arxiv.org/pdf/2509.22965", "abs": "https://arxiv.org/abs/2509.22965", "authors": ["Yousef Tahboub", "Anthony Revilla", "Jaydon Lynch", "Greg Floyd"], "title": "Blockchain Voting System", "comment": null, "summary": "Casting a ballot from a phone or laptop sounds appealing, but only if voters\ncan be confident their choice remains secret and results cannot be altered in\nthe dark. This paper proposes a hybrid blockchain-based voting model that\nstores encrypted votes on a private blockchain maintained by election\norganizers and neutral observers, while periodically anchoring hashes of these\nvotes onto a public blockchain as a tamper-evident seal. The system issues\nvoters one-time blind-signed tokens to protect anonymity, and provides receipts\nso they can confirm their vote was counted. We implemented a live prototype\nusing common web technologies (Next.js, React, Firebase) to demonstrate\nend-to-end functionality, accessibility, and cost efficiency. Our contributions\ninclude developing a working demo, a complete election workflow, a hybrid\nblockchain design, and a user-friendly interface that balances privacy,\nsecurity, transparency, and practicality. This research highlights the\nfeasibility of secure, verifiable, and scalable online voting for organizations\nranging from small groups to larger institutions."}
{"id": "2509.24215", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.24215", "abs": "https://arxiv.org/abs/2509.24215", "authors": ["Wenxuan Wang", "Yongjiang Wu", "Junyuan Zhang", "Shuqing Li", "Yun Peng", "Wenting Chen", "Shuai Wang", "Michael R. Lyu"], "title": "Metamorphic Testing for Audio Content Moderation Software", "comment": "Accepted by ASE 2025", "summary": "The rapid growth of audio-centric platforms and applications such as WhatsApp\nand Twitter has transformed the way people communicate and share audio content\nin modern society. However, these platforms are increasingly misused to\ndisseminate harmful audio content, such as hate speech, deceptive\nadvertisements, and explicit material, which can have significant negative\nconsequences (e.g., detrimental effects on mental health). In response,\nresearchers and practitioners have been actively developing and deploying audio\ncontent moderation tools to tackle this issue. Despite these efforts, malicious\nactors can bypass moderation systems by making subtle alterations to audio\ncontent, such as modifying pitch or inserting noise. Moreover, the\neffectiveness of modern audio moderation tools against such adversarial inputs\nremains insufficiently studied. To address these challenges, we propose MTAM, a\nMetamorphic Testing framework for Audio content Moderation software.\nSpecifically, we conduct a pilot study on 2000 audio clips and define 14\nmetamorphic relations across two perturbation categories: Audio Features-Based\nand Heuristic perturbations. MTAM applies these metamorphic relations to toxic\naudio content to generate test cases that remain harmful while being more\nlikely to evade detection. In our evaluation, we employ MTAM to test five\ncommercial textual content moderation software and an academic model against\nthree kinds of toxic content. The results show that MTAM achieves up to 38.6%,\n18.3%, 35.1%, 16.7%, and 51.1% error finding rates (EFR) when testing\ncommercial moderation software provided by Gladia, Assembly AI, Baidu,\nNextdata, and Tencent, respectively, and it obtains up to 45.7% EFR when\ntesting the state-of-the-art algorithms from the academy."}
{"id": "2509.22986", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.22986", "abs": "https://arxiv.org/abs/2509.22986", "authors": ["Jingyao Zhang", "Elaheh Sadredini"], "title": "CryptoSRAM: Enabling High-Throughput Cryptography on MCUs via In-SRAM Computing", "comment": "To appear in 2025 IEEE Cross-disciplinary Conference on\n  Memory-Centric Computing (CCMCC)", "summary": "Secure communication is a critical requirement for Internet of Things (IoT)\ndevices, which are often based on Microcontroller Units (MCUs). Current\ncryptographic solutions, which rely on software libraries or dedicated hardware\naccelerators, are fundamentally limited by the performance and energy costs of\ndata movement between memory and processing units. This paper introduces\nCryptoSRAM, an in-SRAM computing architecture that performs cryptographic\noperations directly within the MCU's standard SRAM array. By repurposing the\nmemory array into a massively parallel processing fabric, CryptoSRAM eliminates\nthe data movement bottleneck. This approach is well-suited to MCUs, which\nutilize physical addressing and Direct Memory Access (DMA) to manage SRAM,\nallowing for seamless integration with minimal hardware overhead. Our analysis\nshows that for common cryptographic kernels, CryptoSRAM achieves throughput\nimprovements of up to 74$\\times$ and 67$\\times$ for AES and SHA3, respectively,\ncompared to a software implementation. Furthermore, our solution delivers up to\n6$\\times$ higher throughput than existing hardware accelerators for AES.\nCryptoSRAM demonstrates a viable and efficient architecture for secure\ncommunication in next-generation IoT systems."}
{"id": "2509.24344", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24344", "abs": "https://arxiv.org/abs/2509.24344", "authors": ["Theo Koraag", "Niklas Wagner", "Felix Dobslaw", "Lucas Gren"], "title": "Comparing Open-Source and Commercial LLMs for Domain-Specific Analysis and Reporting: Software Engineering Challenges and Design Trade-offs", "comment": null, "summary": "Context: Large Language Models (LLMs) enable automation of complex natural\nlanguage processing across domains, but research on domain-specific\napplications like Finance remains limited. Objectives: This study explored\nopen-source and commercial LLMs for financial report analysis and commentary\ngeneration, focusing on software engineering challenges in implementation.\nMethods: Using Design Science Research methodology, an exploratory case study\niteratively designed and evaluated two LLM-based systems: one with local\nopen-source models in a multi-agent workflow, another using commercial GPT-4o.\nBoth were assessed through expert evaluation of real-world financial reporting\nuse cases. Results: LLMs demonstrated strong potential for automating financial\nreporting tasks, but integration presented significant challenges. Iterative\ndevelopment revealed issues including prompt design, contextual dependency, and\nimplementation trade-offs. Cloud-based models offered superior fluency and\nusability but raised data privacy and external dependency concerns. Local\nopen-source models provided better data control and compliance but required\nsubstantially more engineering effort for reliability and usability.\nConclusion: LLMs show strong potential for financial reporting automation, but\nsuccessful integration requires careful attention to architecture, prompt\ndesign, and system reliability. Implementation success depends on addressing\ndomain-specific challenges through tailored validation mechanisms and\nengineering strategies that balance accuracy, control, and compliance."}
{"id": "2509.23019", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23019", "abs": "https://arxiv.org/abs/2509.23019", "authors": ["Jeongyeon Hwang", "Sangdon Park", "Jungseul Ok"], "title": "LLM Watermark Evasion via Bias Inversion", "comment": null, "summary": "Watermarking for large language models (LLMs) embeds a statistical signal\nduring generation to enable detection of model-produced text. While\nwatermarking has proven effective in benign settings, its robustness under\nadversarial evasion remains contested. To advance a rigorous understanding and\nevaluation of such vulnerabilities, we propose the \\emph{Bias-Inversion\nRewriting Attack} (BIRA), which is theoretically motivated and model-agnostic.\nBIRA weakens the watermark signal by suppressing the logits of likely\nwatermarked tokens during LLM-based rewriting, without any knowledge of the\nunderlying watermarking scheme. Across recent watermarking methods, BIRA\nachieves over 99\\% evasion while preserving the semantic content of the\noriginal text. Beyond demonstrating an attack, our results reveal a systematic\nvulnerability, emphasizing the need for stress testing and robust defenses."}
{"id": "2509.24347", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24347", "abs": "https://arxiv.org/abs/2509.24347", "authors": ["Junjie Meng", "Jie An", "Yong Li", "Andrea Turrini", "Fanjiang Xu", "Naijun Zhan", "Miaomiao Zhang"], "title": "Efficient Decomposition Identification of Deterministic Finite Automata from Examples", "comment": null, "summary": "The identification of deterministic finite automata (DFAs) from labeled\nexamples is a cornerstone of automata learning, yet traditional methods focus\non learning monolithic DFAs, which often yield a large DFA lacking simplicity\nand interoperability. Recent work addresses these limitations by exploring DFA\ndecomposition identification problems (DFA-DIPs), which model system behavior\nas intersections of multiple DFAs, offering modularity for complex tasks.\nHowever, existing DFA-DIP approaches depend on SAT encodings derived from\nAugmented Prefix Tree Acceptors (APTAs), incurring scalability limitations due\nto their inherent redundancy.\n  In this work, we advance DFA-DIP research through studying two variants: the\ntraditional Pareto-optimal DIP and the novel states-optimal DIP, which\nprioritizes a minimal number of states. We propose a novel framework that\nbridges DFA decomposition with recent advancements in automata representation.\nOne of our key innovations replaces APTA with 3-valued DFA (3DFA) derived\ndirectly from labeled examples. This compact representation eliminates\nredundancies of APTA, thus drastically reducing variables in the improved SAT\nencoding. Experimental results demonstrate that our 3DFA-based approach\nachieves significant efficiency gains for the Pareto-optimal DIP while enabling\na scalable solution for the states-optimal DIP."}
{"id": "2509.23041", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23041", "abs": "https://arxiv.org/abs/2509.23041", "authors": ["Zi Liang", "Qingqing Ye", "Xuan Liu", "Yanyun Wang", "Jianliang Xu", "Haibo Hu"], "title": "Virus Infection Attack on LLMs: Your Poisoning Can Spread \"VIA\" Synthetic Data", "comment": "NeurIPS 2025 Spotlight. Source code:\n  https://github.com/liangzid/VirusInfectionAttack", "summary": "Synthetic data refers to artificial samples generated by models. While it has\nbeen validated to significantly enhance the performance of large language\nmodels (LLMs) during training and has been widely adopted in LLM development,\npotential security risks it may introduce remain uninvestigated. This paper\nsystematically evaluates the resilience of synthetic-data-integrated training\nparadigm for LLMs against mainstream poisoning and backdoor attacks. We reveal\nthat such a paradigm exhibits strong resistance to existing attacks, primarily\nthanks to the different distribution patterns between poisoning data and\nqueries used to generate synthetic samples. To enhance the effectiveness of\nthese attacks and further investigate the security risks introduced by\nsynthetic data, we introduce a novel and universal attack framework, namely,\nVirus Infection Attack (VIA), which enables the propagation of current attacks\nthrough synthetic data even under purely clean queries. Inspired by the\nprinciples of virus design in cybersecurity, VIA conceals the poisoning payload\nwithin a protective \"shell\" and strategically searches for optimal hijacking\npoints in benign samples to maximize the likelihood of generating malicious\ncontent. Extensive experiments on both data poisoning and backdoor attacks show\nthat VIA significantly increases the presence of poisoning content in synthetic\ndata and correspondingly raises the attack success rate (ASR) on downstream\nmodels to levels comparable to those observed in the poisoned upstream models."}
{"id": "2509.24352", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24352", "abs": "https://arxiv.org/abs/2509.24352", "authors": ["Minghua He", "Tong Jia", "Chiming Duan", "Pei Xiao", "Lingzhe Zhang", "Kangjin Wang", "Yifan Wu", "Ying Li", "Gang Huang"], "title": "Walk the Talk: Is Your Log-based Software Reliability Maintenance System Really Reliable?", "comment": "Accepted by ASE 2025 (NIER Track)", "summary": "Log-based software reliability maintenance systems are crucial for sustaining\nstable customer experience. However, existing deep learning-based methods\nrepresent a black box for service providers, making it impossible for providers\nto understand how these methods detect anomalies, thereby hindering trust and\ndeployment in real production environments. To address this issue, this paper\ndefines a trustworthiness metric, diagnostic faithfulness, for models to gain\nservice providers' trust, based on surveys of SREs at a major cloud provider.\nWe design two evaluation tasks: attention-based root cause localization and\nevent perturbation. Empirical studies demonstrate that existing methods perform\npoorly in diagnostic faithfulness. Consequently, we propose FaithLog, a\nfaithful log-based anomaly detection system, which achieves faithfulness\nthrough a carefully designed causality-guided attention mechanism and\nadversarial consistency learning. Evaluation results on two public datasets and\none industrial dataset demonstrate that the proposed method achieves\nstate-of-the-art performance in diagnostic faithfulness."}
{"id": "2509.23091", "categories": ["cs.CR", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23091", "abs": "https://arxiv.org/abs/2509.23091", "authors": ["Xiangchen Meng", "Yangdi Lyu"], "title": "FedBit: Accelerating Privacy-Preserving Federated Learning via Bit-Interleaved Packing and Cross-Layer Co-Design", "comment": null, "summary": "Federated learning (FL) with fully homomorphic encryption (FHE) effectively\nsafeguards data privacy during model aggregation by encrypting local model\nupdates before transmission, mitigating threats from untrusted servers or\neavesdroppers in transmission. However, the computational burden and ciphertext\nexpansion associated with homomorphic encryption can significantly increase\nresource and communication overhead. To address these challenges, we propose\nFedBit, a hardware/software co-designed framework optimized for the\nBrakerski-Fan-Vercauteren (BFV) scheme. FedBit employs bit-interleaved data\npacking to embed multiple model parameters into a single ciphertext\ncoefficient, thereby minimizing ciphertext expansion and maximizing\ncomputational parallelism. Additionally, we integrate a dedicated FPGA\naccelerator to handle cryptographic operations and an optimized dataflow to\nreduce the memory overhead. Experimental results demonstrate that FedBit\nachieves a speedup of two orders of magnitude in encryption and lowers average\ncommunication overhead by 60.7%, while maintaining high accuracy."}
{"id": "2509.24364", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24364", "abs": "https://arxiv.org/abs/2509.24364", "authors": ["Minghua He", "Chiming Duan", "Pei Xiao", "Tong Jia", "Siyu Yu", "Lingzhe Zhang", "Weijie Hong", "Jin Han", "Yifan Wu", "Ying Li", "Gang Huang"], "title": "United We Stand: Towards End-to-End Log-based Fault Diagnosis via Interactive Multi-Task Learning", "comment": "ASE 2025 (Research Track)", "summary": "Log-based fault diagnosis is essential for maintaining software system\navailability. However, existing fault diagnosis methods are built using a\ntask-independent manner, which fails to bridge the gap between anomaly\ndetection and root cause localization in terms of data form and diagnostic\nobjectives, resulting in three major issues: 1) Diagnostic bias accumulates in\nthe system; 2) System deployment relies on expensive monitoring data; 3) The\ncollaborative relationship between diagnostic tasks is overlooked. Facing this\nproblems, we propose a novel end-to-end log-based fault diagnosis method,\nChimera, whose key idea is to achieve end-to-end fault diagnosis through\nbidirectional interaction and knowledge transfer between anomaly detection and\nroot cause localization. Chimera is based on interactive multi-task learning,\ncarefully designing interaction strategies between anomaly detection and root\ncause localization at the data, feature, and diagnostic result levels, thereby\nachieving both sub-tasks interactively within a unified end-to-end framework.\nEvaluation on two public datasets and one industrial dataset shows that Chimera\noutperforms existing methods in both anomaly detection and root cause\nlocalization, achieving improvements of over 2.92% - 5.00% and 19.01% - 37.09%,\nrespectively. It has been successfully deployed in production, serving an\nindustrial cloud platform."}
{"id": "2509.23305", "categories": ["cs.CR", "93C95", "I.6.5"], "pdf": "https://arxiv.org/pdf/2509.23305", "abs": "https://arxiv.org/abs/2509.23305", "authors": ["Jaxson Brown", "Duc-Son Pham", "Sie-Teng Soh", "Foad Motalebi", "Sivaraman Eswaran", "Mahathir Almashor"], "title": "ICS-SimLab: A Containerized Approach for Simulating Industrial Control Systems for Cyber Security Research", "comment": "This is the 10-page extended version of a paper accepted to the First\n  International Workshop on Secure Industrial Control Systems and\n  Industrial-IoT, IEEE CNS 2025 (the conference version was 6 pages)", "summary": "Industrial Control Systems (ICSs) are complex interconnected systems used to\nmanage process control within industrial environments, such as chemical\nprocessing plants and water treatment facilities. As the modern industrial\nenvironment moves towards Internet-facing services, ICSs face an increased risk\nof attacks that necessitates ICS-specific Intrusion Detection Systems (IDS).\nThe development of such IDS relies significantly on a simulated testbed as it\nis unrealistic and sometimes hazardous to utilize an operational control\nsystem. Whilst some testbeds have been proposed, they often use a limited\nselection of virtual ICS simulations to test and verify cyber security\nsolutions. There is a lack of investigation done on developing systems that can\nefficiently simulate multiple ICS architectures. Currently, the trend within\nresearch involves developing security solutions on just one ICS simulation,\nwhich can result in bias to its specific architecture. We present ICS-SimLab,\nan end-to-end software suite that utilizes Docker containerization technology\nto create a highly configurable ICS simulation environment. This software\nframework enables researchers to rapidly build and customize different ICS\nenvironments, facilitating the development of security solutions across\ndifferent systems that adhere to the Purdue Enterprise Reference Architecture.\nTo demonstrate its capability, we present three virtual ICS simulations: a\nsolar panel smart grid, a water bottle filling facility, and a system of\nintelligent electronic devices. Furthermore, we run cyber-attacks on these\nsimulations and construct a dataset of recorded malicious and benign network\ntraffic to be used for IDS development."}
{"id": "2509.24380", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24380", "abs": "https://arxiv.org/abs/2509.24380", "authors": ["Shuiguang Deng", "Hailiang Zhao", "Ziqi Wang", "Guanjie Cheng", "Peng Chen", "Wenzhuo Qian", "Zhiwei Ling", "Jianwei Yin", "Albert Y. Zomaya", "Schahram Dustdar"], "title": "Agentic Services Computing", "comment": null, "summary": "The rise of LLM-powered agents is driving a fundamental transformation in\nservices computing: from static, request-response functions to dynamic,\ngoal-oriented, and autonomous multi-agent ecosystems. In response to this\nshift, we introduce Agentic Service Computing (ASC), a new paradigm that\nreimagines services as intelligent, self-adaptive, and socially embedded\nentities. This comprehensive survey presents a lifecycle-driven framework for\nASC, structured around four core phases: Design, Deployment, Operation, and\nEvolution. We systematically analyze ASC through four foundational research\ndimensions: (1) Perception, Context, and Environment Modeling, (2) Autonomous\nDecision-Making and Task Execution, (3) Multi-Agent Collaboration and\nOrganization, and (4) Evaluation, Value Alignment, and Trustworthiness. We\nexamine how these dimensions are instantiated, integrated, and continuously\nadapted across the service lifecycle. Our synthesis reveals that agentic\nservices are not merely assembled but orchestrated: contextual awareness\nenables robust deployment; autonomous reasoning supports real-time operation;\ncollaborative structures emerge and evolve through interaction; and\ntrustworthiness must be upheld as a cross-cutting, lifelong imperative. We\nfurther identify and discuss emerging trends shaping the future of ASC. By\nintegrating classical principles of services computing with advances in\nLLM-based multi-agent systems, this work establishes a holistic and\nforward-looking foundation for ASC. It provides a unified reference for\nresearchers and practitioners aiming to develop adaptive, accountable, and\nhuman-centered intelligent services."}
{"id": "2509.23418", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.23418", "abs": "https://arxiv.org/abs/2509.23418", "authors": ["Ummay Kulsum", "Aafaq Sabir", "Abhinaya S. B.", "Anupam Das"], "title": "Detecting YouTube Scam Videos via Multimodal Signals and Policy Reasoning", "comment": null, "summary": "YouTube has emerged as a dominant platform for both information dissemination\nand entertainment. However, its vast accessibility has also made it a target\nfor scammers, who frequently upload deceptive or malicious content. Prior\nresearch has documented a range of scam types, and detection approaches rely\nprimarily on textual or statistical metadata. Although effective to some\nextent, these signals are easy to evade and potentially overlook other\nmodalities, such as visual cues.\n  In this study, we present the first systematic investigation of multimodal\napproaches for YouTube scam detection. Our dataset consolidates established\nscam categories and augments them with full length video content and policy\ngrounded reasoning annotations. Our experimental evaluation demonstrates that a\ntext-only model using video titles and descriptions (fine-tuned BERT) achieves\nmoderate effectiveness (76.61% F1), with modest improvements when incorporating\naudio transcripts (77.98% F1). In contrast, visual analysis using a fine-tuned\nLLaVA-Video model yields stronger results (79.61% F1). Finally, a multimodal\nframework that integrates titles, descriptions, and video frames achieves the\nhighest performance (80.53% F1). Beyond improving detection accuracy, our\nmultimodal framework produces interpretable reasoning grounded in YouTube\ncontent policies, thereby enhancing transparency and supporting potential\napplications in automated moderation. Moreover, we validate our approach on\nin-the-wild YouTube data by analyzing 6,374 videos, thereby contributing a\nvaluable resource for future research on scam detection."}
{"id": "2509.24419", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24419", "abs": "https://arxiv.org/abs/2509.24419", "authors": ["Yuanhe Zhang", "Zhiquan Yang", "Shengyi Pan", "Zhongxin Liu"], "title": "Unit Test Update through LLM-Driven Context Collection and Error-Type-Aware Refinement", "comment": null, "summary": "Unit testing is critical for ensuring software quality and software system\nstability. The current practice of manually maintaining unit tests suffers from\nlow efficiency and the risk of delayed or overlooked fixes. Therefore, an\nautomated approach is required to instantly update unit tests, with the\ncapability to both repair and enhance unit tests. However, existing automated\ntest maintenance methods primarily focus on repairing broken tests, neglecting\nthe scenario of enhancing existing tests to verify new functionality.\nMeanwhile, due to their reliance on rule-based context collection and the lack\nof verification mechanisms, existing approaches struggle to handle complex code\nchanges and often produce test cases with low correctness. To address these\nchallenges, we propose TESTUPDATER, a novel LLM based approach that enables\nautomated just-in-time test updates in response to production code changes.\nTESTUPDATER first leverages the LLM to analyze code changes and identify\nrelevant context, which it then extracts and filters. Then, through carefully\ndesigned prompts, TESTUPDATER guides the LLM step by step to handle various\ntypes of code changes and introduce new dependencies, enabling both test repair\nand enhancement. Finally, we introduce an error-type-aware iterative refinement\nmechanism that executes the LLM-updated tests and repairs failures, which\nsignificantly improves the overall correctness of test updates. Since existing\ntest repair datasets lack scenarios of test enhancement, we further construct a\nnew benchmark, UPDATES4J, with 195 real-world samples from 7 projects.\nExperimental results show that TESTUPDATER achieves a compilation pass rate of\n94.4% and a test pass rate of 86.7%, outperforming the state-of-the-art method\nSYNTER by 15.9% and 20.0%, respectively. Furthermore, TESTUPDATER exhibits\n12.9% higher branch coverage and 15.2% greater line coverage than SYNTER."}
{"id": "2509.23427", "categories": ["cs.CR", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.23427", "abs": "https://arxiv.org/abs/2509.23427", "authors": ["Rowdy Chotkan", "Bulat Nasrulin", "Jérémie Decouchant", "Johan Pouwelse"], "title": "StarveSpam: Mitigating Spam with Local Reputation in Permissionless Blockchains", "comment": "Preprint. Accepted for publication in the proceedings of the 7th\n  Conference on Blockchain Research & Applications for Innovative Networks and\n  Services (BRAINS 2025). The final version will be available on IEEE Xplore", "summary": "Spam poses a growing threat to blockchain networks. Adversaries can easily\ncreate multiple accounts to flood transaction pools, inflating fees and\ndegrading service quality. Existing defenses against spam, such as fee markets\nand staking requirements, primarily rely on economic deterrence, which fails to\ndistinguish between malicious and legitimate users and often exclude low-value\nbut honest activity. To address these shortcomings, we present StarveSpam, a\ndecentralized reputation-based protocol that mitigates spam by operating at the\ntransaction relay layer. StarveSpam combines local behavior tracking, peer\nscoring, and adaptive rate-limiting to suppress abusive actors, without\nrequiring global consensus, protocol changes, or trusted infrastructure. We\nevaluate StarveSpam using real Ethereum data from a major NFT spam event and\nshow that it outperforms existing fee-based and rule-based defenses, allowing\neach node to block over 95% of spam while dropping just 3% of honest traffic,\nand reducing the fraction of the network exposed to spam by 85% compared to\nexisting rule-based methods. StarveSpam offers a scalable and deployable\nalternative to traditional spam defenses, paving the way toward more resilient\nand equitable blockchain infrastructure."}
{"id": "2509.24485", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24485", "abs": "https://arxiv.org/abs/2509.24485", "authors": ["Vlad Stirbu", "Mateen Ahmed Abbasi", "Teerath Das", "Jesse Haimi", "Niko Iljin", "Pyry Kotilainen", "Petrus Lipsanen", "Niko Mäkitalo", "Maiju Sipilä", "Venla Veijalainen", "Tommi Mikkonen"], "title": "Towards Shift-Up: A Framework and a Prestudy on High-Value Activities in GenAI Native Software Development", "comment": null, "summary": "Generative AI (GenAI) has significantly influenced software engineering.\nAssociated tools have created a shift in software engineering, where\nspecialized agents, based on user-provided prompts, are replacing human\ndevelopers. In this paper, we propose a framework for GenAI native development\nthat we call \\textit{shift-up}, which helps software teams focus on high-value\nwork while being supported by GenAI. Furthermore, we also present a preliminary\nstudy testing these ideas with current GenAI tools. Towards the end of the\npaper, we propose future research goals to study shift-up in more detail."}
{"id": "2509.23459", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23459", "abs": "https://arxiv.org/abs/2509.23459", "authors": ["Sepideh Abedini", "Shubhankar Mohapatra", "D. B. Emerson", "Masoumeh Shafieinejad", "Jesse C. Cresswell", "Xi He"], "title": "MaskSQL: Safeguarding Privacy for LLM-Based Text-to-SQL via Abstraction", "comment": "Accepted to the NeurIPS 2025 Workshop on Regulatable Machine Learning\n  (Regulatable ML @ NeurIPS 2025). Code available at\n  https://github.com/sepideh-abedini/MaskSQL", "summary": "Large language models (LLMs) have shown promising performance on tasks that\nrequire reasoning, such as text-to-SQL, code generation, and debugging.\nHowever, regulatory frameworks with strict privacy requirements constrain their\nintegration into sensitive systems. State-of-the-art LLMs are also proprietary,\ncostly, and resource-intensive, making local deployment impractical.\nConsequently, utilizing such LLMs often requires sharing data with third-party\nproviders, raising privacy concerns and risking noncompliance with regulations.\nAlthough fine-tuned small language models (SLMs) can outperform LLMs on certain\ntasks and be deployed locally to mitigate privacy concerns, they underperform\non more complex tasks such as text-to-SQL translation. In this work, we\nintroduce MaskSQL, a text-to-SQL framework that utilizes abstraction as a\nprivacy protection mechanism to mask sensitive information in LLM prompts.\nUnlike redaction, which removes content entirely, or generalization, which\nbroadens tokens, abstraction retains essential information while discarding\nunnecessary details, striking an effective privacy-utility balance for the\ntext-to-SQL task. Moreover, by providing mechanisms to control the\nprivacy-utility tradeoff, MaskSQL facilitates adoption across a broader range\nof use cases. Our experimental results show that MaskSQL outperforms leading\nSLM-based text-to-SQL models and achieves performance approaching\nstate-of-the-art LLM-based models, while preserving privacy."}
{"id": "2509.24498", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24498", "abs": "https://arxiv.org/abs/2509.24498", "authors": ["Zhihao Li", "Chaozheng Wang", "Zongjie Li", "Xinyong Peng", "Zelin Su", "Qun Xia", "Haochuan Lu", "Ting Xiong", "Man Ho Lam", "Shuzheng Gao", "Yuchong Xie", "Cuiyun Gao", "Shuai Wang", "Yuetang Deng", "Huafeng Ma"], "title": "JSProtect: A Scalable Obfuscation Framework for Mini-Games in WeChat", "comment": "10 pages", "summary": "The WeChat mini-game ecosystem faces rampant intellectual property theft to\nother platforms via secondary development, yet existing JavaScript obfuscation\ntools are ill-equipped for large-scale applications, suffering from prohibitive\nprocessing times, severe runtime performance degradation, and unsustainable\ncode size inflation. This paper introduces JSProtect, a high-throughput\nparallelized obfuscation framework designed to overcome these fundamental\nlimitations. At the core of our framework is the Parallel-Aware Scope Analysis\n(PASA) algorithm, which enables two key optimizations: independent code\npartitioning for multi-core processing and independent namespace management\nthat aggressively reuses short identifiers to combat code bloat. Our evaluation\ndemonstrates that JSProtectprocesses 20MB codebases in minutes, maintaining\n100\\% semantic equivalence while controlling code size inflation to as low as\n20\\% compared to over 1,000\\% with baseline tools. Furthermore, it preserves\nnear-native runtime performance and provides superior security effectiveness\nagainst both static analysis tools and large language models. This work\npresents a new paradigm for industrial-scale JavaScript protection that\neffectively balances robust security with high performance and scalability."}
{"id": "2509.23519", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23519", "abs": "https://arxiv.org/abs/2509.23519", "authors": ["Zeyu Shen", "Basileal Imana", "Tong Wu", "Chong Xiang", "Prateek Mittal", "Aleksandra Korolova"], "title": "ReliabilityRAG: Effective and Provably Robust Defense for RAG-based Web-Search", "comment": "Accepted to NeurIPS 2025", "summary": "Retrieval-Augmented Generation (RAG) enhances Large Language Models by\ngrounding their outputs in external documents. These systems, however, remain\nvulnerable to attacks on the retrieval corpus, such as prompt injection.\nRAG-based search systems (e.g., Google's Search AI Overview) present an\ninteresting setting for studying and protecting against such threats, as\ndefense algorithms can benefit from built-in reliability signals -- like\ndocument ranking -- and represent a non-LLM challenge for the adversary due to\ndecades of work to thwart SEO.\n  Motivated by, but not limited to, this scenario, this work introduces\nReliabilityRAG, a framework for adversarial robustness that explicitly\nleverages reliability information of retrieved documents.\n  Our first contribution adopts a graph-theoretic perspective to identify a\n\"consistent majority\" among retrieved documents to filter out malicious ones.\nWe introduce a novel algorithm based on finding a Maximum Independent Set (MIS)\non a document graph where edges encode contradiction. Our MIS variant\nexplicitly prioritizes higher-reliability documents and provides provable\nrobustness guarantees against bounded adversarial corruption under natural\nassumptions. Recognizing the computational cost of exact MIS for large\nretrieval sets, our second contribution is a scalable weighted sample and\naggregate framework. It explicitly utilizes reliability information, preserving\nsome robustness guarantees while efficiently handling many documents.\n  We present empirical results showing ReliabilityRAG provides superior\nrobustness against adversarial attacks compared to prior methods, maintains\nhigh benign accuracy, and excels in long-form generation tasks where prior\nrobustness-focused methods struggled. Our work is a significant step towards\nmore effective, provably robust defenses against retrieved corpus corruption in\nRAG."}
{"id": "2509.24507", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24507", "abs": "https://arxiv.org/abs/2509.24507", "authors": ["Qinglin Wang", "Zhihong Sun", "Ruyun Wang", "Tao Huang", "Zhi Jin", "Ge Li", "Chen Lyu"], "title": "SemGuard: Real-Time Semantic Evaluator for Correcting LLM-Generated Code", "comment": "Accepted by the 40th IEEE/ACM Automated Software Engineering\n  Conference (ASE 2025)", "summary": "Large Language Models (LLMs) can translate natural language requirements into\ncode, yet empirical analyses of representative models reveal that semantic\nerrors-programs that compile but behave incorrectly-constitute the majority of\nobserved faults (e.g., >60% on DeepSeek-Coder-6.7B and QwenCoder-7B). Post-hoc\nrepair pipelines detect such faults only after execution, incurring latency,\nrelying on incomplete test suites, and often mis-localizing the defect. Since\nsemantic drift originates in the autoregressive decoding process, intervening\nwhile the code is being generated is a direct way to stop error propagation.\nConstrained-decoding approaches such as ROCODE attempt this, but still wait\nuntil the entire program runs to obtain feedback and use entropy heuristics\nthat do not truly capture semantics. A more effective solution must inject\nsemantic signals-early and precisely-into the decoding process.We present\nSemGuard, a semantic-evaluator-driven framework that performs real-time,\nline-level semantic supervision. To train the evaluator, we build SemDiff, the\nfirst dataset with fine-grained annotations that mark the exact line where a\ncorrect and an incorrect implementation diverge. The evaluator, once embedded\nin the LLM's decoder, flags deviations on partial code, rolls back to the\nfaulty line, and guides regeneration-without executing the program or requiring\ntest cases. Across four benchmarks, SemGuard consistently outperforms\nstate-of-the-art baselines. It lowers the semantic error rate by 19.86% on\nSemDiff relative to ROCODE, and lifts Pass@1 by 48.92% on the real-world\nLiveCodeBench with CodeLlama-7B. Similar gains hold for StarCoder2-7B on MBPP\nand for DeepSeekCoder-6.7B on the Java benchmark SemDiff-Java, demonstrating\nmodel- and language-agnostic effectiveness."}
{"id": "2509.23571", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23571", "abs": "https://arxiv.org/abs/2509.23571", "authors": ["Yuqiao Meng", "Luoxi Tang", "Feiyang Yu", "Xi Li", "Guanhua Yan", "Ping Yang", "Zhaohan Xi"], "title": "Benchmarking LLM-Assisted Blue Teaming via Standardized Threat Hunting", "comment": null, "summary": "As cyber threats continue to grow in scale and sophistication, blue team\ndefenders increasingly require advanced tools to proactively detect and\nmitigate risks. Large Language Models (LLMs) offer promising capabilities for\nenhancing threat analysis. However, their effectiveness in real-world blue team\nthreat-hunting scenarios remains insufficiently explored. This paper presents\nCyberTeam, a benchmark designed to guide LLMs in blue teaming practice.\nCyberTeam constructs a standardized workflow in two stages. First, it models\nrealistic threat-hunting workflows by capturing the dependencies among\nanalytical tasks from threat attribution to incident response. Next, each task\nis addressed through a set of operational modules tailored to its specific\nanalytical requirements. This transforms threat hunting into a structured\nsequence of reasoning steps, with each step grounded in a discrete operation\nand ordered according to task-specific dependencies. Guided by this framework,\nLLMs are directed to perform threat-hunting tasks through modularized steps.\nOverall, CyberTeam integrates 30 tasks and 9 operational modules to guide LLMs\nthrough standardized threat analysis. We evaluate both leading LLMs and\nstate-of-the-art cybersecurity agents, comparing CyberTeam against open-ended\nreasoning strategies. Our results highlight the improvements enabled by\nstandardized design, while also revealing the limitations of open-ended\nreasoning in real-world threat hunting."}
{"id": "2509.24515", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.24515", "abs": "https://arxiv.org/abs/2509.24515", "authors": ["Yu-Fu Fu", "Meng Xu", "Taesoo Kim"], "title": "Agentic Specification Generator for Move Programs", "comment": "18 pages; Extended version of ASE'25 paper with extra appendices", "summary": "While LLM-based specification generation is gaining traction, existing tools\nprimarily focus on mainstream programming languages like C, Java, and even\nSolidity, leaving emerging and yet verification-oriented languages like Move\nunderexplored. In this paper, we introduce MSG, an automated specification\ngeneration tool designed for Move smart contracts. MSG aims to highlight key\ninsights that uniquely present when applying LLM-based specification generation\nto a new ecosystem. Specifically, MSG demonstrates that LLMs exhibit robust\ncode comprehension and generation capabilities even for non-mainstream\nlanguages. MSG successfully generates verifiable specifications for 84% of\ntested Move functions and even identifies clauses previously overlooked by\nexperts. Additionally, MSG shows that explicitly leveraging specification\nlanguage features through an agentic, modular design improves specification\nquality substantially (generating 57% more verifiable clauses than conventional\ndesigns). Incorporating feedback from the verification toolchain further\nenhances the effectiveness of MSG, leading to a 30% increase in generated\nverifiable specifications."}
{"id": "2509.23573", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23573", "abs": "https://arxiv.org/abs/2509.23573", "authors": ["Yuqiao Meng", "Luoxi Tang", "Feiyang Yu", "Jinyuan Jia", "Guanhua Yan", "Ping Yang", "Zhaohan Xi"], "title": "Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence", "comment": null, "summary": "Large Language Models (LLMs) are intensively used to assist security analysts\nin counteracting the rapid exploitation of cyber threats, wherein LLMs offer\ncyber threat intelligence (CTI) to support vulnerability assessment and\nincident response. While recent work has shown that LLMs can support a wide\nrange of CTI tasks such as threat analysis, vulnerability detection, and\nintrusion defense, significant performance gaps persist in practical\ndeployments. In this paper, we investigate the intrinsic vulnerabilities of\nLLMs in CTI, focusing on challenges that arise from the nature of the threat\nlandscape itself rather than the model architecture. Using large-scale\nevaluations across multiple CTI benchmarks and real-world threat reports, we\nintroduce a novel categorization methodology that integrates stratification,\nautoregressive refinement, and human-in-the-loop supervision to reliably\nanalyze failure instances. Through extensive experiments and human inspections,\nwe reveal three fundamental vulnerabilities: spurious correlations,\ncontradictory knowledge, and constrained generalization, that limit LLMs in\neffectively supporting CTI. Subsequently, we provide actionable insights for\ndesigning more robust LLM-powered CTI systems to facilitate future research."}
{"id": "2509.24637", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24637", "abs": "https://arxiv.org/abs/2509.24637", "authors": ["Zhensu Sun", "Chengran Yang", "Chao Peng", "Pengfei Gao", "Xiaoning Du", "Li Li", "David Lo"], "title": "Bridging Developer Instructions and Code Completion Through Instruction-Aware Fill-in-the-Middle Paradigm", "comment": "10 pages", "summary": "Large Language Models (LLMs) have significantly advanced code completion, yet\nthey often fail when the developer's intent is underspecified in the code\ncontext. To address this, developers usually add natural language instructions\n(e.g., comments) into the code context to clarify their intent. However,\nexisting code LLMs applied for code completion systems merely undergo a\nfill-in-the-middle (FIM) pre-training, which struggles to leverage this\ninformation effectively due to the lack of instruction-like training data.\nExisting instruction-tuning techniques, which improve instruction-following in\ngeneral code generation, paradoxically degrade FIM performance, forcing a\ntrade-off between instruction-following and infilling capabilities. To address\nthis gap, we introduce Instruction-aware Fill-in-the-Middle (IFIM), an\ninstruction-tuning method specifically designed to enhance FIM code completion\nmodels. IFIM extends the conventional FIM training objective by incorporating\nan explicit instruction section into the input, enabling the model to learn\nfrom (prefix, instruction, suffix) triplets. This approach allows the model to\neffectively leverage developer-provided directives while preserving its core\ncompletion abilities when no instructions are present. To facilitate this, we\nconstructed a large-scale dataset by using GPT-4o to generate concise,\nintent-focused instructions for code infilling examples. We evaluated IFIM by\napplying it to two popular base models, Deepseek-Coder and Qwen2.5-Coder, on\nthe benchmarks derived from HumanEval-infilling and RepoMasterEval. The results\ndemonstrate that IFIM significantly improves instruction-following\ncapabilities, boosting the Pass@1 score from 84.6% to 93.6% on\nHumanEval-infilling. Moreover, this enhancement does not compromise the models'\noriginal performance on FIM code completion tasks with no instructions\nprovided."}
{"id": "2509.23594", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23594", "abs": "https://arxiv.org/abs/2509.23594", "authors": ["Yixu Wang", "Yan Teng", "Yingchun Wang", "Xingjun Ma"], "title": "StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data", "comment": "ICCV 2025", "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have transformed\nvision model adaptation, enabling the rapid deployment of customized models.\nHowever, the compactness of LoRA adaptations introduces new safety concerns,\nparticularly their vulnerability to model extraction attacks. This paper\nintroduces a new focus of model extraction attacks named LoRA extraction that\nextracts LoRA-adaptive models based on a public pre-trained model. We then\npropose a novel extraction method called StolenLoRA which trains a substitute\nmodel to extract the functionality of a LoRA-adapted model using synthetic\ndata. StolenLoRA leverages a Large Language Model to craft effective prompts\nfor data generation, and it incorporates a Disagreement-based Semi-supervised\nLearning (DSL) strategy to maximize information gain from limited queries. Our\nexperiments demonstrate the effectiveness of StolenLoRA, achieving up to a\n96.60% attack success rate with only 10k queries, even in cross-backbone\nscenarios where the attacker and victim models utilize different pre-trained\nbackbones. These findings reveal the specific vulnerability of LoRA-adapted\nmodels to this type of extraction and underscore the urgent need for robust\ndefense mechanisms tailored to PEFT methods. We also explore a preliminary\ndefense strategy based on diversified LoRA deployments, highlighting its\npotential to mitigate such attacks."}
{"id": "2509.24694", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24694", "abs": "https://arxiv.org/abs/2509.24694", "authors": ["Gangda Xiong", "Tao Chen"], "title": "CoTune: Co-evolutionary Configuration Tuning", "comment": "Accepted by ASE 2025", "summary": "To automatically tune configurations for the best possible system performance\n(e.g., runtime or throughput), much work has been focused on designing\nintelligent heuristics in a tuner. However, existing tuner designs have mostly\nignored the presence of complex performance requirements (e.g., the latency\nshall ideally be 2 seconds), but simply assume that better performance is\nalways more preferred. This would not only waste valuable information in a\nrequirement but might also consume extensive resources to tune for a goal with\nlittle gain. Yet, prior studies have shown that simply incorporating the\nrequirement as a tuning objective is problematic since the requirement might be\ntoo strict, harming convergence; or its highly diverse satisfactions might lead\nto premature convergence. In this paper, we propose CoTune, a tool that takes\nthe information of a given target performance requirement into account through\nco-evolution. CoTune is unique in the sense that it creates an auxiliary\nperformance requirement to be co-evolved with the configurations, which assists\nthe target performance requirement when it becomes ineffective or even\nmisleading, hence allowing the tuning to be guided by the requirement while\nbeing robust to its harm. Experiment results on 162 cases (nine systems and 18\nrequirements) reveal that CoTune considerably outperforms existing tuners,\nranking as the best for 90% cases (against the 0%--35% for other tuners) with\nup to 2.9x overall improvements, while doing so under a much better efficiency."}
{"id": "2509.23621", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.23621", "abs": "https://arxiv.org/abs/2509.23621", "authors": ["Sherif Saad", "Kevin Shi", "Mohammed Mamun", "Hythem Elmiligi"], "title": "AutoML in Cybersecurity: An Empirical Study", "comment": null, "summary": "Automated machine learning (AutoML) has emerged as a promising paradigm for\nautomating machine learning (ML) pipeline design, broadening AI adoption. Yet\nits reliability in complex domains such as cybersecurity remains underexplored.\nThis paper systematically evaluates eight open-source AutoML frameworks across\n11 publicly available cybersecurity datasets, spanning intrusion detection,\nmalware classification, phishing, fraud detection, and spam filtering. Results\nshow substantial performance variability across tools and datasets, with no\nsingle solution consistently superior. A paradigm shift is observed: the\nchallenge has moved from selecting individual ML models to identifying the most\nsuitable AutoML framework, complicated by differences in runtime efficiency,\nautomation capabilities, and supported features. AutoML tools frequently favor\ntree-based models, which perform well but risk overfitting and limit\ninterpretability. Key challenges identified include adversarial vulnerability,\nmodel drift, and inadequate feature engineering. We conclude with best\npractices and research directions to strengthen robustness, interpretability,\nand trust in AutoML for high-stakes cybersecurity applications."}
{"id": "2509.24782", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24782", "abs": "https://arxiv.org/abs/2509.24782", "authors": ["Muhammad Laiq"], "title": "Large language models for behavioral modeling: A literature survey", "comment": "9 pages", "summary": "In recent years, large language models (LLMs) have been extensively utilized\nfor behavioral modeling, for example, to automatically generate sequence\ndiagrams. However, no overview of this work has been published yet. Such an\noverview will help identify future research directions and inform practitioners\nand educators about the effectiveness of LLMs in assisting behavioral modeling.\nThis study aims to provide an overview of the existing research on the use of\nLLMs for behavioral modeling, particularly focusing on use case and sequence\ndiagrams. Through a term-based search, we filtered and identified 14 relevant\nprimary studies. Our analysis of the selected primary studies reveals that LLMs\nhave demonstrated promising results in automatically generating use case and\nsequence diagrams. In addition, we found that most of the current literature\nlacks expert-based evaluations and has mainly used GPT-based models. Therefore,\nfuture work should evaluate a broader range of LLMs for behavioral modeling and\ninvolve domain experts to evaluate the output of LLMs."}
{"id": "2509.23680", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23680", "abs": "https://arxiv.org/abs/2509.23680", "authors": ["Shidong Pan", "Yikai Ge", "Xiaoyu Sun"], "title": "A First Look at Privacy Risks of Android Task-executable Voice Assistant Applications", "comment": "Accepted by APSEC 2025", "summary": "With the development of foundation AI technologies, task-executable voice\nassistants (VAs) have become more popular, enhancing user convenience and\nexpanding device functionality. Android task-executable VAs are applications\nthat are capable of understanding complex tasks and performing corresponding\noperations. Given their prevalence and great autonomy, there is no existing\nwork examine the privacy risks within the voice assistants from the\ntask-execution pattern in a holistic manner. To fill this research gap, this\npaper presents a user-centric comprehensive empirical study on privacy risks in\nAndroid task-executable VA applications. We collect ten mainstream VAs as our\nresearch target and analyze their operational characteristics. We then\ncross-check their privacy declarations across six sources, including privacy\nlabels, policies, and manifest files, and our findings reveal widespread\ninconsistencies. Moreover, we uncover three significant privacy threat models:\n(1) privacy misdisclosure in mega apps, where integrated mini apps such as\nAlexa skills are inadequately represented; (2) privilege escalation via\ninter-application interactions, which exploit Android's communication\nmechanisms to bypass user consent; and (3) abuse of Google system applications,\nenabling apps to evade the declaration of dangerous permissions. Our study\ncontributes actionable recommendations for practitioners and underscores\nbroader relevance of these privacy risks to emerging autonomous AI agents."}
{"id": "2509.24828", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24828", "abs": "https://arxiv.org/abs/2509.24828", "authors": ["Joshua Heisler", "Johannes Reisinger", "Andreas Fischer"], "title": "Evaluating SAP Joule for Code Generation", "comment": null, "summary": "SAP has released its own proprietary generative model SAP Joule, intended for\nvarious generative tasks, including serving as a code assistant for software\nengineers. While Joule is yet not focused on SAP-specific ABAP code generation,\nit can be used for other common languages, including Javascript. This paper\ncompares SAP Joules Javascript coding capabilities against a total of 29 other\nmodels using the HumanEval-X Javascript benchmark. SAP Joule achieves a strict\naccuracy of 80.49% as the fifth best model in our evaluation. To the best of\nour knowledge, this is the first comparative evaluation of SAP Joule code\ngeneration capabilities."}
{"id": "2509.23834", "categories": ["cs.CR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.23834", "abs": "https://arxiv.org/abs/2509.23834", "authors": ["Haochen Sun", "Xi He"], "title": "GPM: The Gaussian Pancake Mechanism for Planting Undetectable Backdoors in Differential Privacy", "comment": "16 pages, 7 figures. Not published yet. Code and raw experimental\n  logs will be available after publication, or upon email request", "summary": "Differential privacy (DP) has become the gold standard for preserving\nindividual privacy in data analysis. However, an implicit yet fundamental\nassumption underlying these rigorous privacy guarantees is the correct\nimplementation and execution of DP mechanisms. Several incidents of unintended\nprivacy loss have occurred due to numerical issues and inappropriate\nconfigurations of DP software, which have been successfully exploited in\nprivacy attacks. To better understand the seriousness of defective DP software,\nwe ask the following question: is it possible to elevate these passive defects\ninto active privacy attacks while maintaining covertness?\n  To address this question, we present the Gaussian pancake mechanism (GPM), a\nnovel mechanism that is computationally indistinguishable from the widely used\nGaussian mechanism (GM), yet exhibits arbitrarily weaker statistical DP\nguarantees. This unprecedented separation enables a new class of backdoor\nattacks: by indistinguishably passing off as the authentic GM, GPM can covertly\ndegrade statistical privacy. Unlike the unintentional privacy loss caused by\nGM's numerical issues, GPM is an adversarial yet undetectable backdoor attack\nagainst data privacy. We formally prove GPM's covertness, characterize its\nstatistical leakage, and demonstrate a concrete distinguishing attack that can\nachieve near-perfect success rates under suitable parameter choices, both\ntheoretically and empirically.\n  Our results underscore the importance of using transparent, open-source DP\nlibraries and highlight the need for rigorous scrutiny and formal verification\nof DP implementations to prevent subtle, undetectable privacy compromises in\nreal-world systems."}
{"id": "2509.24975", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24975", "abs": "https://arxiv.org/abs/2509.24975", "authors": ["Lekang Yang", "Yuetong Liu", "Yitong Zhang", "Jia Li"], "title": "DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern", "comment": null, "summary": "Software development relies heavily on extensive unit testing, which makes\nthe efficiency of automated Unit Test Generation (UTG) particularly important.\nHowever, most existing LLMs generate test cases one token at a time in each\nforward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs)\nhave emerged, offering promising parallel generation capabilities and showing\nstrong potential for efficient UTG. Despite this advantage, their application\nto UTG is still constrained by a clear trade-off between efficiency and test\nquality, since increasing the number of tokens generated in each step often\ncauses a sharp decline in the quality of test cases. To overcome this\nlimitation, we present DiffTester, an acceleration framework specifically\ntailored for dLLMs in UTG. The key idea of DiffTester is that unit tests\ntargeting the same focal method often share repetitive structural patterns. By\ndynamically identifying these common patterns through abstract syntax tree\nanalysis during generation, DiffTester adaptively increases the number of\ntokens produced at each step without compromising the quality of the output. To\nenable comprehensive evaluation, we extend the original TestEval benchmark,\nwhich was limited to Python, by introducing additional programming languages\nincluding Java and C++. Extensive experiments on three benchmarks with two\nrepresentative models show that DiffTester delivers significant acceleration\nwhile preserving test coverage. Moreover, DiffTester generalizes well across\ndifferent dLLMs and programming languages, providing a practical and scalable\nsolution for efficient UTG in software development. Code and data are publicly\navailable at https://github.com/wellbeingyang/DLM4UTG-open ."}
{"id": "2509.23871", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23871", "abs": "https://arxiv.org/abs/2509.23871", "authors": ["Yukun Chen", "Boheng Li", "Yu Yuan", "Leyi Qi", "Yiming Li", "Tianwei Zhang", "Zhan Qin", "Kui Ren"], "title": "Taught Well Learned Ill: Towards Distillation-conditional Backdoor Attack", "comment": "The first three authors contributed equally to this work. To appear\n  in NeurIPS 2025. 35 pages", "summary": "Knowledge distillation (KD) is a vital technique for deploying deep neural\nnetworks (DNNs) on resource-constrained devices by transferring knowledge from\nlarge teacher models to lightweight student models. While teacher models from\nthird-party platforms may undergo security verification (\\eg, backdoor\ndetection), we uncover a novel and critical threat: distillation-conditional\nbackdoor attacks (DCBAs). DCBA injects dormant and undetectable backdoors into\nteacher models, which become activated in student models via the KD process,\neven with clean distillation datasets. While the direct extension of existing\nmethods is ineffective for DCBA, we implement this attack by formulating it as\na bilevel optimization problem and proposing a simple yet effective method\n(\\ie, SCAR). Specifically, the inner optimization simulates the KD process by\noptimizing a surrogate student model, while the outer optimization leverages\noutputs from this surrogate to optimize the teacher model for implanting the\nconditional backdoor. Our SCAR addresses this complex optimization utilizing an\nimplicit differentiation algorithm with a pre-optimized trigger injection\nfunction. Extensive experiments across diverse datasets, model architectures,\nand KD techniques validate the effectiveness of our SCAR and its resistance\nagainst existing backdoor detection, highlighting a significant yet previously\noverlooked vulnerability in the KD process. Our code is available at\nhttps://github.com/WhitolfChen/SCAR."}
{"id": "2509.25043", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25043", "abs": "https://arxiv.org/abs/2509.25043", "authors": ["Cristian Augusto", "Antonia Bertolino", "Guglielmo De Angelis", "Francesca Lonetti", "Jesús Morán"], "title": "Large Language Models for Software Testing: A Research Roadmap", "comment": "40 pages & 10 figures Submitted on 29th September 2025", "summary": "Large Language Models (LLMs) are starting to be profiled as one of the most\nsignificant disruptions in the Software Testing field.\n  Specifically, they have been successfully applied in software testing tasks\nsuch as generating test code, or summarizing documentation.\n  This potential has attracted hundreds of researchers, resulting in dozens of\nnew contributions every month, hardening researchers to\n  stay at the forefront of the wave. Still, to the best of our knowledge, no\nprior work has provided a structured vision of the progress\n  and most relevant research trends in LLM-based testing. In this article, we\naim to provide a roadmap that illustrates its current state,\n  grouping the contributions into different categories, and also sketching the\nmost promising and active research directions for the field.\n  To achieve this objective, we have conducted a semi-systematic literature\nreview, collecting articles and mapping them into the most\n  prominent categories, reviewing the current and ongoing status, and analyzing\nthe open challenges of LLM-based software testing.\n  Lastly, we have outlined several expected long-term impacts of LLMs over the\nwhole software testing field."}
{"id": "2509.23970", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.23970", "abs": "https://arxiv.org/abs/2509.23970", "authors": ["Meet Udeshi", "Venkata Sai Charan Putrevu", "Prashanth Krishnamurthy", "Prashant Anantharaman", "Sean Carrick", "Ramesh Karri", "Farshad Khorrami"], "title": "Binary Diff Summarization using Large Language Models", "comment": null, "summary": "Security of software supply chains is necessary to ensure that software\nupdates do not contain maliciously injected code or introduce vulnerabilities\nthat may compromise the integrity of critical infrastructure. Verifying the\nintegrity of software updates involves binary differential analysis (binary\ndiffing) to highlight the changes between two binary versions by incorporating\nbinary analysis and reverse engineering. Large language models (LLMs) have been\napplied to binary analysis to augment traditional tools by producing natural\nlanguage summaries that cybersecurity experts can grasp for further analysis.\nCombining LLM-based binary code summarization with binary diffing can improve\nthe LLM's focus on critical changes and enable complex tasks such as automated\nmalware detection. To address this, we propose a novel framework for binary\ndiff summarization using LLMs. We introduce a novel functional sensitivity\nscore (FSS) that helps with automated triage of sensitive binary functions for\ndownstream detection tasks. We create a software supply chain security\nbenchmark by injecting 3 different malware into 6 open-source projects which\ngenerates 104 binary versions, 392 binary diffs, and 46,023 functions. On this,\nour framework achieves a precision of 0.98 and recall of 0.64 for malware\ndetection, displaying high accuracy with low false positives. Across malicious\nand benign functions, we achieve FSS separation of 3.0 points, confirming that\nFSS categorization can classify sensitive functions. We conduct a case study on\nthe real-world XZ utils supply chain attack; our framework correctly detects\nthe injected backdoor functions with high FSS."}
{"id": "2509.25117", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.25117", "abs": "https://arxiv.org/abs/2509.25117", "authors": ["Sogol Masoumzadeh", "Keheliya Gallaba", "Dayi Lin", "Ahmed E. Hassan"], "title": "Towards Reliable Generation of Executable Workflows by Foundation Models", "comment": null, "summary": "Recent advancements in Foundation Models (FMs) have demonstrated significant\nprogress in comprehending complex natural language to perform intricate tasks.\nSuccessfully executing these tasks often requires orchestrating calls to FMs\nalongside other software components. However, manually decomposing a task into\na coherent sequence of smaller, logically aggregated steps, commonly referred\nto as workflows, demands considerable effort and specialized domain knowledge.\nWhile FMs can assist in generating such workflows specified in domain-specific\nlanguages (DSLs), achieving accuracy and reliability in this process remains a\nchallenge.\n  This work introduces a framework that leverages static analysis feedback to\nenable FMs to detect and repair defects in the DSL-based workflows they\ngenerate. We begin by presenting the first-ever taxonomy of incidences of\ndefects in FM-generated DSL workflows, categorizing them into 18 distinct\ntypes. Furthermore, we observe a high prevalence of defects across FM-generated\nDSL workflows, with 87.27% of the studied instances containing at least one\ndefect. This, in turn, emphasizes the magnitude of the problem in practice and\nunderscores the necessity for implementing mitigation strategies. Following\nthis, we demonstrate that nine types of these defects can be effectively\nidentified through static analysis of the workflows. For this purpose, we\ndevelop Timon, the first-of-its-kind static analyzer specifically designed for\nFM-generated DSL workflows. Finally, we show that by incorporating feedback\nfrom Timon, we can guide Pumbaa, an FM-based tool, to repair the detected\ndefect incidences. By systematically detecting and repairing defects, our work\nprovides a crucial step towards the reliable and automated generation of\nexecutable workflows from natural language requirements."}
{"id": "2509.23984", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.23984", "abs": "https://arxiv.org/abs/2509.23984", "authors": ["Pranav Garimidi", "Joachim Neu", "Max Resnick"], "title": "Multiple Concurrent Proposers: Why and How", "comment": null, "summary": "Traditional single-proposer blockchains suffer from miner extractable value\n(MEV), where validators exploit their serial monopoly on transaction inclusion\nand ordering to extract rents from users. While there have been many\ndevelopments at the application layer to reduce the impact of MEV, these\napproaches largely require auctions as a subcomponent. Running auctions\nefficiently on chain requires two key properties of the underlying consensus\nprotocol: selective-censorship resistance and hiding. These properties\nguarantee that an adversary can neither selectively delay transactions nor see\ntheir contents before they are confirmed. We propose a multiple concurrent\nproposer (MCP) protocol offering exactly these properties."}
{"id": "2509.22900", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22900", "abs": "https://arxiv.org/abs/2509.22900", "authors": ["Haochen Gong", "Zhen Tao", "Shidong Pan", "Zhenchang Xing", "Xiaoyu Sun"], "title": "Towards Context-aware Mobile Privacy Notice: Implementation of A Deployable Contextual Privacy Policies Generator", "comment": "Accepted by ASE 2025, Tool Demonstration Track", "summary": "Lengthy and legally phrased privacy policies impede users' understanding of\nhow mobile applications collect and process personal data. Prior work proposed\nContextual Privacy Policies (CPPs) for mobile apps to display shorter policy\nsnippets only in the corresponding user interface contexts, but the pipeline\ncould not be deployable in real-world mobile environments. In this paper, we\npresent PrivScan, the first deployable CPP Software Development Kit (SDK) for\nAndroid. It captures live app screenshots to identify GUI elements associated\nwith types of personal data and displays CPPs in a concise, user-facing format.\nWe provide a lightweight floating button that offers low-friction, on-demand\ncontrol. The architecture leverages remote deployment to decouple the\nmultimodal backend pipeline from a mobile client comprising five modular\ncomponents, thereby reducing on-device resource demands and easing\ncross-platform portability. A feasibility-oriented evaluation shows an average\nexecution time of 9.15\\,s, demonstrating the practicality of our approach. The\nsource code of PrivScan is available at https://github.com/buyanghc/PrivScan\nand the demo video can be found at https://www.youtube.com/watch?v=ck-25otfyHc."}
{"id": "2509.24037", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24037", "abs": "https://arxiv.org/abs/2509.24037", "authors": ["Alireza Lotfi", "Charalampos Katsis", "Elisa Bertino"], "title": "Automated Vulnerability Validation and Verification: A Large Language Model Approach", "comment": null, "summary": "Software vulnerabilities remain a critical security challenge, providing\nentry points for attackers into enterprise networks. Despite advances in\nsecurity practices, the lack of high-quality datasets capturing diverse exploit\nbehavior limits effective vulnerability assessment and mitigation. This paper\nintroduces an end-to-end multi-step pipeline leveraging generative AI,\nspecifically large language models (LLMs), to address the challenges of\norchestrating and reproducing attacks to known software vulnerabilities. Our\napproach extracts information from CVE disclosures in the National\nVulnerability Database, augments it with external public knowledge (e.g.,\nthreat advisories, code snippets) using Retrieval-Augmented Generation (RAG),\nand automates the creation of containerized environments and exploit code for\neach vulnerability. The pipeline iteratively refines generated artifacts,\nvalidates attack success with test cases, and supports complex multi-container\nsetups. Our methodology overcomes key obstacles, including noisy and incomplete\nvulnerability descriptions, by integrating LLMs and RAG to fill information\ngaps. We demonstrate the effectiveness of our pipeline across different\nvulnerability types, such as memory overflows, denial of service, and remote\ncode execution, spanning diverse programming languages, libraries and years. In\ndoing so, we uncover significant inconsistencies in CVE descriptions,\nemphasizing the need for more rigorous verification in the CVE disclosure\nprocess. Our approach is model-agnostic, working across multiple LLMs, and we\nopen-source the artifacts to enable reproducibility and accelerate security\nresearch. To the best of our knowledge, this is the first system to\nsystematically orchestrate and exploit known vulnerabilities in containerized\nenvironments by combining general-purpose LLM reasoning with CVE data and\nRAG-based context enrichment."}
{"id": "2509.23680", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23680", "abs": "https://arxiv.org/abs/2509.23680", "authors": ["Shidong Pan", "Yikai Ge", "Xiaoyu Sun"], "title": "A First Look at Privacy Risks of Android Task-executable Voice Assistant Applications", "comment": "Accepted by APSEC 2025", "summary": "With the development of foundation AI technologies, task-executable voice\nassistants (VAs) have become more popular, enhancing user convenience and\nexpanding device functionality. Android task-executable VAs are applications\nthat are capable of understanding complex tasks and performing corresponding\noperations. Given their prevalence and great autonomy, there is no existing\nwork examine the privacy risks within the voice assistants from the\ntask-execution pattern in a holistic manner. To fill this research gap, this\npaper presents a user-centric comprehensive empirical study on privacy risks in\nAndroid task-executable VA applications. We collect ten mainstream VAs as our\nresearch target and analyze their operational characteristics. We then\ncross-check their privacy declarations across six sources, including privacy\nlabels, policies, and manifest files, and our findings reveal widespread\ninconsistencies. Moreover, we uncover three significant privacy threat models:\n(1) privacy misdisclosure in mega apps, where integrated mini apps such as\nAlexa skills are inadequately represented; (2) privilege escalation via\ninter-application interactions, which exploit Android's communication\nmechanisms to bypass user consent; and (3) abuse of Google system applications,\nenabling apps to evade the declaration of dangerous permissions. Our study\ncontributes actionable recommendations for practitioners and underscores\nbroader relevance of these privacy risks to emerging autonomous AI agents."}
{"id": "2509.24043", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24043", "abs": "https://arxiv.org/abs/2509.24043", "authors": ["Yihan Wu", "Ruibo Chen", "Georgios Milis", "Heng Huang"], "title": "An Ensemble Framework for Unbiased Language Model Watermarking", "comment": null, "summary": "As large language models become increasingly capable and widely deployed,\nverifying the provenance of machine-generated content is critical to ensuring\ntrust, safety, and accountability. Watermarking techniques have emerged as a\npromising solution by embedding imperceptible statistical signals into the\ngeneration process. Among them, unbiased watermarking is particularly\nattractive due to its theoretical guarantee of preserving the language model's\noutput distribution, thereby avoiding degradation in fluency or detectability\nthrough distributional shifts. However, existing unbiased watermarking schemes\noften suffer from weak detection power and limited robustness, especially under\nshort text lengths or distributional perturbations. In this work, we propose\nENS, a novel ensemble framework that enhances the detectability and robustness\nof logits-based unbiased watermarks while strictly preserving their\nunbiasedness. ENS sequentially composes multiple independent watermark\ninstances, each governed by a distinct key, to amplify the watermark signal. We\ntheoretically prove that the ensemble construction remains unbiased in\nexpectation and demonstrate how it improves the signal-to-noise ratio for\nstatistical detectors. Empirical evaluations on multiple LLM families show that\nENS substantially reduces the number of tokens needed for reliable detection\nand increases resistance to smoothing and paraphrasing attacks without\ncompromising generation quality."}
{"id": "2509.24272", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24272", "abs": "https://arxiv.org/abs/2509.24272", "authors": ["Weibo Zhao", "Jiahao Liu", "Bonan Ruan", "Shaofei Li", "Zhenkai Liang"], "title": "When MCP Servers Attack: Taxonomy, Feasibility, and Mitigation", "comment": null, "summary": "Model Context Protocol (MCP) servers enable AI applications to connect to\nexternal systems in a plug-and-play manner, but their rapid proliferation also\nintroduces severe security risks. Unlike mature software ecosystems with\nrigorous vetting, MCP servers still lack standardized review mechanisms, giving\nadversaries opportunities to distribute malicious implementations. Despite this\npressing risk, the security implications of MCP servers remain underexplored.\nTo address this gap, we present the first systematic study that treats MCP\nservers as active threat actors and decomposes them into core components to\nexamine how adversarial developers can implant malicious intent. Specifically,\nwe investigate three research questions: (i) what types of attacks malicious\nMCP servers can launch, (ii) how vulnerable MCP hosts and Large Language Models\n(LLMs) are to these attacks, and (iii) how feasible it is to carry out MCP\nserver attacks in practice. Our study proposes a component-based taxonomy\ncomprising twelve attack categories. For each category, we develop\nProof-of-Concept (PoC) servers and demonstrate their effectiveness across\ndiverse real-world host-LLM settings. We further show that attackers can\ngenerate large numbers of malicious servers at virtually no cost. We then test\nstate-of-the-art scanners on the generated servers and found that existing\ndetection approaches are insufficient. These findings highlight that malicious\nMCP servers are easy to implement, difficult to detect with current tools, and\ncapable of causing concrete damage to AI agent systems. Addressing this threat\nrequires coordinated efforts among protocol designers, host developers, LLM\nproviders, and end users to build a more secure and resilient MCP ecosystem."}
{"id": "2509.24048", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24048", "abs": "https://arxiv.org/abs/2509.24048", "authors": ["Yihan Wu", "Xuehao Cui", "Ruibo Chen", "Heng Huang"], "title": "Analyzing and Evaluating Unbiased Language Model Watermark", "comment": null, "summary": "Verifying the authenticity of AI-generated text has become increasingly\nimportant with the rapid advancement of large language models, and unbiased\nwatermarking has emerged as a promising approach due to its ability to preserve\noutput distribution without degrading quality. However, recent work reveals\nthat unbiased watermarks can accumulate distributional bias over multiple\ngenerations and that existing robustness evaluations are inconsistent across\nstudies. To address these issues, we introduce UWbench, the first open-source\nbenchmark dedicated to the principled evaluation of unbiased watermarking\nmethods. Our framework combines theoretical and empirical contributions: we\npropose a statistical metric to quantify multi-batch distribution drift, prove\nan impossibility result showing that no unbiased watermark can perfectly\npreserve the distribution under infinite queries, and develop a formal analysis\nof robustness against token-level modification attacks. Complementing this\ntheory, we establish a three-axis evaluation protocol: unbiasedness,\ndetectability, and robustness, and show that token modification attacks provide\nmore stable robustness assessments than paraphrasing-based methods. Together,\nUWbench offers the community a standardized and reproducible platform for\nadvancing the design and evaluation of unbiased watermarking algorithms."}
{"id": "2509.24153", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24153", "abs": "https://arxiv.org/abs/2509.24153", "authors": ["Philip Sjösvärd", "Hongyu Jin", "Panos Papadimitratos"], "title": "DNS in the Time of Curiosity: A Tale of Collaborative User Privacy Protection", "comment": "Twenty-ninth International Workshop on Security Protocols", "summary": "The Domain Name System (DNS) is central to all Internet user activity,\nresolving accessed domain names into Internet Protocol (IP) addresses. As a\nresult, curious DNS resolvers can learn everything about Internet users'\ninterests. Public DNS resolvers are rising in popularity, offering low-latency\nresolution, high reliability, privacy-preserving policies, and support for\nencrypted DNS queries. However, client-resolver traffic encryption,\nincreasingly deployed to protect users from eavesdroppers, does not protect\nusers against curious resolvers. Similarly, privacy-preserving policies are\nbased solely on written commitments and do not provide technical safeguards.\nAlthough DNS query relay schemes can separate duties to limit data accessible\nby each entity, they cannot prevent colluding entities from sharing user\ntraffic logs. Thus, a key challenge remains: organizations operating public DNS\nresolvers, accounting for the majority of DNS resolutions, can potentially\ncollect and analyze massive volumes of Internet user activity data. With DNS\ninfrastructure that cannot be fully trusted, can we safeguard user privacy? We\nanswer positively and advocate for a user-driven approach to reduce exposure to\nDNS services. We will discuss key ideas of the proposal, which aims to achieve\na high level of privacy without sacrificing performance: maintaining low\nlatency, network bandwidth, memory/storage overhead, and computational\noverhead."}
{"id": "2509.24173", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.24173", "abs": "https://arxiv.org/abs/2509.24173", "authors": ["Sun-Moon Yoon", "Hyun-Young Park", "Seung-Hyun Nam", "Si-Hyeon Lee"], "title": "Fundamental Limit of Discrete Distribution Estimation under Utility-Optimized Local Differential Privacy", "comment": "20 pages, 7 figures, 1 table. This work has been submitted to the\n  IEEE for possible publication", "summary": "We study the problem of discrete distribution estimation under\nutility-optimized local differential privacy (ULDP), which enforces local\ndifferential privacy (LDP) on sensitive data while allowing more accurate\ninference on non-sensitive data. In this setting, we completely characterize\nthe fundamental privacy-utility trade-off. The converse proof builds on several\nkey ideas, including a generalized uniform asymptotic Cram\\'er-Rao lower bound,\na reduction showing that it suffices to consider a newly defined class of\nextremal ULDP mechanisms, and a novel distribution decomposition technique\ntailored to ULDP constraints. For the achievability, we propose a class of\nutility-optimized block design (uBD) schemes, obtained as nontrivial\nmodifications of the block design mechanism known to be optimal under standard\nLDP constraints, while incorporating the distribution decomposition idea used\nin the converse proof and a score-based linear estimator. These results provide\na tight characterization of the estimation accuracy achievable under ULDP and\nreveal new insights into the structure of optimal mechanisms for\nprivacy-preserving statistical inference."}
{"id": "2509.24174", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24174", "abs": "https://arxiv.org/abs/2509.24174", "authors": ["Philip Sjösvärd", "Hongyu Jin", "Panos Papadimitratos"], "title": "LLUAD: Low-Latency User-Anonymized DNS", "comment": "24th Workshop on Privacy in the Electronic Society", "summary": "The Domain Name System (DNS) is involved in practically all web activity,\ntranslating easy-to-remember domain names into Internet Protocol (IP)\naddresses. Due to its central role on the Internet, DNS exposes user web\nactivity in detail. The privacy challenge is honest-but-curious DNS\nservers/resolvers providing the translation/lookup service. In particular, with\nthe majority of DNS queries handled by public DNS resolvers, the organizations\nrunning them can track, collect, and analyze massive user activity data.\nExisting solutions that encrypt DNS traffic between clients and resolvers are\ninsufficient, as the resolver itself is the privacy threat. While DNS query\nrelays separate duties among multiple entities, to limit the data accessible by\neach entity, they cannot prevent colluding entities from sharing user traffic\nlogs. To achieve near-zero-trust DNS privacy compatible with the existing DNS\ninfrastructure, we propose LLUAD: it locally stores a Popularity List, the most\npopular DNS records, on user devices, formed in a privacy-preserving manner\nbased on user interests. In this way, LLUAD can both improve privacy and reduce\naccess times to web content. The Popularity List is proactively retrieved from\na (curious) public server that continually updates and refreshes the records\nbased on user popularity votes, while efficiently broadcasting record\nupdates/changes to adhere to aggressive load-balancing schemes (i.e., name\nservers actively load-balancing user connections by changing record IP\naddresses). User votes are anonymized using a novel, efficient, and highly\nscalable client-driven Voting Mix Network - with packet lengths independent of\nthe number of hops, centrally enforced limit on number of votes cast per user,\nand robustness against poor client participation - to ensure a geographically\nrelevant and correctly/securely instantiated Popularity List."}
{"id": "2509.24240", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24240", "abs": "https://arxiv.org/abs/2509.24240", "authors": ["Eunkyu Lee", "Donghyeon Kim", "Wonyoung Kim", "Insu Yun"], "title": "Takedown: How It's Done in Modern Coding Agent Exploits", "comment": null, "summary": "Coding agents, which are LLM-driven agents specialized in software\ndevelopment, have become increasingly prevalent in modern programming\nenvironments. Unlike traditional AI coding assistants, which offer simple code\ncompletion and suggestions, modern coding agents tackle more complex tasks with\ngreater autonomy, such as generating entire programs from natural language\ninstructions. To enable such capabilities, modern coding agents incorporate\nextensive functionalities, which in turn raise significant concerns over their\nsecurity and privacy. Despite their growing adoption, systematic and in-depth\nsecurity analysis of these agents has largely been overlooked.\n  In this paper, we present a comprehensive security analysis of eight\nreal-world coding agents. Our analysis addresses the limitations of prior\napproaches, which were often fragmented and ad hoc, by systematically examining\nthe internal workflows of coding agents and identifying security threats across\ntheir components. Through the analysis, we identify 15 security issues,\nincluding previously overlooked or missed issues, that can be abused to\ncompromise the confidentiality and integrity of user systems. Furthermore, we\nshow that these security issues are not merely individual vulnerabilities, but\ncan collectively lead to end-to-end exploitations. By leveraging these security\nissues, we successfully achieved arbitrary command execution in five agents and\nglobal data exfiltration in four agents, all without any user interaction or\napproval. Our findings highlight the need for a comprehensive security analysis\nin modern LLM-driven agents and demonstrate how insufficient security\nconsiderations can lead to severe vulnerabilities."}
{"id": "2509.24257", "categories": ["cs.CR", "cs.LG", "C.2.1"], "pdf": "https://arxiv.org/pdf/2509.24257", "abs": "https://arxiv.org/abs/2509.24257", "authors": ["Ke Wang", "Felix Qu", "Libin Xia", "Zishuo Zhao", "Chris Tong", "Lynn Ai", "Eric Yang"], "title": "VeriLLM: A Lightweight Framework for Publicly Verifiable Decentralized Inference", "comment": "13 pages, 4 figures, 2 tables", "summary": "Decentralized inference is an appealing paradigm for serving large language\nmodels (LLMs), offering strong security, high efficiency, and lower operating\ncosts. Yet the permissionless setting admits no a priori trust in participating\nnodes, making output verifiability a prerequisite for secure deployment. We\npresent VeriLLM, a publicly verifiable protocol for decentralized LLM inference\nthat (i) achieves security under a one-honest-verifier assumption, (ii) attains\nnear-negligible verification cost (about 1% of the underlying inference) via a\nlightweight verification algorithm designed explicitly for LLMs, and (iii)\nenforces honest checking through a peer-prediction mechanism that mitigates\nlazy verification in naive voting. We further introduce an isomorphic\ninference-verification network that multiplexes both roles on the same set of\nGPU workers. This architecture (i) increases GPU utilization and thereby\nimproves end-to-end throughput for both inference and verification, (ii)\nexpands the effective pool of available validators, strengthening robustness\nand security, and (iii) enforces task indistinguishability at the worker\nboundary to prevent job-type-conditioned behavior. Finally, we provide a formal\ngame-theoretic analysis and prove that, under our incentives, honest inference\nand verification constitute a Nash equilibrium, ensuring incentive\ncompatibility against rational adversaries. To our knowledge, this is the first\ndecentralized inference verification protocol with an end-to-end game-theoretic\nsecurity proof."}
{"id": "2509.24272", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24272", "abs": "https://arxiv.org/abs/2509.24272", "authors": ["Weibo Zhao", "Jiahao Liu", "Bonan Ruan", "Shaofei Li", "Zhenkai Liang"], "title": "When MCP Servers Attack: Taxonomy, Feasibility, and Mitigation", "comment": null, "summary": "Model Context Protocol (MCP) servers enable AI applications to connect to\nexternal systems in a plug-and-play manner, but their rapid proliferation also\nintroduces severe security risks. Unlike mature software ecosystems with\nrigorous vetting, MCP servers still lack standardized review mechanisms, giving\nadversaries opportunities to distribute malicious implementations. Despite this\npressing risk, the security implications of MCP servers remain underexplored.\nTo address this gap, we present the first systematic study that treats MCP\nservers as active threat actors and decomposes them into core components to\nexamine how adversarial developers can implant malicious intent. Specifically,\nwe investigate three research questions: (i) what types of attacks malicious\nMCP servers can launch, (ii) how vulnerable MCP hosts and Large Language Models\n(LLMs) are to these attacks, and (iii) how feasible it is to carry out MCP\nserver attacks in practice. Our study proposes a component-based taxonomy\ncomprising twelve attack categories. For each category, we develop\nProof-of-Concept (PoC) servers and demonstrate their effectiveness across\ndiverse real-world host-LLM settings. We further show that attackers can\ngenerate large numbers of malicious servers at virtually no cost. We then test\nstate-of-the-art scanners on the generated servers and found that existing\ndetection approaches are insufficient. These findings highlight that malicious\nMCP servers are easy to implement, difficult to detect with current tools, and\ncapable of causing concrete damage to AI agent systems. Addressing this threat\nrequires coordinated efforts among protocol designers, host developers, LLM\nproviders, and end users to build a more secure and resilient MCP ecosystem."}
{"id": "2509.24408", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24408", "abs": "https://arxiv.org/abs/2509.24408", "authors": ["Yuzhen Long", "Songze Li"], "title": "FuncPoison: Poisoning Function Library to Hijack Multi-agent Autonomous Driving Systems", "comment": null, "summary": "Autonomous driving systems increasingly rely on multi-agent architectures\npowered by large language models (LLMs), where specialized agents collaborate\nto perceive, reason, and plan. A key component of these systems is the shared\nfunction library, a collection of software tools that agents use to process\nsensor data and navigate complex driving environments. Despite its critical\nrole in agent decision-making, the function library remains an under-explored\nvulnerability. In this paper, we introduce FuncPoison, a novel poisoning-based\nattack targeting the function library to manipulate the behavior of LLM-driven\nmulti-agent autonomous systems. FuncPoison exploits two key weaknesses in how\nagents access the function library: (1) agents rely on text-based instructions\nto select tools; and (2) these tools are activated using standardized command\nformats that attackers can replicate. By injecting malicious tools with\ndeceptive instructions, FuncPoison manipulates one agent s decisions--such as\nmisinterpreting road conditions--triggering cascading errors that mislead other\nagents in the system. We experimentally evaluate FuncPoison on two\nrepresentative multi-agent autonomous driving systems, demonstrating its\nability to significantly degrade trajectory accuracy, flexibly target specific\nagents to induce coordinated misbehavior, and evade diverse defense mechanisms.\nOur results reveal that the function library, often considered a simple\ntoolset, can serve as a critical attack surface in LLM-based autonomous driving\nsystems, raising elevated concerns on their reliability."}
{"id": "2509.24418", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24418", "abs": "https://arxiv.org/abs/2509.24418", "authors": ["Haoran Li", "Yulin Chen", "Jingru Zeng", "Hao Peng", "Huihao Jing", "Wenbin Hu", "Xi Yang", "Ziqian Zeng", "Sirui Han", "Yangqiu Song"], "title": "GSPR: Aligning LLM Safeguards as Generalizable Safety Policy Reasoners", "comment": null, "summary": "As large language models (LLMs) are increasingly integrated into numerous\napplications across various domains, LLMs' safety becomes a critical concern\nfor both application developers and intended users. Currently, great efforts\nhave been made to develop safety benchmarks with fine-grained taxonomies.\nHowever, these benchmarks' taxonomies are disparate with different safety\npolicies. Thus, existing safeguards trained on these benchmarks are either\ncoarse-grained to only distinguish between safe and unsafe, or constrained by\nthe narrow risk taxonomies of a single benchmark. To leverage these\nfine-grained safety taxonomies across multiple safety benchmarks, in this\npaper, we propose GSPR, a Generalizable Safety Policy Reasoner to identify\nunsafe input prompts and LLMs' outputs with violated safety taxonomies through\nGroup Relative Policy Optimization (GRPO). Unlike prior safeguards which only\ncover a fixed set of risk factors, our GSPR incentivizes its reasoning\ncapability with varied safety taxonomies through our careful cold-start\nstrategy and reward design. Consequently, our GSPR can be trained across\nmultiple safety benchmarks with distinct taxonomies and naturally exhibits\npowerful generalization ability. We conduct extensive experiments to show that\nour GSPR significantly improves existing safety guardrails' reasoning\ncapabilities for both safety and category prediction tasks. Moreover, our GSPR\nnot only demonstrates powerful safety generalization abilities but also\nachieves the least inference token costs with explanations."}
{"id": "2509.24440", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24440", "abs": "https://arxiv.org/abs/2509.24440", "authors": ["Antonis Selentis", "Nikolas Makris", "Alkinoos Papageorgopoulos", "Persefoni Konteli", "Konstantinos Christodoulopoulos", "George T. Kanellos", "Dimitris Syvridis"], "title": "Evaluating Relayed and Switched Quantum Key Distribution (QKD) Network Architectures", "comment": null, "summary": "We evaluate the performance of two architectures for network-wide quantum key\ndistribution (QKD): Relayed QKD, which relays keys over multi-link QKD paths\nfor non-adjacent nodes, and Switched QKD, which uses optical switches to\ndynamically connect arbitrary QKD modules to form direct QKD links between\nthem. An advantage of Switched QKD is that it distributes quantum keys\nend-to-end, whereas Relayed relies on trusted nodes. However, Switched depends\non arbitrary matching of QKD modules. We first experimentally evaluate the\nperformance of commercial DV-QKD modules; for each of three vendors we\nbenchmark the performance in standard/matched module pairs and in unmatched\npairs to emulate configurations in the Switched QKD network architecture. The\nanalysis reveals that in some cases a notable variation in the generated secret\nkey rate (SKR) between the matched and unmatched pairs is observed. Driven by\nthese experimental findings, we conduct a comprehensive theoretical analysis\nthat evaluates the network-wide performance of the two architectures. Our\nanalysis is based on uniform ring networks, where we derive optimal key\nmanagement configurations and analytical formulas for the achievable consumed\nSKR. We compare network performance under varying ring sizes, QKD link losses,\nQKD receivers' sensitivity and performance penalties of unmatched modules. Our\nfindings indicate that Switched QKD performs better in dense rings (short\ndistances, large node counts), while Relayed QKD is more effective in longer\ndistances and large node counts. Moreover, we confirm that unmatched QKD\nmodules penalties significantly impact the efficiency of Switched QKD\narchitecture."}
{"id": "2509.24444", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.24444", "abs": "https://arxiv.org/abs/2509.24444", "authors": ["Yury Yanovich", "Victoria Kovalevskaya", "Maksim Egorov", "Elizaveta Smirnova", "Matvey Mishuris", "Yash Madhwal", "Kirill Ziborov", "Vladimir Gorgadze", "Subodh Sharma"], "title": "BugMagnifier: TON Transaction Simulator for Revealing Smart Contract Vulnerabilities", "comment": null, "summary": "The Open Network (TON) blockchain employs an asynchronous execution model\nthat introduces unique security challenges for smart contracts, particularly\nrace conditions arising from unpredictable message processing order. While\nprevious work established vulnerability patterns through static analysis of\naudit reports, dynamic detection of temporal dependencies through systematic\ntesting remains an open problem. We present BugMagnifier, a transaction\nsimulation framework that systematically reveals vulnerabilities in TON smart\ncontracts through controlled message orchestration. Built atop TON Sandbox and\nintegrated with the TON Virtual Machine (TVM), our tool combines precise\nmessage queue manipulation with differential state analysis and probabilistic\npermutation testing to detect asynchronous execution flaws. Experimental\nevaluation demonstrates BugMagnifier's effectiveness through extensive\nparametric studies on purpose-built vulnerable contracts, revealing message\nratio-dependent detection complexity that aligns with theoretical predictions.\nThis quantitative model enables predictive vulnerability assessment while\nshifting discovery from manual expert analysis to automated evidence\ngeneration. By providing reproducible test scenarios for temporal\nvulnerabilities, BugMagnifier addresses a critical gap in the TON security\ntooling, offering practical support for safer smart contract development in\nasynchronous blockchain environments."}
{"id": "2509.24623", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24623", "abs": "https://arxiv.org/abs/2509.24623", "authors": ["Carlos Benitez"], "title": "Mapping Quantum Threats: An Engineering Inventory of Cryptographic Dependencies", "comment": "13 pages, to be submitted to IEEE Privacy and Security", "summary": "The emergence of large-scale quantum computers, powered by algorithms like\nShor's and Grover's, poses an existential threat to modern public-key\ncryptography. This vulnerability stems from the ability of these machines to\nefficiently solve the hard mathematical problems - such as integer\nfactorization and the elliptic curve discrete logarithm problem - that underpin\nwidely used cryptographic primitives. This includes RSA, Diffie-Hellman (DH),\nElliptic Curve Diffie-Hellman (ECDH), and Elliptic Curve Digital Signature\nAlgorithm (ECDSA), which are foundational to security across the digital\necosystem. Once Shor's algorithm becomes practically realizable, these\nprimitives will fail, undermining both retrospective confidentiality and\ncryptographic authenticity - enabling adversaries to decrypt previously\ncaptured communications and forge digital signatures. This paper presents a\nsystematic inventory of technologies exposed to quantum threats from the\nengineering perspective, organized by both technology domain and by\nimplementation environment. While prior research has emphasized theoretical\nbreaks or protocol-level adaptations, this work focuses on the practical\nlandscape - mapping quantum-vulnerable systems across diverse digital\ninfrastructures. The contribution is a cross-domain, cross-environment threat\nmap to guide practitioners, vendors, and policymakers in identifying exposed\ntechnologies before the arrival of cryptographically relevant quantum\ncomputers."}
{"id": "2509.24624", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24624", "abs": "https://arxiv.org/abs/2509.24624", "authors": ["Thomas Fargues", "Ye Dong", "Tianwei Zhang", "Jin-Song Dong"], "title": "PRIVMARK: Private Large Language Models Watermarking with MPC", "comment": "8 pages, 4 figures, under peer-review", "summary": "The rapid growth of Large Language Models (LLMs) has highlighted the pressing\nneed for reliable mechanisms to verify content ownership and ensure\ntraceability. Watermarking offers a promising path forward, but it remains\nlimited by privacy concerns in sensitive scenarios, as traditional approaches\noften require direct access to a model's parameters or its training data. In\nthis work, we propose a secure multi-party computation (MPC)-based private LLMs\nwatermarking framework, PRIVMARK, to address the concerns. Concretely, we\ninvestigate PostMark (EMNLP'2024), one of the state-of-the-art LLMs\nWatermarking methods, and formulate its basic operations. Then, we construct\nefficient protocols for these operations using the MPC primitives in a\nblack-box manner. In this way, PRIVMARK enables multiple parties to\ncollaboratively watermark an LLM's output without exposing the model's weights\nto any single computing party. We implement PRIVMARK using SecretFlow-SPU\n(USENIX ATC'2023) and evaluate its performance using the ABY3 (CCS'2018)\nbackend. The experimental results show that PRIVMARK achieves semantically\nidentical results compared to the plaintext baseline without MPC and is\nresistant against paraphrasing and removing attacks with reasonable efficiency."}
{"id": "2509.24698", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24698", "abs": "https://arxiv.org/abs/2509.24698", "authors": ["Izaiah Sun", "Daniel Tan", "Andy Deng"], "title": "LISA Technical Report: An Agentic Framework for Smart Contract Auditing", "comment": "A technical report with 10 pages", "summary": "We present LISA, an agentic smart contract vulnerability detection framework\nthat combines rule-based and logic-based methods to address a broad spectrum of\nvulnerabilities in smart contracts. LISA leverages data from historical audit\nreports to learn the detection experience (without model fine-tuning), enabling\nit to generalize learned patterns to unseen projects and evolving threat\nprofiles. In our evaluation, LISA significantly outperforms both LLM-based\napproaches and traditional static analysis tools, achieving superior coverage\nof vulnerability types and higher detection accuracy. Our results suggest that\nLISA offers a compelling solution for industry: delivering more reliable and\ncomprehensive vulnerability detection while reducing the dependence on manual\neffort."}
{"id": "2509.24807", "categories": ["cs.CR", "K.6.5"], "pdf": "https://arxiv.org/pdf/2509.24807", "abs": "https://arxiv.org/abs/2509.24807", "authors": ["Dong Hyun Roh", "Rajesh Kumar"], "title": "Active Authentication via Korean Keystrokes Under Varying LLM Assistance and Cognitive Contexts", "comment": "Accepted for publication at IEEE-ICMLA 2025. Contains nine pages, six\n  figures, and two tables", "summary": "Keystroke dynamics is a promising modality for active user authentication,\nbut its effectiveness under varying LLM-assisted typing and cognitive\nconditions remains understudied. Using data from 50 users and cognitive labels\nfrom Bloom's Taxonomy, we evaluate keystroke-based authentication in Korean\nacross three realistic typing scenarios: bona fide composition, LLM content\nparaphrasing, and transcription. Our pipeline incorporates continuity-aware\nsegmentation, feature extraction, and classification via SVM, MLP, and XGB.\nResults show that the system maintains reliable performance across varying LLM\nusages and cognitive contexts, with Equal Error Rates ranging from 5.1% to\n10.4%. These findings demonstrate the feasibility of behavioral authentication\nunder modern writing conditions and offer insights into designing more\ncontext-resilient models."}
{"id": "2509.24823", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24823", "abs": "https://arxiv.org/abs/2509.24823", "authors": ["Benedetta Tondi", "Andrea Costanzo", "Mauro Barni"], "title": "Of-SemWat: High-payload text embedding for semantic watermarking of AI-generated images with arbitrary size", "comment": "5 pages, 2 figures", "summary": "We propose a high-payload image watermarking method for textual embedding,\nwhere a semantic description of the image - which may also correspond to the\ninput text prompt-, is embedded inside the image. In order to be able to\nrobustly embed high payloads in large-scale images - such as those produced by\nmodern AI generators - the proposed approach builds upon a traditional\nwatermarking scheme that exploits orthogonal and turbo codes for improved\nrobustness, and integrates frequency-domain embedding and perceptual masking\ntechniques to enhance watermark imperceptibility. Experiments show that the\nproposed method is extremely robust against a wide variety of image processing,\nand the embedded text can be retrieved also after traditional and AI\ninpainting, permitting to unveil the semantic modification the image has\nundergone via image-text mismatch analysis."}
{"id": "2509.24955", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24955", "abs": "https://arxiv.org/abs/2509.24955", "authors": ["Tereza Burianová", "Martin Perešíni", "Ivan Homoliak"], "title": "Secret Leader Election in Ethereum PoS: An Empirical Security Analysis of Whisk and Homomorphic Sortition under DoS on the Leader and Censorship Attacks", "comment": null, "summary": "Proposer anonymity in Proof-of-Stake (PoS) blockchains is a critical concern\ndue to the risk of targeted attacks such as malicious denial-of-service (DoS)\nand censorship attacks. While several Secret Single Leader Election (SSLE)\nmechanisms have been proposed to address these threats, their practical impact\nand trade-offs remain insufficiently explored. In this work, we present a\nunified experimental framework for evaluating SSLE mechanisms under adversarial\nconditions, grounded in a simplified yet representative model of Ethereum's PoS\nconsensus layer. The framework includes configurable adversaries capable of\nlaunching targeted DoS and censorship attacks, including coordinated strategies\nthat simultaneously compromise groups of validators. We simulate and compare\nkey protection mechanisms - Whisk, and homomorphic sortition. To the best of\nour knowledge, this is the first comparative study to examine adversarial DoS\nscenarios involving multiple attackers under diverse protection mechanisms. Our\nresults show that while both designs offer strong protection against targeted\nDoS attacks on the leader, neither defends effectively against coordinated\nattacks on validator groups. Moreover, Whisk simplifies a DoS attack by\nnarrowing the target set from all validators to a smaller list of known\ncandidates. Homomorphic sortition, despite its theoretical strength, remains\nimpractical due to the complexity of cryptographic operations over large\nvalidator sets."}
{"id": "2509.24967", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24967", "abs": "https://arxiv.org/abs/2509.24967", "authors": ["Yupei Liu", "Yanting Wang", "Yuqi Jia", "Jinyuan Jia", "Neil Zhenqiang Gong"], "title": "SecInfer: Preventing Prompt Injection via Inference-time Scaling", "comment": null, "summary": "Prompt injection attacks pose a pervasive threat to the security of Large\nLanguage Models (LLMs). State-of-the-art prevention-based defenses typically\nrely on fine-tuning an LLM to enhance its security, but they achieve limited\neffectiveness against strong attacks. In this work, we propose \\emph{SecInfer},\na novel defense against prompt injection attacks built on \\emph{inference-time\nscaling}, an emerging paradigm that boosts LLM capability by allocating more\ncompute resources for reasoning during inference. SecInfer consists of two key\nsteps: \\emph{system-prompt-guided sampling}, which generates multiple responses\nfor a given input by exploring diverse reasoning paths through a varied set of\nsystem prompts, and \\emph{target-task-guided aggregation}, which selects the\nresponse most likely to accomplish the intended task. Extensive experiments\nshow that, by leveraging additional compute at inference, SecInfer effectively\nmitigates both existing and adaptive prompt injection attacks, outperforming\nstate-of-the-art defenses as well as existing inference-time scaling\napproaches."}
{"id": "2509.25072", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25072", "abs": "https://arxiv.org/abs/2509.25072", "authors": ["Yaman Jandali", "Ruisi Zhang", "Nojan Sheybani", "Farinaz Koushanfar"], "title": "Optimizing Privacy-Preserving Primitives to Support LLM-Scale Applications", "comment": null, "summary": "Privacy-preserving technologies have introduced a paradigm shift that allows\nfor realizable secure computing in real-world systems. The significant barrier\nto the practical adoption of these primitives is the computational and\ncommunication overhead that is incurred when applied at scale. In this paper,\nwe present an overview of our efforts to bridge the gap between this overhead\nand practicality for privacy-preserving learning systems using multi-party\ncomputation (MPC), zero-knowledge proofs (ZKPs), and fully homomorphic\nencryption (FHE). Through meticulous hardware/software/algorithm co-design, we\nshow progress towards enabling LLM-scale applications in privacy-preserving\nsettings. We demonstrate the efficacy of our solutions in several contexts,\nincluding DNN IP ownership, ethical LLM usage enforcement, and transformer\ninference."}
{"id": "2509.25113", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.25113", "abs": "https://arxiv.org/abs/2509.25113", "authors": ["Wai Ming Chan", "Remi Chou", "Taejoon Kim"], "title": "Two-Dimensional XOR-Based Secret Sharing for Layered Multipath Communication", "comment": null, "summary": "This paper introduces the first two-dimensional XOR-based secret sharing\nscheme for layered multipath communication networks. We present a construction\nthat guarantees successful message recovery and perfect privacy when an\nadversary observes and disrupts any single path at each transmission layer. The\nscheme achieves information-theoretic security using only bitwise XOR\noperations with linear $O(|S|)$ complexity, where $|S|$ is the message length.\nWe provide mathematical proofs demonstrating that the scheme maintains\nunconditional security regardless of computational resources available to\nadversaries. Unlike encryption-based approaches vulnerable to quantum computing\nadvances, our construction offers provable security suitable for\nresource-constrained military environments where computational assumptions may\nfail."}
{"id": "2509.24032", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24032", "abs": "https://arxiv.org/abs/2509.24032", "authors": ["Jialun Zhang", "Merve Gülmez", "Thomas Nyman", "Gang Tan"], "title": "SandCell: Sandboxing Rust Beyond Unsafe Code", "comment": null, "summary": "Rust is a modern systems programming language that ensures memory safety by\nenforcing ownership and borrowing rules at compile time. While the unsafe\nkeyword allows programmers to bypass these restrictions, it introduces\nsignificant risks. Various approaches for isolating unsafe code to protect safe\nRust from vulnerabilities have been proposed, yet these methods provide only\nfixed isolation boundaries and do not accommodate expressive policies that\nrequire sandboxing both safe and unsafe code. This paper presents SandCell for\nflexible and lightweight isolation in Rust by leveraging existing syntactic\nboundaries. SandCell allows programmers to specify which components to sandbox\nwith minimal annotation effort, enabling fine-grained control over isolation.\nThe system also introduces novel techniques to minimize overhead when\ntransferring data between sandboxes. Our evaluation demonstrates SandCell's\neffectiveness in preventing vulnerabilities across various Rust applications\nwhile maintaining reasonable performance overheads."}
{"id": "2509.24515", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.24515", "abs": "https://arxiv.org/abs/2509.24515", "authors": ["Yu-Fu Fu", "Meng Xu", "Taesoo Kim"], "title": "Agentic Specification Generator for Move Programs", "comment": "18 pages; Extended version of ASE'25 paper with extra appendices", "summary": "While LLM-based specification generation is gaining traction, existing tools\nprimarily focus on mainstream programming languages like C, Java, and even\nSolidity, leaving emerging and yet verification-oriented languages like Move\nunderexplored. In this paper, we introduce MSG, an automated specification\ngeneration tool designed for Move smart contracts. MSG aims to highlight key\ninsights that uniquely present when applying LLM-based specification generation\nto a new ecosystem. Specifically, MSG demonstrates that LLMs exhibit robust\ncode comprehension and generation capabilities even for non-mainstream\nlanguages. MSG successfully generates verifiable specifications for 84% of\ntested Move functions and even identifies clauses previously overlooked by\nexperts. Additionally, MSG shows that explicitly leveraging specification\nlanguage features through an agentic, modular design improves specification\nquality substantially (generating 57% more verifiable clauses than conventional\ndesigns). Incorporating feedback from the verification toolchain further\nenhances the effectiveness of MSG, leading to a 30% increase in generated\nverifiable specifications."}
