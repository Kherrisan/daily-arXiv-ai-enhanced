{"id": "2509.03711", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03711", "abs": "https://arxiv.org/abs/2509.03711", "authors": ["Siddharth Muralee", "Sourag Cherupattamoolayil", "James C. Davis", "Antonio Bianchi", "Aravind Machiry"], "title": "Reactive Bottom-Up Testing", "comment": null, "summary": "Modern computing systems remain rife with software vulnerabilities. Engineers\napply many means to detect them, of which dynamic testing is one of the most\ncommon and effective. However, most dynamic testing techniques follow a\ntop-down paradigm, and struggle to reach and exercise functions deep within the\ncall graph. While recent works have proposed Bottom-Up approaches to address\nthese limitations, they face challenges with false positives and generating\nvalid inputs that adhere to the context of the entire program.\n  In this work, we introduce a new paradigm that we call Reactive Bottom-Up\nTesting. Our insight is that function-level testing is necessary but not\nsufficient for the validation of vulnerabilities in functions. What we need is\na systematic approach that not only tests functions in isolation but also\nvalidates their behavior within the broader program context, ensuring that\ndetected vulnerabilities are both reachable and triggerable. We develop a\nthree-stage bottom-up testing scheme: (1) identify likely-vulnerable functions\nand generate type- and context-aware harnesses; (2) fuzz to find crashes and\nextract input constraints via symbolic execution; (3) verify crashes by\ncombining constraints to remove false positives. We implemented an automated\nprototype, which we call Griller. We evaluated Griller in a controlled setting\nusing a benchmark of 48 known vulnerabilities across 5 open-source projects,\nwhere we successfully detected 28 known vulnerabilities. Additionally, we\nevaluated Griller on several real-world applications such as Pacman, and it\ndiscovered 6 previously unknown vulnerabilities. Our findings suggest that\nReactive Bottom-Up Testing can significantly enhance the detection of\nvulnerabilities in complex systems, paving the way for more robust security\npractices.", "AI": {"tldr": "Reactive Bottom-Up Testing improves vulnerability detection by combining context-aware harnesses, fuzzing, and constraint verification. Griller, its prototype, found 28 known and 6 new vulnerabilities, demonstrating significant potential for secure systems.", "motivation": "Existing dynamic testing struggles to reach deep functions in call graphs, while bottom-up approaches face false positives and context validity issues. The paper aims to address these limitations through a systematic program-aware testing paradigm.", "method": "The method involves a three-stage bottom-up testing scheme: (1) identifying likely-vulnerable functions and generating context-aware harnesses, (2) fuzzing and extracting input constraints via symbolic execution, and (3) verifying crashes by combining constraints to eliminate false positives. This is implemented in the Griller prototype.", "result": "Griller successfully detected 28 known vulnerabilities in a benchmark of 48 and discovered 6 previously unknown vulnerabilities in real-world applications like Pacman.", "conclusion": "The paper concludes that Reactive Bottom-Up Testing enhances vulnerability detection in complex systems by addressing reachability and context, leading to more robust security practices."}}
{"id": "2509.03744", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.03744", "abs": "https://arxiv.org/abs/2509.03744", "authors": ["Hamid Barati"], "title": "A Quantum Genetic Algorithm-Enhanced Self-Supervised Intrusion Detection System for Wireless Sensor Networks in the Internet of Things", "comment": null, "summary": "The rapid expansion of the Internet of Things (IoT) and Wireless Sensor\nNetworks (WSNs) has significantly increased the attack surface of such systems,\nmaking them vulnerable to a wide range of cyber threats. Traditional Intrusion\nDetection Systems (IDS) often fail to meet the stringent requirements of\nresource-constrained IoT environments due to their high computational cost and\nreliance on large labeled datasets. To address these challenges, this paper\nproposes a novel hybrid Intrusion Detection System that integrates a Quantum\nGenetic Algorithm (QGA) with Self-Supervised Learning (SSL). The QGA leverages\nquantum-inspired evolutionary operators to optimize feature selection and\nfine-tune model parameters, ensuring lightweight yet efficient detection in\nresource-limited networks. Meanwhile, SSL enables the system to learn robust\nrepresentations from unlabeled data, thereby reducing dependency on manually\nlabeled training sets. The proposed framework is evaluated on benchmark IoT\nintrusion datasets, demonstrating superior performance in terms of detection\naccuracy, false positive rate, and computational efficiency compared to\nconventional evolutionary and deep learning-based IDS models. The results\nhighlight the potential of combining quantum-inspired optimization with\nself-supervised paradigms to design next-generation intrusion detection\nsolutions for IoT and WSN environments.", "AI": {"tldr": "A hybrid IDS combining Quantum Genetic Algorithm (QGA) and Self-Supervised Learning (SSL) is proposed to address resource constraints and data labeling challenges in IoT/WSN environments.", "motivation": "Traditional IDS solutions lack scalability for resource-constrained IoT/WSN systems due to high computational demands and reliance on labeled datasets, while IoT's growth increases cyber threat surfaces.", "method": "The framework integrates QGA for optimized feature selection and parameter tuning with SSL for robust representation learning from unlabeled data, tested on benchmark IoT intrusion datasets.", "result": "Outperforms conventional IDS models in detection accuracy, false positive rate, and computational efficiency on benchmark IoT datasets.", "conclusion": "The hybrid approach demonstrates superior detection accuracy, lower false positives, and higher computational efficiency, validating the potential of quantum-inspired and self-supervised methods for next-gen IoT security.uestions"}}
{"id": "2509.03806", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.03806", "abs": "https://arxiv.org/abs/2509.03806", "authors": ["Hao Nie", "Wei Wang", "Peng Xu", "Wei Chen", "Laurence T. Yang", "Mauro Conti", "Kaitai Liang"], "title": "Peekaboo, I See Your Queries: Passive Attacks Against DSSE Via Intermittent Observations", "comment": null, "summary": "Dynamic Searchable Symmetric Encryption (DSSE) allows secure searches over a\ndynamic encrypted database but suffers from inherent information leakage.\nExisting passive attacks against DSSE rely on persistent leakage monitoring to\ninfer leakage patterns, whereas this work targets intermittent observation - a\nmore practical threat model. We propose Peekaboo - a new universal attack\nframework - and the core design relies on inferring the search pattern and\nfurther combining it with auxiliary knowledge and other leakage. We instantiate\nPeekaboo over the SOTA attacks, Sap (USENIX' 21) and Jigsaw (USENIX' 24), to\nderive their \"+\" variants (Sap+ and Jigsaw+). Extensive experiments demonstrate\nthat our design achieves >0.9 adjusted rand index for search pattern recovery\nand 90% query accuracy vs. FMA's 30% (CCS' 23). Peekaboo's accuracy scales with\nobservation rounds and the number of observed queries but also it resists SOTA\ncountermeasures, with >40% accuracy against file size padding and >80% against\nobfuscation.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.03807", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03807", "abs": "https://arxiv.org/abs/2509.03807", "authors": ["Junhui Li", "Chengbin Feng", "Zhiwei Yang", "Qi Mo", "Wei Wang"], "title": "BIDO: A Unified Approach to Address Obfuscation and Concept Drift Challenges in Image-based Malware Detection", "comment": null, "summary": "To identify malicious Android applications, various malware detection\ntechniques have been proposed. Among them, image-based approaches are\nconsidered potential alternatives due to their efficiency and scalability.\nRecent studies have reported that these approaches suffer significant\nperformance declines when confronted with obfuscation or concept drift.\nHowever, existing solutions often treat these two challenges as different\nproblems, offering independent solutions. These techniques overlook the fact\nthat both challenges share a common statistical root, out-of-distribution, and\nresearch from this perspective remains limited. In response, we propose BIDO, a\nhybrid image-based malware detector designed to enhance robustness against both\nobfuscation and concept drift simultaneously. Specifically, to improve the\ndiscriminative power of image features, we introduce a local feature selection\nmodule that identifies informative subregions within malware images. Second, to\nenhance feature robustness, we model pairwise cross-modal dependencies in an\nouter product space, enabling the extraction of stable co-occurrence patterns.\nThird, to ensure feature compactness, we design a learnable metric that pulls\nsamples with identical labels closer while pushing apart those with different\nlabels, regardless of obfuscation or concept drift. Extensive experiments on\nthe real-world datasets demonstrate that BIDO significantly outperforms\nexisting baselines, achieving higher robustness against both concept drift and\nobfuscation. The source code is available at:\nhttps://github.com/whatishope/BIDO/.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.03541", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03541", "abs": "https://arxiv.org/abs/2509.03541", "authors": ["Chong Wang", "Haoning Wu", "Peng Liang", "Maya Daneva", "Marten van Sinderen"], "title": "Towards the Datasets Used in Requirements Engineering of Mobile Apps: Preliminary Findings from a Systematic Mapping Study", "comment": null, "summary": "[Background] Research on requirements engineering (RE) for mobile apps\nemploys datasets formed by app users, developers or vendors. However, little is\nknown about the sources of these datasets in terms of platforms and the RE\nactivities that were researched with the help of the respective datasets.\n[Aims] The goal of this paper is to investigate the state-of-the-art of the\ndatasets of mobile apps used in existing RE research. [Method] We carried out a\nsystematic mapping study by following the guidelines of Kitchenham et al.\n[Results] Based on 43 selected papers, we found that Google Play and Apple App\nStore provide the datasets for more than 90% of published research in RE for\nmobile apps. We also found that the most investigated RE activities - based on\ndatasets, are requirements elicitation and requirements analysis. [Conclusions]\nOur most important conclusions are: (1) there is a growth in the use of\ndatasets for RE research of mobile apps since 2012, (2) the RE knowledge for\nmobile apps might be skewed due to the overuse of Google Play and Apple App\nStore, (3) there are attempts to supplement reviews of apps from repositories\nwith other data sources, (4) there is a need to expand the alternative sources\nand experiments with complimentary use of multiple sources, if the community\nwants more generalizable results. Plus, it is expected to expand the research\non other RE activities, beyond elicitation and analysis.", "AI": {"tldr": "This study maps 43 mobile app RE research papers, revealing heavy reliance on Google Play and Apple App Store data for requirements elicitation and analysis, with recommendations to diversify data sources for more robust research outcomes.", "motivation": "The study addresses the need to understand the sources and scope of datasets used in mobile app requirements engineering (RE) research, as prior work overlooks the diversity of data origins and potential biases in RE activity analysis. This is critical for assessing the validity and generalizability of RE research findings.", "method": "A systematic mapping study following Kitchenham et al.'s guidelines, analyzing 43 selected papers.", "result": "Over 90% of RE research relies on Google Play and Apple App Store datasets, with a focus on requirements elicitation and analysis. Research growth since 2012 is documented, alongside emerging efforts to combine repository reviews with alternative data sources.", "conclusion": "Key takeaways include a growing but potentially biased dataset usage, calls for diversifying data sources, and expanding RE research beyond elicitation and analysis to enhance generalizability."}}
{"id": "2509.03821", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.03821", "abs": "https://arxiv.org/abs/2509.03821", "authors": ["Rui Zhao", "Muhammad Shoaib", "Viet Tung Hoang", "Wajih Ul Hassan"], "title": "Rethinking Tamper-Evident Logging: A High-Performance, Co-Designed Auditing System", "comment": "This paper has been accepted to the ACM Conference on Computer and\n  Communications Security (CCS) 2025", "summary": "Existing tamper-evident logging systems suffer from high overhead and severe\ndata loss in high-load settings, yet only provide coarse-grained tamper\ndetection. Moreover, installing such systems requires recompiling kernel code.\nTo address these challenges, we present Nitro, a high-performance,\ntamper-evident audit logging system that supports fine-grained detection of log\ntampering. Even better, our system avoids kernel recompilation by using the\neBPF technology. To formally justify the security of Nitro, we provide a new\ndefinitional framework for logging systems, and give a practical cryptographic\nconstruction meeting this new goal. Unlike prior work that focus only on the\ncryptographic processing, we codesign the cryptographic part with the pre- and\npost-processing of the logs to exploit all system-level optimizations. Our\nevaluations demonstrate Nitro's superior performance, achieving 10X-25X\nimprovements in high-stress conditions and 2X-10X in real-world scenarios while\nmaintaining near-zero data loss. We also provide an advanced variant, Nitro-R\nthat introduces in-kernel log reduction techniques to reduce runtime overhead\neven further.", "AI": {"tldr": "Nitro uses eBPF and integrated cryptographic-design to create a high-performance, tamper-evident logging system that avoids kernel recompilation and achieves 10\u201325\u00d7 performance gains in stressful conditions with nearly zero data loss.", "motivation": "Existing systems suffer from high overhead, data loss during high loads, coarse-grained tamper detection, and require kernel recompilation. These limitations motivate the need for a high-performance, fine-grained tamper-evident logging system.", "method": "Nitro employs eBPF to avoid kernel recompilation, introduces a new definitional framework for secure logging, and integrates cryptographic processing with pre-/post-processing of logs. An advanced variant, Nitro-R, further reduces overhead via in-kernel log reduction.", "result": "Nitro achieves 10\u201325\u00d7 performance improvements in high-stress conditions and 2\u201310\u00d7 gains in real-world scenarios with near-zero data loss. Nitro-R further reduces runtime overhead through in-kernel optimizations.", "conclusion": "Nitro effectively addresses the limitations of existing tamper-evident logging systems by leveraging eBPF technology for kernel compilation avoidance, formal security frameworks, and system-level optimizations. It achieves significant performance improvements with minimal data loss, validated by rigorous evaluations."}}
{"id": "2509.03554", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03554", "abs": "https://arxiv.org/abs/2509.03554", "authors": ["Cheng-Yang Tsai", "Tzu-Wei Huang", "Jen-Wei Shih", "I-Hsiang Wang", "Yu-Cheng Lin", "Rung-Bin Lin"], "title": "A Multi-stage Error Diagnosis for APB Transaction", "comment": null, "summary": "Functional verification and debugging are critical bottlenecks in modern\nSystem-on-Chip (SoC) design, with manual detection of Advanced Peripheral Bus\n(APB) transaction errors in large Value Change Dump (VCD) files being\ninefficient and error-prone. Addressing the 2025 ICCAD Contest Problem D, this\nstudy proposes an automated error diagnosis framework using a hierarchical\nRandom Forest-based architecture. The multi-stage error diagnosis employs four\npre-trained binary classifiers to sequentially detect Out-of-Range Access,\nAddress Corruption, and Data Corruption errors, prioritizing high-certainty\naddress-related faults before tackling complex data errors to enhance\nefficiency. Experimental results show an overall accuracy of 91.36%, with\nnear-perfect precision and recall for address errors and robust performance for\ndata errors. Although the final results of the ICCAD 2025 CAD Contest are yet\nto be announced as of the submission date, our team achieved first place in the\nbeta stage, highlighting the method's competitive strength. This research\nvalidates the potential of hierarchical machine learning as a powerful\nautomated tool for hardware debugging in Electronic Design Automation (EDA).", "AI": {"tldr": "A hierarchical Random Forest-based framework automates APB transaction error detection in SoC design, achieving 91.36% accuracy and competitive performance in a design contest.", "motivation": "Functional verification and debugging are critical bottlenecks in modern SoC design, with manual detection of APB transaction errors in large VCD files being inefficient and error-prone.", "method": "The study proposes an automated error diagnosis framework using a hierarchical Random Forest-based architecture. The multi-stage error diagnosis employs four pre-trained binary classifiers to sequentially detect Out-of-Range Access, Address Corruption, and Data Corruption errors, prioritizing high-certainty address-related faults before tackling complex data errors.", "result": "The framework achieved an overall accuracy of 91.36%, with near-perfect precision and recall for address errors and robust performance for data errors. The team secured first place in the beta stage of the ICCAD 2025 CAD Contest.", "conclusion": "This research validates the potential of hierarchical machine learning as a powerful automated tool for hardware debugging in Electronic Design Automation (EDA)."}}
{"id": "2509.03860", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.03860", "abs": "https://arxiv.org/abs/2509.03860", "authors": ["Yifan Jia", "Ye Tian", "Liguo Zhang", "Yanbin Wang", "Jianguo Sun", "Liangliang Song"], "title": "KGBERT4Eth: A Feature-Complete Transformer Powered by Knowledge Graph for Multi-Task Ethereum Fraud Detection", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Ethereum's rapid ecosystem expansion and transaction anonymity have triggered\na surge in malicious activity. Detection mechanisms currently bifurcate into\nthree technical strands: expert-defined features, graph embeddings, and\nsequential transaction patterns, collectively spanning the complete feature\nsets of Ethereum's native data layer. Yet the absence of cross-paradigm\nintegration mechanisms forces practitioners to choose between sacrificing\nsequential context awareness, structured fund-flow patterns, or human-curated\nfeature insights in their solutions. To bridge this gap, we propose KGBERT4Eth,\na feature-complete pre-training encoder that synergistically combines two key\ncomponents: (1) a Transaction Semantic Extractor, where we train an enhanced\nTransaction Language Model (TLM) to learn contextual semantic representations\nfrom conceptualized transaction records, and (2) a Transaction Knowledge Graph\n(TKG) that incorporates expert-curated domain knowledge into graph node\nembeddings to capture fund flow patterns and human-curated feature insights. We\njointly optimize pre-training objectives for both components to fuse these\ncomplementary features, generating feature-complete embeddings. To emphasize\nrare anomalous transactions, we design a biased masking prediction task for TLM\nto focus on statistical outliers, while the Transaction TKG employs link\nprediction to learn latent transaction relationships and aggregate knowledge.\nFurthermore, we propose a mask-invariant attention coordination module to\nensure stable dynamic information exchange between TLM and TKG during\npre-training. KGBERT4Eth significantly outperforms state-of-the-art baselines\nin both phishing account detection and de-anonymization tasks, achieving\nabsolute F1-score improvements of 8-16% on three phishing detection benchmarks\nand 6-26% on four de-anonymization datasets.", "AI": {"tldr": "KGBERT4Eth integrates TLM and TKG for Ethereum transaction analysis, achieving state-of-the-art results by fusing sequential semantics, fund-flow patterns, and expert knowledge.", "motivation": "Existing Ethereum transaction detection methods are constrained by the need to sacrifice sequential context, fund-flow patterns, or expert features due to lack of cross-paradigm integration.", "method": "Proposes KGBERT4Eth with two components: (1) a Transaction Language Model (TLM) using biased masking to focus on anomalies, and (2) a Transaction Knowledge Graph (TKG) embedding expert-curated domain knowledge. These are jointly optimized with a coordination module for dynamic information exchange.", "result": "Outperforms SOTA baselines with 8-16% absolute F1-score improvement on phishing detection and 6-26% on de-anonymization across multiple datasets.", "conclusion": "KGBERT4Eth effectively integrates transaction semantic learning and structured knowledge graphs, achieving superior performance in phishing and de-anonymization tasks."}}
{"id": "2509.03668", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03668", "abs": "https://arxiv.org/abs/2509.03668", "authors": ["Matt Rau", "Chris Brown", "John Edwards"], "title": "Parse Tree Tracking Through Time for Programming Process Analysis at Scale", "comment": null, "summary": "Background and Context: Programming process data can be utilized to\nunderstand the processes students use to write computer programming\nassignments. Keystroke- and line-level event logs have been used in the past in\nvarious ways, primarily in high-level descriptive statistics (e.g., timings,\ncharacter deletion rate, etc). Analysis of behavior in context (e.g., how much\ntime students spend working on loops) has been cumbersome because of our\ninability to automatically track high-level code representations, such as\nabstract syntax trees, through time and unparseable states.\n  Objective: Our study has two goals. The first is to design the first\nalgorithm that tracks parse tree nodes through time. Second, we utilize this\nalgorithm to perform a partial replication study of prior work that used manual\ntracking of code representations, as well as other novel analyses of student\nprogramming behavior that can now be done at scale.\n  Method: We use two algorithms presented in this paper to track parse tree\nnodes through time and construct tree representations for unparseable code\nstates. We apply these algorithms to a public keystroke data from student\ncoursework in a 2021 CS1 course and conduct analysis on the resulting parse\ntrees.\n  Findings: We discover newly observable statistics at scale, including that\ncode is deleted at similar rates inside and outside of conditionals and loops,\na third of commented out code is eventually restored, and that frequency with\nwhich students jump around in their code may not be indicative of struggle.\n  Implications: The ability to track parse trees through time opens the door to\nunderstanding new dimensions of student programming, such as best practices of\nstructural development of code over time, quantitative measurement of what\nsyntactic constructs students struggle most with, refactoring behavior, and\nattention shifting within the code.", "AI": {"tldr": "This paper introduces algorithms to track parse trees through time, enabling scalable analysis of student programming behavior. Key findings show code deletion rates are consistent across structures, commented code often gets restored, and code navigation does not imply struggle, opening new research directions in understanding coding practices.", "motivation": "Previous methods relied on high-level statistics (e.g., timings, deletion rates) but lacked the ability to track high-level code representations (e.g., abstract syntax trees) through time or handle unparseable states. This limited contextual analysis of student programming behavior (e.g., work on loops/conditionals).", "method": "The authors designed two algorithms to track parse tree nodes through time and construct tree representations for unparseable code states. These algorithms were applied to public keystroke data from a 2021 CS1 course, enabling analysis of parse trees to uncover new behavioral insights.", "result": "Key findings include: (1) code deletion rates are similar within and outside conditionals/loops; (2) approximately one-third of commented-out code is restored; (3) code navigation frequency does not reliably indicate struggle. These results demonstrate scalable, context-aware insights previously infeasible.", "conclusion": "The paper concludes that tracking parse trees through time enables a deeper understanding of student programming behaviors, offering new dimensions like structural development of code, syntactic challenges, refactoring habits, and attention shifts. This method allows for scalable, context-aware analysis previously unattainable."}}
{"id": "2509.03879", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.03879", "abs": "https://arxiv.org/abs/2509.03879", "authors": ["Gang Liu", "Ningjie Li", "Cen Chen"], "title": "ShieldMMU: Detecting and Defending against Controlled-Channel Attacks in Shielding Memory System", "comment": null, "summary": "Intel SGX and hypervisors isolate non-privileged programs from other\nsoftware, ensuring confidentiality and integrity. However, side-channel attacks\ncontinue to threaten Intel SGX's security, enabling malicious OS to manipulate\nPTE present bits, induce page faults, and steal memory access traces. Despite\nextensive research, existing defenses focus on detection or rely on impractical\nsolutions. This paper presents ShieldMMU, a comprehensive solution for\nmitigating controlled channel attacks, balancing compatibility, performance,\nand usability. Leveraging a Merkle Tree-inspired Defense Tree (DD-Tree),\nShieldMMU protects PTE integrity by detecting, locating, and restoring attacked\nPTEs. It identifies MMU page table lookup events and side-channel attacks,\npromptly restoring PTE parameters to prevent page fault traps and ensure secure\nnon-privileged application operation within SGX. Our experiments confirm\nShieldMMU's enhanced security and acceptable latency performance.", "AI": {"tldr": "ShieldMMU is a comprehensive solution to mitigate controlled channel attacks on Intel SGX by using a Merkle Tree-inspired DD-Tree to protect PTE integrity.", "motivation": "Side-channel attacks threaten Intel SGX's security by exploiting PTE present bits and memory access traces, while current defenses are either detection-focused or impractical.", "method": "ShieldMMU employs a Defense Tree (DD-Tree) to detect, locate, and restore attacked PTEs. It identifies MMU page table lookup events and side-channel attacks, then restores PTE parameters to prevent page fault traps and secure non-privileged application operations within SGX.", "result": "Experiments confirm ShieldMMU achieves improved security while maintaining acceptable latency performance.", "conclusion": "ShieldMMU provides a balanced approach to mitigating controlled channel attacks on Intel SGX, offering compatibility, performance, and usability through proactive PTE integrity protection."}}
{"id": "2509.03848", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.03848", "abs": "https://arxiv.org/abs/2509.03848", "authors": ["Rodrigo Oliveira Zacarias", "Rodrigo Pereira dos Santos", "Patricia Lago"], "title": "Towards an Understanding of Developer Experience-Driven Transparency in Software Ecosystems", "comment": "36 pages Submitted to the ACM Transactions on Software Engineering\n  and Methodology. 2025", "summary": "Software ecosystems (SECO) have become a dominant paradigm in the software\nindustry, enabling third-party developers to co-create value through\ncomplementary components and services. While Developer Experience (DX) is\nincreasingly recognized as critical for sustainable SECO, transparency remains\nan underexplored factor shaping how developers perceive and interact with\necosystems. Existing studies acknowledge transparency as essential for trust,\nfairness, and engagement, yet its relationship with DX has not been\nsystematically conceptualized. Hence, this work aims to advance the\nunderstanding of transparency in SECO from a developer-centered perspective. To\nthis end, we propose SECO-TransDX (Transparency in Software Ecosystems from a\nDeveloper Experience Perspective), a conceptual model that introduces the\nnotion of DX-driven transparency. The model identifies 63 interrelated\nconcepts, including conditioning factors, ecosystem procedures, artifacts, and\nrelational dynamics that influence how transparency is perceived and\nconstructed during developer interactions. SECO-TransDX was built upon prior\nresearch and refined through a Delphi study with experts from academia and\nindustry. It offers a structured lens to examine how transparency mediates DX\nacross technical, social, and organizational layers. For researchers, it lays\nthe groundwork for future studies and tool development; for practitioners, it\nsupports the design of trustworthy, developer-centered platforms that improve\ntransparency and foster long-term engagement in SECO.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.03939", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03939", "abs": "https://arxiv.org/abs/2509.03939", "authors": ["Yifan Jia", "Yanbin Wang", "Jianguo Sun", "Ye Tian", "Peng Qian"], "title": "LMAE4Eth: Generalizable and Robust Ethereum Fraud Detection by Exploring Transaction Semantics and Masked Graph Embedding", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Current Ethereum fraud detection methods rely on context-independent,\nnumerical transaction sequences, failing to capture semantic of account\ntransactions. Furthermore, the pervasive homogeneity in Ethereum transaction\nrecords renders it challenging to learn discriminative account embeddings.\nMoreover, current self-supervised graph learning methods primarily learn node\nrepresentations through graph reconstruction, resulting in suboptimal\nperformance for node-level tasks like fraud account detection, while these\nmethods also encounter scalability challenges. To tackle these challenges, we\npropose LMAE4Eth, a multi-view learning framework that fuses transaction\nsemantics, masked graph embedding, and expert knowledge. We first propose a\ntransaction-token contrastive language model (TxCLM) that transforms\ncontext-independent numerical transaction records into logically cohesive\nlinguistic representations. To clearly characterize the semantic differences\nbetween accounts, we also use a token-aware contrastive learning pre-training\nobjective together with the masked transaction model pre-training objective,\nlearns high-expressive account representations. We then propose a masked\naccount graph autoencoder (MAGAE) using generative self-supervised learning,\nwhich achieves superior node-level account detection by focusing on\nreconstructing account node features. To enable MAGAE to scale for large-scale\ntraining, we propose to integrate layer-neighbor sampling into the graph, which\nreduces the number of sampled vertices by several times without compromising\ntraining quality. Finally, using a cross-attention fusion network, we unify the\nembeddings of TxCLM and MAGAE to leverage the benefits of both. We evaluate our\nmethod against 21 baseline approaches on three datasets. Experimental results\nshow that our method outperforms the best baseline by over 10% in F1-score on\ntwo of the datasets.", "AI": {"tldr": "LMAE4Eth tackles Ethereum fraud detection by integrating semantic transaction modeling and scalable graph learning, achieving state-of-the-art results.", "motivation": "Existing methods fail to capture transaction semantics, struggle with dataset homogeneity, and lack scalability in self-supervised graph learning for fraud detection.", "method": "LMAE4Eth combines a transaction-token contrastive language model (TxCLM), a masked account graph autoencoder (MAGAE), layer-neighbor sampling for scalability, and cross-attention fusion to produce discriminative account embeddings.", "result": "The method outperforms 21 baselines by over 10% F1-score on two datasets, demonstrating superior fraud detection performance.", "conclusion": "The proposed framework addresses the limitations of current Ethereum fraud detection methods by integrating transaction semantics and scalable graph learning, achieving significant performance improvements."}}
{"id": "2509.03875", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03875", "abs": "https://arxiv.org/abs/2509.03875", "authors": ["Ziyou Jiang", "Mingyang Li", "Guowei Yang", "Lin Shi", "Qing Wang"], "title": "VulRTex: A Reasoning-Guided Approach to Identify Vulnerabilities from Rich-Text Issue Report", "comment": "25 pages, 7 figures, submitting to TOSEM journal", "summary": "Software vulnerabilities exist in open-source software (OSS), and the\ndevelopers who discover these vulnerabilities may submit issue reports (IRs) to\ndescribe their details. Security practitioners need to spend a lot of time\nmanually identifying vulnerability-related IRs from the community, and the time\ngap may be exploited by attackers to harm the system. Previously, researchers\nhave proposed automatic approaches to facilitate identifying these\nvulnerability-related IRs, but these works focus on textual descriptions but\nlack the comprehensive analysis of IR's rich-text information. In this paper,\nwe propose VulRTex, a reasoning-guided approach to identify\nvulnerability-related IRs with their rich-text information. In particular,\nVulRTex first utilizes the reasoning ability of the Large Language Model (LLM)\nto prepare the Vulnerability Reasoning Database with historical IRs. Then, it\nretrieves the relevant cases from the prepared reasoning database to generate\nreasoning guidance, which guides LLM to identify vulnerabilities by reasoning\nanalysis on target IRs' rich-text information. To evaluate the performance of\nVulRTex, we conduct experiments on 973,572 IRs, and the results show that\nVulRTex achieves the highest performance in identifying the\nvulnerability-related IRs and predicting CWE-IDs when the dataset is\nimbalanced, outperforming the best baseline with +11.0% F1, +20.2% AUPRC, and\n+10.5% Macro-F1, and 2x lower time cost than baseline reasoning approaches.\nFurthermore, VulRTex has been applied to identify 30 emerging vulnerabilities\nacross 10 representative OSS projects in 2024's GitHub IRs, and 11 of them are\nsuccessfully assigned CVE-IDs, which illustrates VulRTex's practicality.", "AI": {"tldr": "VulRTex uses LLM reasoning and rich-text analysis to automatically detect vulnerability-related issue reports (IRs) in OSS, outperforming existing methods in accuracy and efficiency.", "motivation": "Manual identification of vulnerability-related IRs is time-consuming and risky, while prior auto-methods neglect rich-text information\u2014leaving systems vulnerable to exploits during lag periods.", "method": "VulRTex leverages LLMs to build a Vulnerability Reasoning Database from historical IRs, then uses retrieved cases as reasoning guidance for LLM to analyze target IRs' rich-text features, enabling context-aware vulnerability identification.", "result": "VulRTex achieves +11.0% F1, +20.2% AUPRC, and +10.5% Macro-F1 over baselines on an imbalanced dataset, with 2\u00d7 faster processing. It successfully identified 30 emerging vulnerabilities in 2024 GitHub IRs, assigning 11 CVE-IDs.", "conclusion": "The paper concludes that VulRTex effectively improves the identification of vulnerability-related IRs using reasoning-guided analysis with LLMs, demonstrating superior performance in imbalanced datasets and practical applicability in real-world emerging vulnerability detection."}}
{"id": "2509.03985", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03985", "abs": "https://arxiv.org/abs/2509.03985", "authors": ["Chuhan Zhang", "Ye Zhang", "Bowen Shi", "Yuyou Gan", "Tianyu Du", "Shouling Ji", "Dazhan Deng", "Yingcai Wu"], "title": "NeuroBreak: Unveil Internal Jailbreak Mechanisms in Large Language Models", "comment": "12 pages, 9 figures", "summary": "In deployment and application, large language models (LLMs) typically undergo\nsafety alignment to prevent illegal and unethical outputs. However, the\ncontinuous advancement of jailbreak attack techniques, designed to bypass\nsafety mechanisms with adversarial prompts, has placed increasing pressure on\nthe security defenses of LLMs. Strengthening resistance to jailbreak attacks\nrequires an in-depth understanding of the security mechanisms and\nvulnerabilities of LLMs. However, the vast number of parameters and complex\nstructure of LLMs make analyzing security weaknesses from an internal\nperspective a challenging task. This paper presents NeuroBreak, a top-down\njailbreak analysis system designed to analyze neuron-level safety mechanisms\nand mitigate vulnerabilities. We carefully design system requirements through\ncollaboration with three experts in the field of AI security. The system\nprovides a comprehensive analysis of various jailbreak attack methods. By\nincorporating layer-wise representation probing analysis, NeuroBreak offers a\nnovel perspective on the model's decision-making process throughout its\ngeneration steps. Furthermore, the system supports the analysis of critical\nneurons from both semantic and functional perspectives, facilitating a deeper\nexploration of security mechanisms. We conduct quantitative evaluations and\ncase studies to verify the effectiveness of our system, offering mechanistic\ninsights for developing next-generation defense strategies against evolving\njailbreak attacks.", "AI": {"tldr": "This paper introduces NeuroBreak, a jailbreak analysis system for LLMs using neuron-level analysis to reveal security vulnerabilities and improve defenses.", "motivation": "LLMs face increasing pressure from evolving jailbreak attack techniques due to their complex structures making internal security analysis challenging.", "method": "The system uses layer-wise representation probing analysis, semantic/function-based critical neuron analysis, and was designed with input from AI security experts to provide insights into LLM decision-making processes.", "result": "Quantitative evaluations and case studies confirm the system's effectiveness, providing actionable insights for next-generation defense strategies against jailbreak attacks.", "conclusion": "NeuroBreak offers a top-down analysis system with mechanistic insights to strengthen LLMs against jailbreak attacks."}}
{"id": "2509.03876", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03876", "abs": "https://arxiv.org/abs/2509.03876", "authors": ["Xingchu Chen", "Chengwei Liu", "Jialun Cao", "Yang Xiao", "Xinyue Cai", "Yeting Li", "Jingyi Shi", "Tianqi Sun", "Haiming Chen ang Wei Huo"], "title": "Vulnerability-Affected Versions Identification: How Far Are We?", "comment": null, "summary": "Identifying which software versions are affected by a vulnerability is\ncritical for patching, risk mitigation.Despite a growing body of tools, their\nreal-world effectiveness remains unclear due to narrow evaluation scopes often\nlimited to early SZZ variants, outdated techniques, and small or\ncoarse-graineddatasets. In this paper, we present the first comprehensive\nempirical study of vulnerability affected versions identification. We curate a\nhigh quality benchmark of 1,128 real-world C/C++ vulnerabilities and\nsystematically evaluate 12 representative tools from both tracing and matching\nparadigms across four dimensions: effectiveness at both vulnerability and\nversion levels, root causes of false positives and negatives, sensitivity to\npatch characteristics, and ensemble potential. Our findings reveal fundamental\nlimitations: no tool exceeds 45.0% accuracy, with key challenges stemming from\nheuristic dependence, limited semantic reasoning, and rigid matching logic.\nPatch structures such as add-only and cross-file changes further hinder\nperformance. Although ensemble strategies can improve results by up to 10.1%,\noverall accuracy remains below 60.0%, highlighting the need for fundamentally\nnew approaches. Moreover, our study offers actionable insights to guide tool\ndevelopment, combination strategies, and future research in this critical area.\nFinally, we release the replicated code and benchmark on our website to\nencourage future contributions.outdated techniques, and small or coarse grained\ndatasets.", "AI": {"tldr": "This paper shows current vulnerability analysis tools fail to accurately identify affected software versions due to fundamental limitations in their approaches.", "motivation": "Existing tools for vulnerability impact analysis are inadequately evaluated due to outdated methods and small datasets, leaving their real-world effectiveness unknown.", "method": "The team curated a benchmark of 1,128 real-world C/C++ vulnerabilities and evaluated 12 tools across four dimensions: vulnerability/version-level effectiveness, error root causes, patch sensitivity, and ensemble potential.", "result": "12 evaluated tools achieved below 45.0% accuracy, with ensemble methods improving results by up to 10.1% but still below 60.0%. Key challenges include heuristic dependence and poor handling of add-only/cross-file patch structures.", "conclusion": "The study highlights the urgent need for fundamentally new approaches to vulnerability affected versions identification as current tools face significant limitations in accuracy and require better semantic reasoning."}}
{"id": "2509.04010", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.04010", "abs": "https://arxiv.org/abs/2509.04010", "authors": ["Olivier Adjonyo", "Sebastien Bardin", "Emanuele Bellini", "Gilbert Ndollane Dione", "Mahmudul Faisal Al Ameen", "Robert Merget", "Frederic Recoules", "Yanis Sellami"], "title": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and Lessons Learned", "comment": "20 pages, 1 figure, to be published and presented at Sixth PQC\n  Standardization Conference by NIST, partially supported by the \"France 2030\"\n  government investment plan managed by the French National Research Agency,\n  under the reference ANR-22-PECY-0005", "summary": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools.", "AI": {"tldr": "This paper introduces an automated toolchain for analyzing constant-time compliance in cryptographic software, using TIMECOP, Binsec/Rel2, dudect, and RTLF. Applied to NIST PQDSS candidates, it identified 26 issues (5 fixed) and evaluates tool effectiveness for binary-level side-channel analysis.", "motivation": "The PQDSS standard requires cryptographic implementations to be free from timing/cache side-channels for both security and fair performance comparisons. Ensuring constant-time properties via manual binary analysis is impractical, necessitating automated tools.", "method": "The authors created an automated toolchain integrating TIMECOP, Binsec/Rel2 for binary-level constant-time policy checks, and dudect/RTLF for statistical side-channel analysis. They applied this to NIST PQDSS round 1 and 2 implementations to identify and report 26 issues.", "result": "26 implementation issues were reported across NIST PQDSS candidates, with 5 already fixed. The study compares tool strengths/weaknesses and demonstrates the practicality of automated analysis for compliance verification.", "conclusion": "The paper concludes that the developed toolchain effectively identifies constant-time compliance issues in cryptographic implementations, emphasizes the importance of automated analysis for both security and fair performance evaluation, and highlights the practical value of integrating multiple tools to detect side-channel vulnerabilities."}}
{"id": "2509.03896", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03896", "abs": "https://arxiv.org/abs/2509.03896", "authors": ["Zushuai Zhang", "Elliott Wen", "Ewan Tempero"], "title": "Analyzing Variations in Dependency Distributions Due to Code Smell Interactions", "comment": null, "summary": "The existence of dependencies between modules, such as classes, can mean that\nchanging a module triggers ripple effects that make maintenance complex and\ncostly, so the advice is to minimize dependencies between modules. It is\ntherefore important to understand the circumstances that can lead to increased\ndependencies. Recent studies suggest that code smells, which are\ncharacteristics of code that indicate potential design issues, may interact in\nways that increase dependencies between modules. In this study, we aim to\nconfirm previous observations and investigate whether and how the distribution\nof static dependencies changes in the presence of code smell interactions. We\nconducted a dependency analysis on 116 open-source Java systems to quantify the\ninteractions, comparing interactions among code smells and interactions between\ncode smells and non-code smells. Our results suggest that while interactions\nbetween code smell pairs are associated with increases in certain dependencies\nand decreases in others, overall, they are associated with an increase in total\ndependencies. For example, the median number of dependencies between Feature\nEnvy methods and Data Classes is seven times as many as when the methods are\nnon-Feature Envy methods, increasing from 1 to 7. This implies that developers\nshould prioritize addressing code smells that interact with each other, rather\nthan code smells that exist only in isolation.", "AI": {"tldr": "Code smell interactions significantly increase software dependencies; fixing interconnected code smells improves maintainability.", "motivation": "Code smell interactions may exacerbate module dependencies, but no prior work has quantified this relationship.", "method": "Dependency analysis of 116 Java systems comparing code smell interactions (e.g., Feature Envy + Data Class) vs. non-interactions.", "result": "Code smell pairs increase net dependencies (e.g., 1\u21927x dependencies between Feature Envy methods and Data Classes)", "conclusion": "Developers should prioritize addressing interacting code smells to reduce module dependencies and maintenance costs."}}
