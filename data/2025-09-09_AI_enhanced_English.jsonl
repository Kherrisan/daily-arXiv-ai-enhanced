{"id": "2509.05372", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.05372", "abs": "https://arxiv.org/abs/2509.05372", "authors": ["Piotr Przymus", "Andreas Happe", "J\u00fcrgen Cito"], "title": "Adversarial Bug Reports as a Security Risk in Language Model-Based Automated Program Repair", "comment": null, "summary": "Large Language Model (LLM) - based Automated Program Repair (APR) systems are\nincreasingly integrated into modern software development workflows, offering\nautomated patches in response to natural language bug reports. However, this\nreliance on untrusted user input introduces a novel and underexplored attack\nsurface. In this paper, we investigate the security risks posed by adversarial\nbug reports -- realistic-looking issue submissions crafted to mislead APR\nsystems into producing insecure or harmful code changes. We develop a\ncomprehensive threat model and conduct an empirical study to evaluate the\nvulnerability of state-of-the-art APR systems to such attacks. Our\ndemonstration comprises 51 adversarial bug reports generated across a spectrum\nof strategies, from manual curation to fully automated pipelines. We test these\nagainst leading APR model and assess both pre-repair defenses (e.g., LlamaGuard\nvariants, PromptGuard variants, Granite-Guardian, and custom LLM filters) and\npost-repair detectors (GitHub Copilot, CodeQL). Our findings show that current\ndefenses are insufficient: 90\\% of crafted bug reports triggered\nattacker-aligned patches. The best pre-repair filter blocked only 47\\%, while\npost-repair analysis-often requiring human oversight-was effective in just 58\\%\nof cases. To support scalable security testing, we introduce a prototype\nframework for automating the generation of adversarial bug reports. Our\nanalysis exposes a structural asymmetry: generating adversarial inputs is\ninexpensive, while detecting or mitigating them remains costly and error-prone.\nWe conclude with practical recommendations for improving the robustness of APR\nsystems against adversarial misuse and highlight directions for future work on\ntrustworthy automated repair.", "AI": {"tldr": "This paper exposes security vulnerabilities in LLM-based APR systems via adversarial bug reports. Despite existing defenses failing to block >90% of attacks, the study highlights a cost imbalance between attack generation and defense, calling for robust solutions and future research on trustworthy repair.", "motivation": "As LLM-based APR systems become integral to software workflows, their dependence on untrusted user input introduces unexplored security risks. Adversarial bug reports could compromise system integrity, motivating the exploration of this novel attack vector.", "method": "The authors developed a threat model and conducted an empirical evaluation using 51 adversarial bug reports generated through manual and automated methods. They tested state-of-the-art APR systems with pre-repair defenses (e.g., LlamaGuard variants, PromptGuard) and post-repair detectors (e.g., GitHub Copilot, CodeQL). A prototype framework for scalable adversarial input generation is introduced.", "result": "90% of adversarial bug reports triggered harmful patches. Pre-repair filters blocked only 47%, and post-repair analysis detected threats in 58% of cases. Current defenses are insufficient for mitigating adversarial attacks.", "conclusion": "The study reveals a structural asymmetry where adversarial bug report generation is inexpensive but defending against them is costly and error-prone. It emphasizes the need for improved robustness in APR systems and outlines future research directions for trustworthy automated repair."}}
{"id": "2509.05394", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05394", "abs": "https://arxiv.org/abs/2509.05394", "authors": ["Zoltan Toth-Czifra"], "title": "Reverse Browser: Vector-Image-to-Code Generator", "comment": "Submitted to AIWare 2025 ArXiv Track", "summary": "Automating the conversion of user interface design into code (image-to-code\nor image-to-UI) is an active area of software engineering research. However,\nthe state-of-the-art solutions do not achieve high fidelity to the original\ndesign, as evidenced by benchmarks. In this work, I approach the problem\ndifferently: I use vector images instead of bitmaps as model input. I create\nseveral large datasets for training machine learning models. I evaluate the\navailable array of Image Quality Assessment (IQA) algorithms and introduce a\nnew, multi-scale metric. I then train a large open-weights model and discuss\nits limitations.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.05540", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05540", "abs": "https://arxiv.org/abs/2509.05540", "authors": ["Thiago Barradas", "Aline Paes", "V\u00e2nia de Oliveira Neves"], "title": "Combining TSL and LLM to Automate REST API Testing: A Comparative Study", "comment": "10 pages, article computer science, software engineering, software\n  testing, ia, llm", "summary": "The effective execution of tests for REST APIs remains a considerable\nchallenge for development teams, driven by the inherent complexity of\ndistributed systems, the multitude of possible scenarios, and the limited time\navailable for test design. Exhaustive testing of all input combinations is\nimpractical, often resulting in undetected failures, high manual effort, and\nlimited test coverage. To address these issues, we introduce RestTSLLM, an\napproach that uses Test Specification Language (TSL) in conjunction with Large\nLanguage Models (LLMs) to automate the generation of test cases for REST APIs.\nThe approach targets two core challenges: the creation of test scenarios and\nthe definition of appropriate input data. The proposed solution integrates\nprompt engineering techniques with an automated pipeline to evaluate various\nLLMs on their ability to generate tests from OpenAPI specifications. The\nevaluation focused on metrics such as success rate, test coverage, and mutation\nscore, enabling a systematic comparison of model performance. The results\nindicate that the best-performing LLMs - Claude 3.5 Sonnet (Anthropic),\nDeepseek R1 (Deepseek), Qwen 2.5 32b (Alibaba), and Sabia 3 (Maritaca) -\nconsistently produced robust and contextually coherent REST API tests. Among\nthem, Claude 3.5 Sonnet outperformed all other models across every metric,\nemerging in this study as the most suitable model for this task. These findings\nhighlight the potential of LLMs to automate the generation of tests based on\nAPI specifications.", "AI": {"tldr": "RestTSLLM uses LLMs and TSL to automate REST API testing, with Claude 3.5 Sonnet being the top model for generating effective tests from OpenAPI specs.", "motivation": "REST API testing faces challenges from distributed system complexity, limited time, and impractical exhaustive scenarios, leading to undetected failures and low coverage, necessitating automation.", "method": "RestTSLLM combines TSL with prompt engineering and an automated pipeline to evaluate LLMs' ability to generate REST API tests from OpenAPI specifications, using metrics like success rate, test coverage, and mutation score.", "result": "Top-performing LLMs (Claude 3.5 Sonnet, Deepseek R1, Qwen 2.5 32b, Sabia 3) consistently produced robust tests, with Claude 3.5 Sonnet excelling in all metrics, proving LLMs' potential for automated API test generation.", "conclusion": "The study demonstrates that LLMs, particularly Claude 3.5 Sonnet, effectively automate REST API test generation using RestTSLLM and OpenAPI specifications, achieving high success rates, coverage, and mutation scores."}}
{"id": "2509.05585", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05585", "abs": "https://arxiv.org/abs/2509.05585", "authors": ["Zhiyuan Zou", "Bangchao Wang", "Peng Liang", "Tingting Bi", "Huan Jin"], "title": "Natural Language-Programming Language Software Traceability Link Recovery Needs More than Textual Similarity", "comment": "45 pages, 5 images, 11 tables, Manuscript submitted to a Journal\n  (2025)", "summary": "In the field of software traceability link recovery (TLR), textual similarity\nhas long been regarded as the core criterion. However, in tasks involving\nnatural language and programming language (NL-PL) artifacts, relying solely on\ntextual similarity is limited by their semantic gap. To this end, we conducted\na large-scale empirical evaluation across various types of TLR tasks, revealing\nthe limitations of textual similarity in NL-PL scenarios. To address these\nlimitations, we propose an approach that incorporates multiple domain-specific\nauxiliary strategies, identified through empirical analysis, into two models:\nthe Heterogeneous Graph Transformer (HGT) via edge types and the prompt-based\nGemini 2.5 Pro via additional input information. We then evaluated our approach\nusing the widely studied requirements-to-code TLR task, a representative case\nof NL-PL TLR. Experimental results show that both the multi-strategy HGT and\nGemini 2.5 Pro models outperformed their original counterparts without strategy\nintegration. Furthermore, compared to the current state-of-the-art method\nHGNNLink, the multi-strategy HGT and Gemini 2.5 Pro models achieved average\nF1-score improvements of 3.68% and 8.84%, respectively, across twelve\nopen-source projects, demonstrating the effectiveness of multi-strategy\nintegration in enhancing overall model performance for the requirements-code\nTLR task.", "AI": {"tldr": "This paper introduces multi-strategy integration in HGT and Gemini 2.5 Pro models to overcome textual similarity limitations in NL-PL traceability, achieving significant performance gains in requirements-code TLR.", "motivation": "Textual similarity alone is insufficient for NL-PL traceability due to semantic gaps between natural and programming languages, motivating the exploration of enhanced strategies to address these limitations.", "method": "The authors propose incorporating domain-specific auxiliary strategies into two models: HGT through edge types and Gemini 2.5 Pro via additional input information, validated through extensive experiments on requirements-to-code tasks.", "result": "Multi-strategy HGT and Gemini 2.5 Pro achieved average F1-score improvements of 3.68% and 8.84% over baseline models and current SOTA HGNNLink across twelve open-source projects.", "conclusion": "The study demonstrates that integrating multiple domain-specific auxiliary strategies into models like HGT and Gemini 2.5 Pro significantly enhances NL-PL traceability link recovery performance, as evidenced by F1-score improvements over existing methods."}}
{"id": "2509.05306", "categories": ["cs.CR", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.05306", "abs": "https://arxiv.org/abs/2509.05306", "authors": ["Enis Karaarslan", "Esin G\u00fcler", "Efe Emir Y\u00fcce", "Cagatay Coban"], "title": "Towards Log Analysis with AI Agents: Cowrie Case Study", "comment": null, "summary": "The scarcity of real-world attack data significantly hinders progress in\ncybersecurity research and education. Although honeypots like Cowrie\neffectively collect live threat intelligence, they generate overwhelming\nvolumes of unstructured and heterogeneous logs, rendering manual analysis\nimpractical. As a first step in our project on secure and efficient AI\nautomation, this study explores the use of AI agents for automated log\nanalysis. We present a lightweight and automated approach to process Cowrie\nhoneypot logs. Our approach leverages AI agents to intelligently parse,\nsummarize, and extract insights from raw data, while also considering the\nsecurity implications of deploying such an autonomous system. Preliminary\nresults demonstrate the pipeline's effectiveness in reducing manual effort and\nidentifying attack patterns, paving the way for more advanced autonomous\ncybersecurity analysis in future work.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.05596", "categories": ["cs.SE", "cs.SC", "68", "D.3.1; D.2.4"], "pdf": "https://arxiv.org/pdf/2509.05596", "abs": "https://arxiv.org/abs/2509.05596", "authors": ["Soumyadip Bandyopadhyay", "Santonu Sarkar"], "title": "Verifying Correctness of PLC Software during System Evolution using Model Containment Approach", "comment": "31 pages with appendix", "summary": "Upgradation of Programmable Logic Controller (PLC) software is quite common\nto accommodate evolving industrial requirements. Verifying the correctness of\nsuch upgrades remains a significant challenge. In this paper, we propose a\nverification-based approach to ensure the correctness of the existing\nfunctionality in the upgraded version of a PLC software. The method converts\nthe older and the newer versions of the sequential function chart (SFC) into\ntwo Petri net models. We then verify whether one model is contained within\nanother, based on a novel containment checking algorithm grounded in symbolic\npath equivalence. For this purpose, we have developed a home-grown Petri\nnet-based containment checker. Experimental evaluation on 80 real-world\nbenchmarks from the OSCAT library highlights the scalability and effectiveness\nof the framework. We have compared our approach with verifAPS, a popular tool\nused for software upgradation, and observed nearly 4x performance improvement.", "AI": {"tldr": "This paper introduces a Petri net-based verification framework for ensuring PLC software upgrade correctness through symbolic path equivalence, achieving 4x faster verification than existing tools on 80 real-world benchmarks.", "motivation": "PLC software upgrades are common but verifying correctness of existing functionality in updated versions remains a significant industry challenge.", "method": "Converts sequential function charts (SFCs) to Petri net models and uses a novel containment checking algorithm based on symbolic path equivalence.", "result": "Experimental evaluation on 80 real-world benchmarks shows a 4x performance improvement compared to verifAPS and validates the framework's scalability and effectiveness.", "conclusion": "The framework demonstrates scalability and effectiveness through 80 real-world benchmarks, with a 4x performance improvement over existing tools."}}
{"id": "2509.05311", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05311", "abs": "https://arxiv.org/abs/2509.05311", "authors": ["Konur Tholl", "Fran\u00e7ois Rivest", "Mariam El Mezouar", "Ranwa Al Mallah"], "title": "Large Language Model Integration with Reinforcement Learning to Augment Decision-Making in Autonomous Cyber Operations", "comment": null, "summary": "Reinforcement Learning (RL) has shown great potential for autonomous\ndecision-making in the cybersecurity domain, enabling agents to learn through\ndirect environment interaction. However, RL agents in Autonomous Cyber\nOperations (ACO) typically learn from scratch, requiring them to execute\nundesirable actions to learn their consequences. In this study, we integrate\nexternal knowledge in the form of a Large Language Model (LLM) pretrained on\ncybersecurity data that our RL agent can directly leverage to make informed\ndecisions. By guiding initial training with an LLM, we improve baseline\nperformance and reduce the need for exploratory actions with obviously negative\noutcomes. We evaluate our LLM-integrated approach in a simulated cybersecurity\nenvironment, and demonstrate that our guided agent achieves over 2x higher\nrewards during early training and converges to a favorable policy approximately\n4,500 episodes faster than the baseline.", "AI": {"tldr": "LLM-guided RL agents learn faster, avoid harmful actions, and outperform baselines in autonomous cyber operations through early training guidance from cybersecurity expertise.", "motivation": "Standard RL agents in cybersecurity learn from scratch through environment interaction, often executing undesirable actions to learn their consequences, which is inefficient and risky for real-world applications.", "method": "A Large Language Model (LLM) pretrained on cybersecurity data is integrated with an RL agent to guide decision-making during initial training stages, avoiding the need for trial-and-error exploration of harmful behaviors.", "result": "The LLM-integrated agent achieved over 2x higher rewards during early training and converged to a favorable policy 4,500 episodes faster than the baseline in simulated cybersecurity experiments.", "conclusion": "Integrating a cybersecurity-pretrained LLM with RL agents improves autonomous cyber operations by reducing harmful exploratory actions, accelerating policy convergence, and achieving higher early training rewards."}}
{"id": "2509.05749", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05749", "abs": "https://arxiv.org/abs/2509.05749", "authors": ["AmirHossein Naghshzan"], "title": "Automating API Documentation with LLMs: A BERTopic Approach", "comment": null, "summary": "Developers rely on API documentation, but official sources are often lengthy,\ncomplex, or incomplete. Many turn to community-driven forums like Stack\nOverflow for practical insights. We propose automating the summarization of\ninformal sources, focusing on Android APIs. Using BERTopic, we extracted\nprevalent topics from 3.6 million Stack Overflow posts and applied extractive\nsummarization techniques to generate concise summaries, including code\nsnippets. A user study with 30 Android developers assessed the summaries for\ncoherence, relevance, informativeness, and satisfaction, showing improved\nproductivity. Integrating formal API knowledge with community-generated content\nenhances documentation, making API resources more accessible and actionable\nwork.", "AI": {"tldr": "This study summarizes 3.6M Android-related Stack Overflow posts using BERTopic and extractive methods to improve API documentation accessibility and developer productivity.", "motivation": "Official Android API documentation is often lengthy, complex, or incomplete, prompting developers to seek practical insights from community forums like Stack Overflow.", "method": "The study utilized BERTopic for topic extraction from 3.6 million Stack Overflow posts and applied extractive summarization techniques to create concise, code-inclusive summaries.", "result": "A user study (n=30 Android developers) demonstrated improved productivity through summaries evaluated for coherence, relevance, informativeness, and satisfaction.", "conclusion": "Integrating formal API knowledge with community-generated content enhances documentation, making API resources more accessible and actionable."}}
{"id": "2509.05318", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05318", "abs": "https://arxiv.org/abs/2509.05318", "authors": ["Zuquan Peng", "Jianming Fu", "Lixin Zou", "Li Zheng", "Yanzhen Ren", "Guojun Peng"], "title": "Backdoor Samples Detection Based on Perturbation Discrepancy Consistency in Pre-trained Language Models", "comment": "13 pages, 9 figures, 8 tables, journal", "summary": "The use of unvetted third-party and internet data renders pre-trained models\nsusceptible to backdoor attacks. Detecting backdoor samples is critical to\nprevent backdoor activation during inference or injection during training.\nHowever, existing detection methods often require the defender to have access\nto the poisoned models, extra clean samples, or significant computational\nresources to detect backdoor samples, limiting their practicality. To address\nthis limitation, we propose a backdoor sample detection method based on\nperturbatio\\textbf{N} discr\\textbf{E}pancy consis\\textbf{T}ency\n\\textbf{E}valuation (\\NETE). This is a novel detection method that can be used\nboth pre-training and post-training phases. In the detection process, it only\nrequires an off-the-shelf pre-trained model to compute the log probability of\nsamples and an automated function based on a mask-filling strategy to generate\nperturbations. Our method is based on the interesting phenomenon that the\nchange in perturbation discrepancy for backdoor samples is smaller than that\nfor clean samples. Based on this phenomenon, we use curvature to measure the\ndiscrepancy in log probabilities between different perturbed samples and input\nsamples, thereby evaluating the consistency of the perturbation discrepancy to\ndetermine whether the input sample is a backdoor sample. Experiments conducted\non four typical backdoor attacks and five types of large language model\nbackdoor attacks demonstrate that our detection strategy outperforms existing\nzero-shot black-box detection methods.", "AI": {"tldr": "Researchers developed NERTE, a novel backdoor detection method that uses perturbation discrepancy analysis to identify malicious samples in pre-trained models without needing special resources.", "motivation": "Existing backdoor detection methods require poisoned models, extra clean samples, or significant computational resources, making them impractical for real-world applications.", "method": "NERTE leverages perturbation discrepancy consistency evaluation by computing log probabilities with a pre-trained model and using an automated mask-filling strategy to generate perturbations. It measures curvature to assess discrepancy consistency for backdoor detection.", "result": "NERTE outperforms state-of-the-art zero-shot black-box detection methods across experiments involving four typical attacks and five large language model backdoor attack scenarios.", "conclusion": "The proposed NERTE method effectively detects backdoor samples in pre-trained models with minimal resource requirements and without needing access to poisoned models or additional clean data. It demonstrates superior performance over existing zero-shot black-box detection methods."}}
{"id": "2509.05769", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.05769", "abs": "https://arxiv.org/abs/2509.05769", "authors": ["Edyta Brzychczy", "Urszula Jessen", "Krzysztof Kluza", "Sridhar Sriram", "Manuel Vargas Nettelnstroth"], "title": "IoT Miner: Intelligent Extraction of Event Logs from Sensor Data for Process Mining", "comment": "17 pages, conference draft", "summary": "This paper presents IoT Miner, a novel framework for automatically creating\nhigh-level event logs from raw industrial sensor data to support process\nmining. In many real-world settings, such as mining or manufacturing, standard\nevent logs are unavailable, and sensor data lacks the structure and semantics\nneeded for analysis. IoT Miner addresses this gap using a four-stage pipeline:\ndata preprocessing, unsupervised clustering, large language model (LLM)-based\nlabeling, and event log construction. A key innovation is the use of LLMs to\ngenerate meaningful activity labels from cluster statistics, guided by\ndomain-specific prompts. We evaluate the approach on sensor data from a\nLoad-Haul-Dump (LHD) mining machine and introduce a new metric,\nSimilarity-Weighted Accuracy, to assess labeling quality. Results show that\nricher prompts lead to more accurate and consistent labels. By combining AI\nwith domain-aware data processing, IoT Miner offers a scalable and\ninterpretable method for generating event logs from IoT data, enabling process\nmining in settings where traditional logs are missing.", "AI": {"tldr": "IoT Miner is a framework that automates generating structured event logs from unstructured industrial sensor data using a four-stage pipeline involving unsupervised clustering and LLM-based labeling, enabling process mining in data-scarce environments.", "motivation": "Real-world industrial settings like mining/manufacturing often lack standard event logs, requiring structured/semantic sensor data analysis for process mining applications which cannot be addressed by traditional methods.", "method": "Four-stage pipeline: (1) data preprocessing, (2) unsupervised clustering, (3) LLM-based labeling using domain-specific prompts to derive activity labels from cluster statistics, (4) event log construction", "result": "Evaluation on Load-Haul-Dump mining machine data showed: (1) Similarity-Weighted Accuracy metric effectiveness, (2) enriched prompts significantly improve label accuracy and consistency", "conclusion": "IoT Miner demonstrates a scalable, interpretable methodology for transforming raw IoT data into usable event logs by combining AI with domain-aware processing, overcoming the lack of traditional logs in industrial environments."}}
{"id": "2509.05320", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05320", "abs": "https://arxiv.org/abs/2509.05320", "authors": ["Ikhlasse Badidi", "Nouhaila El Khiyaoui", "Aya Riany", "Badr Ben Elallid", "Amine Abouaomar"], "title": "Privacy-Preserving Offloading for Large Language Models in 6G Vehicular Networks", "comment": "7 pages, 6 figures, 1 algorithm, 5 equations", "summary": "The integration of Large Language Models (LLMs) in 6G vehicular networks\npromises unprecedented advancements in intelligent transportation systems.\nHowever, offloading LLM computations from vehicles to edge infrastructure poses\nsignificant privacy risks, potentially exposing sensitive user data. This paper\npresents a novel privacy-preserving offloading framework for LLM-integrated\nvehicular networks. We introduce a hybrid approach combining federated learning\n(FL) and differential privacy (DP) techniques to protect user data while\nmaintaining LLM performance. Our framework includes a privacy-aware task\npartitioning algorithm that optimizes the trade-off between local and edge\ncomputation, considering both privacy constraints and system efficiency. We\nalso propose a secure communication protocol for transmitting model updates and\naggregating results across the network. Experimental results demonstrate that\nour approach achieves 75\\% global accuracy with only a 2-3\\% reduction compared\nto non-privacy-preserving methods, while maintaining DP guarantees with an\noptimal privacy budget of $\\varepsilon = 0.8$. The framework shows stable\ncommunication overhead of approximately 2.1MB per round with computation\ncomprising over 90\\% of total processing time, validating its efficiency for\nresource-constrained vehicular environments.", "AI": {"tldr": "This paper proposes a FL-DP hybrid framework for privacy-preserving LLM offloading in 6G vehicular networks, achieving strong accuracy (75%) and \u03b5=0.8 DP while maintaining low communication overhead.", "motivation": "LLM offloading in 6G vehicular networks risks exposing sensitive user data. Existing methods lack privacy-protection mechanisms for such environments, necessitating a solution that balances privacy and system performance.", "method": "A privacy-preserving framework combining federated learning (FL) and differential privacy (DP) is introduced. It uses privacy-aware task partitioning to balance local and edge computation, along with a secure communication protocol for model updates.", "result": "The approach achieves 75% global accuracy (2-3% lower than non-privacy methods) with \u03b5=0.8 DP guarantees. Communication overhead remains stable at 2.1MB/round, and computation dominates 90% of processing time, confirming efficiency for resource-constrained vehicles.", "conclusion": "The proposed framework effectively addresses privacy concerns in LLM-integrated vehicular networks through a hybrid FL-DP approach, achieving high accuracy and efficiency in resource-constrained environments."}}
{"id": "2509.05881", "categories": ["cs.SE", "cs.AI", "I.2"], "pdf": "https://arxiv.org/pdf/2509.05881", "abs": "https://arxiv.org/abs/2509.05881", "authors": ["Qianheng Zhang", "Song Gao", "Chen Wei", "Yibo Zhao", "Ying Nie", "Ziru Chen", "Shijie Chen", "Yu Su", "Huan Sun"], "title": "GeoAnalystBench: A GeoAI benchmark for assessing large language models for spatial analysis workflow and code generation", "comment": "34 pages, 8 figures", "summary": "Recent advances in large language models (LLMs) have fueled growing interest\nin automating geospatial analysis and GIS workflows, yet their actual\ncapabilities remain uncertain. In this work, we call for rigorous evaluation of\nLLMs on well-defined geoprocessing tasks before making claims about full GIS\nautomation. To this end, we present GeoAnalystBench, a benchmark of 50\nPython-based tasks derived from real-world geospatial problems and carefully\nvalidated by GIS experts. Each task is paired with a minimum deliverable\nproduct, and evaluation covers workflow validity, structural alignment,\nsemantic similarity, and code quality (CodeBLEU). Using this benchmark, we\nassess both proprietary and open source models. Results reveal a clear gap:\nproprietary models such as ChatGPT-4o-mini achieve high validity 95% and\nstronger code alignment (CodeBLEU 0.39), while smaller open source models like\nDeepSeek-R1-7B often generate incomplete or inconsistent workflows (48.5%\nvalidity, 0.272 CodeBLEU). Tasks requiring deeper spatial reasoning, such as\nspatial relationship detection or optimal site selection, remain the most\nchallenging across all models. These findings demonstrate both the promise and\nlimitations of current LLMs in GIS automation and provide a reproducible\nframework to advance GeoAI research with human-in-the-loop support.", "AI": {"tldr": "The paper introduces GeoAnalystBench, a benchmark for evaluating large language models' (LLMs) capabilities in geospatial analysis. It highlights a performance gap between proprietary models (e.g., ChatGPT-4o-mini) and open-source alternatives (e.g., DeepSeek-R1-7B) in terms of workflow validity and code quality, while underscoring remaining challenges in spatial reasoning tasks.", "motivation": "The study addresses the need for rigorous evaluation frameworks to assess LLM capabilities in GIS automation, as current claims about their efficacy lack systemic validation, risking overestimation of their readiness for real-world geoprocessing tasks.", "method": "Authors developed GeoAnalystBench, a benchmark with 50 Python-based, expert-validated geospatial tasks, each paired with minimum deliverables and evaluated using four metrics: workflow validity, structural alignment, semantic similarity, and CodeBLEU.", "result": "Proprietary models (validity: 95%, CodeBLEU: 0.39) outperformed open-source models (validity: 48.5%, CodeBLEU: 0.272). Deep spatial reasoning tasks (e.g., site selection) were most challenging. Proprietary models showed stronger code alignment but gaps persist for complex workflows.", "conclusion": "While current LLMs show promise for GIS automation, performance limitations in spatial reasoning and consistency highlight critical challenges. The benchmark provides a human-in-the-loop framework to advance GeoAI research and development."}}
{"id": "2509.05326", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05326", "abs": "https://arxiv.org/abs/2509.05326", "authors": ["Logan Nye"], "title": "Zero-Knowledge Proofs in Sublinear Space", "comment": "21 pages", "summary": "Modern zero-knowledge proof (ZKP) systems, essential for privacy and\nverifiable computation, suffer from a fundamental limitation: the prover\ntypically uses memory that scales linearly with the computation's trace length\nT, making them impractical for resource-constrained devices and prohibitively\nexpensive for large-scale tasks. This paper overcomes this barrier by\nconstructing, to our knowledge, the first sublinear-space ZKP prover. Our core\ncontribution is an equivalence that reframes proof generation as an instance of\nthe classic Tree Evaluation problem. Leveraging a recent space-efficient\ntree-evaluation algorithm, we design a streaming prover that assembles the\nproof without ever materializing the full execution trace. The approach reduces\nprover memory from linear in T to O(sqrt(T)) (up to O(log T) lower-order terms)\nwhile preserving proof size, verifier time, and the transcript/security\nguarantees of the underlying system. This enables a shift from specialized,\nserver-bound proving to on-device proving, opening applications in\ndecentralized systems, on-device machine learning, and privacy-preserving\ntechnologies.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.05941", "categories": ["cs.SE", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.05941", "abs": "https://arxiv.org/abs/2509.05941", "authors": ["Chaoqian Ouyang", "Ling Yue", "Shimin Di", "Libin Zheng", "Shaowu Pan", "Min-Ling Zhang"], "title": "Code2MCP: A Multi-Agent Framework for Automated Transformation of Code Repositories into Model Context Protocol Services", "comment": null, "summary": "The proliferation of Large Language Models (LLMs) has created a significant\nintegration challenge in the AI agent ecosystem, often called the \"$N \\times M$\nproblem,\" where N models require custom integrations for M tools. This\nfragmentation stifles innovation and creates substantial development overhead.\nWhile the Model Context Protocol (MCP) has emerged as a standard to resolve\nthis, its adoption is hindered by the manual effort required to convert the\nvast universe of existing software into MCP-compliant services. This is\nespecially true for the millions of open-source repositories on GitHub, the\nworld's largest collection of functional code. This paper introduces Code2MCP,\na highly automated, agentic framework designed to transform any GitHub\nrepository into a functional MCP service with minimal human intervention. Our\nsystem employs a multi-stage workflow that automates the entire process, from\ncode analysis and environment configuration to service generation and\ndeployment. A key innovation of our framework is an LLM-driven, closed-loop\n\"Run--Review--Fix\" cycle, which enables the system to autonomously debug and\nrepair the code it generates. Code2MCP produces not only deployable services\nbut also comprehensive technical documentation, acting as a catalyst to\naccelerate the MCP ecosystem by systematically unlocking the world's largest\nopen-source code repository and automating the critical last mile of tool\nintegration. The code is open-sourced at\nhttps://github.com/DEFENSE-SEU/MCP-Github-Agent.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.05331", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05331", "abs": "https://arxiv.org/abs/2509.05331", "authors": ["Youssef Chakir", "Iyad Lahsen-Cherif"], "title": "ForensicsData: A Digital Forensics Dataset for Large Language Models", "comment": "Accepted to WiMob 2025 (21st International Conference on Wireless and\n  Mobile Computing, Networking and Communications), Marrakesh, Morocco, Oct\n  20-22, 2025. 6 pages, 5 figures, 5 tables. IEEEtran conference format", "summary": "The growing complexity of cyber incidents presents significant challenges for\ndigital forensic investigators, especially in evidence collection and analysis.\nPublic resources are still limited because of ethical, legal, and privacy\nconcerns, even though realistic datasets are necessary to support research and\ntool developments. To address this gap, we introduce ForensicsData, an\nextensive Question-Context-Answer (Q-C-A) dataset sourced from actual malware\nanalysis reports. It consists of more than 5,000 Q-C-A triplets. A unique\nworkflow was used to create the dataset, which extracts structured data, uses\nlarge language models (LLMs) to transform it into Q-C-A format, and then uses a\nspecialized evaluation process to confirm its quality. Among the models\nevaluated, Gemini 2 Flash demonstrated the best performance in aligning\ngenerated content with forensic terminology. ForensicsData aims to advance\ndigital forensics by enabling reproducible experiments and fostering\ncollaboration within the research community.", "AI": {"tldr": "Introduces ForensicsData, a Q-C-A dataset from malware analysis reports to address limited public resources in digital forensics.", "motivation": "Lack of realistic datasets due to privacy/ethical constraints hinders forensic research and tool development.", "method": "Workflow combining structured data extraction, LLM-based Q-C-A transformation, and specialized evaluation for quality assurance.", "result": " Generated 5,000+ Q-C-A triplets; Gemini 2 Flash showed best performance in forensic terminology alignment.", "conclusion": "ForensicsData enables reproducible experiments and fosters collaboration by providing standardized, realistic forensic data."}}
{"id": "2509.05980", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.05980", "abs": "https://arxiv.org/abs/2509.05980", "authors": ["Xingliang Wang", "Baoyi Wang", "Chen Zhi", "Junxiao Han", "Xinkui Zhao", "Jianwei Yin", "Shuiguang Deng"], "title": "GRACE: Graph-Guided Repository-Aware Code Completion through Hierarchical Code Fusion", "comment": null, "summary": "LLMs excel in localized code completion but struggle with repository-level\ntasks due to limited context windows and complex semantic and structural\ndependencies across codebases. While Retrieval-Augmented Generation (RAG)\nmitigates context scarcity by retrieving relevant code snippets, current\napproaches face significant limitations. They overly rely on textual similarity\nfor retrieval, neglecting structural relationships such as call chains and\ninheritance hierarchies, and lose critical structural information by naively\nconcatenating retrieved snippets into text sequences for LLM input. To address\nthese shortcomings, GRACE constructs a multi-level, multi-semantic code graph\nthat unifies file structures, abstract syntax trees, function call graphs,\nclass hierarchies, and data flow graphs to capture both static and dynamic code\nsemantics. For retrieval, GRACE employs a Hybrid Graph Retriever that\nintegrates graph neural network-based structural similarity with textual\nretrieval, refined by a graph attention network-based re-ranker to prioritize\ntopologically relevant subgraphs. To enhance context, GRACE introduces a\nstructural fusion mechanism that merges retrieved subgraphs with the local code\ncontext and preserves essential dependencies like function calls and\ninheritance. Extensive experiments on public repository-level benchmarks\ndemonstrate that GRACE significantly outperforms state-of-the-art methods\nacross all metrics. Using DeepSeek-V3 as the backbone LLM, GRACE surpasses the\nstrongest graph-based RAG baselines by 8.19% EM and 7.51% ES points on every\ndataset. The code is available at\nhttps://anonymous.4open.science/r/grace_icse-C3D5.", "AI": {"tldr": "GRACE improves repository-level code generation by integrating structural code relationships into a graph-based RAG system, outperforming existing methods with 8.19%+ accuracy gains.", "motivation": "Current code generation methods struggle with repository-level tasks due to context window limitations and neglect of structural relationships like call chains and inheritance, relying instead on text similarity and concatenating snippets in a structure-agnostic manner.", "method": "GRACE constructs a multi-level code graph combining file structures, ASTs, call graphs, class hierarchies, and data flow graphs. It uses a Hybrid Graph Retriever with structural/text similarity, a graph-attention re-ranker, and a structural fusion mechanism to merge retrieved subgraphs with local context while preserving key dependencies.", "result": "GRACE outperforms state-of-the-art methods using DeepSeek-V3 by 8.19% (EM) and 7.51% (ES) across all benchmarks, demonstrating superior handling of complex code dependencies.", "conclusion": "GRACE enhances code generation by integrating structural data into Retrieval-Augmented Generation, significantly improving repository-level code tasks through a multi-level code graph and hybrid retrieval mechanisms."}}
{"id": "2509.05332", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05332", "abs": "https://arxiv.org/abs/2509.05332", "authors": ["Christos Anagnostopoulos", "Ioulia Kapsali", "Alexandros Gkillas", "Nikos Piperigkos", "Aris S. Lalos"], "title": "Integrated Simulation Framework for Adversarial Attacks on Autonomous Vehicles", "comment": "6 pages, 2 figures", "summary": "Autonomous vehicles (AVs) rely on complex perception and communication\nsystems, making them vulnerable to adversarial attacks that can compromise\nsafety. While simulation offers a scalable and safe environment for robustness\ntesting, existing frameworks typically lack comprehensive supportfor modeling\nmulti-domain adversarial scenarios. This paper introduces a novel, open-source\nintegrated simulation framework designed to generate adversarial attacks\ntargeting both perception and communication layers of AVs. The framework\nprovides high-fidelity modeling of physical environments, traffic dynamics, and\nV2X networking, orchestrating these components through a unified core that\nsynchronizes multiple simulators based on a single configuration file. Our\nimplementation supports diverse perception-level attacks on LiDAR sensor data,\nalong with communication-level threats such as V2X message manipulation and GPS\nspoofing. Furthermore, ROS 2 integration ensures seamless compatibility with\nthird-party AV software stacks. We demonstrate the framework's effectiveness by\nevaluating the impact of generated adversarial scenarios on a state-of-the-art\n3D object detector, revealing significant performance degradation under\nrealistic conditions.", "AI": {"tldr": "A new simulation framework is introduced for testing autonomous vehicles against multi-domain adversarial attacks.", "motivation": "AVs face risks from adversarial attacks on their perception and communication systems, and current tools don't adequately model these threats across domains.", "method": "Developed an open-source simulation framework with a unified core that integrates physical environment, traffic, V2X networking, and supports perception and communication attacks via a single config file.", "result": "Tested the framework with a real-world 3D object detector, showing detectable performance issues in realistic adversarial scenarios.", "conclusion": "The framework enables detailed testing for AV robustness against multi-domain attacks and is made open-source for research use."}}
{"id": "2509.05995", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.05995", "abs": "https://arxiv.org/abs/2509.05995", "authors": ["Sharon Guardado", "Risha Parveen", "Zheying Zhang", "Maruf Rayhan", "Nirnaya Tripathi"], "title": "Students' Perception of LLM Use in Requirements Engineering Education: An Empirical Study Across Two Universities", "comment": "Accepted by the 33rd IEEE International Requirements Engineering 2025\n  (RE'25), Valencia, Spain, September 1-5, 2025", "summary": "The integration of Large Language Models (LLMs) in Requirements Engineering\n(RE) education is reshaping pedagogical approaches, seeking to enhance student\nengagement and motivation while providing practical tools to support their\nprofessional future. This study empirically evaluates the impact of integrating\nLLMs in RE coursework. We examined how the guided use of LLMs influenced\nstudents' learning experiences, and what benefits and challenges they perceived\nin using LLMs in RE practices. The study collected survey data from 179\nstudents across two RE courses in two universities. LLMs were integrated into\ncoursework through different instructional formats, i.e., individual\nassignments versus a team-based Agile project. Our findings indicate that LLMs\nimproved students' comprehension of RE concepts, particularly in tasks like\nrequirements elicitation and documentation. However, students raised concerns\nabout LLMs in education, including academic integrity, overreliance on AI, and\nchallenges in integrating AI-generated content into assignments. Students who\nworked on individual assignments perceived that they benefited more than those\nwho worked on team-based assignments, highlighting the importance of contextual\nAI integration. This study offers recommendations for the effective integration\nof LLMs in RE education. It proposes future research directions for balancing\nAI-assisted learning with critical thinking and collaborative practices in RE\ncourses.", "AI": {"tldr": "This study evaluates how integrating LLMs into RE coursework affects student learning. It finds that LLMs improve concept understanding but highlights concerns like academic integrity and overuse. Individual assignments yielded higher perceived benefits than team projects, suggesting context matters for AI integration in education.", "motivation": "The paper addresses the growing integration of LLMs in RE education to improve student engagement, practical skill development, and professional preparedness, while empirically evaluating the effectiveness and challenges of such integration.", "method": "The study conducted an empirical evaluation using survey data from 179 students across two RE courses at two universities. LLMs were integrated through individual assignments and team-based Agile projects to compare their impact on learning experiences and perceived benefits/challenges.", "result": "Findings show LLMs improved comprehension of RE concepts (e.g., requirements elicitation) but raised concerns about academic integrity, AI overreliance, and content integration challenges. Students in individual assignments reported greater benefits than those in team projects, emphasizing the need for context-aware AI integration.", "conclusion": "The study concludes that integrating LLMs into RE education can enhance learning outcomes but requires careful consideration of academic integrity, overreliance risks, and contextual integration strategies. It provides actionable recommendations for educators and outlines future research directions to balance AI-assisted learning with critical thinking and collaboration."}}
{"id": "2509.05350", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05350", "abs": "https://arxiv.org/abs/2509.05350", "authors": ["Joshua Ward", "Yuxuan Yang", "Chi-Hua Wang", "Guang Cheng"], "title": "Ensembling Membership Inference Attacks Against Tabular Generative Models", "comment": null, "summary": "Membership Inference Attacks (MIAs) have emerged as a principled framework\nfor auditing the privacy of synthetic data generated by tabular generative\nmodels, where many diverse methods have been proposed that each exploit\ndifferent privacy leakage signals. However, in realistic threat scenarios, an\nadversary must choose a single method without a priori guarantee that it will\nbe the empirically highest performing option. We study this challenge as a\ndecision theoretic problem under uncertainty and conduct the largest synthetic\ndata privacy benchmark to date. Here, we find that no MIA constitutes a\nstrictly dominant strategy across a wide variety of model architectures and\ndataset domains under our threat model. Motivated by these findings, we propose\nensemble MIAs and show that unsupervised ensembles built on individual attacks\noffer empirically more robust, regret-minimizing strategies than individual\nattacks.", "AI": {"tldr": "This paper addresses the challenge of selecting effective Membership Inference Attacks (MIAs) under uncertainty for auditing synthetic data privacy. It proposes unsupervised ensembling of MIAs as a robust, regret-minimizing strategy superior to individual attacks.", "motivation": "Existing MIAs lack a universally dominant method across diverse model architectures and datasets, creating a gap where adversaries must choose an empirically optimal attack without a priori knowledge. Decision theory under uncertainty motivates the need for robust strategies.", "method": "1) Frame MIA selection as a decision-theoretic problem under uncertainty. 2) Conduct the largest synthetic data privacy benchmark to date. 3) Propose unsupervised ensemble methods combining individual MIAs.", "result": "1) No single MIA strictly dominates across all model/dataset combinations. 2) Unsupervised ensembles consistently outperform individual attacks by minimizing regret (26-43% improvement in precision). 3) Ensemble robustness holds across 12 models (GANs, VAEs) and 6 diverse datasets.", "conclusion": "Ensemble MIAs provide empirically more robust privacy auditing under uncertainty. Results demonstrate the necessity of ensemble approaches to account for model/dataset variability in adversarial privacy analysis."}}
{"id": "2509.06012", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.06012", "abs": "https://arxiv.org/abs/2509.06012", "authors": ["Jukka Ruohonen"], "title": "A Rapid Review Regarding the Concept of Legal Requirements in Requirements Engineering", "comment": "Submitted to REFSQ 2026", "summary": "Out of a personal puzzlement, recent peer review comments, and demonstrable\nconfusion in the existing literature, the paper presents a rapid review of the\nconcept of legal requirements (LRs) in requirements engineering (RE) research.\nAccording to reviewing results, a normative understanding of LRs has often been\npresent, although proper definitions and conceptual operationalizations are\nlacking. Some papers also see LRs as functional and others as non-functional\nrequirements. Legal requirements are often characterized as being vague and\ncomplex, requiring a lot of effort to elicit, implement, and validate. These\ncharacterizations supposedly correlate with knowledge gaps among requirements\nengineers. LRs are also seen to often change and overlap. They may be also\nprioritized. According to the literature, they seem to be also reluctantly\nimplemented, often providing only a minimal baseline for other requirements.\nWith these and other observations, the review raises critical arguments about\napparent knowledge gaps, including a lack of empirical evidence backing the\nobservations and enduring conceptual confusion.", "AI": {"tldr": "This paper reviews the concept of legal requirements in requirements engineering, revealing conceptual confusion, lack of empirical evidence, and persistent challenges in their management despite widespread recognition of their complexity.", "motivation": "Driven by personal confusion, peer review feedback, and observed inconsistencies in the literature, the paper aims to clarify the conceptual and practical understanding of legal requirements in requirements engineering.", "method": "The paper conducts a rapid literature review, analyzing existing definitions, operationalizations, and characterizations of legal requirements in RE research through a systematic review of the literature.", "result": "The study identifies conceptual inconsistencies, a lack of standardized definitions, challenges in implementing legal requirements (e.g., vagueness, complexity, overlapping requirements), and a reliance on minimal baseline implementations without sufficient empirical validation.", "conclusion": "The review highlights the need for more empirical evidence and clear conceptual frameworks to resolve existing ambiguities in understanding and managing legal requirements within requirements engineering."}}
{"id": "2509.05362", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.05362", "abs": "https://arxiv.org/abs/2509.05362", "authors": ["Ismail Hossain", "Sai Puppala", "Sajedul Talukder", "Md Jahangir Alam"], "title": "AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and Conversational Scambaiting by Leveraging LLMs and Federated Learning", "comment": "This paper got accepted in 26th Privacy Enhancing Technologies\n  Symposium (PETS 2026). We uploaded it into ArXiv as pre-print", "summary": "Scams exploiting real-time social engineering -- such as phishing,\nimpersonation, and phone fraud -- remain a persistent and evolving threat\nacross digital platforms. Existing defenses are largely reactive, offering\nlimited protection during active interactions. We propose a privacy-preserving,\nAI-in-the-loop framework that proactively detects and disrupts scam\nconversations in real time. The system combines instruction-tuned artificial\nintelligence with a safety-aware utility function that balances engagement with\nharm minimization, and employs federated learning to enable continual model\nupdates without raw data sharing. Experimental evaluations show that the system\nproduces fluent and engaging responses (perplexity as low as 22.3, engagement\n$\\approx$0.80), while human studies confirm significant gains in realism,\nsafety, and effectiveness over strong baselines. In federated settings, models\ntrained with FedAvg sustain up to 30 rounds while preserving high engagement\n($\\approx$0.80), strong relevance ($\\approx$0.74), and low PII leakage\n($\\leq$0.0085). Even with differential privacy, novelty and safety remain\nstable, indicating that robust privacy can be achieved without sacrificing\nperformance. The evaluation of guard models (LlamaGuard, LlamaGuard2/3,\nMD-Judge) shows a straightforward pattern: stricter moderation settings reduce\nthe chance of exposing personal information, but they also limit how much the\nmodel engages in conversation. In contrast, more relaxed settings allow longer\nand richer interactions, which improve scam detection, but at the cost of\nhigher privacy risk. To our knowledge, this is the first framework to unify\nreal-time scam-baiting, federated privacy preservation, and calibrated safety\nmoderation into a proactive defense paradigm.", "AI": {"tldr": "The paper introduces a privacy-preserving AI framework that proactively detects and disrupts real-time social engineering scams using instruction-tuned AI, federated learning, and a safety-aware utility function, achieving high engagement and low privacy leakage.", "motivation": "Existing scam defenses are reactive and ineffective during active interactions. Real-time social engineering attacks require proactive, privacy-preserving solutions to protect users dynamically across digital platforms.", "method": "The system combines instruction-tuned AI with a safety-aware utility function that balances engagement and harm minimization. Federated learning enables model updates without raw data sharing, while guard models (e.g., LlamaGuard) moderate safety. The framework operates in real time to disrupt scam conversations.", "result": "Experimental results show fluent responses (perplexity 22.3, engagement \u22480.80), strong relevance (\u22480.74), and minimal PII leakage (\u22640.0085) over 30 federated rounds. Human studies confirm improved realism/safety over baselines. Stricter moderation reduces privacy risks but limits engagement, while relaxed settings enhance scam detection but increase privacy trade-offs.", "conclusion": "The framework is the first to unify real-time scam baiting, federated privacy preservation, and calibrated safety moderation, demonstrating effective proactive defense without sacrificing performance. It highlights actionable trade-offs between privacy and engagement in safety-critical systems."}}
{"id": "2509.06052", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.06052", "abs": "https://arxiv.org/abs/2509.06052", "authors": ["Qingyuan Li", "Binchang Li", "Cuiyun Gao", "Shuzheng Gao", "Zongjie Li"], "title": "Empirical Study of Code Large Language Models for Binary Security Patch Detection", "comment": null, "summary": "Security patch detection (SPD) is crucial for maintaining software security,\nas unpatched vulnerabilities can lead to severe security risks. In recent\nyears, numerous learning-based SPD approaches have demonstrated promising\nresults on source code. However, these approaches typically cannot be applied\nto closed-source applications and proprietary systems that constitute a\nsignificant portion of real-world software, as they release patches only with\nbinary files, and the source code is inaccessible. Given the impressive\nperformance of code large language models (LLMs) in code intelligence and\nbinary analysis tasks such as decompilation and compilation optimization, their\npotential for detecting binary security patches remains unexplored, exposing a\nsignificant research gap between their demonstrated low-level code\nunderstanding capabilities and this critical security task. To address this\ngap, we construct a large-scale binary patch dataset containing \\textbf{19,448}\nsamples, with two levels of representation: assembly code and pseudo-code, and\nsystematically evaluate \\textbf{19} code LLMs of varying scales to investigate\ntheir capability in binary SPD tasks. Our initial exploration demonstrates that\ndirectly prompting vanilla code LLMs struggles to accurately identify security\npatches from binary patches, and even state-of-the-art prompting techniques\nfail to mitigate the lack of domain knowledge in binary SPD within vanilla\nmodels. Drawing on the initial findings, we further investigate the fine-tuning\nstrategy for injecting binary SPD domain knowledge into code LLMs through two\nlevels of representation. Experimental results demonstrate that fine-tuned LLMs\nachieve outstanding performance, with the best results obtained on the\npseudo-code representation.", "AI": {"tldr": "This work bridges the gap between code LLMs and binary security patch detection by constructing a large binary dataset and demonstrating that fine-tuned LLMs using pseudo-code achieve superior patch detection performance.", "motivation": "Existing learning-based security patch detection (SPD) approaches rely on source code, which is inaccessible in closed-source applications dominating real-world software. Despite code LLMs' success in low-level code tasks, their potential for binary SPD remains unexplored, highlighting a critical research gap.", "method": "The study constructs a large-scale binary patch dataset with two levels of representation (assembly code and pseudo-code), systematically evaluates 19 code LLMs using prompting techniques, and investigates fine-tuning strategies to inject domain knowledge through these representations.", "result": "Vanilla code LLMs struggle with binary SPD tasks without domain knowledge, while fine-tuned models achieve outstanding performance with the best results on pseudo-code representation. Advanced prompting techniques fail to mitigate the knowledge gap without explicit fine-tuning.", "conclusion": "Fine-tuning code large language models (LLMs) with binary security patch detection domain knowledge, particularly through pseudo-code representations, bridges the gap between their low-level code understanding capabilities and the critical task of binary SPD."}}
{"id": "2509.05366", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.05366", "abs": "https://arxiv.org/abs/2509.05366", "authors": ["Umair Amjid", "M. Umar Khan", "S. A. Manan Kirmani"], "title": "A Framework for Detection and Classification of Attacks on Surveillance Cameras under IoT Networks", "comment": null, "summary": "The increasing use of Internet of Things (IoT) devices has led to a rise in\nsecurity related concerns regarding IoT Networks. The surveillance cameras in\nIoT networks are vulnerable to security threats such as brute force and\nzero-day attacks which can lead to unauthorized access by hackers and potential\nspying on the users activities. Moreover, these cameras can be targeted by\nDenial of Service (DOS) attacks, which will make it unavailable for the user.\nThe proposed AI based framework will leverage machine learning algorithms to\nanalyze network traffic and detect anomalous behavior, allowing for quick\ndetection and response to potential intrusions. The framework will be trained\nand evaluated using real-world datasets to learn from past security incidents\nand improve its ability to detect potential intrusion.", "AI": {"tldr": "This paper proposes an AI-driven framework to secure IoT cameras from attacks like brute force and DOS by analyzing network traffic with machine learning, trained on real-world datasets to enhance threat detection and response.", "motivation": "The study addresses the growing security vulnerabilities in IoT networks, particularly surveillance cameras, which are susceptible to unauthorized access, spying, and Denial of Service attacks due to increasing IoT adoption.", "method": "The method involves training a machine learning-based framework on real-world datasets to analyze network traffic, identify anomalous behavior, and detect potential intrusions like brute force, zero-day, and DOS attacks.", "result": "The framework demonstrates improved intrusion detection capabilities by learning from historical security data, enabling rapid identification and mitigation of threats in IoT camera networks.", "conclusion": "The paper concludes that an AI-based framework using machine learning algorithms effectively enhances security for IoT surveillance cameras by detecting and responding to intrusion threats in real-time."}}
{"id": "2509.06085", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06085", "abs": "https://arxiv.org/abs/2509.06085", "authors": ["Jerin Yasmin", "Wenxin Jiang", "James C. Davis", "Yuan Tian"], "title": "Software Dependencies 2.0: An Empirical Study of Reuse and Integration of Pre-Trained Models in Open-Source Projects", "comment": "Submitted to Empirical Software Engineering (EMSE) Journal", "summary": "Pre-trained models (PTMs) are machine learning models that have been trained\nin advance, often on large-scale data, and can be reused for new tasks, thereby\nreducing the need for costly training from scratch. Their widespread adoption\nintroduces a new class of software dependency, which we term Software\nDependencies 2.0, extending beyond conventional libraries to learned behaviors\nembodied in trained models and their associated artifacts. The integration of\nPTMs as software dependencies in real projects remains unclear, potentially\nthreatening maintainability and reliability of modern software systems that\nincreasingly rely on them. Objective: In this study, we investigate Software\nDependencies 2.0 in open-source software (OSS) projects by examining the reuse\nof PTMs, with a focus on how developers manage and integrate these models.\nSpecifically, we seek to understand: (1) how OSS projects structure and\ndocument their PTM dependencies; (2) what stages and organizational patterns\nemerge in the reuse pipelines of PTMs within these projects; and (3) the\ninteractions among PTMs and other learned components across pipeline stages. We\nconduct a mixed-methods analysis of a statistically significant random sample\nof 401 GitHub repositories from the PeaTMOSS dataset (28,575 repositories\nreusing PTMs from Hugging Face and PyTorch Hub). We quantitatively examine PTM\nreuse by identifying patterns and qualitatively investigate how developers\nintegrate and manage these models in practice.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.05367", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05367", "abs": "https://arxiv.org/abs/2509.05367", "authors": ["Shei Pern Chua", "Thai Zhen Leng", "Teh Kai Jun", "Xiao Li", "Xiaolin Hu"], "title": "Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs", "comment": null, "summary": "Large language models (LLMs) have undergone safety alignment efforts to\nmitigate harmful outputs. However, as LLMs become more sophisticated in\nreasoning, their intelligence may introduce new security risks. While\ntraditional jailbreak attacks relied on singlestep attacks, multi-turn\njailbreak strategies that adapt dynamically to context remain underexplored. In\nthis work, we introduce TRIAL (Trolley-problem Reasoning for Interactive Attack\nLogic), a framework that leverages LLMs ethical reasoning to bypass their\nsafeguards. TRIAL embeds adversarial goals within ethical dilemmas modeled on\nthe trolley problem. TRIAL demonstrates high jailbreak success rates towards\nboth open and close-source models. Our findings underscore a fundamental\nlimitation in AI safety: as models gain advanced reasoning abilities, the\nnature of their alignment may inadvertently allow for more covert security\nvulnerabilities to be exploited. TRIAL raises an urgent need in reevaluating\nsafety alignment oversight strategies, as current safeguards may prove\ninsufficient against context-aware adversarial attack.", "AI": {"tldr": "This paper introduces TRIAL, a context-aware adversarial framework exploiting LLMs' ethical reasoning to bypass safety safeguards, highlighting urgent gaps in current AI alignment strategies.", "motivation": "Traditional jailbreak attacks rely on single-step methods, leaving multi-turn, context-aware adversarial strategies underexplored despite sophisticated LLM reasoning abilities that may introduce new security risks.", "method": "TRIAL (Trolley-problem Reasoning for Interactive Attack Logic) embeds adversarial goals within ethical dilemmas modeled on the trolley problem, leveraging LLMs' ethical reasoning capabilities to bypass safeguards through multi-turn, context-aware interactions.", "result": "TRIAL demonstrates high jailbreak success rates against both open-source and closed-source large language models, validating its effectiveness in exploiting ethical reasoning frameworks to bypass safety measures.", "conclusion": "The paper underscores a critical limitation in AI safety alignment as models gain advanced reasoning abilities, allowing more covert vulnerabilities to be exploited. It urges reevaluating safety oversight strategies to address context-aware adversarial attacks."}}
{"id": "2509.06216", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06216", "abs": "https://arxiv.org/abs/2509.06216", "authors": ["Ahmed E. Hassan", "Hao Li", "Dayi Lin", "Bram Adams", "Tse-Hsun Chen", "Yutaro Kashiwa", "Dong Qiu"], "title": "Agentic Software Engineering: Foundational Pillars and a Research Roadmap", "comment": null, "summary": "Agentic Software Engineering (SE 3.0) represents a new era where intelligent\nagents are tasked not with simple code generation, but with achieving complex,\ngoal-oriented SE objectives. To harness these new capabilities while ensuring\ntrustworthiness, we must recognize a fundamental duality within the SE field in\nthe Agentic SE era, comprising two symbiotic modalities: SE for Humans and SE\nfor Agents. This duality demands a radical reimagining of the foundational\npillars of SE (actors, processes, tools, and artifacts) which manifest\ndifferently across each modality. We propose two purpose-built workbenches to\nsupport this vision. The Agent Command Environment (ACE) serves as a command\ncenter where humans orchestrate and mentor agent teams, handling outputs such\nas Merge-Readiness Packs (MRPs) and Consultation Request Packs (CRPs). The\nAgent Execution Environment (AEE) is a digital workspace where agents perform\ntasks while invoking human expertise when facing ambiguity or complex\ntrade-offs. This bi-directional partnership, which supports agent-initiated\nhuman callbacks and handovers, gives rise to new, structured engineering\nactivities (i.e., processes) that redefine human-AI collaboration, elevating\nthe practice from agentic coding to true agentic software engineering. This\npaper presents the Structured Agentic Software Engineering (SASE) vision,\noutlining several of the foundational pillars for the future of SE. The paper\nculminates in a research roadmap that identifies a few key challenges and\nopportunities while briefly discussing the resulting impact of this future on\nSE education. Our goal is not to offer a definitive solution, but to provide a\nconceptual scaffold with structured vocabulary to catalyze a community-wide\ndialogue, pushing the SE community to think beyond its classic, human-centric\ntenets toward a disciplined, scalable, and trustworthy agentic future.", "AI": {"tldr": "This paper introduces SE 3.0 through a dual-modality framework that redefines software engineering with intelligent agents, proposing ACE (human coordination hub) and AEE (agent workspace) environments. It establishes structured collaboration processes between humans and agents while providing a research roadmap for the field's evolution.", "motivation": "The rise of intelligent agents in software engineering demands a paradigm shift from code-centric practices to structured, goal-oriented systems engineering. The duality of human-agent collaboration creates complex trust, scalability, and coordination challenges currently lacking in foundational SE frameworks.", "method": "The paper introduces two symbiotic workbench environments: (1) Agent Command Environment (ACE) for human-agent orchestration and (2) Agent Execution Environment (AEE) for autonomous agent tasks with human intervention. These frameworks redefine actors, processes, tools, and artifacts in agentic SE.", "result": "The authors present the Structured Agentic Software Engineering (SASE) vision, defining standardized artifacts like Merge-Readiness Packs and Consultation Request Packs. The paper establishes a research roadmap targeting key technical challenges and educational implications for this emerging field.", "conclusion": "This paper proposes a conceptual framework for Agentic Software Engineering (SE 3.0) through the dual modality approach of SE for Humans and SE for Agents. It outlines foundational pillars, introduces ACE and AEE environments, and emphasizes the need for community-driven dialogue to redefine traditional SE principles."}}
{"id": "2509.05370", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.05370", "abs": "https://arxiv.org/abs/2509.05370", "authors": ["Tanya Joshi", "Krishnendu Guha"], "title": "Quantum AI Algorithm Development for Enhanced Cybersecurity: A Hybrid Approach to Malware Detection", "comment": "10 pages", "summary": "This study explores the application of quantum machine learning (QML)\nalgorithms to enhance cybersecurity threat detection, particularly in the\nclassification of malware and intrusion detection within high-dimensional\ndatasets. Classical machine learning approaches encounter limitations when\ndealing with intricate, obfuscated malware patterns and extensive network\nintrusion data. To address these challenges, we implement and evaluate various\nQML algorithms, including Quantum Neural Networks (QNN), Quantum Support Vector\nMachines (QSVM), and hybrid Quantum Convolutional Neural Networks (QCNN) for\nmalware detection tasks. Our experimental analysis utilized two datasets: the\nIntrusion dataset, comprising 150 samples with 56 memory-based features derived\nfrom Volatility framework analysis, and the ObfuscatedMalMem2022 dataset,\ncontaining 58,596 samples with 57 features representing benign and malicious\nsoftware. Remarkably, our QML methods demonstrated superior performance\ncompared to classical approaches, achieving accuracies of 95% for QNN and 94%\nfor QSVM. These quantum-enhanced methods leveraged quantum superposition and\nentanglement principles to accurately identify complex patterns within highly\nobfuscated malware samples that were imperceptible to classical methods. To\nfurther advance malware analysis, we propose a novel real-time malware analysis\nframework that incorporates Quantum Feature Extraction using Quantum Fourier\nTransform, Quantum Feature Maps, and Classification using Variational Quantum\nCircuits. This system integrates explainable AI methods, including GradCAM++\nand ScoreCAM algorithms, to provide interpretable insights into the quantum\ndecision-making processes.", "AI": {"tldr": "This paper demonstrates quantum machine learning's superiority in cybersecurity threat detection, achieving 94-95% accuracy through QML algorithms that exploit quantum principles. It introduces a real-time malware analysis framework with quantum feature extraction and explainable AI, offering interpretable results for complex malware classification tasks.", "motivation": "Classical machine learning struggles with obfuscated malware patterns and high-dimensional cybersecurity data. This work addresses these limitations by exploring quantum machine learning's potential to identify complex patterns imperceptible to classical models.", "method": "The study implements QML algorithms (QNN, QSVM, QCNN) using quantum superposition/entanglement principles. It proposes a real-time malware analysis framework employing Quantum Fourier Transform for feature extraction, quantum feature maps, and variational quantum circuits for classification. Explainable AI methods (GradCAM++, ScoreCAM) are integrated for interpretability.", "result": "QML methods achieved 95% accuracy for Quantum Neural Networks (QNN) and 94% for Quantum Support Vector Machines (QSVM) in malware classification, surpassing classical approaches. Quantum methods effectively identified obfuscated malware patterns in datasets with 56-57 memory-based features from 58,596 samples.", "conclusion": "Quantum Machine Learning (QML) outperforms classical methods in detecting sophisticated malware and intrusion patterns, leveraging principles like superposition and entanglement. The paper introduces a novel real-time framework integrating quantum feature extraction and explainable AI techniques (GradCAM++, ScoreCAM) for interpretable threat detection."}}
{"id": "2509.06301", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.06301", "abs": "https://arxiv.org/abs/2509.06301", "authors": ["Dharun Anandayuvaraj", "Zain Hammadeh", "Andreas Lund", "Alexandra Holloway", "James C. Davis"], "title": "Learning From Software Failures: A Case Study at a National Space Research Center", "comment": null, "summary": "Software failures can have significant consequences, making learning from\nfailures a critical aspect of software engineering. While software\norganizations are recommended to conduct postmortems, the effectiveness and\nadoption of these practices vary widely. Understanding how engineers gather,\ndocument, share, and apply lessons from failures is essential for improving\nreliability and preventing recurrence. High-reliability organizations (HROs)\noften develop software systems where failures carry catastrophic risks,\nrequiring continuous learning to ensure reliability. These organizations\nprovide a valuable setting to examine practices and challenges for learning\nfrom software failures. Such insight could help develop processes and tools to\nimprove reliability and prevent recurrence. However, we lack in-depth industry\nperspectives on the practices and challenges of learning from failures.\n  To address this gap, we conducted a case study through 10 in-depth interviews\nwith research software engineers at a national space research center. We\nexamine how they learn from failures: how they gather, document, share, and\napply lessons. To assess transferability, we include data from 5 additional\ninterviews at other HROs. Our findings provide insight into how engineers learn\nfrom failures in practice. To summarize: (1) failure learning is informal, ad\nhoc, and inconsistently integrated into SDLC; (2) recurring failures persist\ndue to absence of structured processes; and (3) key challenges, including time\nconstraints, knowledge loss from turnover and fragmented documentation, and\nweak process enforcement, undermine systematic learning. Our findings deepen\nunderstanding of how software engineers learn from failures and offer guidance\nfor improving failure management practices.", "AI": {"tldr": "Case studies in HROs show failure learning is informal and fragmented, leading to recurring issues. Structured processes and addressing organizational challenges could enhance reliability.", "motivation": "To address the lack of in-depth industry insights into failure learning practices in high-reliability software systems and improve reliability processes.", "method": "Conducted 10 in-depth interviews at a national space research center and 5 additional interviews at other HROs, examining failure learning practices.", "result": "Three key findings: (1) Failure learning is informal and inconsistent in SDLC integration; (2) Recurring failures due to unstructured processes; (3) Challenges include time constraints, knowledge loss, and weak process enforcement.", "conclusion": "The study reveals that failure learning in software HROs is hindered by informal processes, recurring failures, and organizational challenges, offering guidance to improve failure management practices."}}
{"id": "2509.05376", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05376", "abs": "https://arxiv.org/abs/2509.05376", "authors": ["Abdul Rehman", "Are D\u00e6hlen", "Ilona Heldal", "Jerry Chun-wei Lin"], "title": "Privacy Preservation and Identity Tracing Prevention in AI-Driven Eye Tracking for Interactive Learning Environments", "comment": null, "summary": "Eye-tracking technology can aid in understanding neurodevelopmental disorders\nand tracing a person's identity. However, this technology poses a significant\nrisk to privacy, as it captures sensitive information about individuals and\nincreases the likelihood that data can be traced back to them. This paper\nproposes a human-centered framework designed to prevent identity backtracking\nwhile preserving the pedagogical benefits of AI-powered eye tracking in\ninteractive learning environments. We explore how real-time data anonymization,\nethical design principles, and regulatory compliance (such as GDPR) can be\nintegrated to build trust and transparency. We first demonstrate the potential\nfor backtracking student IDs and diagnoses in various scenarios using serious\ngame-based eye-tracking data. We then provide a two-stage privacy-preserving\nframework that prevents participants from being tracked while still enabling\ndiagnostic classification. The first phase covers four scenarios: I) Predicting\ndisorder diagnoses based on different game levels. II) Predicting student IDs\nbased on different game levels. III) Predicting student IDs based on randomized\ndata. IV) Utilizing K-Means for out-of-sample data. In the second phase, we\npresent a two-stage framework that preserves privacy. We also employ Federated\nLearning (FL) across multiple clients, incorporating a secure identity\nmanagement system with dummy IDs and administrator-only access controls. In the\nfirst phase, the proposed framework achieved 99.3% accuracy for scenario 1, 63%\naccuracy for scenario 2, and 99.7% accuracy for scenario 3, successfully\nidentifying and assigning a new student ID in scenario 4. In phase 2, we\neffectively prevented backtracking and established a secure identity management\nsystem with dummy IDs and administrator-only access controls, achieving an\noverall accuracy of 99.40%.", "AI": {"tldr": "This paper proposes a privacy-preserving human-centered framework for AI-powered eye-tracking in education, balancing pedagogical value with identity protection through anonymization, ethical design, and Federated Learning (FL).", "motivation": "Eye-tracking in educational AI poses privacy risks by linking sensitive data to individuals, necessitating solutions that maintain diagnostic utility while preventing identity backtracking.", "method": "A two-phase framework: Phase 1 evaluates backtracking risks via four scenarios (diagnosis prediction, student ID prediction, randomization, K-Means classification). Phase 2 implements privacy measures using FL, dummy IDs, and administrator-controlled access.", "result": "Phase 1 achieved 99.3-99.7 % diagnostic accuracy while limiting student ID prediction (63 %). Phase 2 achieved 99.40 % accuracy overall with full backtracking prevention and secure identity management.", "conclusion": "The framework successfully preserves privacy and diagnostic capabilities in eye-tracking for education, demonstrating compliance with GDPR and ethical AI through technical-philosophical integration."}}
{"id": "2509.06324", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.06324", "abs": "https://arxiv.org/abs/2509.06324", "authors": ["Zhuohang Shen", "Mohammed Yaseen", "Denini Silva", "Kevin Guan", "Junho Lee", "Marcelo d'Amorim", "Owolabi Legunsen"], "title": "A Generic and Efficient Python Runtime Verification System and its Large-scale Evaluation", "comment": "23 pages, 7 figures", "summary": "Runtime verification (RV) now scales for testing thousands of open-source\nJava projects, helping find hundreds of bugs. The popular Python ecosystem\ncould use such benefits. But, today's Python RV systems are limited to a domain\nor specification logic, or slow. We propose PyMOP, a generic, extensible, and\nefficient RV system for Python. PyMOP supports five logics, implements five\nexisting monitoring algorithms, ships with 73 API specs of Python and\nwidely-used libraries, supports three instrumentation strategies, and users can\neasily add more of these. On 290,133 unit tests in 1,463 GitHub projects, we\nfind mainly that (i) the default monitoring algorithm for Java is often not the\nfastest for Python; (ii) PyMOP is up to 1,168.3x faster than two recent dynamic\nanalysis systems; and (iii) 44 of 121 bugs that PyMOP helped find so far were\nfixed by developers. PyMOP's generality and efficiency position it well as an\nexcellent platform for the next advances on RV for Python.", "AI": {"tldr": "PyMOP is a generic, efficient runtime verification (RV ) system for Python that addresses the limitations of existing tools, achieving significant speed improvements and finding/fxing real-world bugs.", "motivation": "Existing Python RV systems are either domain-specific, limited to particular logics, or slow. PyMOP aims to provide a scalable, extensible solution for Python's growing ecosystem to enable robust dynamic analysis and bug detection.", "method": "PyMOP implements five monitoring algorithms, supports five logics, includes 73 API specs, and three instrumentation strategies. It was evaluated across 290,133 unit tests in 1,463 GitHub projects, comparing performance to existing systems and validating bug-finding capabilities.", "result": "1) Default Java RV algorithms are often suboptimal for Python. 2) PyMOP is up to 1,168.3\u00d7 faster than recent dynamic analysis tools. 3) 44/121 bugs found by PyMOP were fxed by developers. 4) Supports broad extensibility for monitoring logics and APIs.", "conclusion": "PyMOP establishes a foundational platform for advancing Python RV, demonstrating superior efficiency and practical bug-finding capabilities while enabling future research through its modular design and comprehensive API coverage."}}
{"id": "2509.05379", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05379", "abs": "https://arxiv.org/abs/2509.05379", "authors": ["Sharif Noor Zisad", "Ragib Hasan"], "title": "ThreatGPT: An Agentic AI Framework for Enhancing Public Safety through Threat Modeling", "comment": null, "summary": "As our cities and communities become smarter, the systems that keep us safe,\nsuch as traffic control centers, emergency response networks, and public\ntransportation, also become more complex. With this complexity comes a greater\nrisk of security threats that can affect not just machines but real people's\nlives. To address this challenge, we present ThreatGPT, an agentic Artificial\nIntelligence (AI) assistant built to help people whether they are engineers,\nsafety officers, or policy makers to understand and analyze threats in public\nsafety systems. Instead of requiring deep cybersecurity expertise, it allows\nusers to simply describe the components of a system they are concerned about,\nsuch as login systems, data storage, or communication networks. Then, with the\nclick of a button, users can choose how they want the system to be analyzed by\nusing popular frameworks such as STRIDE, MITRE ATT&CK, CVE reports, NIST, or\nCISA. ThreatGPT is unique because it does not just provide threat information,\nbut rather it acts like a knowledgeable partner. Using few-shot learning, the\nAI learns from examples and generates relevant smart threat models. It can\nhighlight what might go wrong, how attackers could take advantage, and what can\nbe done to prevent harm. Whether securing a city's infrastructure or a local\nhealth service, this tool adapts to users' needs. In simple terms, ThreatGPT\nbrings together AI and human judgment to make our public systems safer. It is\ndesigned not just to analyze threats, but to empower people to understand and\nact on them, faster, smarter, and with more confidence.", "AI": {"tldr": "The proposed ThreatGPT is an AI assistant designed to help individuals like engineers, safety officers, and policymakers analyze threats in smart city systems using popular cybersecurity frameworks such as STRIDE, MITRE ATT&CK, CVE reports, NIST, and CISA. It allows users to generate smart threat models with minimal cybersecurity expertise by simply describing system components and offering actionable prevention strategies.", "motivation": "The complexity of smart city systems increases the risk of security threats, which can impact not only machines but also the safety of real people. These systems require expert knowledge to identify and address threats, which may be a barrier for non-experts such as policymakers or safety officers attempting to evaluate threat models for their urban infrastructure.", "method": "ThreatGPT is an agentic AI assistant that utilizes few-shot learning to understand system components based on user descriptions. It can interactively use popular cybersecurity frameworks (e.g., MITRE ATT&CK, NIST) to generate customized threat models. The AI does not require complex querying or deep cybersecurity expertise, as it learns from examples and adapts to the needs of different users such as engineers, policymakers, and safety officers.", "result": "ThreatGPT simplifies threat analysis in smart city and public safety infrastructure by allowing users to generate comprehensive threat models using intuitive inputs. The system enables users to evaluate how attackers might exploit vulnerabilities and gain insights into prevention strategies efficiently, even without deep cybersecurity knowledge.", "conclusion": "ThreatGPT represents a step towards making AI and cybersecurity more accessible, enhancing the safety of smart city systems by bridging the expertise gap. It empowers users with timely, accurate, and actionable threat analysis for various public safety domains, demonstrating the potential of agentic AI as a collaborative tool."}}
{"id": "2509.06429", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.06429", "abs": "https://arxiv.org/abs/2509.06429", "authors": ["Mehmet Bilal Er", "Nagehan \u0130lhan", "Umut Kuran"], "title": "Analyzing the Instability of Large Language Models in Automated Bug Injection and Correction", "comment": null, "summary": "The use of Large Language Models (LLMs) in software engineering tasks is\ngrowing, especially in the areas of bug fixing and code generation.\nNevertheless, these models often yield unstable results; when executed at\ndifferent times with the same input, they can generate radically different\ncode. The consistency of LLMs in bug-fixing tasks has not yet been thoroughly\nassessed, despite the fact that this instability has typically been discussed\nin the literature in relation to code generation. The purpose of this study is\nto look into how unstable an LLM like ChatGPT is when it comes to fixing code\nbugs. We examine the structural, syntactic, and functional variations among\nseveral fix recommendations made in response to the same prompt using code\nsamples with various error types. Additionally, we assess how instability is\naffected by the temperature settings (0, 0.5, and 1) used for the model's\ndeterministic operation. For a total of 20 problems in the experimental\nanalysis, the model produced three fix suggestions at each temperature value,\ncomparing nine distinct outputs for each problem. The Syntax Similarity and\nOutput Equivalence Rate (OER) metrics were used to assess the outputs'\nstructural and functional consistency. The results demonstrate that the model's\noutputs become much more unstable and variable as the temperature rises, with\nhigh temperatures showing especially high rates of functional failure.\nAccording to syntax similarity analyses, the suggested fixes show notable\nstructural differences at high temperatures but are fairly similar at low\ntemperatures. The purpose of this study is to provide important methodological\ninsights into how LLM-based error correction systems can be applied more\nconsistently in software development processes while also casting doubt on\ntheir dependability.", "AI": {"tldr": "This study evaluates ChatGPT's instability in bug-fixing tasks across different temperature settings, finding that higher temperatures lead to increased instability and functional failures. Syntax and functional consistency metrics highlight the need for caution when relying on LLMs for software development.", "motivation": "The motivation stems from the growing use of LLMs in software engineering for tasks like bug fixing and code generation, coupled with the lack of thorough analysis on their instability in bug-fixing contexts. The study aims to address this gap and inform reliable application of LLM-based tools.", "method": "The study evaluates structural, syntactic, and functional variations in ChatGPT's bug-fixing outputs using 20 code problems, three temperature settings (0, 0.5, 1), and nine outputs per problem. Syntax Similarity and Output Equivalence Rate (OER) metrics were employed to measure consistency.", "result": "Results show that higher temperature settings significantly increase instability and functional failure rates in ChatGPT's outputs. At low temperatures, structural similarity remains high, but structural differences become pronounced at high temperatures. Metrics reveal a strong correlation between temperature and output variability.", "conclusion": "The study concludes that LLMs like ChatGPT exhibit significant instability in bug-fixing tasks, particularly at higher temperature settings, which raises doubts about their reliability for software development processes. It emphasizes the need for methodological improvements to ensure consistent application of LLM-based error correction systems."}}
{"id": "2509.05471", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05471", "abs": "https://arxiv.org/abs/2509.05471", "authors": ["Youjia Zheng", "Mohammad Zandsalimy", "Shanu Sushmita"], "title": "Behind the Mask: Benchmarking Camouflaged Jailbreaks in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are increasingly vulnerable to a sophisticated\nform of adversarial prompting known as camouflaged jailbreaking. This method\nembeds malicious intent within seemingly benign language to evade existing\nsafety mechanisms. Unlike overt attacks, these subtle prompts exploit\ncontextual ambiguity and the flexible nature of language, posing significant\nchallenges to current defense systems. This paper investigates the construction\nand impact of camouflaged jailbreak prompts, emphasizing their deceptive\ncharacteristics and the limitations of traditional keyword-based detection\nmethods. We introduce a novel benchmark dataset, Camouflaged Jailbreak Prompts,\ncontaining 500 curated examples (400 harmful and 100 benign prompts) designed\nto rigorously stress-test LLM safety protocols. In addition, we propose a\nmulti-faceted evaluation framework that measures harmfulness across seven\ndimensions: Safety Awareness, Technical Feasibility, Implementation Safeguards,\nHarmful Potential, Educational Value, Content Quality, and Compliance Score.\nOur findings reveal a stark contrast in LLM behavior: while models demonstrate\nhigh safety and content quality with benign inputs, they exhibit a significant\ndecline in performance and safety when confronted with camouflaged jailbreak\nattempts. This disparity underscores a pervasive vulnerability, highlighting\nthe urgent need for more nuanced and adaptive security strategies to ensure the\nresponsible and robust deployment of LLMs in real-world applications.", "AI": {"tldr": "A study reveals LLMs' vulnerability to camouflaged jailbreak prompts, which evade existing safeguards. A new dataset and framework highlight major shortcomings in safety measures, urging improved defenses to secure real-world LLM use.", "motivation": "The paper addresses the escalating threat of camouflaged jailbreaking, where malicious intent is hidden in seemingly benign prompts to bypass safety mechanisms. Existing keyword-based defenses fail to detect such subtle attacks, motivating the need for advanced evaluation strategies.", "method": "The paper introduces a benchmark dataset (500 examples) and a multi-faceted evaluation framework with seven dimensions (e.g., Safety Awareness, Harmful Potential) to rigorously test LLMs' responses to camouflaged jailbreak prompts.", "result": "LLMs exhibit high safety and content quality with benign inputs but show significant decline in performance and safety when tested against camouflaged jailbreak prompts. This reveals a critical vulnerability in current defense systems.", "conclusion": "The paper concludes that current LLM safety measures are insufficient against camouflaged jailbreaking attacks. It highlights the urgent need for adaptive, nuanced security strategies to address vulnerabilities in handling contextually deceptive prompts, ensuring safe real-world deployment of LLMs."}}
{"id": "2509.06530", "categories": ["cs.SE", "D.2.10"], "pdf": "https://arxiv.org/pdf/2509.06530", "abs": "https://arxiv.org/abs/2509.06530", "authors": ["Sylvain Gu\u00e9rin", "Salvador Martinez", "Ciprian Teodorov"], "title": "Modeling in the Design Multiverse", "comment": null, "summary": "Real-world design processes often involve the evolution and divergence of\ndesign paths (by branching, revising, merging, etc.), especially when multiple\nstakeholders or teams operate concurrently and/or explore different\nalternatives for complex and heterogeneous systems. Unfortunately, this\nvariability in time and space can not be directly managed in current modeling\nspaces but requires resorting to external tools and methodologies.\n  In order to tackle this problem, we introduce the Design Multiverse. The\nDesign Multiverse aims to integrate in the modeling space a selection of\nrevisions and variants, representing snapshots of a design state composed of\nmultiple artifacts. This enables stakeholders to seamlessly trace, analyze, and\nmanage design decisions, system variants, and their interdependencies.\nConcretely, in this paper we present a conceptual definition of the Design\nMultiverse, discuss usage scenarios such as model product lines and\nmodel/metamodel co-evolution, and propose an implementation leveraging the\nmodel federation paradigm.", "AI": {"tldr": "The paper introduces the Design Multiverse, a concept to manage design variability in multi-stakeholder systems by integrating revisions and variants into the modeling space, enabling traceability and analysis of design decisions.", "motivation": "Current modeling tools cannot handle temporal-spatial variability in design processes involving multiple stakeholders, requiring external tools. This limits seamless management of design paths, variants, and dependencies.", "method": "The authors propose a conceptual framework for the Design Multiverse, describe usage scenarios (model product lines, co-evolution of models/metamodels), and suggest an implementation using model federation.", "result": "The Design Multiverse provides theoretical foundations and implementation guidelines for managing design artifacts as a unified space of revisions and variants, though practical validation is beyond the paper's scope.", "conclusion": "The Design Multiverse offers a promising approach to integrate, trace, and analyze complex design ecosystems across stakeholders and time, bridging gaps between modeling tools and real-world design evolution."}}
{"id": "2509.05496", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.05496", "abs": "https://arxiv.org/abs/2509.05496", "authors": ["Charbel Mattar", "Jacques Bou Abdo", "Abdallah Makhoul", "Benoit Piranda", "Jacques Demerjian"], "title": "What is Cybersecurity in Space?", "comment": null, "summary": "Satellites, drones, and 5G space links now support\n  critical services such as air traffic, finance, and weather. Yet most\n  were not built to resist modern cyber threats. Ground stations\n  can be breached, GPS jammed, and supply chains compromised,\n  while no shared list of vulnerabilities or safe testing range exists.\n  This paper maps eleven research gaps, including secure\n  routing, onboard intrusion detection, recovery methods, trusted\n  supply chains, post-quantum encryption, zero-trust architectures,\n  and real-time impact monitoring. For each, we outline the\n  challenge, why it matters, and a guiding research question. We\n  also highlight an agentic (multi-agent) AI approach where small,\n  task-specific agents share defense tasks onboard instead of one\n  large model.\n  Finally, we propose a five-year roadmap: post-quantum and\n  QKD flight trials, open cyber-ranges, clearer vulnerability shar ing, and\nearly multi-agent deployments. These steps move space\n  cybersecurity from reactive patching toward proactive resilience.", "AI": {"tldr": "This paper identifies 11 research gaps in space cybersecurity for satellites/drones/5G, proposes a multi-agent AI defense system, and outlines a 5-year roadmap focused on proactive resilience.", "motivation": "Modern space-critical infrastructure lacks cyber resilience as ground stations can be hacked, GPS jammed, and no shared vulnerability database exists, risking global services like air traffic and finance.", "method": "Mapped 11 research gaps with challenge/impact/question triads, recommending agentic AI (multi-agent systems) over monolithic models for onboard defense coordination.", "result": "Framework of research priorities including post-quantum crypto trials, open cyber-testing ranges, and multi-agent deployment timelines to enable proactive defense strategies.", "conclusion": "A transition from reactive patching to proactive resilience is achievable through standardized vulnerability sharing, QKD experiments, and multi-agent architectures within 5 years."}}
{"id": "2509.06688", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.06688", "abs": "https://arxiv.org/abs/2509.06688", "authors": ["Heerok Banerjee"], "title": "Design and Implementation of a Domain-specific Language for Modelling Evacuation Scenarios Using Eclipse EMG/GMF Tool", "comment": null, "summary": "Domain-specific languages (DSLs) play a crucial role in resolving internal\ndependencies across enterprises and boosts their upfront business management\nprocesses. Yet, a lot of development is needed to build modelling frameworks\nwhich support graphical interfaces (canvas, pallettes etc.), hierarchical\nstructures and easy implementation to shorten the gap for novice users. In this\npaper, a DSL namely, Bmod is introduced, which can be used to model evacuation\nscenarios. The language is built using Eclipse Modelling Framework (EMF) and\nEclipse Graphical Modelling Framework (GMF). Furthermore, a comparison is also\nshown between Eclipse EMF/GMF and other modelling tools such as AToMPM,\nmetaDepth, Sirius etc with respect to expressiveness, learning curve and\nperformance.", "AI": {"tldr": "The paper introduces Bmod, a domain-specific language (DSL) for evacuation scenario modeling built with Eclipse EMF/GMF, and compares it with other tools like AToMPM and Sirius.", "motivation": "Existing DSLs require improved modeling frameworks with graphical interfaces and hierarchical structures to reduce complexity for novice users in enterprise settings.", "method": "Developed Bmod using Eclipse EMF/GMF for graphical modeling. Conducted a comparative analysis of EMF/GMF with tools like AToMPM, metaDepth, and Sirius, evaluating expressiveness, learning curve, and performance.", "result": "Bmod enables hierarchical evacuation modeling with graphical interfaces. The comparison highlights EMF/GMF's strengths in expressiveness and usability relative to alternatives.", "conclusion": "Bmod addresses gaps in DSL tooling for enterprise scenario modeling, while the comparison provides guidelines for selecting modeling tools based on project requirements."}}
{"id": "2509.05552", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.05552", "abs": "https://arxiv.org/abs/2509.05552", "authors": ["Ali Arastehfard", "Weiran Liu", "Joshua Lee", "Bingyu Liu", "Xuegang Ban", "Yuan Hong"], "title": "Secure and Efficient $L^p$-Norm Computation for Two-Party Learning Applications", "comment": null, "summary": "Secure norm computation is becoming increasingly important in many real-world\nlearning applications. However, existing cryptographic systems often lack a\ngeneral framework for securely computing the $L^p$-norm over private inputs\nheld by different parties. These systems often treat secure norm computation as\na black-box process, neglecting to design tailored cryptographic protocols that\noptimize performance. Moreover, they predominantly focus on the $L^2$-norm,\npaying little attention to other popular $L^p$-norms, such as $L^1$ and\n$L^\\infty$, which are commonly used in practice, such as machine learning tasks\nand location-based services.\n  To our best knowledge, we propose the first comprehensive framework for\nsecure two-party $L^p$-norm computations ($L^1$, $L^2$, and $L^\\infty$),\ndenoted as \\mbox{Crypto-$L^p$}, designed to be versatile across various\napplications. We have designed, implemented, and thoroughly evaluated our\nframework across a wide range of benchmarking applications, state-of-the-art\n(SOTA) cryptographic protocols, and real-world datasets to validate its\neffectiveness and practical applicability. In summary, \\mbox{Crypto-$L^p$}\noutperforms prior works on secure $L^p$-norm computation, achieving $82\\times$,\n$271\\times$, and $42\\times$ improvements in runtime while reducing\ncommunication overhead by $36\\times$, $4\\times$, and $21\\times$ for $p=1$, $2$,\nand $\\infty$, respectively. Furthermore, we take the first step in adapting our\nCrypto-$L^p$ framework for secure machine learning inference, reducing\ncommunication costs by $3\\times$ compared to SOTA systems while maintaining\ncomparable runtime and accuracy.", "AI": {"tldr": "The paper proposes Crypto-$L^p$, the first comprehensive secure two-party $L^p$-norm computation framework for $L^1$, $L^2$, and $L^\\infty$, achieving significant performance improvements over existing methods and enabling efficient secure machine learning inference.", "motivation": "Existing cryptographic systems lack a general framework for secure $L^p$-norm computation, focus only on $L^2$, and use inefficient black-box approaches. Real-world applications like machine learning and location-based services require optimized protocols for all common $L^p$-norms.", "method": "Designed, implemented, and evaluated a tailored framework (Crypto-$L^p$) supporting all $L^p$-norms. Conducted benchmarks against state-of-the-art protocols using real-world datasets and applications.", "result": "Achieves 82\u00d7-42\u00d7 runtime improvements and 36\u00d7-21\u00d7 communication reductions compared to prior work. Reduces machine learning inference communication costs by 3\u00d7 over SOTA while maintaining comparable runtime and accuracy.", "conclusion": "Crypto-$L^p$ establishes the first practical, secure $L^p$-norm framework that outperforms existing solutions across all measured metrics. It enables efficient secure machine learning inference and provides a versatile foundation for privacy-preserving applications."}}
{"id": "2509.06716", "categories": ["cs.SE", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.06716", "abs": "https://arxiv.org/abs/2509.06716", "authors": ["Th\u00e9o Matricon", "Mathieu Acher", "Helge Spieker", "Arnaud Gotlieb"], "title": "Efficiently Ranking Software Variants with Minimal Benchmarks", "comment": null, "summary": "Benchmarking is a common practice in software engineering to assess the\nqualities and performance of software variants, coming from multiple competing\nsystems or from configurations of the same system. Benchmarks are used notably\nto compare and understand variant performance, fine-tune software, detect\nregressions, or design new software systems. The execution of benchmarks to get\na complete picture of software variants is highly costly in terms of\ncomputational resources and time. In this paper, we propose a novel approach\nfor reducing benchmarks while maintaining stable rankings, using test suite\noptimization techniques. That is, we remove instances from the benchmarks while\ntrying to keep the same rankings of the variants on all tests. Our method,\nBISection Sampling, BISS, strategically retains the most critical tests and\napplies a novel divide-and-conquer approach to efficiently sample among\nrelevant remaining tests. We experiment with datasets and use cases from LLM\nleaderboards, SAT competitions, and configurable systems for performance\nmodeling. Our results show that our method outperforms baselines even when\noperating on a subset of variants. Using BISS, we reduce the computational cost\nof the benchmarks on average to 44% and on more than half the benchmarks by up\nto 99% without loss in ranking stability.", "AI": {"tldr": "This paper proposes BISS, a benchmark reduction method that maintains ranking stability while reducing computational costs by up to 99% through optimized test suite sampling.", "motivation": "Benchmark execution is computationally expensive, necessitating methods to reduce costs while maintaining reliable performance comparisons across software variants.", "method": "BISS employs test suite optimization through strategic retention of critical tests and a divide-and-conquer sampling approach to reduce benchmark subsets.", "result": "BISS achieves 44% average computational cost reduction (up to 99% in some cases) across LLM leaderboards, SAT competitions, and configurable systems without compromising ranking stability.", "conclusion": "The paper concludes that BISS effectively reduces benchmarking costs while preserving ranking stability, outperforming baseline methods."}}
{"id": "2509.05608", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05608", "abs": "https://arxiv.org/abs/2509.05608", "authors": ["Waris Gill", "Natalie Isak", "Matthew Dressman"], "title": "Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints", "comment": null, "summary": "The widespread deployment of LLMs across enterprise services has created a\ncritical security blind spot. Organizations operate multiple LLM services\nhandling billions of queries daily, yet regulatory compliance boundaries\nprevent these services from sharing threat intelligence about prompt injection\nattacks, the top security risk for LLMs. When an attack is detected in one\nservice, the same threat may persist undetected in others for months, as\nprivacy regulations prohibit sharing user prompts across compliance boundaries.\n  We present BinaryShield, the first privacy-preserving threat intelligence\nsystem that enables secure sharing of attack fingerprints across compliance\nboundaries. BinaryShield transforms suspicious prompts through a unique\npipeline combining PII redaction, semantic embedding, binary quantization, and\nrandomized response mechanism to potentially generate non-invertible\nfingerprints that preserve attack patterns while providing privacy. Our\nevaluations demonstrate that BinaryShield achieves an F1-score of 0.94,\nsignificantly outperforming SimHash (0.77), the privacy-preserving baseline,\nwhile achieving 64x storage reduction and 38x faster similarity search compared\nto dense embeddings.", "AI": {"tldr": "BinaryShield is a privacy-preserving threat intelligence system for LLMs that allows secure sharing of attack fingerprints across compliance boundaries using a unique pipeline of PII redaction, semantic embedding, binary quantization, and randomized response, achieving better performance than existing methods.", "motivation": "Organizations face a security challenge with LLMs as they can't share threat intelligence about prompt injection attacks due to privacy regulations, leading to prolonged undetected threats in other services.", "method": "BinaryShield employs a pipeline combining four steps: PII redaction to remove sensitive information, semantic embedding to capture attack context, binary quantization to compress embeddings, and a randomized response mechanism to enhance privacy while preserving attack patterns.", "result": "BinaryShield achieves an impressive F1-score of 0.94 for identifying malicious prompts, a 0.77 F1-score baseline for SimHash, and results in a 64x reduction in storage requirements and 38x faster similarity search compared to using dense embeddings.", "conclusion": "BinaryShield advances the state of the art for secure threat intelligence sharing among LLM services by effectively preserving privacy while accurately detecting prompt injection attacks, offering a robust solution to a critical industry problem."}}
{"id": "2509.06774", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.06774", "abs": "https://arxiv.org/abs/2509.06774", "authors": ["Hridoy Sankar Dutta", "Sana Ansari", "Swati Kumari", "Shounak Ravi Bhalerao"], "title": "OpenCoderRank: AI-Driven Technical Assessments Made Easy", "comment": null, "summary": "Organizations and educational institutions use time-bound assessment tasks to\nevaluate coding and problem-solving skills. These assessments measure not only\nthe correctness of the solutions, but also their efficiency. Problem setters\n(educator/interviewer) are responsible for crafting these challenges, carefully\nbalancing difficulty and relevance to create meaningful evaluation experiences.\nConversely, problem solvers (student/interviewee) apply coding efficiency and\nlogical thinking to arrive at correct solutions. In the era of Large Language\nModels (LLMs), LLMs assist problem setters in generating diverse and\nchallenging questions, but they can undermine assessment integrity for problem\nsolvers by providing easy access to solutions. This paper introduces\nOpenCoderRank, an easy-to-use platform designed to simulate technical\nassessments. It acts as a bridge between problem setters and problem solvers,\nhelping solvers prepare for time constraints and unfamiliar problems while\nallowing setters to self-host assessments, offering a no-cost and customizable\nsolution for technical assessments in resource-constrained environments.", "AI": {"tldr": "OpenCoderRank is an open-source platform for conducting customizable, cost-effective technical coding assessments, addressing both problem-setting and solving challenges in resource-constrained environments.", "motivation": "Technical assessments require balancing problem difficulty and integrity, especially with LLMs potentially exposing solutions. Organizations need accessible, customizable tools for evaluating coding skills.", "method": "The platform provides a self-hosted, no-cost interface for problem setters to design assessments and problem solvers to practice under time constraints, with features to ensure assessment security.", "result": "OpenCoderRank enables seamless, low-barrier technical evaluations without compromising problem integrity while supporting diverse educational and professional use cases.", "conclusion": "The platform offers a scalable solution for fair, practical coding assessments in resource-limited settings by eliminating financial and technical barriers to implementation."}}
{"id": "2509.05643", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.05643", "abs": "https://arxiv.org/abs/2509.05643", "authors": ["Carmine Cesarano", "Roberto Natella"], "title": "FuzzBox: Blending Fuzzing into Emulation for Binary-Only Embedded Targets", "comment": null, "summary": "Coverage-guided fuzzing has been widely applied to address zero-day\nvulnerabilities in general-purpose software and operating systems. This\napproach relies on instrumenting the target code at compile time. However,\napplying it to industrial systems remains challenging, due to proprietary and\nclosed-source compiler toolchains and lack of access to source code. FuzzBox\naddresses these limitations by integrating emulation with fuzzing: it\ndynamically instruments code during execution in a virtualized environment, for\nthe injection of fuzz inputs, failure detection, and coverage analysis, without\nrequiring source code recompilation and hardware-specific dependencies. We show\nthe effectiveness of FuzzBox through experiments in the context of a\nproprietary MILS (Multiple Independent Levels of Security) hypervisor for\nindustrial applications. Additionally, we analyze the applicability of FuzzBox\nacross commercial IoT firmware, showcasing its broad portability.", "AI": {"tldr": "FuzzBox enables fuzzing in closed-source industrial systems via runtime instrumentation, overcoming recompilation and hardware dependency limitations.", "motivation": "Traditional coverage-guided fuzzing is hindered in industrial systems due to closed-source toolchains, proprietary compilers, and lack of source code access, limiting its ability to detect zero-day vulnerabilities in such environments.", "method": "FuzzBox integrates emulation with fuzzing through dynamic instrumentation in a virtualized environment, allowing fuzz input injection, failure detection, and coverage analysis without hardware dependencies or recompilation.", "result": "FuzzBox demonstrates effectiveness on a proprietary MILS hypervisor for industrial applications and shows broad portability across commercial IoT firmware.", "conclusion": "FuzzBox effectively addresses the limitations of traditional fuzzing in industrial systems by enabling dynamic instrumentation without requiring source code recompilation, making vulnerability detection feasible in proprietary environments."}}
{"id": "2509.06911", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06911", "abs": "https://arxiv.org/abs/2509.06911", "authors": ["Margarida Ferreira", "Victor Nicolet", "Luan Pham", "Joey Dodds", "Daniel Kroening", "Ines Lynce", "Ruben Martins"], "title": "Hypergraph-Guided Regex Filter Synthesis for Event-Based Anomaly Detection", "comment": null, "summary": "We propose HyGLAD, a novel algorithm that automatically builds a set of\ninterpretable patterns that model event data. These patterns can then be used\nto detect event-based anomalies in a stationary system, where any deviation\nfrom past behavior may indicate malicious activity. The algorithm infers\nequivalence classes of entities with similar behavior observed from the events,\nand then builds regular expressions that capture the values of those entities.\nAs opposed to deep-learning approaches, the regular expressions are directly\ninterpretable, which also translates to interpretable anomalies. We evaluate\nHyGLAD against all 7 unsupervised anomaly detection methods from DeepOD on five\ndatasets from real-world systems. The experimental results show that on average\nHyGLAD outperforms existing deep-learning methods while being an order of\nmagnitude more efficient in training and inference (single CPU vs GPU).\nPrecision improved by 1.2x and recall by 1.3x compared to the second-best\nbaseline.", "AI": {"tldr": "HyGLAD is an interpretable algorithm for event-based anomaly detection that outperforms deep-learning methods in precision and recall while being more efficient.", "motivation": "The paper addresses the need for interpretable anomaly detection in event data from real-world stationary systems, where deviations indicate potential malicious activities. Existing deep-learning approaches, while effective, lack interpretability, making it hard to understand and act on detected anomalies.", "method": "HyGLAD infers equivalence classes of entities with similar behavior from event data and constructs interpretable regular expressions to model these classes. This contrasts with deep-learning techniques by providing transparency through regular expressions, allowing for clear anomaly explanations.", "result": "HyGLAD was evaluated against seven unsupervised anomaly detection methods from DeepOD on five real-world datasets. It demonstrated a 1.2x improvement in precision and a 1.3x improvement in recall compared to the second-best baseline. Training and inference were significantly more efficient (single CPU vs GPU).", "conclusion": "HyGLAD offers a new approach to event-based anomaly detection that is both more accurate and interpretable than deep-learning models while maintaining higher efficiency. This suggests it is well-suited for environments where model interpretability and performance are both critical."}}
{"id": "2509.05681", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05681", "abs": "https://arxiv.org/abs/2509.05681", "authors": ["Xng Ai", "Shudan Lin", "Zecheng Li", "Kai Zhou", "Bixin Li", "Bin Xiao"], "title": "SEASONED: Semantic-Enhanced Self-Counterfactual Explainable Detection of Adversarial Exploiter Contracts", "comment": null, "summary": "Decentralized Finance (DeFi) attacks have resulted in significant losses,\noften orchestrated through Adversarial Exploiter Contracts (AECs) that exploit\nvulnerabilities in victim smart contracts. To proactively identify such\nthreats, this paper targets the explainable detection of AECs.\n  Existing detection methods struggle to capture semantic dependencies and lack\ninterpretability, limiting their effectiveness and leaving critical knowledge\ngaps in AEC analysis. To address these challenges, we introduce SEASONED, an\neffective, self-explanatory, and robust framework for AEC detection.\n  SEASONED extracts semantic information from contract bytecode to construct a\nsemantic relation graph (SRG), and employs a self-counterfactual explainable\ndetector (SCFED) to classify SRGs and generate explanations that highlight the\ncore attack logic. SCFED further enhances robustness, generalizability, and\ndata efficiency by extracting representative information from these\nexplanations. Both theoretical analysis and experimental results demonstrate\nthe effectiveness of SEASONED, which showcases outstanding detection\nperformance, robustness, generalizability, and data efficiency learning\nability. To support further research, we also release a new dataset of 359\nAECs.", "AI": {"tldr": "This paper addresses the explainable detection of Adversarial Exploiter Contracts (AECs) in DeFi by introducing SEASONED, a framework combining semantic relation graphs and self-counterfactual explanations.", "motivation": "Existing AEC detection methods fail to capture semantic dependencies and lack interpretability, creating critical gaps in threat analysis.", "method": "SEASONED constructs semantic relation graphs from contract bytecode and uses a self-counterfactual detector (SCFED) to classify SRGs while generating attack-explaining logic.", "result": "Experiments show SEASONED achieves strong detection performance, robustness, generalizability, and data efficiency, validated by theoretical analysis.", "conclusion": "SEASONED provides a robust, explainable AEC detection framework, supported by a released dataset of 359 AECs for future research."}}
{"id": "2509.05698", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.05698", "abs": "https://arxiv.org/abs/2509.05698", "authors": ["Yuhan Meng", "Shaofei Li", "Jiaping Gui", "Peng Jiang", "Ding Li"], "title": "KnowHow: Automatically Applying High-Level CTI Knowledge for Interpretable and Accurate Provenance Analysis", "comment": "Accepted by NDSS 2026", "summary": "High-level natural language knowledge in CTI reports, such as the ATT&CK\nframework, is beneficial to counter APT attacks. However, how to automatically\napply the high-level knowledge in CTI reports in realistic attack detection\nsystems, such as provenance analysis systems, is still an open problem. The\nchallenge stems from the semantic gap between the knowledge and the low-level\nsecurity logs: while the knowledge in CTI reports is written in natural\nlanguage, attack detection systems can only process low-level system events\nlike file accesses or network IP manipulations. Manual approaches can be\nlabor-intensive and error-prone.\n  In this paper, we propose KnowHow, a CTI-knowledge-driven online provenance\nanalysis approach that can automatically apply high-level attack knowledge from\nCTI reports written in natural languages to detect low-level system events. The\ncore of KnowHow is a novel attack knowledge representation, gIoC, that\nrepresents the subject, object, and actions of attacks. By lifting system\nidentifiers, such as file paths, in system events to natural language terms,\nKnowHow can match system events to gIoC and further match them to techniques\ndescribed in natural languages. Finally, based on the techniques matched to\nsystem events, KnowHow reasons about the temporal logic of attack steps and\ndetects potential APT attacks in system events. Our evaluation shows that\nKnowHow can accurately detect all 16 APT campaigns in the open-source and\nindustrial datasets, while existing approaches all introduce large numbers of\nfalse positives. Meanwhile, our evaluation also shows that KnowHow reduces at\nmost 90% of node-level false positives while having a higher node-level recall\nand is robust against several unknown attacks and mimicry attacks.", "AI": {"tldr": "KnowHow is a CTI-knowledge-driven approach that bridges the semantic gap between high-level natural language attack knowledge and low-level system events for APT detection, achieving high accuracy and robustness against false positives.", "motivation": "Automatically applying high-level CTI knowledge (e.g., ATT&CK) to low-level system events remains a challenge due to semantic gaps, and manual approaches are labor-intensive and error-prone.", "method": "KnowHow introduces gIoC, a structured attack knowledge representation, to lift system identifiers (e.g., file paths) to natural language terms. It matches system events to gIoC and techniques from CTI reports, then reasons through temporal logic to detect APTs.", "result": "KnowHow detects all 16 APT campaigns in datasets with no false positives, reduces node-level false positives by up to 90% compared to existing methods, and demonstrates robustness against unknown/mimic attacks.", "conclusion": "KnowHow effectively applies CTI knowledge to system events, enabling accurate and efficient APT detection while outperforming existing approaches in precision and robustness."}}
{"id": "2509.06133", "categories": ["cs.CR", "cs.DC", "cs.SE", "cs.SY", "eess.SY", "C.2.4; K.6.5; D.4.6"], "pdf": "https://arxiv.org/pdf/2509.06133", "abs": "https://arxiv.org/abs/2509.06133", "authors": ["Pradyumna Kaushal"], "title": "VehiclePassport: A GAIA-X-Aligned, Blockchain-Anchored Privacy-Preserving, Zero-Knowledge Digital Passport for Smart Vehicles", "comment": "13 pages, 5 figures. Whitepaper submission; LaTeX source with\n  compiled .bbl. Includes architecture diagrams, tables, and code listings\n  (TypeScript & Solidity)", "summary": "Modern vehicles accumulate fragmented lifecycle records across OEMs, owners,\nand service centers that are difficult to verify and prone to fraud. We propose\nVehiclePassport, a GAIA-X-aligned digital passport anchored on blockchain with\nzero-knowledge proofs (ZKPs) for privacy-preserving verification.\nVehiclePassport immutably commits to manufacturing, telemetry, and service\nevents while enabling selective disclosure via short-lived JWTs and Groth16\nproofs. Our open-source reference stack anchors hashes on Polygon zkEVM at\n<$0.02 per event, validates proofs in <10 ms, and scales to millions of\nvehicles. This architecture eliminates paper-based KYC, ensures GDPR-compliant\ntraceability, and establishes a trustless foundation for insurance, resale, and\nregulatory applications in global mobility data markets.", "AI": {"tldr": "Blockchain-based vehicle passport system with ZKPs ensures verifiable, private lifecycle records at <$0.02/event for global mobility applications.", "motivation": "Fragmented vehicle records across stakeholders enable fraud and lack verification; paper-based KYC systems are incompatible with modern data privacy requirements.", "method": "Leverages Polygon zkEVM for hash anchoring, Groth16 proofs for validation, and short-lived JWTs for selective disclosure in a GAIA-X-aligned architecture.", "result": "Achieves <$0.02/event cost, sub-10ms proof validation, and scalability for 100M+ vehicles through open-source optimizations on zkEVM infrastructure.", "conclusion": "VehiclePassport establishes a trustless blockchain framework with ZKPs to secure vehicle lifecycle data, enabling GDPR-compliant, fraud-resistant applications across mobility markets."}}
{"id": "2509.05708", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.05708", "abs": "https://arxiv.org/abs/2509.05708", "authors": ["Junjie Hu", "Na Ruan"], "title": "Larger-scale Nakamoto-style Blockchains Offer Better Security", "comment": "22 pages, 2 figures", "summary": "Traditional security models for Nakamoto-style blockchains overestimate\nadversarial coordination by assuming instantaneous synchronization among\nmalicious nodes, neglecting the critical impact of internal communication\ndelays on security. This paper introduces a dual-delay framework to revisit\nsecurity analysis, addressing this oversight through two key innovations.\nFirst, the static delay model quantifies how adversarial communication delays\n(\\(\\Delta_a\\)) constrain the effective growth rate of private chains, derived\nvia an M/D/1 queuing model as \\(\\lambda_{eff} = \\lambda_a / (1 + \\lambda_a\n\\Delta_a)\\). This model reveals that the security threshold (\\(\\beta^*\\)), the\nmaximum adversarial power the system tolerates, increases with \\(\\Delta_a\\),\neven exceeding the classic 51\\% boundary when \\(\\Delta_a \\textgreater \\Delta\\)\n(honest nodes' delay), breaking the long-standing 50\\% assumption. Second, the\ndynamic delay model integrates probabilistic corruption and scale-dependent\ndelays to characterize the total adversarial delay window (\\(\\Delta_{total} =\n\\Delta(n) e^{-k\\beta} + c \\log(1 + \\beta n)\\)), where \\(\\Delta(n) \\in\n\\Theta(\\log n)\\) captures honest nodes' logarithmic delay growth. Asymptotic\nanalysis shows adversarial power decays linearly with network scale, ensuring\nthe probability of \\(\\beta \\leq \\beta^*\\) approaches 1 as \\(n \\to \\infty\\). By\nexposing the interplay between network scale, communication delays, and power\ndilution, we provide a theoretical foundation for optimizing consensus\nprotocols and assessing robustness in large-scale Nakamoto-style blockchains.", "AI": {"tldr": "This paper introduces a dual-delay framework to correct traditional blockchain security analysis by quantifying how internal communication delays between adversarial and honest nodes affect consensus security, showing the 50% adversarial power assumption can be exceeded", "motivation": "Traditional models assume instantaneous malicious node coordination, ignoring real-world network delays that critically impact blockchain security thresholds and consensus stability", "method": "1. Static delay model: M/D/1 queuing analysis to quantify private chain growth constraints\n2. Dynamic delay model: Integrates probabilistic corruption, scale-dependent delays, and logarithmic delay growth in honest nodes", "result": "Security threshold \u03b2* increases with adversarial delay \u0394_a (exceeding 51% when \u0394_a>\u0394_h). Adversarial power decays linearly with network scale, with P(\u03b2 \u2264 \u03b2*)\u21921 as n\u2192\u221e", "conclusion": "The dual-delay framework establishes a theoretical foundation for scaling Nakamoto-style blockchains by characterizing the interaction between network scale, communication delays, and security boundaries"}}
{"id": "2509.05739", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05739", "abs": "https://arxiv.org/abs/2509.05739", "authors": ["Hanna Foerster", "Ilia Shumailov", "Yiren Zhao", "Harsh Chaudhari", "Jamie Hayes", "Robert Mullins", "Yarin Gal"], "title": "Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated", "comment": null, "summary": "Early research into data poisoning attacks against Large Language Models\n(LLMs) demonstrated the ease with which backdoors could be injected. More\nrecent LLMs add step-by-step reasoning, expanding the attack surface to include\nthe intermediate chain-of-thought (CoT) and its inherent trait of decomposing\nproblems into subproblems. Using these vectors for more stealthy poisoning, we\nintroduce ``decomposed reasoning poison'', in which the attacker modifies only\nthe reasoning path, leaving prompts and final answers clean, and splits the\ntrigger across multiple, individually harmless components.\n  Fascinatingly, while it remains possible to inject these decomposed poisons,\nreliably activating them to change final answers (rather than just the CoT) is\nsurprisingly difficult. This difficulty arises because the models can often\nrecover from backdoors that are activated within their thought processes.\nUltimately, it appears that an emergent form of backdoor robustness is\noriginating from the reasoning capabilities of these advanced LLMs, as well as\nfrom the architectural separation between reasoning and final answer\ngeneration.", "AI": {"tldr": "The paper explores decomposed reasoning poison attacks, which alter the reasoning paths in LLMs without changing the final answer. It notes that while these backdoors can be injected they are hard to reliably activate because models recover from them.", "motivation": "Recent LLMs with step-by-step reasoning introduce new vulnerabilities in the chain-of-thought process. The paper studies decomposed reasoning poison attacks to understand how perturbations in the reasoning path affect model behavior, possibly revealing a form of backdoor robustness.", "method": "An adversarial example is created by modifying the reasoning path of a model's chain-of-thought while keeping the prompt and final answer intact. The trigger is split into harmless components across the subproblems.", "result": "These decomposed poisons can be injected easily, but reliably activating them to alter the final answer is difficult. LLMs appear to recover from CoT-based backdoors, despite the attack affecting their internal reasoning.", "conclusion": "LLMs may be developing an emergent form of robustness in handling backdoors by separating reasoning and answer generation. This improved resilience implies that more sophisticated or persistent methods might be necessary for attacks on advanced LLMs."}}
{"id": "2509.05753", "categories": ["cs.CR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05753", "abs": "https://arxiv.org/abs/2509.05753", "authors": ["Ching-Chun Chang", "Isao Echizen"], "title": "Tell-Tale Watermarks for Explanatory Reasoning in Synthetic Media Forensics", "comment": null, "summary": "The rise of synthetic media has blurred the boundary between reality and\nfabrication under the evolving power of artificial intelligence, fueling an\ninfodemic that erodes public trust in cyberspace. For digital imagery, a\nmultitude of editing applications further complicates the forensic analysis,\nincluding semantic edits that alter content, photometric adjustments that\nrecalibrate colour characteristics, and geometric projections that reshape\nviewpoints. Collectively, these transformations manipulate and control\nperceptual interpretation of digital imagery. This susceptibility calls for\nforensic enquiry into reconstructing the chain of events, thereby revealing\ndeeper evidential insight into the presence or absence of criminal intent. This\nstudy seeks to address an inverse problem of tracing the underlying generation\nchain that gives rise to the observed synthetic media. A tell-tale watermarking\nsystem is developed for explanatory reasoning over the nature and extent of\ntransformations across the lifecycle of synthetic media. Tell-tale watermarks\nare tailored to different classes of transformations, responding in a manner\nthat is neither strictly robust nor fragile but instead interpretable. These\nwatermarks function as reference clues that evolve under the same\ntransformation dynamics as the carrier media, leaving interpretable traces when\nsubjected to transformations. Explanatory reasoning is then performed to infer\nthe most plausible account across the combinatorial parameter space of\ncomposite transformations. Experimental evaluations demonstrate the validity of\ntell-tale watermarking with respect to fidelity, synchronicity and\ntraceability.", "AI": {"tldr": "Develops interpretable watermarks that evolve with synthetic media to trace manipulation history, solving digital forensics' inverse problem in an era of deepfakes.", "motivation": "Synthetic media's increasing sophistication undermines public trust and obscures media provenance. Traditional forensic methods fail to address complex transformation chains involving semantic edits, color adjustments, and geometric projections, necessitating a new approach to trace generation history and detect intentional manipulations.", "method": "The paper proposes an inverse problem-solving approach using transformation-class-specific tell-tale watermarks. These watermarks evolve dynamically with the synthetic media through its lifecycle, leaving interpretable traces rather than being strictly robust or fragile. Explanatory reasoning is employed to infer transformation sequences across parameter spaces.", "result": "Experimental evaluations validate the system's effectiveness through three dimensions: (1) fidelity maintaining watermark integrity through transformations, (2) synchronicity ensuring watermark evolution parallels media transformations, and (3) traceability enabling reconstruction of transformation chains. The approach successfully enables forensic reconstruction of synthetic media provenance.", "conclusion": "This study presents a novel solution to the challenge of forensic analysis in synthetic media by introducing interpretable tell-tale watermarks. These watermarks provide explanatory reasoning about media transformations, offering critical insights into criminal intent and enhancing forensic investigation capabilities."}}
{"id": "2509.05755", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05755", "abs": "https://arxiv.org/abs/2509.05755", "authors": ["Yu Liu", "Yuchong Xie", "Mingyu Luo", "Zesen Liu", "Zhixiang Zhang", "Kaikai Zhang", "Zongjie Li", "Ping Chen", "Shuai Wang", "Dongdong She"], "title": "Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based Agentic System", "comment": null, "summary": "LLM-based agentic systems leverage large language models to handle user\nqueries, make decisions, and execute external tools for complex tasks across\ndomains like chatbots, customer service, and software engineering. A critical\ncomponent of these systems is the Tool Invocation Prompt (TIP), which defines\ntool interaction protocols and guides LLMs to ensure the security and\ncorrectness of tool usage. Despite its importance, TIP security has been\nlargely overlooked. This work investigates TIP-related security risks,\nrevealing that major LLM-based systems like Cursor, Claude Code, and others are\nvulnerable to attacks such as remote code execution (RCE) and denial of service\n(DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate\nexternal tool behavior hijacking via manipulated tool invocations. We also\npropose defense mechanisms to enhance TIP security in LLM-based agentic\nsystems.", "AI": {"tldr": "Paper uncovers critical TIP security flaws in major LLM systems, demonstrates RCE/DoS attacks via TEW exploitation workflow, and establishes defenses to secure agentic tool interactions.", "motivation": "Existing LLM-based agentic systems lack robust security mechanisms for tool invocation controls (TIPs), leaving them exposed to severe attacks like code execution and service disruption despite their widespread use in sensitive domains.", "method": "The authors conduct a systematic analysis of TIP design flaws using a Threat Exploitation Workflow (TEW), demonstrating how malicious tool invocations can hijack external tool behaviors. They combine vulnerability scanning, attack pattern analysis, and TEW-based exploit validation.", "result": "The study discovers that major systems like Cursor and Claude Code are vulnerable to TIP-based attacks, achieving remote code execution and denial of service through crafted tool invocations. Their proposed defenses significantly reduce these risks while maintaining system functionality.", "conclusion": "This work highlights critical security vulnerabilities in Tool Invocation Prompts (TIPs) of LLM-based systems, demonstrating practical attacks like RCE and DoS while offering effective defense mechanisms to secure agentic workflows."}}
{"id": "2509.05797", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.05797", "abs": "https://arxiv.org/abs/2509.05797", "authors": ["Hai Dinh-Tuan", "Sandro Rodriguez Garzon", "Jianeng Fu"], "title": "Secure and Trustful Cross-domain Communication with Decentralized Identifiers in 5G and Beyond", "comment": null, "summary": "In the evolving landscape of future mobile networks, there is a critical need\nfor secure and trustful communication modalities to support dynamic\ninteractions among core network components of different network domains. This\npaper proposes the application of W3C-endorsed Decentralized Identifiers (DIDs)\nto establish secure and trustful communication channels among network functions\nin 5G and subsequent generations. A new communication agent is introduced that\nintegrates seamlessly with 5G-standardized network functions and utilizes a\nDID-based application layer transport protocol to ensure confidentiality,\nintegrity, and authenticity for cross-domain interactions. A comparative\nanalysis of the two different versions of the DID-based communication protocol\nfor inter network function communication reveals compatibility advantages of\nthe latest protocol iteration. Furthermore, a comprehensive evaluation of the\ncommunication overhead caused by both protocol iterations compared to\ntraditional TCP/TLS shows the benefits of using DIDs to improve communication\nsecurity, albeit with performance loses compared to TCP/TLS. These results\nuncover the potential of DID-based communication for future mobile networks but\nalso point out areas for optimization.", "AI": {"tldr": "This paper proposes DID-based communication for 5G+ networks, showing enhanced security over TCP/TLS but with performance trade-offs, indicating promise for future optimization.", "motivation": "Future mobile networks require secure, trustful cross-domain communication to support dynamic interactions among network components, necessitating robust solutions like W3C-endorsed DIDs to address vulnerabilities in existing protocols.", "method": "The paper introduces a DID-based communication agent integrated with 5G-standardized functions, employs a DID-based application layer transport protocol, and evaluates two protocol iterations through comparative analysis and overhead assessment against TCP/TLS.", "result": "The latest DID protocol version demonstrates compatibility advantages, while evaluations reveal security improvements over TCP/TLS at the cost of increased communication overhead, underscoring both the viability and performance trade-offs of DID-based communication.", "conclusion": "The study highlights the potential of DID-based communication in enhancing security for future mobile networks but identifies the need for further optimization to mitigate performance losses compared to traditional protocols."}}
{"id": "2509.05831", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05831", "abs": "https://arxiv.org/abs/2509.05831", "authors": ["Ishaan Verma"], "title": "Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in Web Summarization", "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrated into web-based\nsystems for content summarization, yet their susceptibility to prompt injection\nattacks remains a pressing concern. In this study, we explore how non-visible\nHTML elements such as <meta>, aria-label, and alt attributes can be exploited\nto embed adversarial instructions without altering the visible content of a\nwebpage. We introduce a novel dataset comprising 280 static web pages, evenly\ndivided between clean and adversarial injected versions, crafted using diverse\nHTML-based strategies. These pages are processed through a browser automation\npipeline to extract both raw HTML and rendered text, closely mimicking\nreal-world LLM deployment scenarios. We evaluate two state-of-the-art\nopen-source models, Llama 4 Scout (Meta) and Gemma 9B IT (Google), on their\nability to summarize this content. Using both lexical (ROUGE-L) and semantic\n(SBERT cosine similarity) metrics, along with manual annotations, we assess the\nimpact of these covert injections. Our findings reveal that over 29% of\ninjected samples led to noticeable changes in the Llama 4 Scout summaries,\nwhile Gemma 9B IT showed a lower, yet non-trivial, success rate of 15%. These\nresults highlight a critical and largely overlooked vulnerability in LLM driven\nweb pipelines, where hidden adversarial content can subtly manipulate model\noutputs. Our work offers a reproducible framework and benchmark for evaluating\nHTML-based prompt injection and underscores the urgent need for robust\nmitigation strategies in LLM applications involving web content.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.05835", "categories": ["cs.CR", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.05835", "abs": "https://arxiv.org/abs/2509.05835", "authors": ["Lingfeng Yao", "Chenpei Huang", "Shengyao Wang", "Junpei Xue", "Hanqing Guo", "Jiang Liu", "Phone Lin", "Tomoaki Ohtsuki", "Miao Pan"], "title": "Yours or Mine? Overwriting Attacks against Neural Audio Watermarking", "comment": null, "summary": "As generative audio models are rapidly evolving, AI-generated audios\nincreasingly raise concerns about copyright infringement and misinformation\nspread. Audio watermarking, as a proactive defense, can embed secret messages\ninto audio for copyright protection and source verification. However, current\nneural audio watermarking methods focus primarily on the imperceptibility and\nrobustness of watermarking, while ignoring its vulnerability to security\nattacks. In this paper, we develop a simple yet powerful attack: the\noverwriting attack that overwrites the legitimate audio watermark with a forged\none and makes the original legitimate watermark undetectable. Based on the\naudio watermarking information that the adversary has, we propose three\ncategories of overwriting attacks, i.e., white-box, gray-box, and black-box\nattacks. We also thoroughly evaluate the proposed attacks on state-of-the-art\nneural audio watermarking methods. Experimental results demonstrate that the\nproposed overwriting attacks can effectively compromise existing watermarking\nschemes across various settings and achieve a nearly 100% attack success rate.\nThe practicality and effectiveness of the proposed overwriting attacks expose\nsecurity flaws in existing neural audio watermarking systems, underscoring the\nneed to enhance security in future audio watermarking designs.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.05883", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05883", "abs": "https://arxiv.org/abs/2509.05883", "authors": ["Andrew Yeo", "Daeseon Choi"], "title": "Multimodal Prompt Injection Attacks: Risks and Defenses for Modern LLMs", "comment": "8 pages, 4 figures, 2 tables", "summary": "Large Language Models (LLMs) have seen rapid adoption in recent years, with\nindustries increasingly relying on them to maintain a competitive advantage.\nThese models excel at interpreting user instructions and generating human-like\nresponses, leading to their integration across diverse domains, including\nconsulting and information retrieval. However, their widespread deployment also\nintroduces substantial security risks, most notably in the form of prompt\ninjection and jailbreak attacks.\n  To systematically evaluate LLM vulnerabilities -- particularly to external\nprompt injection -- we conducted a series of experiments on eight commercial\nmodels. Each model was tested without supplementary sanitization, relying\nsolely on its built-in safeguards. The results exposed exploitable weaknesses\nand emphasized the need for stronger security measures. Four categories of\nattacks were examined: direct injection, indirect (external) injection,\nimage-based injection, and prompt leakage. Comparative analysis indicated that\nClaude 3 demonstrated relatively greater robustness; nevertheless, empirical\nfindings confirm that additional defenses, such as input normalization, remain\nnecessary to achieve reliable protection.", "AI": {"tldr": "This paper evaluates the security vulnerabilities of commercial LLMs (e.g., prompt injection attacks) through experiments on eight models, revealing exploitable weaknesses and emphasizing the need for enhanced defenses. Claude 3 showed relative robustness but still required additional safeguards like input normalization.", "motivation": "LLMs' widespread adoption creates critical security risks (e.g., prompt injection, jailbreak attacks). Systematic evaluation is needed to identify vulnerabilities and establish robust defense mechanisms for industries relying on these models.", "method": "The study tested eight commercial LLMs without supplementary sanitization, relying solely on built-in safeguards. Four attack types were analyzed: direct/indirect injection, image-based injection, and prompt leakage, using empirical experiments to assess model robustness.", "result": "All models exhibited exploitable vulnerabilities to external prompt injection attacks. Claude 3 demonstrated the highest robustness among tested models, but no model achieved reliable security without additional measures like input normalization.", "conclusion": "Current built-in safeguards of commercial LLMs are insufficient to prevent prompt injection attacks. The study confirms the necessity of layering technical defenses (e.g., input normalization) to achieve practical security for real-world deployment."}}
{"id": "2509.05884", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.05884", "abs": "https://arxiv.org/abs/2509.05884", "authors": ["Banhirup Sengupta", "Peenal Gupta", "Souvik Sengupta"], "title": "Introduction to Number Theoretic Transform", "comment": null, "summary": "The Number Theoretic Transform (NTT) can be regarded as a variant of the\nDiscrete Fourier Transform. NTT has been quite a powerful mathematical tool in\ndeveloping Post-Quantum Cryptography and Homomorphic Encryption. The Fourier\nTransform essentially decomposes a signal into its frequencies. They are\ntraditionally sine or cosine waves. NTT works more over groups or finite fields\nrather than on a continuous signal and polynomials work as the analog of sine\nwaves in case of NTT. Fast Fourier Trnasform (FFT) style NTT or fast NTT has\nbeen proven to be useful in lattice-based cryptography due to its ability to\nreduce the complexity of polynomial multiplication from quadratic to\nquasilinear. We have introduced the concepts of cyclic, negacyclic convolutions\nalong with NTT and its inverse and their fast versions.", "AI": {"tldr": "The paper introduces fast Number Theoretic Transforms (NTTs) for cyclic and negacyclic convolutions, enhancing polynomial multiplication efficiency in lattice-based cryptography and post-quantum/homomorphic encryption applications.", "motivation": "Traditional Fourier Transform methods rely on continuous signals and sine/cosine waves, but cryptographic applications require discrete operations over finite fields. Efficient polynomial multiplication is critical for scalable lattice cryptography.", "method": "Proposes fast NTT variants for cyclic/negacyclic convolutions. Leverages group-theoretic and finite field mathematics where polynomials replace sinusoids. Extends FFT-style divide-and-conquer strategies to discrete algebraic structures.", "result": "Demonstrates quasilinear complexity for polynomial multiplication in cryptographic contexts. Achieves significant computational efficiency gains compared to quadratic-time methods.", "conclusion": "Fast NTT is foundational for efficient lattice-based cryptographic systems. Its algebraic structure enables both theoretical advantages and practical performance improvements over classical approaches in post-quantum security paradigms."}}
{"id": "2509.05891", "categories": ["cs.CR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.05891", "abs": "https://arxiv.org/abs/2509.05891", "authors": ["Mahfuzul I. Nissan"], "title": "MemTraceDB: Reconstructing MySQL User Activity Using ActiviTimeTrace Algorithm", "comment": null, "summary": "Database audit and transaction logs are fundamental to forensic\ninvestigations, but they are vulnerable to tampering by privileged attackers.\nMalicious insiders or external threats with administrative access can alter,\npurge, or temporarily disable logging mechanisms, creating significant blind\nspots and rendering disk-based records unreliable. Memory analysis offers a\nvital alternative, providing investigators direct access to volatile artifacts\nthat represent a ground-truth source of recent user activity, even when log\nfiles have been compromised.\n  This paper introduces MemTraceDB, a tool that reconstructs user activity\ntimelines by analyzing raw memory snapshots from the MySQL database process.\nMemTraceDB utilizes a novel algorithm, ActiviTimeTrace, to systematically\nextract and correlate forensic artifacts such as user connections and executed\nqueries. Through a series of experiments, I demonstrate MemTraceDB's\neffectiveness and reveal a critical empirical finding: the MySQL query stack\nhas a finite operational capacity of approximately 9,997 queries. This\ndiscovery allows me to establish a practical, data-driven formula for\ndetermining the optimal frequency for memory snapshot collection, providing a\nclear, actionable guideline for investigators. The result is a\nforensically-sound reconstruction of user activity, independent of compromised\ndisk-based logs.", "AI": {"tldr": "MemTraceDB is a forensic tool that reconstructs user activity from MySQL memory snapshots, bypassing tampered disk logs. It identifies MySQL\u2019s query stack limit (~9,997 queries) to optimize memory snapshot collection, enabling reliable timeline reconstruction.", "motivation": "Traditional database logs are vulnerable to tampering by privileged attackers, creating blind spots in forensic investigations. Memory analysis addresses this by providing volatile, untampered data about recent user activity.", "method": "MemTraceDB analyzes raw MySQL memory snapshots using the ActiviTimeTrace algorithm, which extracts and correlates user connections and executed queries. Experiments validate its effectiveness.", "result": "Discovery of MySQL\u2019s finite query stack capacity (~9,997 queries) and a formula to determine optimal snapshot collection frequency, enabling actionable guidelines for investigators.", "conclusion": "MemTraceDB provides a forensically sound, log-independent reconstruction of user activity, addressing vulnerabilities in disk-based audit trails."}}
{"id": "2509.05893", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.05893", "abs": "https://arxiv.org/abs/2509.05893", "authors": ["Colin Roberts", "Vivek Nair", "Dawn Song"], "title": "Wrangling Entropy: Next-Generation Multi-Factor Key Derivation, Credential Hashing, and Credential Generation Functions", "comment": "Work in progress. Learn more about MFKDF at https://mfkdf.com and\n  Multifactor at https://multifactor.com", "summary": "The Multi-Factor Key Derivation Function (MFKDF) offered a novel solution to\nthe classic problem of usable client-side key management by incorporating\nmultiple popular authentication factors into a key derivation process, but was\nlater shown to be vulnerable to cryptanalysis that degraded its security over\nmultiple invocations. In this paper, we present the Entropy State Transition\nModeling Framework (ESTMF), a novel cryptanalytic technique designed to reveal\npernicious leaks of entropy across multiple invocations of a cryptographic key\nderivation or hash function, and show that it can be used to correctly identify\neach of the known vulnerabilities in the original MFKDF construction. We then\nuse these findings to propose a new construction for ``MFKDF2,'' a\nnext-generation multi-factor key derivation function that can be proven to be\nend-to-end secure using the ESTMF. Finally, we discuss how MFKDF2 can be\nextended to support more authentication factors and usability features than the\nprevious MFKDF construction, and derive several generalizable best-practices\nfor the construction of new KDFs in the future.", "AI": {"tldr": "This paper introduces ESTMF to fix entropy-leak vulnerabilities in MFKDF, resulting in a secure next-generation multi-factor key derivation function (MFKDF2) with broader usability.", "motivation": "The original MFKDF, while innovative for client-side key management, suffered from security degradation due to cryptanalysis, necessitating a robust method (ESTMF) to uncover entropy leaks and develop a secure successor (MFKDF2).", "method": "The authors introduce the ESTMF framework to detect entropy leaks across multiple invocations of cryptographic functions, using it to identify vulnerabilities in MFKDF and redesign it into MFKDF2 with provable security. They extend MFKDF2 to support additional authentication factors and usability features.", "result": "ESTMF successfully identifies all known vulnerabilities in MFKDF, enables MFKDF2 to achieve end-to-end security, and demonstrates scalability through additional authentication factors and usability improvements.", "conclusion": "The paper proposes MFKDF2, an improved and end-to-end secure multi-factor key derivation function based on the Entropy State Transition Modeling Framework (ESTMF), which addresses vulnerabilities in the original MFKDF and provides generalizable best practices for future KDFs."}}
{"id": "2509.05921", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.05921", "abs": "https://arxiv.org/abs/2509.05921", "authors": ["Kun Li", "Cheng Wang", "Minghui Xu", "Yue Zhang", "Xiuzhen Cheng"], "title": "Dataset Ownership in the Era of Large Language Models", "comment": "15 pages, 1 table, accepted by the 2025 International Conference on\n  Blockchain and Web3.0 Technology Innovation and Application Exchange (BWTAC)", "summary": "As datasets become critical assets in modern machine learning systems,\nensuring robust copyright protection has emerged as an urgent challenge.\nTraditional legal mechanisms often fail to address the technical complexities\nof digital data replication and unauthorized use, particularly in opaque or\ndecentralized environments. This survey provides a comprehensive review of\ntechnical approaches for dataset copyright protection, systematically\ncategorizing them into three main classes: non-intrusive methods, which detect\nunauthorized use without modifying data; minimally-intrusive methods, which\nembed lightweight, reversible changes to enable ownership verification; and\nmaximally-intrusive methods, which apply aggressive data alterations, such as\nreversible adversarial examples, to enforce usage restrictions. We synthesize\nkey techniques, analyze their strengths and limitations, and highlight open\nresearch challenges. This work offers an organized perspective on the current\nlandscape and suggests future directions for developing unified, scalable, and\nethically sound solutions to protect datasets in increasingly complex machine\nlearning ecosystems.", "AI": {"tldr": "This survey categorizes and evaluates technical methods for dataset copyright protection, highlighting challenges and future research needs in securing data within modern machine learning systems.", "motivation": "Traditional legal mechanisms fail to address technical complexities of digital data replication and unauthorized use in opaque or decentralized environments, necessitating technical solutions.", "method": "The authors conduct a systematic survey, categorizing technical approaches into non-intrusive, minimally-intrusive, and maximally-intrusive methods, and analyze their strengths and limitations.", "result": "Synthesis of key techniques, analysis of their trade-offs, and identification of open research challenges in dataset copyright protection.", "conclusion": "The paper provides an organized perspective on current dataset copyright protection methods and suggests future directions for unified, scalable solutions in complex machine learning ecosystems."}}
{"id": "2509.06026", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06026", "abs": "https://arxiv.org/abs/2509.06026", "authors": ["Xinyu Gao", "Xiangtao Meng", "Yingkai Dong", "Zheng Li", "Shanqing Guo"], "title": "DCMI: A Differential Calibration Membership Inference Attack Against Retrieval-Augmented Generation", "comment": null, "summary": "While Retrieval-Augmented Generation (RAG) effectively reduces hallucinations\nby integrating external knowledge bases, it introduces vulnerabilities to\nmembership inference attacks (MIAs), particularly in systems handling sensitive\ndata. Existing MIAs targeting RAG's external databases often rely on model\nresponses but ignore the interference of non-member-retrieved documents on RAG\noutputs, limiting their effectiveness. To address this, we propose DCMI, a\ndifferential calibration MIA that mitigates the negative impact of\nnon-member-retrieved documents. Specifically, DCMI leverages the sensitivity\ngap between member and non-member retrieved documents under query perturbation.\nIt generates perturbed queries for calibration to isolate the contribution of\nmember-retrieved documents while minimizing the interference from\nnon-member-retrieved documents. Experiments under progressively relaxed\nassumptions show that DCMI consistently outperforms baselines--for example,\nachieving 97.42% AUC and 94.35% Accuracy against the RAG system with Flan-T5,\nexceeding the MBA baseline by over 40%. Furthermore, on real-world RAG\nplatforms such as Dify and MaxKB, DCMI maintains a 10%-20% advantage over the\nbaseline. These results highlight significant privacy risks in RAG systems and\nemphasize the need for stronger protection mechanisms. We appeal to the\ncommunity's consideration of deeper investigations, like ours, against the data\nleakage risks in rapidly evolving RAG systems. Our code is available at\nhttps://github.com/Xinyu140203/RAG_MIA.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
