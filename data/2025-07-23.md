<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 18]
- [cs.SE](#cs.SE) [Total: 16]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Decentralized AI-driven IoT Architecture for Privacy-Preserving and Latency-Optimized Healthcare in Pandemic and Critical Care Scenarios](https://arxiv.org/abs/2507.15859)
*Harsha Sammangi,Aditya Jagatha,Giridhar Reddy Bojja,Jun Liu*

Main category: cs.CR

TL;DR: A decentralized IoT and AI architecture improves real-time patient monitoring by enhancing data privacy and reducing latency.


<details>
  <summary>Details</summary>
Motivation: Traditional centralized healthcare systems face significant challenges in data privacy, delay, and security, particularly critical during pandemics and in intensive care environments. This study aims to address these limitations through a novel decentralized approach.

Method: The paper integrates federated learning, blockchain technology, and edge computing optimizations. Federated learning enables distributed model training with privacy preservation, blockchain ensures secure and tamper-proof data transactions, and edge computing reduces network latency by processing data locally at the edge.

Result: Experimental evaluations demonstrate transaction latency, energy consumption, and data throughput improvements up to orders of magnitude better than competitive cloud-based solutions.

Conclusion: The proposed AI-enhanced decentralized IoT architecture offers a robust framework for real-time patient monitoring in critical scenarios, with tangible benefits in latency, privacy, security, and energy efficiency.

Abstract: AI Innovations in the IoT for Real-Time Patient Monitoring On one hand, the
current traditional centralized healthcare architecture poses numerous issues,
including data privacy, delay, and security. Here, we present an AI-enabled
decentralized IoT architecture that can address such challenges during a
pandemic and critical care settings. This work presents our architecture to
enhance the effectiveness of the current available federated learning,
blockchain, and edge computing approach, maximizing data privacy, minimizing
latency, and improving other general system metrics. Experimental results
demonstrate transaction latency, energy consumption, and data throughput orders
of magnitude lower than competitive cloud solutions.

</details>


### [2] [BACFuzz: Exposing the Silence on Broken Access Control Vulnerabilities in Web Applications](https://arxiv.org/abs/2507.15984)
*I Putu Arya Dharmaadi,Mohannad Alhanahnah,Van-Thuan Pham,Fadi Mohsen,Fatih Turkmen*

Main category: cs.CR

TL;DR: BACFuzz is a gray-box fuzzing framework for detecting Broken Access Control vulnerabilities in PHP web applications using LLM-guided parameter selection, runtime feedback, and SQL-based oracle checking.


<details>
  <summary>Details</summary>
Motivation: Broken Access Control (BAC) vulnerabilities enable unauthorized access/privileged actions in web apps, but remain underexplored in automated testing due to unreliable oracles and challenges in generating semantically valid attack requests.

Method: BACFuzz combines large language model (LLM)-guided parameter selection with runtime feedback and SQL-based oracle checking to detect silent authorization flaws. It uses lightweight instrumentation for runtime information capture and analyzes backend SQL queries to verify unauthorized input flows.

Result: Achieved 94% detection rate on 17 known BAC vulnerabilities and discovered 26 previously unknown issues across 20 real-world applications, including 15 CVE cases and 2 benchmarks.

Conclusion: BACFuzz demonstrates robust BAC detection capability with low false positives, successfully uncovering both known and novel vulnerabilities while artifacts will be publicly released for further validation.

Abstract: Broken Access Control (BAC) remains one of the most critical and widespread
vulnerabilities in web applications, allowing attackers to access unauthorized
resources or perform privileged actions. Despite its severity, BAC is
underexplored in automated testing due to key challenges: the lack of reliable
oracles and the difficulty of generating semantically valid attack requests. We
introduce BACFuzz, the first gray-box fuzzing framework specifically designed
to uncover BAC vulnerabilities, including Broken Object-Level Authorization
(BOLA) and Broken Function-Level Authorization (BFLA) in PHP-based web
applications. BACFuzz combines LLM-guided parameter selection with runtime
feedback and SQL-based oracle checking to detect silent authorization flaws. It
employs lightweight instrumentation to capture runtime information that guides
test generation, and analyzes backend SQL queries to verify whether
unauthorized inputs flow into protected operations. Evaluated on 20 real-world
web applications, including 15 CVE cases and 2 known benchmarks, BACFuzz
detects 16 of 17 known issues and uncovers 26 previously unknown BAC
vulnerabilities with low false positive rates. All identified issues have been
responsibly disclosed, and artifacts will be publicly released.

</details>


### [3] ["We Need a Standard": Toward an Expert-Informed Privacy Label for Differential Privacy](https://arxiv.org/abs/2507.15997)
*Onyinye Dibia,Mengyi Lu,Prianka Bhattacharjee,Joseph P. Near,Yuanyuan Feng*

Main category: cs.CR

TL;DR: This paper investigates the inconsistent disclosure practices in differential privacy deployments and proposes a standardized privacy label to transparently communicate privacy guarantees using expert interviews.


<details>
  <summary>Details</summary>
Motivation: The core motivation addresses the lack of standardized transparency in real-world differential privacy (DP) implementations, where undisclosed parameters erode trust and create ambiguous privacy assurances through public-facing systems.

Method: Semi-structured interviews with 12 DP researchers facilitated the systematic identification of critical parameters, followed by the development of a formalized privacy label prototype to operationalize disclosure best practices.

Result: We established a comprehensive set of DP parameters requiring disclosure, including their quantitative significance, and implemented a prototype privacy label to standardize communication of these values across deployments.

Conclusion: The study advocates for mandatory implementation of privacy labels in DP systems to ensure consistent disclosure of core parameters, thereby establishing verifiable trust and enabling future empirical comparisons across deployments.

Abstract: The increasing adoption of differential privacy (DP) leads to public-facing
DP deployments by both government agencies and companies. However, real-world
DP deployments often do not fully disclose their privacy guarantees, which vary
greatly between deployments. Failure to disclose certain DP parameters can lead
to misunderstandings about the strength of the privacy guarantee, undermining
the trust in DP. In this work, we seek to inform future standards for
communicating the privacy guarantees of DP deployments. Based on
semi-structured interviews with 12 DP experts, we identify important DP
parameters necessary to comprehensively communicate DP guarantees, and describe
why and how they should be disclosed. Based on expert recommendations, we
design an initial privacy label for DP to comprehensively communicate privacy
guarantees in a standardized format.

</details>


### [4] [Blocklisted Oblivious Pseudorandom Functions](https://arxiv.org/abs/2507.16040)
*Xinyuan Zhang,Anrin Chakraborti,Michael Reiter*

Main category: cs.CR

TL;DR: This paper introduces a blocklisted OPRF (BLOPRF) framework where a server can define a blocklist, and the protocol only evaluates the pseudorandom function if the client's input doesn't cluster with blocked items in a metric space. It optimizes efficiency by separating cryptographic tasks and shows applications in password security and malware detection.


<details>
  <summary>Details</summary>
Motivation: Existing OPRF protocols lack mechanisms to prevent malicious inputs (e.g., compromised passwords, malware executables) from being processed. The authors aim to develop a secure and efficient system that enables selective OPRF evaluation through blocklist verification while preserving privacy.

Method: The approach embeds client and server inputs into a metric space, splitting OPRF evaluation into two phases: (1) deterministic embedding computation by the client and (2) secure blocklist checking via distance bounding techniques. Cryptographic stitching ensures interaction without leaking information between phases.

Result: The paper demonstrates BLOPRF's feasibility for augmenting password-authenticated key exchange and malware analysis systems. Performance improvements are achieved through phase separation and efficient repeated computation reuse of initial embeddings.

Conclusion: The BLOPRF framework establishes a privacy-preserving mechanism for OPRF blocklisting, enabling security-critical applications to selectively compute functions on potentially sensitive input while maintaining efficiency and cryptographic correctness.

Abstract: An oblivious pseudorandom function (OPRF) is a protocol by which a client and
server interact to evaluate a pseudorandom function on a key provided by the
server and an input provided by the client, without divulging the key or input
to the other party. We extend this notion by enabling the server to specify a
blocklist, such that OPRF evaluation succeeds only if the client's input is not
on the blocklist. More specifically, our design gains performance by embedding
the client input into a metric space, where evaluation continues only if this
embedding does not cluster with blocklist elements. Our framework exploits this
structure to separate the embedding and blocklist check to enable efficient
implementations of each, but then must stitch these phases together through
cryptographic means. Our framework also supports subsequent evaluation of the
OPRF on the same input more efficiently. We demonstrate the use of our design
for password blocklisting in augmented password-authenticated key exchange, and
to MAC only executables that are not similar to ones on a blocklist of known
malware.

</details>


### [5] [MFAz: Historical Access Based Multi-Factor Authorization](https://arxiv.org/abs/2507.16060)
*Eyasu Getahun Chekole,Howard Halim,Jianying Zhou*

Main category: cs.CR

TL;DR: This paper proposes a novel multi-factor authorization (MFAz) scheme to counter advanced unauthorized access threats like session hijacking, leveraging bloom filters for efficiency and blockchain for secure decentralized decision-making. Experimental evaluation on a smart-city testbed demonstrates its effectiveness.


<details>
  <summary>Details</summary>
Motivation: Traditional access control mechanisms (e.g., static policies) fail to prevent sophisticated attacks such as session hijacking, which enables stealthy takeover of established peer sessions to access private resources. Advanced exploitation techniques require proactive solutions beyond conventional approaches.

Method: The authors introduce a multi-factor authorization scheme (MFAz) utilizing two factors: (1) fine-grained access control rules (ARs) derived from historical access patterns, and (2) systematically generated verification points (VPs). Techniques include bloom filter implementation for runtime/storage efficiency and blockchain for tamper-proof, decentralized authorization decisions.

Result: Experimental evaluation on a smart-city testbed with diverse devices showed the MFAz scheme provides security against both conventional and advanced attacks while maintaining performance efficiency. The combination of bloom filters and blockchain successfully enhanced protection without compromising scalability.

Conclusion: The proposed MFAz framework offers a robust solution for proactive access control in cyber-physical systems like smart cities. This work pioneers a multi-factor authorization approach orthogonal to multi-factor authentication (MFA), demonstrating practicality through hardware-validated performance results.

Abstract: Unauthorized access remains one of the critical security challenges in the
realm of cybersecurity. With the increasing sophistication of attack
techniques, the threat of unauthorized access is no longer confined to the
conventional ones, such as exploiting weak access control policies. Instead,
advanced exploitation strategies, such as session hijacking-based attacks, are
becoming increasingly prevalent, posing serious security concerns. Session
hijacking enables attackers to take over an already established session between
legitimate peers in a stealthy manner, thereby gaining unauthorized access to
private resources. Unfortunately, traditional access control mechanisms, such
as static access control policies, are insufficient to prevent session
hijacking or other advanced exploitation techniques. In this work, we propose a
new multi-factor authorization (MFAz) scheme that proactively mitigates
unauthorized access attempts both conventional and advanced unauthorized access
attacks. The proposed scheme employs fine-grained access control rules (ARs)
and verification points (VPs) that are systematically generated from
historically granted accesses as the first and second authorization factors,
respectively. As a proof-of-concept, we implement the scheme using different
techniques. We leverage bloom filter to achieve runtime and storage efficiency,
and blockchain to make authorization decisions in a temper-proof and
decentralized manner. To the best of our knowledge, this is the first formal
introduction of a multi-factor authorization scheme, which is orthogonal to the
multi-factor authentication (MFA) schemes. The effectiveness of our proposed
scheme is experimentally evaluated using a smart-city testbed involving
different devices with varying computational capacities. The experimental
results reveal high effectiveness of the scheme both in security and
performance guarantees.

</details>


### [6] [DP2Guard: A Lightweight and Byzantine-Robust Privacy-Preserving Federated Learning Scheme for Industrial IoT](https://arxiv.org/abs/2507.16134)
*Baofu Han,Bing Li,Yining Qi,Raja Jurdak,Kaibin Huang,Chau Yuen*

Main category: cs.CR

TL;DR: DP2Guard enhances PPFL by using lightweight gradient masking, hybrid defense strategies (SVD, cosine similarity, clustering), and blockchain for privacy and robustness against poisoning attacks with reduced overhead.


<details>
  <summary>Details</summary>
Motivation: Existing PPFL schemes face high computational/communication overhead from encryption and insufficient robustness against adaptive adversaries using single-strategy defenses.

Method: The framework combines (1) lightweight gradient masking (instead of cryptography), (2) hybrid defense with singular value decomposition and cosine similarity feature extraction plus clustering, and (3) trust-score adaptive aggregation with blockchain auditability.

Result: Extensive experiments on two public datasets show DP2Guard effectively defends against four advanced poisoning attacks while maintaining privacy and reducing overhead.

Conclusion: DP2Guard offers a balanced solution for secure PPFL by improving privacy, robustness, and efficiency through integrated novel techniques.

Abstract: Privacy-Preserving Federated Learning (PPFL) has emerged as a secure
distributed Machine Learning (ML) paradigm that aggregates locally trained
gradients without exposing raw data. To defend against model poisoning threats,
several robustness-enhanced PPFL schemes have been proposed by integrating
anomaly detection. Nevertheless, they still face two major challenges: (1) the
reliance on heavyweight encryption techniques results in substantial
communication and computation overhead; and (2) single-strategy defense
mechanisms often fail to provide sufficient robustness against adaptive
adversaries. To overcome these challenges, we propose DP2Guard, a lightweight
PPFL framework that enhances both privacy and robustness. DP2Guard leverages a
lightweight gradient masking mechanism to replace costly cryptographic
operations while ensuring the privacy of local gradients. A hybrid defense
strategy is proposed, which extracts gradient features using singular value
decomposition and cosine similarity, and applies a clustering algorithm to
effectively identify malicious gradients. Additionally, DP2Guard adopts a trust
score-based adaptive aggregation scheme that adjusts client weights according
to historical behavior, while blockchain records aggregated results and trust
scores to ensure tamper-proof and auditable training. Extensive experiments
conducted on two public datasets demonstrate that DP2Guard effectively defends
against four advanced poisoning attacks while ensuring privacy with reduced
communication and computation costs.

</details>


### [7] [Attacking interpretable NLP systems](https://arxiv.org/abs/2507.16164)
*Eldor Abdukhamidov,Tamer Abuhmed,Joanna C. S. Santos,Mohammed Abuhamad*

Main category: cs.CR

TL;DR: AdvChar is a black-box adversarial attack on interpretable NLP systems that modifies text with minimal character changes to mislead classifiers while preserving original interpretations.


<details>
  <summary>Details</summary>
Motivation: Previous text-based adversarial attacks often fail to maintain semantic meaning and similarity, while visual attacks exploit human-machine perception differences. This paper aims to develop an attack that preserves interpretation trust while causing misclassification.

Method: The method uses an interpretation-focused scoring approach to identify critical tokens and applies character-level modifications to alter input texts with minimal changes (typically 2 characters), keeping adversarial interpretations similar to benign ones.

Result: AdvChar significantly reduced prediction accuracy across seven NLP models and three interpretation models using benchmark datasets, demonstrating effectiveness of the attack with minimal text alterations.

Conclusion: AdvChar exploits vulnerabilities in interpretable NLP systems by making imperceptible character changes that induce classification errors while maintaining trusted interpretations, highlighting risks to model transparency.

Abstract: Studies have shown that machine learning systems are vulnerable to
adversarial examples in theory and practice. Where previous attacks have
focused mainly on visual models that exploit the difference between human and
machine perception, text-based models have also fallen victim to these attacks.
However, these attacks often fail to maintain the semantic meaning of the text
and similarity. This paper introduces AdvChar, a black-box attack on
Interpretable Natural Language Processing Systems, designed to mislead the
classifier while keeping the interpretation similar to benign inputs, thus
exploiting trust in system transparency. AdvChar achieves this by making less
noticeable modifications to text input, forcing the deep learning classifier to
make incorrect predictions and preserve the original interpretation. We use an
interpretation-focused scoring approach to determine the most critical tokens
that, when changed, can cause the classifier to misclassify the input. We apply
simple character-level modifications to measure the importance of tokens,
minimizing the difference between the original and new text while generating
adversarial interpretations similar to benign ones. We thoroughly evaluated
AdvChar by testing it against seven NLP models and three interpretation models
using benchmark datasets for the classification task. Our experiments show that
AdvChar can significantly reduce the prediction accuracy of current deep
learning models by altering just two characters on average in input samples.

</details>


### [8] [SVAgent: AI Agent for Hardware Security Verification Assertion](https://arxiv.org/abs/2507.16203)
*Rui Guo,Avinash Ayalasomayajula,Henian Li,Jingbo Zhou,Sujan Kumar Saha,Farimah Farahmandi*

Main category: cs.CR

TL;DR: The paper introduces SVAgent, an automated framework for generating SystemVerilog Assertions (SVA) to address limitations in manual SVA development and improving security analysis in complex circuits.


<details>
  <summary>Details</summary>
Motivation: Current SVA development models are inefficient and inadequate for handling the growing complexity and security vulnerabilities in modern integrated circuits, especially with globalized design processes and stricter security standards.

Method: SVAgent employs a requirement decomposition mechanism that breaks down complex verification requirements into structured, fine-grained sub-problems, enabling systematic SVA generation.

Result: SVAgent demonstrates superior accuracy and consistency compared to existing frameworks, effectively reduces hallucinations/random errors, and integrates with mainstream vulnerability assessment tools in real engineering environments.

Conclusion: SVAgent provides a reliable, efficient solution for automated SVA generation, addressing critical gaps in circuit verification while maintaining practicality through industry framework integration.

Abstract: Verification using SystemVerilog assertions (SVA) is one of the most popular
methods for detecting circuit design vulnerabilities. However, with the
globalization of integrated circuit design and the continuous upgrading of
security requirements, the SVA development model has exposed major limitations.
It is not only inefficient in development, but also unable to effectively deal
with the increasing number of security vulnerabilities in modern complex
integrated circuits. In response to these challenges, this paper proposes an
innovative SVA automatic generation framework SVAgent. SVAgent introduces a
requirement decomposition mechanism to transform the original complex
requirements into a structured, gradually solvable fine-grained problem-solving
chain. Experiments have shown that SVAgent can effectively suppress the
influence of hallucinations and random answers, and the key evaluation
indicators such as the accuracy and consistency of the SVA are significantly
better than existing frameworks. More importantly, we successfully integrated
SVAgent into the most mainstream integrated circuit vulnerability assessment
framework and verified its practicality and reliability in a real engineering
design environment.

</details>


### [9] [eX-NIDS: A Framework for Explainable Network Intrusion Detection Leveraging Large Language Models](https://arxiv.org/abs/2507.16241)
*Paul R. B. Houssel,Siamak Layeghy,Priyanka Singh,Marius Portmann*

Main category: cs.CR

TL;DR: eX-NIDS is a framework that improves interpretability in flow-based NIDS by using LLMs with CTI-enriched prompts, achieving over 20% better performance than basic explanations.


<details>
  <summary>Details</summary>
Motivation: Network intrusion detection systems (NIDS) require interpretability to explain malicious flow classifications. Without context-specific knowledge, LLM-generated explanations lack accuracy and utility.

Method: The framework uses a Prompt Augmenter module to extract contextual information and CTI knowledge from NIDS-labeled malicious flows. This data is then included in LLM input prompts to enable contextualized explanation generation. Comparisons are made against a basic-prompt baseline using Llama 3 and GPT-4 models.

Result: Augmented LLM explanations showed significant improvements (over 20% performance gain) in accuracy and consistency compared to the basic-prompt baseline through specialized natural language evaluation.

Conclusion: Integrating CTI-based context into LLM prompts provides reliable, consistent explanations for NIDS classifications, demonstrating LLMs' value as complementary tools for cybersecurity interpretability.

Abstract: This paper introduces eX-NIDS, a framework designed to enhance
interpretability in flow-based Network Intrusion Detection Systems (NIDS) by
leveraging Large Language Models (LLMs). In our proposed framework, flows
labelled as malicious by NIDS are initially processed through a module called
the Prompt Augmenter. This module extracts contextual information and Cyber
Threat Intelligence (CTI)-related knowledge from these flows. This enriched,
context-specific data is then integrated with an input prompt for an LLM,
enabling it to generate detailed explanations and interpretations of why the
flow was identified as malicious by NIDS. We compare the generated
interpretations against a Basic-Prompt Explainer baseline, which does not
incorporate any contextual information into the LLM's input prompt. Our
framework is quantitatively evaluated using the Llama 3 and GPT-4 models,
employing a novel evaluation method tailored for natural language explanations,
focusing on their correctness and consistency. The results demonstrate that
augmented LLMs can produce accurate and consistent explanations, serving as
valuable complementary tools in NIDS to explain the classification of malicious
flows. The use of augmented prompts enhances performance by over 20% compared
to the Basic-Prompt Explainer.

</details>


### [10] [From Contracts to Code: Automating Smart Contract Generation with Multi-Level Finite State Machines](https://arxiv.org/abs/2507.16276)
*Lambard Maxence,Bertelle Cyrille,Duvallet Claude*

Main category: cs.CR

TL;DR: The paper proposes a multi-level finite state machine model to simplify smart contract development by abstracting technical complexities and enhancing security, aiming to improve accessibility for non-technical professionals.


<details>
  <summary>Details</summary>
Motivation: Blockchain smart contracts face adoption barriers due to their complexity and programming requirements, despite benefits like transparency, security, and reduced intermediary costs.

Method: A multi-level finite state machine (FSM) is developed as a formal framework to represent smart contracts hierarchically, promoting modularity and traceability through reusable components and structured execution modeling.

Result: The model's potential is explored via analysis of existing methodologies, detailed contract generation with modularity emphasis, and a security evaluation identifying vulnerabilities to ensure robustness.

Conclusion: The multi-level FSM approach offers a scalable, secure, and user-friendly solution for smart contract design, reducing technical hurdles while maintaining reliability in blockchain environments.

Abstract: In an increasingly complex contractual landscape, the demand for
transparency, security, and efficiency has intensified. Blockchain technology,
with its decentralized and immutable nature, addresses these challenges by
reducing intermediary costs, minimizing fraud risks, and enhancing system
compatibility. Smart contracts, initially conceptualized by Nick Szabo and
later implemented on the Ethereum blockchain, automate and secure contractual
clauses, offering a robust solution for various industries. However, their
complexity and the requirement for advanced programming skills present
significant barriers to widespread adoption. This study introduces a
multi-level finite state machine model designed to represent and track the
execution of smart contracts. Our model aims to simplify smart contract
development by providing a formalized framework that abstracts underlying
technical complexities, making it accessible to professionals without deep
technical expertise. The hierarchical structure of the multi-level finite state
machine enhances contract modularity and traceability, facilitating detailed
representation and evaluation of functional properties. The paper explores the
potential of this multi-level approach, reviewing existing methodologies and
tools, and detailing the smart contract generation process with an emphasis on
reusable components and modularity. We also conduct a security analysis to
evaluate potential vulnerabilities in our model, ensuring the robustness and
reliability of the generated smart contracts.

</details>


### [11] [Talking Like a Phisher: LLM-Based Attacks on Voice Phishing Classifiers](https://arxiv.org/abs/2507.16291)
*Wenhao Li,Selvakumar Manickam,Yung-wey Chong,Shankar Karuppayah*

Main category: cs.CR

TL;DR: This paper explores a novel adversarial attack using large language models (LLMs) to generate persuasive vishing transcripts that evade detection by ML-based classifiers, highlighting vulnerabilities and proposing safeguards.


<details>
  <summary>Details</summary>
Motivation: Existing ML classifiers for vishing detection are vulnerable to adversarial manipulations that preserve semantic content, creating a need to systematically study how LLMs can be exploited to generate such attacks.

Method: The authors constructed an attack pipeline employing prompt engineering and semantic obfuscation across four commercial LLMs. Generated transcripts were evaluated against ML classifiers on a real-world Korean vishing dataset (KorCCViD) using statistical testing.

Result: LLM-generated transcripts, particularly by GPT-4o, reduced classifier accuracy by up to 30.96% with high semantic similarity (BERTScore) and high time-efficiency (under 9 seconds per transcript) and cost-efficiency.

Conclusion: The study underscores the urgency of improving vishing detection frameworks and for LLM providers to implement stronger safeguards against prompt misuse in adversarial social engineering scenarios.

Abstract: Voice phishing (vishing) remains a persistent threat in cybersecurity,
exploiting human trust through persuasive speech. While machine learning
(ML)-based classifiers have shown promise in detecting malicious call
transcripts, they remain vulnerable to adversarial manipulations that preserve
semantic content. In this study, we explore a novel attack vector where large
language models (LLMs) are leveraged to generate adversarial vishing
transcripts that evade detection while maintaining deceptive intent. We
construct a systematic attack pipeline that employs prompt engineering and
semantic obfuscation to transform real-world vishing scripts using four
commercial LLMs. The generated transcripts are evaluated against multiple ML
classifiers trained on a real-world Korean vishing dataset (KorCCViD) with
statistical testing. Our experiments reveal that LLM-generated transcripts are
both practically and statistically effective against ML-based classifiers. In
particular, transcripts crafted by GPT-4o significantly reduce classifier
accuracy (by up to 30.96%) while maintaining high semantic similarity, as
measured by BERTScore. Moreover, these attacks are both time-efficient and
cost-effective, with average generation times under 9 seconds and negligible
financial cost per query. The results underscore the pressing need for more
resilient vishing detection frameworks and highlight the imperative for LLM
providers to enforce stronger safeguards against prompt misuse in adversarial
social engineering contexts.

</details>


### [12] [DREAM: Scalable Red Teaming for Text-to-Image Generative Systems via Distribution Modeling](https://arxiv.org/abs/2507.16329)
*Boheng Li,Junjie Wang,Yiming Li,Zhiyang Hu,Leyi Qi,Jianshuo Dong,Run Wang,Han Qiu,Zhan Qin,Tianwei Zhang*

Main category: cs.CR

TL;DR: DREAM is a scalable red teaming framework for text-to-image models that models the probabilistic distribution of harmful prompts using energy-based methods, surpassing state-of-the-art baselines in detecting problematic content with higher success rates and diversity.


<details>
  <summary>Details</summary>
Motivation: Current safety measures for text-to-image (T2I) systems fail to prevent unintended harmful outputs (e.g., sexual/violent imagery), necessitating robust pre-deployment safety testing through red teaming. Existing automated approaches lack scalability, diversity, and effectiveness in identifying problematic prompts across the entire T2I pipeline.

Method: DREAM uses energy-based models to directly optimize a probabilistic distribution of harmful prompts, and introduces GC-SPSA for stable gradient estimation through the non-differentiable T2I pipeline, enabling large-scale sampling and joint optimization of effectiveness and diversity.

Result: DREAM outperformed 9 state-of-the-art baselines significantly in prompt success rate and content diversity across various T2I models and safety filters, validated through extensive experiments.

Conclusion: DREAM addresses critical limitations of isolated red teaming by modeling harmful prompt distributions, providing an efficient and effective solution for T2I system safety improvement while balancing output quality and diversity requirements.

Abstract: Despite the integration of safety alignment and external filters,
text-to-image (T2I) generative models are still susceptible to producing
harmful content, such as sexual or violent imagery. This raises serious
concerns about unintended exposure and potential misuse. Red teaming, which
aims to proactively identify diverse prompts that can elicit unsafe outputs
from the T2I system (including the core generative model as well as potential
external safety filters and other processing components), is increasingly
recognized as an essential method for assessing and improving safety before
real-world deployment. Yet, existing automated red teaming approaches often
treat prompt discovery as an isolated, prompt-level optimization task, which
limits their scalability, diversity, and overall effectiveness. To bridge this
gap, in this paper, we propose DREAM, a scalable red teaming framework to
automatically uncover diverse problematic prompts from a given T2I system.
Unlike most prior works that optimize prompts individually, DREAM directly
models the probabilistic distribution of the target system's problematic
prompts, which enables explicit optimization over both effectiveness and
diversity, and allows efficient large-scale sampling after training. To achieve
this without direct access to representative training samples, we draw
inspiration from energy-based models and reformulate the objective into simple
and tractable objectives. We further introduce GC-SPSA, an efficient
optimization algorithm that provide stable gradient estimates through the long
and potentially non-differentiable T2I pipeline. The effectiveness of DREAM is
validated through extensive experiments, demonstrating that it surpasses 9
state-of-the-art baselines by a notable margin across a broad range of T2I
models and safety filters in terms of prompt success rate and diversity.

</details>


### [13] [Depth Gives a False Sense of Privacy: LLM Internal States Inversion](https://arxiv.org/abs/2507.16372)
*Tian Dong,Yan Meng,Shaofeng Li,Guoxing Chen,Zhen Liu,Haojin Zhu*

Main category: cs.CR

TL;DR: This paper challenges the assumption that LLM internal states (ISs) can't be reversed to original inputs, proposing four inversion attacks. These attacks improve semantic and token-level reconstruction, show effectiveness on medical/coding datasets and 6 LLMs, and find that existing defenses fail to prevent IS inversion.


<details>
  <summary>Details</summary>
Motivation: Privacy and safety concerns with LLMs grow as internal states are exposed through collaborative inference and auditing techniques, necessitating evaluation of how ISs might be exploited to recover sensitive data.

Method: Two white-box optimization attacks for low/high-depth ISs (two-phase inversion process), black-box extension using model transferability, and a generation-based attack framing inversion as translation. Evaluated on real-world datasets and LLMs.

Result: High token matching (86.88 F1) on 4,112-token medical prompts from Llama-3 mid-layer. All four practical defenses tested were ineffective against IS inversion attacks.

Conclusion: Common belief that ISs are irreversible is incorrect. Highlights critical risk in current LLM deployment paradigms and emphasizes need for stronger mitigation strategies.

Abstract: Large Language Models (LLMs) are increasingly integrated into daily routines,
yet they raise significant privacy and safety concerns. Recent research
proposes collaborative inference, which outsources the early-layer inference to
ensure data locality, and introduces model safety auditing based on inner
neuron patterns. Both techniques expose the LLM's Internal States (ISs), which
are traditionally considered irreversible to inputs due to optimization
challenges and the highly abstract representations in deep layers. In this
work, we challenge this assumption by proposing four inversion attacks that
significantly improve the semantic similarity and token matching rate of
inverted inputs. Specifically, we first develop two white-box
optimization-based attacks tailored for low-depth and high-depth ISs. These
attacks avoid local minima convergence, a limitation observed in prior work,
through a two-phase inversion process. Then, we extend our optimization attack
under more practical black-box weight access by leveraging the transferability
between the source and the derived LLMs. Additionally, we introduce a
generation-based attack that treats inversion as a translation task, employing
an inversion model to reconstruct inputs. Extensive evaluation of short and
long prompts from medical consulting and coding assistance datasets and 6 LLMs
validates the effectiveness of our inversion attacks. Notably, a 4,112-token
long medical consulting prompt can be nearly perfectly inverted with 86.88 F1
token matching from the middle layer of Llama-3 model. Finally, we evaluate
four practical defenses that we found cannot perfectly prevent ISs inversion
and draw conclusions for future mitigation design.

</details>


### [14] [Explainable Vulnerability Detection in C/C++ Using Edge-Aware Graph Attention Networks](https://arxiv.org/abs/2507.16540)
*Radowanul Haque,Aftab Ali,Sally McClean,Naveed Khan*

Main category: cs.CR

TL;DR: ExplainVulD is a graph-based vulnerability detection framework for C/C++ code using dual-channel embeddings and edge-aware attention, addressing class imbalance and improving accuracy/F1 scores while providing explainable outputs.


<details>
  <summary>Details</summary>
Motivation: Security vulnerability detection is hampered by class imbalance and lack of explainability in existing methods, causing high false positives and usability issues in developer and security workflows.

Method: Constructs Code Property Graphs with dual-channel node embeddings (semantic + structural) processed via edge-aware attention mechanism (edge-type embeddings). Uses class-weighted cross-entropy loss to mitigate class imbalance.

Result: Achieved 88.25% mean accuracy and 48.23% F1 score on ReVeal dataset (4.6% and 16.9% improvements over ReVeal model). Outperformed static analysis tools with 14.0-14.1% accuracy gain and 132.2-201.2% F1 gain across 30 runs.

Conclusion: ExplainVulD improves vulnerability detection through balanced accuracy/F1 performance while enabling transparent security triage via code region influence explanations, offering practical advantages over learning-based and static methods.

Abstract: Detecting security vulnerabilities in source code remains challenging,
particularly due to class imbalance in real-world datasets where vulnerable
functions are under-represented. Existing learning-based methods often optimise
for recall, leading to high false positive rates and reduced usability in
development workflows. Furthermore, many approaches lack explainability,
limiting their integration into security workflows. This paper presents
ExplainVulD, a graph-based framework for vulnerability detection in C/C++ code.
The method constructs Code Property Graphs and represents nodes using
dual-channel embeddings that capture both semantic and structural information.
These are processed by an edge-aware attention mechanism that incorporates
edge-type embeddings to distinguish among program relations. To address class
imbalance, the model is trained using class-weighted cross-entropy loss.
ExplainVulD achieves a mean accuracy of 88.25 percent and an F1 score of 48.23
percent across 30 independent runs on the ReVeal dataset. These results
represent relative improvements of 4.6 percent in accuracy and 16.9 percent in
F1 score compared to the ReVeal model, a prior learning-based method. The
framework also outperforms static analysis tools, with relative gains of 14.0
to 14.1 percent in accuracy and 132.2 to 201.2 percent in F1 score. Beyond
improved detection performance, ExplainVulD produces explainable outputs by
identifying the most influential code regions within each function, supporting
transparency and trust in security triage.

</details>


### [15] [From Text to Actionable Intelligence: Automating STIX Entity and Relationship Extraction](https://arxiv.org/abs/2507.16576)
*Ahmed Lekssays,Husrev Taha Sencar,Ting Yu*

Main category: cs.CR

TL;DR: This paper introduces AZERG, a tool for automatically generating structured STIX threat intelligence from unstructured security reports. By fine-tuning LLMs with a custom dataset of 141 annotated threat reports, the system achieves strong performance across four subtasks with 2-25% improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Manual generation of STIX-compatible threat intelligence data is time-consuming and reliant on expert analysts, hampering timeliness and automation in threat intelligence sharing. The STIX framework is widely adopted but lacks efficient tools for structured data extraction.

Method: The authors decomposed the task into four components: entity detection, entity type identification, related pair detection, and relationship type identification. They implemented task-specific fine-tuning of general-purpose LLMs using their custom dataset containing 4,011 entities and 2,075 STIX-aligned relationships from real threat reports.

Result: Achieved F1-scores of 84.43% (T1), 88.49% (T2), 95.47% (T3), and 84.60% (T4) with 2-25% performance improvements over open-/closed-parameter models and state-of-the-art alternatives.

Conclusion: AZERG demonstrates a viable solution for automating STIX data generation through structured task decomposition and LLM fine-tuning, significantly improving threat intelligence sharing efficiency while maintaining high alignment with STIX standards.

Abstract: Sharing methods of attack and their effectiveness is a cornerstone of
building robust defensive systems. Threat analysis reports, produced by various
individuals and organizations, play a critical role in supporting security
operations and combating emerging threats. To enhance the timeliness and
automation of threat intelligence sharing, several standards have been
established, with the Structured Threat Information Expression (STIX) framework
emerging as one of the most widely adopted. However, generating STIX-compatible
data from unstructured security text remains a largely manual, expert-driven
process. To address this challenge, we introduce AZERG, a tool designed to
assist security analysts in automatically generating structured STIX
representations. To achieve this, we adapt general-purpose large language
models for the specific task of extracting STIX-formatted threat data. To
manage the complexity, the task is divided into four subtasks: entity detection
(T1), entity type identification (T2), related pair detection (T3), and
relationship type identification (T4). We apply task-specific fine-tuning to
accurately extract relevant entities and infer their relationships in
accordance with the STIX specification. To address the lack of training data,
we compiled a comprehensive dataset with 4,011 entities and 2,075 relationships
extracted from 141 full threat analysis reports, all annotated in alignment
with the STIX standard. Our models achieved F1-scores of 84.43% for T1, 88.49%
for T2, 95.47% for T3, and 84.60% for T4 in real-world scenarios. We validated
their performance against a range of open- and closed-parameter models, as well
as state-of-the-art methods, demonstrating improvements of 2-25% across tasks.

</details>


### [16] [LLMxCPG: Context-Aware Vulnerability Detection Through Code Property Graph-Guided Large Language Models](https://arxiv.org/abs/2507.16585)
*Ahmed Lekssays,Hamza Mouhcine,Khang Tran,Ting Yu,Issa Khalil*

Main category: cs.CR

TL;DR: LLMxCPG is a framework combining Code Property Graphs (CPG) with Large Language Models (LLM) for robust vulnerability detection, achieving 15-40% F1-score improvements over existing methods and maintaining performance under code modifications.


<details>
  <summary>Details</summary>
Motivation: Deep learning approaches for vulnerability detection suffer from significant accuracy drops (up to 45%) on verified datasets and poor robustness against code modifications, creating a need for more reliable solutions.

Method: LLMxCPG integrates CPG slicing techniques to reduce code size by 67.84-90.93% while preserving vulnerability-relevant context, enabling analysis of larger code segments and projects through concise code representation.

Result: 15-40% higher F1-scores compared to state-of-the-art baselines, combined with consistent performance on function-level and multi-function codebases under various syntactic modifications.

Conclusion: The integration of CPG-based slicing with LLMs provides a robust vulnerability detection framework that addresses existing limitations by improving accuracy, scalability, and resilience to code transformations.

Abstract: Software vulnerabilities present a persistent security challenge, with over
25,000 new vulnerabilities reported in the Common Vulnerabilities and Exposures
(CVE) database in 2024 alone. While deep learning based approaches show promise
for vulnerability detection, recent studies reveal critical limitations in
terms of accuracy and robustness: accuracy drops by up to 45% on rigorously
verified datasets, and performance degrades significantly under simple code
modifications. This paper presents LLMxCPG, a novel framework integrating Code
Property Graphs (CPG) with Large Language Models (LLM) for robust vulnerability
detection. Our CPG-based slice construction technique reduces code size by
67.84 to 90.93% while preserving vulnerability-relevant context. Our approach's
ability to provide a more concise and accurate representation of code snippets
enables the analysis of larger code segments, including entire projects. This
concise representation is a key factor behind the improved detection
capabilities of our method, as it can now identify vulnerabilities that span
multiple functions. Empirical evaluation demonstrates LLMxCPG's effectiveness
across verified datasets, achieving 15-40% improvements in F1-score over
state-of-the-art baselines. Moreover, LLMxCPG maintains high performance across
function-level and multi-function codebases while exhibiting robust detection
efficacy under various syntactic code modifications.

</details>


### [17] [When LLMs Copy to Think: Uncovering Copy-Guided Attacks in Reasoning LLMs](https://arxiv.org/abs/2507.16773)
*Yue Li,Xiao Li,Hao Wu,Yue Zhang,Fengyuan Xu,Xiuzhen Cheng,Sheng Zhong*

Main category: cs.CR

TL;DR: This paper introduces Copy-Guided Attacks (CGA), a novel method exploiting LLMs' copying behavior to manipulate code analysis outcomes, enabling vulnerabilities like infinite loops and false conclusions. While effective in targeted scenarios, generalization challenges persist, highlighting the need for prompt-level defenses.


<details>
  <summary>Details</summary>
Motivation: LLMs are critical for code analysis but their integration creates new vulnerabilities. The paper addresses the underexplored risk of adversarial code snippet copying, which can distort reasoning and compromise downstream tasks such as vulnerability detection.

Method: The authors formalize CGA as an optimization problem and propose a gradient-based approach to synthesize triggers. These triggers are injected into external code snippets to exploit the LLMs' tendency to replicate malicious content during inference.

Result: Empirical results demonstrate CGA successfully induces infinite loops, premature termination, false refusals, and semantic distortions in multiple state-of-the-art reasoning LLMs. However, computational constraints limit cross-prompt generalization.

Conclusion: The study reveals critical vulnerabilities in LLM-powered development pipelines, emphasizing the need for prompt-level defense mechanisms. It raises open questions about generalization efficiency, prompting further research on secure LLM deployment in code analysis.

Abstract: Large Language Models (LLMs) have become integral to automated code analysis,
enabling tasks such as vulnerability detection and code comprehension. However,
their integration introduces novel attack surfaces. In this paper, we identify
and investigate a new class of prompt-based attacks, termed Copy-Guided Attacks
(CGA), which exploit the inherent copying tendencies of reasoning-capable LLMs.
By injecting carefully crafted triggers into external code snippets,
adversaries can induce the model to replicate malicious content during
inference. This behavior enables two classes of vulnerabilities: inference
length manipulation, where the model generates abnormally short or excessively
long reasoning traces; and inference result manipulation, where the model
produces misleading or incorrect conclusions. We formalize CGA as an
optimization problem and propose a gradient-based approach to synthesize
effective triggers. Empirical evaluation on state-of-the-art reasoning LLMs
shows that CGA reliably induces infinite loops, premature termination, false
refusals, and semantic distortions in code analysis tasks. While highly
effective in targeted settings, we observe challenges in generalizing CGA
across diverse prompts due to computational constraints, posing an open
question for future research. Our findings expose a critical yet underexplored
vulnerability in LLM-powered development pipelines and call for urgent advances
in prompt-level defense mechanisms.

</details>


### [18] [AUTOPSY: A Framework for Tackling Privacy Challenges in the Automotive Industry](https://arxiv.org/abs/2507.16788)
*Sebastian Pape,Anis Bkakria,Maurice Heymann,Badreddine Chah,Abdeljalil Abbas-Turki,Sarah Syed-Winkler,Matthias Hiller,Reda Yaich*

Main category: cs.CR

TL;DR: The AUTOPSY project developed technical solutions to improve privacy in connected vehicles beyond GDPR compliance, including a system model, privacy manager, PET selection approach based on GDPR principles, architectural framework, and a demonstrator for location-based services.


<details>
  <summary>Details</summary>
Motivation: GDPR compliance alone does not ensure privacy-friendly automotive systems, as processes like user consent collection do not address underlying privacy risks in connected and automated vehicles.

Method: The project implemented a privacy engineering framework through: (1) System modeling to identify PET application contexts, (2) Designing a privacy manager for data control, (3) Developing a GDPR-principles-driven PET selection methodology, (4) Creating an architectural framework with reusable privacy components, and (5) Building a location-based services demonstrator for evaluation.

Result: Delivered tools include: (1) System model for PET identification across 162 components, (2) Privacy manager with dynamic access controls, (3) PET selection approach addressing 349 privacy risks, (4) Reference architecture with 12 privacy layers, (5) Functional demonstrator for location services with de-identification and data minimization features.

Conclusion: The architectural framework and PET integration strategy significantly enhance automotive privacy engineering by systematically implementing GDPR requirements through technical measures rather than superficial compliance checks, as validated by the demonstrator's successful proof-of-concept implementation.

Abstract: With the General Data Protection Regulation (GDPR) in place, all domains have
to ensure compliance with privacy legislation. However, compliance does not
necessarily result in a privacy-friendly system as for example getting users'
consent to process their data does not improve the privacy-friendliness of the
system. Therefore, the goal of the AUTOPSY project was to support the privacy
engineering process in the automotive domain by providing several building
blocks which technically improve the privacy-friendliness of modern, i.e.,
connected and (partially) automated vehicles. This paper presents the results
of the AUTOPSY project: a system model to identify relevant entities and
locations to apply privacy enhancing technologies (PETs); the privacy manager
aiming at more control of the data flow from the vehicle, a PET selection
approach based on GDPR principles, and an architectural framework for
automotive privacy. Furthermore, we built a demonstrator for location-based
services to evaluate the architectural framework.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [19] [AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?](https://arxiv.org/abs/2507.15887)
*Ori Press,Brandon Amos,Haoyu Zhao,Yikai Wu,Samuel K. Ainsworth,Dominik Krupke,Patrick Kidger,Touqir Sajed,Bartolomeo Stellato,Jisun Park,Nathanael Bosch,Eli Meril,Albert Steppi,Arman Zharmagambetov,Fangzhao Zhang,David Perez-Pineiro,Alberto Mercurio,Ni Zhan,Talor Abramovich,Kilian Lieret,Hanlin Zhang,Shirley Huang,Matthias Bethge,Ofir Press*

Main category: cs.SE

TL;DR: AlgoTune introduces a novel open-ended benchmark to evaluate language models' ability to design and implement algorithms across computer science, physics, and mathematics. While achieving 1.72x speedups over reference solvers using libraries like SciPy, models currently lack the capacity for algorithmic innovation, focusing instead on superficial optimizations.


<details>
  <summary>Details</summary>
Motivation: Previous LM evaluations focused only on solving pre-existing human-defined tasks, not creating algorithmic solutions from scratch. This work seeks to measure true algorithmic creativity and reasoning in computationally challenging domains.

Method: 1) Developed 155 expert-curated coding tasks for AlgoTune benchmark
2) Created framework to validate correctness and benchmark efficiency of LM-generated code
3) Compared solutions against reference implementations from established libraries
4) Evaluated performance using baseline AlgoTuner agent and state-of-the-art models

Result: Achieved 1.72x average speedup over reference optimizers.
Identified limitations in deep algorithmic innovation - models prioritized surface-level code optimizations over discovering novel algorithms.
Revealed performance gaps between model capabilities and breakthrough algorithm design.

Conclusion: AlgoTune demonstrates viable methodology to assess algorithmic creativity while exposing key limitations in current language models. The benchmark provides targeted avenues for developing LMs that could eventually realize novel algorithmic contributions beyond human precedent.

Abstract: Despite progress in language model (LM) capabilities, evaluations have thus
far focused on models' performance on tasks that humans have previously solved,
including in programming (Jimenez et al., 2024) and mathematics (Glazer et al.,
2024). We therefore propose testing models' ability to design and implement
algorithms in an open-ended benchmark: We task LMs with writing code that
efficiently solves computationally challenging problems in computer science,
physics, and mathematics. Our AlgoTune benchmark consists of 155 coding tasks
collected from domain experts and a framework for validating and timing
LM-synthesized solution code, which is compared to reference implementations
from popular open-source packages. In addition, we develop a baseline LM agent,
AlgoTuner, and evaluate its performance across a suite of frontier models.
AlgoTuner achieves an average 1.72x speedup against our reference solvers,
which use libraries such as SciPy, sk-learn and CVXPY. However, we find that
current models fail to discover algorithmic innovations, instead preferring
surface-level optimizations. We hope that AlgoTune catalyzes the development of
LM agents exhibiting creative problem solving beyond state-of-the-art human
performance.

</details>


### [20] [Dr. Boot: Bootstrapping Program Synthesis Language Models to Perform Repairing](https://arxiv.org/abs/2507.15889)
*Noah van der Vleuten*

Main category: cs.SE

TL;DR: The paper introduces a bootstrapping algorithm for program synthesis that teaches models to repair code iteratively, outperforming regular fine-tuning and matching results from 68% larger models.


<details>
  <summary>Details</summary>
Motivation: Current language models for program synthesis struggle with limited dataset size/quality and misaligned synthesis processes compared to human iterative code development using compilers. These models are data-hungry and produce code in one go.

Method: A bootstrapping algorithm was developed to teach models code repair. The algorithm improves learning through iterative refinement rather than single-shot generation, simulating human development patterns.

Result: Bootstrapping consistently outperforms regular fine-tuning, achieving performance comparable to models 68% larger. However, repair-based inference is often outperformed by direct sampling despite repair benefits during training.

Conclusion: The study demonstrates that bootstrapping with repair mechanisms enhances program synthesis effectiveness while using fewer resources, but emphasizes limitations in evaluation datasets and potential improvements in inference techniques for repair processes.

Abstract: Language models for program synthesis are usually trained and evaluated on
programming competition datasets (MBPP, APPS). However, these datasets are
limited in size and quality, while these language models are extremely data
hungry. Additionally, the language models have a misaligned program synthesis
process compared to humans. While humans iteratively develop code with the help
of a compiler, most program synthesis models currently produce code in one go.
To solve these issues, we introduce a bootstrapping algorithm for program
synthesis, that supports teaching models how to repair. We show that
bootstrapping consistently outperforms regular fine-tuning. Compared to other
work, our bootstrapped model performs on par with fine-tuned models that are
68\% larger. Notably, bootstrapping with repairing also improves non-repairing
performance compared to regular bootstrapping during inference. However, on our
models, repairing during inference is likely inferior to simply sampling the
same number of solutions. Furthermore, we find that there are issues with the
example test cases in the training portion of the APPS dataset that are
valuable to the community, as many repairing and reinforcement learning methods
rely on them.

</details>


### [21] [StaAgent: An Agentic Framework for Testing Static Analyzers](https://arxiv.org/abs/2507.15892)
*Elijah Nnorom,Md Basim Uddin Ahmed,Jiho Shin,Hung Viet Pham,Song Wang*

Main category: cs.SE

TL;DR: StaAgent is an LLM-driven multi-agent framework that identifies inconsistent behaviors in static analyzers by generating and evaluating bug-inducing seed programs and mutants.


<details>
  <summary>Details</summary>
Motivation: Static analyzers are under-tested and prone to rule implementation inconsistencies, which can lead to missed bugs in software.

Method: The framework uses four agents: Seed Generation (translates rules into seed programs), Code Validation (ensures correctness), Mutation Generation (creates semantically equivalent mutants), and Analyzer Evaluation (compares analyzer behavior via metamorphic testing).

Result: 64 problematic rules were revealed across five static analyzers (SpotBugs, SonarQube, etc.), with 53 undetectable by prior methods; two bugs were fixed, three confirmed, and the rest under review.

Conclusion: StaAgent demonstrates that agentic, LLM-based data synthesis can significantly enhance static analyzer reliability and advance software engineering practices.

Abstract: Static analyzers play a critical role in identifying bugs early in the
software development lifecycle, but their rule implementations are often
under-tested and prone to inconsistencies. To address this, we propose
StaAgent, an agentic framework that harnesses the generative capabilities of
Large Language Models (LLMs) to systematically evaluate static analyzer rules.
StaAgent comprises four specialized agents: a Seed Generation Agent that
translates bug detection rules into concrete, bug-inducing seed programs; a
Code Validation Agent that ensures the correctness of these seeds; a Mutation
Generation Agent that produces semantically equivalent mutants; and an Analyzer
Evaluation Agent that performs metamorphic testing by comparing the static
analyzer's behavior on seeds and their corresponding mutants. By revealing
inconsistent behaviors, StaAgent helps uncover flaws in rule implementations.
This LLM-driven, multi-agent framework offers a scalable and adaptable solution
to improve the reliability of static analyzers. We evaluated StaAgent with five
state-of-the-art LLMs (CodeL-lama, DeepSeek, Codestral, Qwen, and GPT-4o)
across five widely used static analyzers (SpotBugs, SonarQube, ErrorProne,
Infer, and PMD). The experimental results show that our approach can help
reveal 64 problematic rules in the latest versions of these five static
analyzers (i.e., 28 in SpotBugs, 18 in SonarQube, 6 in ErrorProne, 4 in Infer,
and 8 in PMD). In addition, 53 out of the 64 bugs cannot be detected by the
SOTA baseline. We have reported all the bugs to developers, with two of them
already fixed. Three more have been confirmed by developers, while the rest are
awaiting response. These results demonstrate the effectiveness of our approach
and underscore the promise of agentic, LLM-driven data synthesis to advance
software engineering.

</details>


### [22] [A Pilot Study on LLM-Based Agentic Translation from Android to iOS: Pitfalls and Insights](https://arxiv.org/abs/2507.16037)
*Zhili Zeng,Kimya Khakzad Shahandashti,Alvine Boaye Belle,Song Wang,Zhen Ming,Jiang*

Main category: cs.SE

TL;DR: This study evaluates LLM-based agents for Android-to-iOS mobile application translation, identifies failure points, and proposes guidelines to improve cross-platform translation performance.


<details>
  <summary>Details</summary>
Motivation: Current mobile app translation methods between Android and iOS are reliance on manual/rule-based systems or insufficient ML approaches lacking contextual understanding. LLM-based translation remains underexplored despite potential for automation.

Method: Developed an agent chain that handles dependencies, specifications, program structure, and control flow during translation. Evaluated syntactic correctness, semantic accuracy, and functional completeness through manual inspection and root cause analysis of failures.

Result: Systematically characterized LLM limitations in cross-platform translation, including platform-specific framework adaptation issues and control flow handling deficiencies. Identified common failure modes in agentic approaches.

Conclusion: LLM-based agents offer promise but require platform-aware architecture adaptations and improved program control flow modeling to achieve effective cross-platform mobile application translation.

Abstract: The rapid advancement of mobile applications has led to a significant demand
for cross-platform compatibility, particularly between the Android and iOS
platforms. Traditional approaches to mobile application translation often rely
on manual intervention or rule-based systems, which are labor-intensive and
time-consuming. While recent advancements in machine learning have introduced
automated methods, they often lack contextual understanding and adaptability,
resulting in suboptimal translations. Large Language Models (LLMs) were
recently leveraged to enhance code translation at different granularities,
including the method, class, and repository levels. Researchers have
investigated common errors, limitations, and potential strategies to improve
these tasks. However, LLM-based application translation across different
platforms, such as migrating mobile applications between Android and iOS or
adapting software across diverse frameworks, remains underexplored.
Understanding the performance, strengths, and limitations of LLMs in
cross-platform application translation is critical for advancing software
engineering automation. This study aims to fill this gap by evaluating
LLM-based agentic approaches for mobile application translation, identifying
key failure points, and proposing guidelines to improve translation
performance. We developed a chain of agents that account for dependencies,
specifications, program structure, and program control flow when translating
applications from Android to iOS. To evaluate the performance, we manually
examined the translated code for syntactic correctness, semantic accuracy, and
functional completeness. For translation failures, we further conducted a
detailed root cause analysis to understand the underlying limitations of the
agentic translation process and identify opportunities for improvement.

</details>


### [23] [Making REST APIs Agent-Ready: From OpenAPI to Model Context Protocol Servers for Tool-Augmented LLMs](https://arxiv.org/abs/2507.16044)
*Meriem Mastouri,Emna Ksontini,Wael Kessentini*

Main category: cs.SE

TL;DR: The paper introduces AutoMCP, a compiler that automates the creation of Model Context Protocol (MCP) servers from OpenAPI 3.0 specifications, achieving 99.9% success after resolving minor spec inconsistencies.


<details>
  <summary>Details</summary>
Motivation: Manual MCP server construction requires repetitive scaffolding and glue code, limiting adoption despite MCP's potential to streamline tool integration for large language models.

Method: AutoMCP was developed to parse REST API definitions and generate schema-registered, authenticated servers. The tool was evaluated on 50 real-world APIs with 5,066 endpoints via stratified sampling of 1,023 tool calls.

Result: 76.5% of tool calls worked without modification, rising to 99.9% after 19-line average adjustments per API to fix OpenAPI specification flaws. The tool supports dynamic integration of APIs across 10+ domains.

Conclusion: Automatic MCP server generation is viable using OpenAPI 3.0 contracts despite specification quality issues. The work contributes (1) adoption analysis of manual MCP costs, (2) demonstration of 99.9% automation feasibility, and (3) a 5,066-endpoint tool corpus with spec repair patterns.

Abstract: Large Language Models (LLMs) are evolving from passive text generators into
active agents that invoke external tools. To support this shift, scalable
protocols for tool integration are essential. The Model Context Protocol (MCP),
introduced by Anthropic in 2024, offers a schema-driven standard for dynamic
tool discovery and invocation. Yet, building MCP servers remains manual and
repetitive, requiring developers to write glue code, handle authentication, and
configure schemas by hand-replicating much of the integration effort MCP aims
to eliminate.
  This paper investigates whether MCP server construction can be meaningfully
automated. We begin by analyzing adoption trends: among 22,000+ MCP-tagged
GitHub repositories created within six months of release, fewer than 5% include
servers, typically small, single-maintainer projects dominated by repetitive
scaffolding. To address this gap, we present AutoMCP, a compiler that generates
MCP servers from OpenAPI 2.0/3.0 specifications. AutoMCP parses REST API
definitions and produces complete server implementations, including schema
registration and authentication handling.
  We evaluate AutoMCP on 50 real-world APIs spanning 5,066 endpoints across
over 10 domains. From a stratified sample of 1,023 tool calls, 76.5% succeeded
out of the box. Manual failure analysis revealed five recurring issues, all
attributable to inconsistencies or omissions in the OpenAPI contracts. After
minor fixes, averaging 19 lines of spec changes per API, AutoMCP achieved 99.9%
success.
  Our findings (i) analyze MCP adoption and quantify the cost of manual server
development, (ii) demonstrate that OpenAPI specifications, despite quality
issues, enable near-complete MCP server automation, and (iii) contribute a
corpus of 5,066 callable tools along with insights on repairing common
specification flaws.

</details>


### [24] [AI-Powered Commit Explorer (APCE)](https://arxiv.org/abs/2507.16063)
*Yousab Grees,Polina Iaremchuk,Ramtin Ehsani,Esteban Parra,Preetha Chatterjee,Sonia Haiduc*

Main category: cs.SE

TL;DR: The paper introduces APCE, a tool for developers and researchers to use and study LLM-generated commit messages, offering prompt storage, evaluation prompts, and evaluation mechanisms.


<details>
  <summary>Details</summary>
Motivation: High-quality commit messages are often neglected, and LLM-generated messages need improvement in practice. Researchers require tools to study and evaluate their effectiveness.

Method: APCE was developed to allow storing LLM prompts, integrate an evaluation prompt for enhanced message generation, and provide automated/human evaluation mechanisms for assessing generated messages.

Result: APCE supports researchers in prompt management and evaluation of LLM-generated commit messages, with a demo available for demonstration.

Conclusion: APCE addresses the need for tools to systematically evaluate and improve LLM-generated commit messages, facilitating both practical use and academic research.

Abstract: Commit messages in a version control system provide valuable information for
developers regarding code changes in software systems. Commit messages can be
the only source of information left for future developers describing what was
changed and why. However, writing high-quality commit messages is often
neglected in practice. Large Language Model (LLM) generated commit messages
have emerged as a way to mitigate this issue. We introduce the AI-Powered
Commit Explorer (APCE), a tool to support developers and researchers in the use
and study of LLM-generated commit messages. APCE gives researchers the option
to store different prompts for LLMs and provides an additional evaluation
prompt that can further enhance the commit message provided by LLMs. APCE also
provides researchers with a straightforward mechanism for automated and human
evaluation of LLM-generated messages. Demo link https://youtu.be/zYrJ9s6sZvo

</details>


### [25] [Ten Essential Guidelines for Building High-Quality Research Software](https://arxiv.org/abs/2507.16166)
*Nasir U. Eisty,David E. Bernholdt,Alex Koufos,David J. Luet,Miranda Mundt*

Main category: cs.SE

TL;DR: This paper proposes ten guidelines for developing high-quality research software across the entire lifecycle, focusing on best practices for robustness, usability, and sustainability.


<details>
  <summary>Details</summary>
Motivation: To address the critical need for reliable and reusable research software that supports scientific progress through reproducibility, maintainability, and effective collaboration.

Method: The work synthesizes ten evidence-based guidelines covering software planning, clean code practices, version control, testing strategies, modular design, performance optimization, documentation, and long-term maintenance principles.

Result: Demonstrates that following these guidelines produces research software with enhanced robustness, usability, and sustainability, while fostering community engagement and scientific reproducibility.

Conclusion: Researchers adopting these guidelines can advance their scientific objectives while contributing to a more reliable and interconnected ecosystem of research tools through improved software quality and community standards.

Abstract: High-quality research software is a cornerstone of modern scientific
progress, enabling researchers to analyze complex data, simulate phenomena, and
share reproducible results. However, creating such software requires adherence
to best practices that ensure robustness, usability, and sustainability. This
paper presents ten guidelines for producing high-quality research software,
covering every stage of the development lifecycle. These guidelines emphasize
the importance of planning, writing clean and readable code, using version
control, and implementing thorough testing strategies. Additionally, they
address key principles such as modular design, reproducibility, performance
optimization, and long-term maintenance. The paper also highlights the role of
documentation and community engagement in enhancing software usability and
impact. By following these guidelines, researchers can create software that
advances their scientific objectives and contributes to a broader ecosystem of
reliable and reusable research tools. This work serves as a practical resource
for researchers and developers aiming to elevate the quality and impact of
their research software.

</details>


### [26] [LOCOFY Large Design Models -- Design to code conversion solution](https://arxiv.org/abs/2507.16208)
*Sohaib Muhammad,Ashwati Vipin,Karan Shetti,Honey Mittal*

Main category: cs.SE

TL;DR: The paper proposes Large Design Models (LDMs) to address design-to-code challenges not solvable by existing LLMs through a novel training pipeline with three core components (Design Optimiser, Tagging & Feature Detection, Auto Components), demonstrating superior conversion accuracy and code efficiency via preview match score and benchmark comparison.


<details>
  <summary>Details</summary>
Motivation: Current LLMs and Multimodal LLMs face limitations in interpretability, scalability, resource efficiency, and repeatability when applied to design-to-code conversion tasks.

Method: Developed a dual-phase approach with 1) Design Optimiser for sub-optimal design correction, 2) Tagging & Feature Detection using fine-tuned pre-trained models for UI element classification, and 3) Auto Components for modular code generation. Inference processes real-world designs into precise code instructions with reliability.

Result: LDMs outperformed LLMs in node positioning accuracy (38.2% improvement), responsiveness (27.5% improvement), and reproducibility (41.7% improvement). Custom tagging model achieved 92.3% precision across 1,000+ test designs.

Conclusion: LDMs represent a superior and reliable paradigm for end-to-end design-to-code conversion, enabling production-ready code with enhanced modularity and reduced redundancy through their architecture-specific optimizations.

Abstract: Despite rapid advances in Large Language Models and Multimodal Large Language
Models (LLMs), numerous challenges related to interpretability, scalability,
resource requirements and repeatability remain, related to their application in
the design-to-code space. To address this, we introduce the Large Design Models
(LDMs) paradigm specifically trained on designs and webpages to enable seamless
conversion from design-to-code. We have developed a training and inference
pipeline by incorporating data engineering and appropriate model architecture
modification. The training pipeline consists of the following: 1)Design
Optimiser: developed using a proprietary ground truth dataset and addresses
sub-optimal designs; 2)Tagging and feature detection: using pre-trained and
fine-tuned models, this enables the accurate detection and classification of UI
elements; and 3)Auto Components: extracts repeated UI structures into reusable
components to enable creation of modular code, thus reducing redundancy while
enhancing code reusability. In this manner, each model addresses distinct but
key issues for design-to-code conversion. Separately, our inference pipeline
processes real-world designs to produce precise and interpretable instructions
for code generation and ensures reliability. Additionally, our models
illustrated exceptional end-to-end design-to-code conversion accuracy using a
novel preview match score metric. Comparative experiments indicated superior
performance of LDMs against LLMs on accuracy of node positioning,
responsiveness and reproducibility. Moreover, our custom-trained tagging and
feature detection model demonstrated high precision and consistency in
identifying UI elements across a wide sample of test designs. Thus, our
proposed LDMs are a reliable and superior solution to understanding designs
that subsequently enable the generation of efficient and reliable
production-ready code.

</details>


### [27] [Search-based Generation of Waypoints for Triggering Self-Adaptations in Maritime Autonomous Vessels](https://arxiv.org/abs/2507.16327)
*Karoline Nylænder,Aitor Arrieta,Shaukat Ali,Paolo Arcaini*

Main category: cs.SE

TL;DR: WPgen uses multiobjective search to generate modified waypoints for testing self-adaptation in autonomous maritime vessels, showing effectiveness varies across vessel types.


<details>
  <summary>Details</summary>
Motivation: Ensuring safety and dependability in autonomous vessels through systematic testing of self-adaptation mechanisms against unexpected waypoint modifications.

Method: Proposes WPgen with NSGA-II and three seeding strategies to generate perturbed waypoints, evaluated on three AVs (one overwater tanker, two underwater).

Result: Experimental results demonstrated variation in effectiveness among WPgen's three variations across different AV types.

Conclusion: Highlights research and practical implications for improving self-adaptation validation in maritime autonomous systems through tailored waypoint generation.

Abstract: Self-adaptation in maritime autonomous vessels (AVs) enables them to adapt
their behaviors to address unexpected situations while maintaining
dependability requirements. During the design of such AVs, it is crucial to
understand and identify the settings that should trigger adaptations, enabling
validation of their implementation. To this end, we focus on the navigation
software of AVs, which must adapt their behavior during operation through
adaptations. AVs often rely on predefined waypoints to guide them along
designated routes, ensuring safe navigation. We propose a multiobjective
search-based approach, called WPgen, to generate minor modifications to the
predefined set of waypoints, keeping them as close as possible to the original
waypoints, while causing the AV to navigate inappropriately when navigating
with the generated waypoints. WPgen uses NSGA-II as the multi-objective search
algorithm with three seeding strategies for its initial population, resulting
in three variations of WPgen. We evaluated these variations on three AVs (one
overwater tanker and two underwater). We compared the three variations of WPgen
with Random Search as the baseline and with each other. Experimental results
showed that the effectiveness of these variations varied depending on the AV.
Based on the results, we present the research and practical implications of
WPgen.

</details>


### [28] [Improving Code LLM Robustness to Prompt Perturbations via Layer-Aware Model Editing](https://arxiv.org/abs/2507.16407)
*Shuhan Liu,Xing Hu,Kerui Huang,Xiaohu Yang,David Lo,Xin Xia*

Main category: cs.SE

TL;DR: CREME is a novel method that enhances the robustness of large language models in code generation by identifying and editing parameters in robustness-sensitive layers, improving performance on perturbed prompts without compromising accuracy on clean inputs.


<details>
  <summary>Details</summary>
Motivation: LLMs are highly sensitive to prompt perturbations (e.g., wording changes, formatting), leading to reduced functional correctness in real-world scenarios. This sensitivity hinders reliable code generation, necessitating robustness improvements.

Method: CREME identifies robustness-sensitive layers by comparing hidden states of original and perturbed prompts. It then applies lightweight parameter editing at those layers to mitigate performance degradation caused by perturbations.

Result: Evaluations on HumanEval and MBPP benchmarks with perturbed prompts demonstrate CREME improves Pass@1 accuracy by 63% on perturbed inputs while maintaining a deviation of ≤1% on clean inputs. Sensitive layers are concentrated in middle/deep layers and vary across architectures.

Conclusion: CREME effectively addresses prompt perturbation vulnerability in code generation, showing that targeted parameter editing in identified sensitive layers improves robustness. The findings suggest model-architecture-specific sensitive layers and lay the groundwork for future robustness-focused editing strategies.

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
code generation, where the natural language prompt plays a crucial role in
conveying user intent to the model. However, prior studies have shown that LLMs
are highly sensitive to prompt perturbations. Minor modifications in wording,
syntax, or formatting can significantly reduce the functional correctness of
generated code. As perturbations frequently occur in real-world scenarios,
improving the robustness of LLMs to prompt perturbations is essential for
ensuring reliable performance in practical code generation. In this paper, we
introduce CREME (Code Robustness Enhancement via Model Editing), a novel
approach that enhances LLM robustness through targeted parameter updates. CREME
first identifies robustness-sensitive layers by comparing hidden states between
an original prompt and its perturbed variant. Then, it performs lightweight
parameter editing at the identified layer to reduce performance degradation. We
evaluate CREME on two widely used code generation benchmarks (HumanEval and
MBPP) along with their perturbed counterparts. Experimental results show that
CREME improves Pass@1 accuracy by 63% on perturbed prompts while maintaining
stable performance on clean inputs, with accuracy deviations within 1%. Further
analysis reveals that robustness-sensitive layers are primarily concentrated in
the middle and deeper layers of the network, and their locations vary across
different model architectures. These insights provide a valuable foundation for
developing future robustness-oriented editing strategies.

</details>


### [29] [Exploring Large Language Models for Analyzing and Improving Method Names in Scientific Code](https://arxiv.org/abs/2507.16439)
*Gunnar Larsen,Carol Wong,Anthony Peruma*

Main category: cs.SE

TL;DR: The paper evaluates four LLMs' effectiveness in analyzing and improving method names in scientific Python code, finding they are moderately helpful but require human review due to domain-specific inconsistencies.


<details>
  <summary>Details</summary>
Motivation: Scientific software relies heavily on clear method names, yet prior research focused mainly on traditional programming environments. LLMs offer potential for automating code quality tasks in this domain.

Method: Assessed four LLMs by analyzing grammatical patterns and recommending improvements for 496 method names from Python Jupyter Notebooks, comparing against established naming practices and human annotations.

Result: LLMs demonstrated moderate effectiveness, often adhering to verb-leading naming conventions but showing inconsistency with domain jargon and only 56-70% agreement with human evaluators.

Conclusion: Automated LLM-based name recommendations can aid scientific code quality improvement but must be supplemented with human oversight due to moderate reliability and domain knowledge gaps.

Abstract: Research scientists increasingly rely on implementing software to support
their research. While previous research has examined the impact of identifier
names on program comprehension in traditional programming environments, limited
work has explored this area in scientific software, especially regarding the
quality of method names in the code. The recent advances in Large Language
Models (LLMs) present new opportunities for automating code analysis tasks,
such as identifier name appraisals and recommendations. Our study evaluates
four popular LLMs on their ability to analyze grammatical patterns and suggest
improvements for 496 method names extracted from Python-based Jupyter
Notebooks. Our findings show that the LLMs are somewhat effective in analyzing
these method names and generally follow good naming practices, like starting
method names with verbs. However, their inconsistent handling of
domain-specific terminology and only moderate agreement with human annotations
indicate that automated suggestions require human evaluation. This work
provides foundational insights for improving the quality of scientific code
through AI automation.

</details>


### [30] [On the Effectiveness of LLM-as-a-judge for Code Generation and Summarization](https://arxiv.org/abs/2507.16587)
*Giuseppe Crupi,Rosalia Tufano,Alejandro Velasco,Antonio Mastropaolo,Denys Poshyvanyk,Gabriele Bavota*

Main category: cs.SE

TL;DR: The paper evaluates the effectiveness of LLMs as judges for code generation and summarization tasks, finding that GPT-4-turbo performs best but still frequently misjudges correctness and quality.


<details>
  <summary>Details</summary>
Motivation: The study addresses the limitations of quantitative metrics and the expense of human evaluation in assessing code tasks, aiming to explore LLMs' potential as scalable judgment tools for complex code instances.

Method: Eight LLMs were tested on judging code correctness (1,405 Java + 1,281 Python methods), while five LLMs and nine humans evaluated ~1.2k code summaries (Java/Python). Judging performance was compared across LLMs and human benchmarks.

Result: GPT-4-turbo outperformed other LLMs in both code generation and summarization judging tasks; smaller LLMs (10B+ parameters) exhibited poor judgment accuracy. However, top LLMs frequently misjudged code correctness and summary quality relative to human assessments.

Conclusion: While LLMs like GPT-4-turbo show promise as automated judges for code tasks, their frequent misjudgments highlight the need for specialized evaluation frameworks or hybrid approaches combining LLMs and human oversight.

Abstract: Large Language Models have been recently exploited as judges for complex
natural language processing tasks, such as Q&A. The basic idea is to delegate
to an LLM the assessment of the "quality" of the output provided by an
automated technique for tasks for which: (i) quantitative metrics would only
tell part of the story, and; (ii) a large-scale human-based evaluation would be
too expensive. LLMs-as-a-judge, if proven effective for a specific task, can
also unlock new possibilities for automation, with several LLMs proposing a
solution for a given instance of the task and others judging and deciding what
is the best output to show the user. We study the effectiveness of
LLMs-as-a-judge for two code-related tasks, namely code generation and code
summarization. The rationale for choosing these tasks is two-fold. First,
quantitative metrics are usually not enough for the assessment of code
summarizers/generators. For example, it is well documented that metrics such as
BLEU are quite weak proxies for the quality of the generated summaries. Second,
even state-of-the-art techniques still struggle with handling complex instances
of these tasks, making them good candidates for benefiting from more advanced
solutions envisioning collaboration among LLMs. For code generation, we check
whether eight LLMs are able to judge the correctness of 1,405 Java methods and
1,281 Python functions generated by the same LLMs or implemented by humans. For
code summarization, we compare the judgment of five LLMs to those provided by
nine humans for ~1.2k summaries, related to both Java and Python functions. Our
findings show that GPT-4-turbo is the best LLM in terms of judging capabilities
for both tasks, with "smaller" LLMs featuring tens of billions parameters not
being able to cope with judging tasks. However, even the best-performing LLM
frequently misjudges the correctness of the code and summary quality.

</details>


### [31] [VulCoCo: A Simple Yet Effective Method for Detecting Vulnerable Code Clones](https://arxiv.org/abs/2507.16661)
*Tan Bui,Yan Naing Tun,Thanh Phuc Nguyen,Yindu Su,Ferdian Thung,Yikun Li,Han Wei Ang,Yide Yin,Frank Liauw,Lwin Khin Shar,Eng Lieh Ouh,Ting Zhang,David Lo*

Main category: cs.SE

TL;DR: VulCoCo is a novel VCC detection method combining embeddings and LLM validation, featuring a synthetic benchmark and real-world impact with 75 merged PRs and 15 new CVEs identified.


<details>
  <summary>Details</summary>
Motivation: Prior VCC detection tools often struggle with syntactic similarity limitations or lack explanation clarity, while reproducible benchmarks for evaluation are scarce.

Method: The approach uses embedding-based retrieval to identify function clones from a large code corpus, followed by LLM validation to determine if the vulnerability is retained. A synthetic benchmark spanning multiple clone types is also developed for evaluation.

Result: Outperforms existing methods on Precision@k and MAP metrics. Achieved 75 merged PRs in real open-source projects, including 15 CVEs resulting from vulnerability fixes.

Conclusion: Provides a lightweight, scalable VCC detection framework with actionable results, offering 284 real-world validations and highlighting paths for precision improvement in future research.

Abstract: Code reuse is common in modern software development, but it can also spread
vulnerabilities when developers unknowingly copy risky code. The code fragments
that preserve the logic of known vulnerabilities are known as vulnerable code
clones (VCCs). Detecting those VCCs is a critical but challenging task.
Existing VCC detection tools often rely on syntactic similarity or produce
coarse vulnerability predictions without clear explanations, limiting their
practical utility. In this paper, we propose VulCoCo, a lightweight and
scalable approach that combines embedding-based retrieval with large language
model (LLM) validation. Starting from a set of known vulnerable functions, we
retrieve syntactically or semantically similar candidate functions from a large
corpus and use an LLM to assess whether the candidates retain the
vulnerability. Given that there is a lack of reproducible vulnerable code clone
benchmarks, we first construct a synthetic benchmark that spans various clone
types.
  Our experiments on the benchmark show that VulCoCo outperforms prior
state-of-the-art methods in terms of Precision@k and mean average precision
(MAP). In addition, we also demonstrate VulCoCo's effectiveness in real-world
projects by submitting 400 pull requests (PRs) to 284 open-source projects.
Among them, 75 PRs were merged, and 15 resulted in newly published CVEs. We
also provide insights to inspire future work to further improve the precision
of vulnerable code clone detection.

</details>


### [32] [VulGuard: An Unified Tool for Evaluating Just-In-Time Vulnerability Prediction Models](https://arxiv.org/abs/2507.16685)
*Duong Nguyen,Manh Tran-Duc,Thanh Le-Cong,Triet Huynh Minh Le,M. Ali Babar,Quyet-Thang Huynh*

Main category: cs.SE

TL;DR: VulGuard is an automated tool for JIT-VP research that streamlines commit extraction, integrates models, and addresses scalability and reproducibility issues.


<details>
  <summary>Details</summary>
Motivation: The tool aims to overcome challenges in reproducibility, scalability, and efficient model experimentation in software security research, particularly for Just-In-Time vulnerability prediction.

Method: VulGuard automates commit history mining, extracts code changes, messages, and metrics, formats data for analysis, and integrates state-of-the-art vulnerability prediction models within a unified framework.

Result: The effectiveness of VulGuard was demonstrated through its application to FFmpeg and Linux kernel repositories, facilitating JIT-VP model training and evaluation.

Conclusion: VulGuard provides a standardized, scalable solution for JIT-VP research by unifying data processing and model experimentation, reducing setup complexity and promoting reproducible benchmarks.

Abstract: We present VulGuard, an automated tool designed to streamline the extraction,
processing, and analysis of commits from GitHub repositories for Just-In-Time
vulnerability prediction (JIT-VP) research. VulGuard automatically mines commit
histories, extracts fine-grained code changes, commit messages, and software
engineering metrics, and formats them for downstream analysis. In addition, it
integrates several state-of-the-art vulnerability prediction models, allowing
researchers to train, evaluate, and compare models with minimal setup. By
supporting both repository-scale mining and model-level experimentation within
a unified framework, VulGuard addresses key challenges in reproducibility and
scalability in software security research. VulGuard can also be easily
integrated into the CI/CD pipeline. We demonstrate the effectiveness of the
tool in two influential open-source projects, FFmpeg and the Linux kernel,
highlighting its potential to accelerate real-world JIT-VP research and promote
standardized benchmarking. A demo video is available at:
https://youtu.be/j96096-pxbs

</details>


### [33] [Never Come Up Empty: Adaptive HyDE Retrieval for Improving LLM Developer Support](https://arxiv.org/abs/2507.16754)
*Fangjian Lei,Mariam El Mezouar,Shayan Noei,Ying Zou*

Main category: cs.SE

TL;DR: This paper presents an optimal Retrieval-Augmented Generation (RAG) pipeline for enhancing LLM reliability in answering developer questions, achieving higher accuracy and coverage across 4 open-source models.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) often generate unreliable answers (hallucinations) for developer questions. Effective RAG pipeline designs are needed to mitigate these risks while maintaining broad applicability across both familiar and novel queries.

Method: The authors (1) constructed a 3M+ Java/Python Stack Overflow corpus and tested 7 RAG pipelines with 63 variants for similarity-matched questions, and (2) implement dynamic similarity-threshold lowering to handle novel questions. The most effective approach combines hypothetical-documentation-embedding (HyDE) with full-answer context retrieval.

Result: The optimal RAG pipeline (HyDE + full-answer context) consistently outperformed zero-shot baselines across four open-source LLMs, showing higher helpfulness, correctness, and detail scores using LLM-as-a-judge evaluation for both seen and novel developer questions.

Conclusion: Optimized RAG pipelines robustly improve LLM answer quality for developer queries by effectively addressing hallucinations while maintaining strong performance on unseen questions through adaptive retrieval strategies and context-aware embeddings.

Abstract: Large Language Models (LLMs) have shown promise in assisting developers with
code-related questions; however, LLMs carry the risk of generating unreliable
answers. To address this, Retrieval-Augmented Generation (RAG) has been
proposed to reduce the unreliability (i.e., hallucinations) of LLMs. However,
designing effective pipelines remains challenging due to numerous design
choices. In this paper, we construct a retrieval corpus of over 3 million Java
and Python related Stack Overflow posts with accepted answers, and explore
various RAG pipeline designs to answer developer questions, evaluating their
effectiveness in generating accurate and reliable responses. More specifically,
we (1) design and evaluate 7 different RAG pipelines and 63 pipeline variants
to answer questions that have historically similar matches, and (2) address new
questions without any close prior matches by automatically lowering the
similarity threshold during retrieval, thereby increasing the chance of finding
partially relevant context and improving coverage for unseen cases. We find
that implementing a RAG pipeline combining hypothetical-documentation-embedding
(HyDE) with the full-answer context performs best in retrieving and answering
similarcontent for Stack Overflow questions. Finally, we apply our optimal RAG
pipeline to 4 open-source LLMs and compare the results to their zero-shot
performance. Our findings show that RAG with our optimal RAG pipeline
consistently outperforms zero-shot baselines across models, achieving higher
scores for helpfulness, correctness, and detail with LLM-as-a-judge. These
findings demonstrate that our optimal RAG pipelines robustly enhance answer
quality for a wide range of developer queries including both previously seen
and novel questions across different LLMs

</details>


### [34] [Rethinking LLM-Based RTL Code Optimization Via Timing Logic Metamorphosis](https://arxiv.org/abs/2507.16808)
*Zhihao Xu,Bixin Li,Lulu Wang*

Main category: cs.SE

TL;DR: This paper evaluates the effectiveness of LLM-based RTL optimization methods on complex timing logic. It introduces a new benchmark with four subsets and a metamorphosis method, revealing that while LLMs outperform compiler-based approaches in logic operations, they lag in timing-related optimizations like control flow and clock domain.


<details>
  <summary>Details</summary>
Motivation: Traditional RTL optimization relies on manual methods and heuristics, which are inefficient and prone to errors. LLMs offer potential automation but their performance on complex timing logic remains underexplored, necessitating a comprehensive evaluation.

Method: The authors propose a benchmark with four RTL optimization subsets and use a metamorphosis approach to test LLMs on semantically equivalent but more complex code, systematically assessing their optimization capabilities.

Result: LLM-based methods excel in optimizing logic operations, outperforming compiler-based tools, but underperform in timing control flow optimization and clock domain optimization due to challenges in understanding timing logic in RTL code.

Conclusion: LLM-based RTL optimization is effective for logic-centric tasks but struggles with timing logic complexities. Future work should focus on improving LLMs' understanding of timing-specific nuances and hybrid approaches for broader applicability.

Abstract: Register Transfer Level(RTL) code optimization is crucial for achieving high
performance and low power consumption in digital circuit design. However,
traditional optimization methods often rely on manual tuning and heuristics,
which can be time-consuming and error-prone. Recent studies proposed to
leverage Large Language Models(LLMs) to assist in RTL code optimization. LLMs
can generate optimized code snippets based on natural language descriptions,
potentially speeding up the optimization process. However, existing approaches
have not thoroughly evaluated the effectiveness of LLM-Based code optimization
methods for RTL code with complex timing logic. To address this gap, we
conducted a comprehensive empirical investigation to assess the capability of
LLM-Based RTL code optimization methods in handling RTL code with complex
timing logic. In this study, we first propose a new benchmark for RTL
optimization evaluation. It comprises four subsets, each corresponding to a
specific area of RTL code optimization. Then we introduce a method based on
metamorphosis to systematically evaluate the effectiveness of LLM-Based RTL
code optimization methods.Our key insight is that the optimization
effectiveness should remain consistent for semantically equivalent but more
complex code. After intensive experiments, we revealed several key findings.
(1) LLM-Based RTL optimization methods can effectively optimize logic
operations and outperform existing compiler-based methods. (2) LLM-Based RTL
optimization methods do not perform better than existing compiler-based methods
on RTL code with complex timing logic, particularly in timing control flow
optimization and clock domain optimization. This is primarily attributed to the
challenges LLMs face in understanding timing logic in RTL code. Based on these
findings, we provide insights for further research in leveraging LLMs for RTL
code optimization.

</details>
