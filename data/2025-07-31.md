<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 16]
- [cs.SE](#cs.SE) [Total: 21]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Prompt Optimization and Evaluation for LLM Automated Red Teaming](https://arxiv.org/abs/2507.22133)
*Michael Freenor,Lauren Alvarez,Milton Leal,Lily Smith,Joel Garrett,Yelyzaveta Husieva,Madeline Woodruff,Ryan Miller,Erich Kummerfeld,Rafael Medeiros,Sander Schulhoff*

Main category: cs.CR

TL;DR: This paper proposes a method for optimizing attack generator prompts by evaluating individual attack success through repeated testing with random target seeds, enhancing the effectiveness of Automated Red Teaming against LLM vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Current evaluation of attack generators using Attack Success Rate (ASR) as a sample mean across all attacks overlooks exploitable patterns in individual vulnerabilities, limiting robust refinement of security measures.

Method: The proposed method calculates the discoverability of each attack by testing it multiple times against a target system with varying random seeds, measuring the expectation of success as a local ASR to guide prompt optimization.

Result: The approach identifies exploitable attack patterns not captured by global ASR metrics, enabling data-driven optimization of attack generators to achieve higher precision and robustness in vulnerability detection.

Conclusion: Local ASR-based optimization of attack generators increases the effectiveness of Automated Red Teaming for LLM applications by systematically uncovering and exploiting system vulnerabilities through repeatable attack testing.

Abstract: Applications that use Large Language Models (LLMs) are becoming widespread,
making the identification of system vulnerabilities increasingly important.
Automated Red Teaming accelerates this effort by using an LLM to generate and
execute attacks against target systems. Attack generators are evaluated using
the Attack Success Rate (ASR) the sample mean calculated over the judgment of
success for each attack. In this paper, we introduce a method for optimizing
attack generator prompts that applies ASR to individual attacks. By repeating
each attack multiple times against a randomly seeded target, we measure an
attack's discoverability the expectation of the individual attack success. This
approach reveals exploitable patterns that inform prompt optimization,
ultimately enabling more robust evaluation and refinement of generators.

</details>


### [2] [Strategic Deflection: Defending LLMs from Logit Manipulation](https://arxiv.org/abs/2507.22160)
*Yassine Rachidy,Jihad Rbaiti,Youssef Hmamouche,Faissal Sehbaoui,Amal El Fallah Seghrouchni*

Main category: cs.CR

TL;DR: This paper introduces Strategic Deflection (SDeflection), a defense mechanism against logit-level jailbreaking attacks on Large Language Models (LLMs), which redirects malicious intent without refusing prompts, thereby reducing attack success rates while preserving benign performance.


<details>
  <summary>Details</summary>
Motivation: Traditional LLM defenses block malicious prompts, but logit-level attacks bypass these by manipulating token selection during generation, necessitating a shift from refusal to proactive response strategies to ensure security.

Method: SDeflection alters the model's response process to produce semantically adjacent outputs that neutralize harmful intent by strategically deflecting the content direction during token selection, as opposed to outright rejecting the input.

Result: Experiments confirm SDeflection significantly lowers Attack Success Rates (ASR) of logit-based jailbreaking attacks while maintaining high accuracy on legitimate user queries.

Conclusion: The study advocates a new paradigm in LLM defense: prioritizing strategic content redirection over passive refusal to neutralize advanced threats effectively.

Abstract: With the growing adoption of Large Language Models (LLMs) in critical areas,
ensuring their security against jailbreaking attacks is paramount. While
traditional defenses primarily rely on refusing malicious prompts, recent
logit-level attacks have demonstrated the ability to bypass these safeguards by
directly manipulating the token-selection process during generation. We
introduce Strategic Deflection (SDeflection), a defense that redefines the
LLM's response to such advanced attacks. Instead of outright refusal, the model
produces an answer that is semantically adjacent to the user's request yet
strips away the harmful intent, thereby neutralizing the attacker's harmful
intent. Our experiments demonstrate that SDeflection significantly lowers
Attack Success Rate (ASR) while maintaining model performance on benign
queries. This work presents a critical shift in defensive strategies, moving
from simple refusal to strategic content redirection to neutralize advanced
threats.

</details>


### [3] [Programmable Data Planes for Network Security](https://arxiv.org/abs/2507.22165)
*Gursimran Singh,H. B. Acharya,Minseok Kwon*

Main category: cs.CR

TL;DR: This paper reviews how programmable data planes (especially P4 switches) enable advanced network security functions through design techniques that overcome hardware limitations like memory and instruction constraints.


<details>
  <summary>Details</summary>
Motivation: Programmable switches initially designed for flexible packet forwarding have evolved to address complex security challenges (e.g., DDoS, spoofing) but face architectural constraints requiring novel workarounds.

Method: Systematic analysis of recent security applications on programmable switches, focusing on capabilities, challenges, and architectural workarounds like recirculate-and-truncate and lookup-table precomputation.

Result: Identification of non-obvious design techniques enabling in-network security (firewalls, cryptography, ML) while navigating hardware limitations, with a structured overview of current capabilities and challenges.

Conclusion: Outlines remaining issues in deploying complex security functions on programmable switches and emerging research directions to address these challenges.

Abstract: The emergence of programmable data planes, and particularly switches
supporting the P4 language, has transformed network security by enabling
customized, line-rate packet processing. These switches, originally intended
for flexible forwarding, now play a broader role: detecting and mitigating
attacks such as DDoS and spoofing, enforcing next-generation firewall policies,
and even supporting in-network cryptography and machine learning. These
capabilities are made possible by techniques such as recirculate-and-truncate
and lookup-table precomputation, which work around architectural constraints
like limited memory and restricted instruction sets. In this paper, we
systematize recent advances in security applications built on programmable
switches, with an emphasis on the capabilities, challenges, and architectural
workarounds. We highlight the non-obvious design techniques that make complex
in-network security functions feasible despite the constraints of the hardware
platform, and also comment on remaining issues and emerging research
directions.

</details>


### [4] [Enhancing Jailbreak Attacks on LLMs via Persona Prompts](https://arxiv.org/abs/2507.22171)
*Zheng Zhang,Peilin Zhao,Deheng Ye,Hao Wang*

Main category: cs.CR

TL;DR: This paper introduces a genetic algorithm approach to craft persona prompts that successfully bypass safety mechanisms in LLMs, reducing refusal rates by 50-70% and enhancing attack success rates by 10-20% when combined with existing methods.


<details>
  <summary>Details</summary>
Motivation: Previous jailbreak attack research has primarily focused on direct harm-inducing intent manipulation while neglecting the potential of persona prompts to compromise LLM defenses. This gap motivates a systematic investigation into persona prompt effectiveness.

Method: The study proposes an automated genetic algorithm framework for evolving optimized persona prompts through iterative evaluation of bypass effectiveness against safety constraints.

Result: Experiments show (1) persona prompts reduce refusal rates by 50-70% across various LLMs, and (2) combining evolved prompts with existing attacks increases success rates by 10-20%. Code is publicly available at the provided GitHub link.

Conclusion: Persona prompts demonstrate significant efficacy in jailbreak attacks, offering both standalone impact and synergistic improvements with conventional methods. This work provides new insights for LLM safety development.

Abstract: Jailbreak attacks aim to exploit large language models (LLMs) by inducing
them to generate harmful content, thereby revealing their vulnerabilities.
Understanding and addressing these attacks is crucial for advancing the field
of LLM safety. Previous jailbreak approaches have mainly focused on direct
manipulations of harmful intent, with limited attention to the impact of
persona prompts. In this study, we systematically explore the efficacy of
persona prompts in compromising LLM defenses. We propose a genetic
algorithm-based method that automatically crafts persona prompts to bypass
LLM's safety mechanisms. Our experiments reveal that: (1) our evolved persona
prompts reduce refusal rates by 50-70% across multiple LLMs, and (2) these
prompts demonstrate synergistic effects when combined with existing attack
methods, increasing success rates by 10-20%. Our code and data are available at
https://github.com/CjangCjengh/Generic_Persona.

</details>


### [5] [POLARIS: Explainable Artificial Intelligence for Mitigating Power Side-Channel Leakage](https://arxiv.org/abs/2507.22177)
*Tanzim Mahfuz,Sudipta Paria,Tasneem Suha,Swarup Bhunia,Prabuddha Chakraborty*

Main category: cs.CR

TL;DR: POLARIS is a novel framework using XAI-guided masking to mitigate power side-channel attacks in microelectronic systems.


<details>
  <summary>Details</summary>
Motivation: Microelectronic systems handling sensitive data (e.g., encryption keys) face vulnerabilities from power side-channel attacks, necessitating effective leakage mitigation solutions.

Method: POLARIS employs an unsupervised process to generate a tailored training dataset for training a masking model, leveraging Explainable Artificial Intelligence (XAI) for guidance.

Result: POLARIS demonstrates superior performance over existing state-of-the-art methods like VALIANT in terms of leakage reduction, execution time, and overhead for large-scale designs.

Conclusion: The POLARIS framework provides a robust solution for enhancing security in microelectronic systems by effectively addressing power side-channel attack vulnerabilities.

Abstract: Microelectronic systems are widely used in many sensitive applications (e.g.,
manufacturing, energy, defense). These systems increasingly handle sensitive
data (e.g., encryption key) and are vulnerable to diverse threats, such as,
power side-channel attacks, which infer sensitive data through dynamic power
profile. In this paper, we present a novel framework, POLARIS for mitigating
power side channel leakage using an Explainable Artificial Intelligence (XAI)
guided masking approach. POLARIS uses an unsupervised process to automatically
build a tailored training dataset and utilize it to train a masking model.The
POLARIS framework outperforms state-of-the-art mitigation solutions (e.g.,
VALIANT) in terms of leakage reduction, execution time, and overhead across
large designs.

</details>


### [6] [Understanding Concept Drift with Deprecated Permissions in Android Malware Detection](https://arxiv.org/abs/2507.22231)
*Ahmed Sabbah,Radi Jarrar,Samer Zein,David Mohaisen*

Main category: cs.CR

TL;DR: This study evaluates the impact of deprecated/restricted Android permissions on ML model performance for malware detection using a 70,000+ app dataset. Excluding these permissions had marginal negative effects, improved CNN accuracy, and enhanced concept drift detection.


<details>
  <summary>Details</summary>
Motivation: Previous Android malware detection research using permissions ignored OS-related changes (protection levels, deprecated/restricted permissions) that cause concept drift, creating a gap in robust ML model evaluation.

Method: Analyzed 166 Android permissions with ML/DL classifiers (including CNN) on 70,000+ apps, applying concept drift detection strategies and year-to-year analysis. Used Kolmogorov-Smirnov test for drift measurement and implemented dataset balancing techniques.

Result: 1) Android permissions remain effective malware features. 2) Removing deprecated/restricted permissions reduced performance impact marginally (<2% change). 3) CNN showed improved accuracy with clean permission set. 4) Annual analysis enhanced concept drift detection. 5) Balanced datasets reduced low-accuracy instances by 37% and improved drift detection robustness.

Conclusion: Legacy permission analysis remains viable despite Android updates. Excluding obsolete permissions slightly improves accuracy and concept drift sensitivity. Dataset balancing is critical for maintaining model performance across OS versions.

Abstract: Permission analysis is a widely used method for Android malware detection. It
involves examining the permissions requested by an application to access
sensitive data or perform potentially malicious actions. In recent years,
various machine learning (ML) algorithms have been applied to Android malware
detection using permission-based features and feature selection techniques,
often achieving high accuracy. However, these studies have largely overlooked
important factors such as protection levels and the deprecation or restriction
of permissions due to updates in the Android OS -- factors that can contribute
to concept drift.
  In this study, we investigate the impact of deprecated and restricted
permissions on the performance of machine learning models. A large dataset
containing 166 permissions was used, encompassing more than 70,000 malware and
benign applications. Various machine learning and deep learning algorithms were
employed as classifiers, along with different concept drift detection
strategies. The results suggest that Android permissions are highly effective
features for malware detection, with the exclusion of deprecated and restricted
permissions having only a marginal impact on model performance. In some cases,
such as with CNN, accuracy improved. Excluding these permissions also enhanced
the detection of concept drift using a year-to-year analysis strategy. Dataset
balancing further improved model performance, reduced low-accuracy instances,
and enhanced concept drift detection via the Kolmogorov-Smirnov test.

</details>


### [7] [Large Language Model-Based Framework for Explainable Cyberattack Detection in Automatic Generation Control Systems](https://arxiv.org/abs/2507.22239)
*Muhammad Sharshar,Ahmad Mohammad Saber,Davor Svetinovic,Amr M. Youssef,Deepa Kundur,Ehab F. El-Saadany*

Main category: cs.CR

TL;DR: This paper introduces a hybrid framework combining lightweight ML models for real-time FDIAs detection in smart grid AGC systems with LLM-generated explanations to enhance operator trust and real-world applicability, achieving high accuracy and latency performance.


<details>
  <summary>Details</summary>
Motivation: Smart grid digitization has created real-time attack detection needs with high accuracy, but current ML/DL models lack interpretability, limiting their adoption in critical infrastructure requiring explainable results.

Method: The system uses LightGBM classifiers (95.13% accuracy, 0.004s latency) for low-latency attack detection and invokes LLMs (GPT-3.5 Turbo, GPT-4 Turbo, GPT-4o mini) with 20-shot prompting to generate real-time human-readable explanations when cyberattacks are detected.

Result: GPT-4o mini reached 93% accuracy in identifying attack targets (100 samples), 0.075 pu MAE in attack magnitude estimation, and 2.19s MAE in onset time estimation while maintaining the low-latency requirements of smart grid operations.

Conclusion: The hybrid framework successfully demonstrates high-performance FDIAs detection with accurate LLM-generated explanations, proving the feasibility of integrating AI transparency with real-time smart grid security requirements in a practical implementation.

Abstract: The increasing digitization of smart grids has improved operational
efficiency but also introduced new cybersecurity vulnerabilities, such as False
Data Injection Attacks (FDIAs) targeting Automatic Generation Control (AGC)
systems. While machine learning (ML) and deep learning (DL) models have shown
promise in detecting such attacks, their opaque decision-making limits operator
trust and real-world applicability. This paper proposes a hybrid framework that
integrates lightweight ML-based attack detection with natural language
explanations generated by Large Language Models (LLMs). Classifiers such as
LightGBM achieve up to 95.13% attack detection accuracy with only 0.004 s
inference latency. Upon detecting a cyberattack, the system invokes LLMs,
including GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o mini, to generate
human-readable explanation of the event. Evaluated on 100 test samples, GPT-4o
mini with 20-shot prompting achieved 93% accuracy in identifying the attack
target, a mean absolute error of 0.075 pu in estimating attack magnitude, and
2.19 seconds mean absolute error (MAE) in estimating attack onset. These
results demonstrate that the proposed framework effectively balances real-time
detection with interpretable, high-fidelity explanations, addressing a critical
need for actionable AI in smart grid cybersecurity.

</details>


### [8] [Invisible Injections: Exploiting Vision-Language Models Through Steganographic Prompt Embedding](https://arxiv.org/abs/2507.22304)
*Chetan Pathade*

Main category: cs.CR

TL;DR: The paper presents the first comprehensive study of steganographic prompt injection attacks against vision-language models (VLMs), demonstrating hidden instructions embedded in images can manipulate model behavior with a 24.3% average success rate.


<details>
  <summary>Details</summary>
Motivation: The authors motivate this work by highlighting the underexplored security risks in VLMs, which are critical for secure AI deployment in sensitive applications.

Method: They propose a multi-domain embedding framework combining spatial, frequency, and neural steganographic methods to inject hidden prompts while maintaining visual imperceptibility (PSNR >38 dB, SSIM >0.94).

Result: Across 12 datasets and 8 models (e.g., GPT-4V, Claude), neural steganography achieves up to 31.8% success rates, revealing vulnerabilities despite imperceptible payload embedding.

Conclusion: The findings indicate moderate but meaningful security gaps in current VLM architectures, emphasizing the need for robust multimodal AI security frameworks to ensure safe deployment.

Abstract: Vision-language models (VLMs) have revolutionized multimodal AI applications
but introduce novel security vulnerabilities that remain largely unexplored. We
present the first comprehensive study of steganographic prompt injection
attacks against VLMs, where malicious instructions are invisibly embedded
within images using advanced steganographic techniques. Our approach
demonstrates that current VLM architectures can inadvertently extract and
execute hidden prompts during normal image processing, leading to covert
behavioral manipulation. We develop a multi-domain embedding framework
combining spatial, frequency, and neural steganographic methods, achieving an
overall attack success rate of 24.3% (plus or minus 3.2%, 95% CI) across
leading VLMs including GPT-4V, Claude, and LLaVA, with neural steganography
methods reaching up to 31.8%, while maintaining reasonable visual
imperceptibility (PSNR greater than 38 dB, SSIM greater than 0.94). Through
systematic evaluation on 12 diverse datasets and 8 state-of-the-art models, we
reveal moderate but meaningful vulnerabilities in current VLM architectures and
propose effective countermeasures. Our findings have significant implications
for VLM deployment in security-critical applications and highlight the need for
proportionate multimodal AI security frameworks.

</details>


### [9] [SleepWalk: Exploiting Context Switching and Residual Power for Physical Side-Channel Attacks](https://arxiv.org/abs/2507.22306)
*Sahan Sanjaya,Aruna Jayasena,Prabhat Mishra*

Main category: cs.CR

TL;DR: The paper introduces a novel physical power side-channel attack exploiting sleep-induced context switch power spikes for cryptographic key recovery on Broadcom BCM2711, simplifying attacks by using single-spike amplitude without external triggers.


<details>
  <summary>Details</summary>
Motivation: Context switching creates measurable power spikes, and residual power from prior workloads persists, enabling new side-channel attack opportunities without relying on traditional complex trace analysis methods.

Method: The attack leverages sleep function-triggered context switches to induce power spikes, develops a power model correlating spike amplitude with cryptographic keys, and performs end-to-end experiments using residual power signatures without full trace analysis.

Result: Successful cryptographic key recovery for AES and SIKE implementations on Broadcom BCM2711 hardware, demonstrating feasibility of attack through single-spike amplitude analysis with no external synchronization triggers required.

Conclusion: This technique expands side-channel attack capabilities beyond register state extraction by exploiting residual power patterns, and simplifies attacks by eliminating the need for full power traces or complex preprocessing methods.

Abstract: Context switching is utilized by operating systems to change the execution
context between application programs. It involves saving and restoring the
states of multiple registers and performing a pipeline flush to remove any
pre-fetched instructions, leading to a higher instantaneous power consumption
compared to typical program execution. In this paper, we introduce a physical
power side-channel leakage source that exploits the power spike observed during
a context switch, triggered by the inbuilt sleep function of the system kernel.
We observed that this power spike directly correlates with both the power
consumption during context switching and the residual power consumption of the
previously executed program. Notably, the persistence of residual power
signatures from previous workloads extends the scope of this side-channel
beyond extracting the data in registers during the context switch. Unlike
traditional approaches that require analyzing full power traces, applying
complex preprocessing, or relying on external synchronization triggers, this
novel technique leverages only the amplitude of a single power spike,
significantly simplifying the attack. We developed a power model to illustrate
the feasibility of mounting end-to-end side-channel attacks using the
sleep-induced power spikes. Experimental evaluation demonstrates that our
framework can successfully perform cryptographic key recovery for both AES and
SIKE implementations on Broadcom BCM2711.

</details>


### [10] [Benchmarking Fraud Detectors on Private Graph Data](https://arxiv.org/abs/2507.22347)
*Alexander Goldberg,Giulia Fanti,Nihar Shah,Zhiwei Steven Wu*

Main category: cs.CR

TL;DR: This paper introduces the challenge of benchmarking fraud detectors on private graph data, identifies a de-anonymization attack exploiting evaluation results, and evaluates differential privacy (DP) benchmarks, finding a bias-variance trade-off that limits both utility and privacy.


<details>
  <summary>Details</summary>
Motivation: Automated fraud detection on graphs often requires third-party evaluation of private datasets, necessitating privacy preservation while enabling fair benchmarking.

Method: 1) Propose a de-anonymization attack using public evaluation results. 2) Study and empirically evaluate subsample-and-aggregate and DP synthetic graph methods for benchmarking under formal differential privacy guarantees.

Result: 1) The de-anonymization attack achieves 0.98 TPR and 0% FPR on simulated NIST facial recognition benchmarks. 2) DP benchmarking approaches show high error rates due to bias from distorted graph structures and variance from noise addition.

Conclusion: Existing DP methods for privacy-preserving benchmarks fail to achieve utility when guaranteeing privacy. The error fundamentally stems from a bias-variance trade-off, with complex methods requiring high-variance noise that undermines usefulness. Improved approaches are needed to balance privacy guarantees with practical efficiency.

Abstract: We introduce the novel problem of benchmarking fraud detectors on private
graph-structured data. Currently, many types of fraud are managed in part by
automated detection algorithms that operate over graphs. We consider the
scenario where a data holder wishes to outsource development of fraud detectors
to third parties (e.g., vendors or researchers). The third parties submit their
fraud detectors to the data holder, who evaluates these algorithms on a private
dataset and then publicly communicates the results. We propose a realistic
privacy attack on this system that allows an adversary to de-anonymize
individuals' data based only on the evaluation results. In simulations of a
privacy-sensitive benchmark for facial recognition algorithms by the National
Institute of Standards and Technology (NIST), our attack achieves near perfect
accuracy in identifying whether individuals' data is present in a private
dataset, with a True Positive Rate of 0.98 at a False Positive Rate of 0.00. We
then study how to benchmark algorithms while satisfying a formal differential
privacy (DP) guarantee. We empirically evaluate two classes of solutions:
subsample-and-aggregate and DP synthetic graph data. We demonstrate through
extensive experiments that current approaches do not provide utility when
guaranteeing DP. Our results indicate that the error arising from DP trades off
between bias from distorting graph structure and variance from adding random
noise. Current methods lie on different points along this bias-variance
trade-off, but more complex methods tend to require high-variance noise
addition, undermining utility.

</details>


### [11] [SAEL: Leveraging Large Language Models with Adaptive Mixture-of-Experts for Smart Contract Vulnerability Detection](https://arxiv.org/abs/2507.22371)
*Lei Yu,Shiqi Cheng,Zhirong Huang,Jingyuan Zhang,Chenjie Shen,Junyi Lu,Li Yang,Fengjun Zhang,Jiajia Ma*

Main category: cs.CR

TL;DR: SAEL is an LLM-based framework that enhances smart contract vulnerability detection by integrating general and specialized models through an adaptive mixture-of-experts architecture.


<details>
  <summary>Details</summary>
Motivation: Traditional static analysis tools struggle with complex scenarios, and existing specialized pre-trained models lack generalization capabilities while general-purpose LLMs underperform on specific vulnerability types despite generating useful explanations.

Method: The framework employs three core components: (1) targeted prompts to guide LLMs for vulnerability detection and explanation generation, (2) prompt-tuning applied to CodeT5/T5 models to process contract code with explanations, and (3) an Adaptive Mixture-of-Experts architecture featuring a Gating Network with TopK filtering, Softmax normalization, and Multi-Head Self-Attention for optimal feature integration through gradient optimization.

Result: Comprehensive experiments demonstrate SAEL outperforms existing methods across various vulnerability types, achieving better performance through the synergistic combination of LLM predictions, explanation features, and code features.

Conclusion: SAEL effectively addresses the limitations of current approaches by combining the adaptability of LLMs with the precision of specialized models, achieving state-of-the-art vulnerability detection performance via novel feature fusion architecture.

Abstract: With the increasing security issues in blockchain, smart contract
vulnerability detection has become a research focus. Existing vulnerability
detection methods have their limitations: 1) Static analysis methods struggle
with complex scenarios. 2) Methods based on specialized pre-trained models
perform well on specific datasets but have limited generalization capabilities.
In contrast, general-purpose Large Language Models (LLMs) demonstrate
impressive ability in adapting to new vulnerability patterns. However, they
often underperform on specific vulnerability types compared to methods based on
specialized pre-trained models. We also observe that explanations generated by
general-purpose LLMs can provide fine-grained code understanding information,
contributing to improved detection performance.
  Inspired by these observations, we propose SAEL, an LLM-based framework for
smart contract vulnerability detection. We first design targeted prompts to
guide LLMs in identifying vulnerabilities and generating explanations, which
serve as prediction features. Next, we apply prompt-tuning on CodeT5 and T5 to
process contract code and explanations, enhancing task-specific performance. To
combine the strengths of each approach, we introduce an Adaptive
Mixture-of-Experts architecture. This dynamically adjusts feature weights via a
Gating Network, which selects relevant features using TopK filtering and
Softmax normalization, and incorporates a Multi-Head Self-Attention mechanism
to enhance cross-feature relationships. This design enables effective
integration of LLM predictions, explanation features, and code features through
gradient optimization. The loss function jointly considers both independent
feature performance and overall weighted predictions. Experiments show that
SAEL outperforms existing methods across various vulnerabilities.

</details>


### [12] [Breaking Obfuscation: Cluster-Aware Graph with LLM-Aided Recovery for Malicious JavaScript Detection](https://arxiv.org/abs/2507.22447)
*Zhihong Liang,Xin Wang,Zhenhuang Hu,Liangliang Song,Lin Chen,Jingjing Guo,Yanbin Wang,Ye Tian*

Main category: cs.CR

TL;DR: The paper proposes DeCoda, a hybrid defense framework combining LLM-based deobfuscation and code graph learning to detect malicious JavaScript code, outperforming state-of-the-art baselines in F1-scores and true positive rates.


<details>
  <summary>Details</summary>
Motivation: Malicious JavaScript detection is challenging due to code obfuscation techniques, nested closure structures, and JavaScript's syntactic flexibility, which create structural noise and semantic ambiguity in static code analysis.

Method: DeCoda uses (1) a multi-stage LLM prompt-learning pipeline for code reconstruction and AST normalization, and (2) a Cluster-wise Graph approach integrating graph transformer networks, node clustering, and node-to-cluster attention to learn hierarchical code representations from AST graphs.

Result: Achieved 94.64% and 97.71% F1-scores on two benchmarks (10.74+13.85% absolute improvements over SOTA). At fixed FPR levels (0.0001/0.001/0.01), delivered 4.82/5.91/2.53 higher TPR than best baseline respectively.

Conclusion: LLM-based deobfuscation and cluster-level modeling in code graphs are crucial for JavaScript malware detection. DeCoda demonstrates significant improvements over existing methods in handling obfuscation and language-specific challenges.

Abstract: With the rapid expansion of web-based applications and cloud services,
malicious JavaScript code continues to pose significant threats to user
privacy, system integrity, and enterprise security. But, detecting such threats
remains challenging due to sophisticated code obfuscation techniques and
JavaScript's inherent language characteristics, particularly its nested closure
structures and syntactic flexibility. In this work, we propose DeCoda, a hybrid
defense framework that combines large language model (LLM)-based deobfuscation
with code graph learning: (1) We first construct a sophisticated
prompt-learning pipeline with multi-stage refinement, where the LLM
progressively reconstructs the original code structure from obfuscated inputs
and then generates normalized Abstract Syntax Tree (AST) representations; (2)
In JavaScript ASTs, dynamic typing scatters semantically similar nodes while
deeply nested functions fracture scope capturing, introducing structural noise
and semantic ambiguity. To address these challenges, we then propose to learn
hierarchical code graph representations via a Cluster-wise Graph that
synergistically integrates graph transformer network, node clustering, and
node-to-cluster attention to simultaneously capture both local node-level
semantics and global cluster-induced structural relationships from AST graph.
Experimental results demonstrate that our method achieves F1-scores of 94.64%
and 97.71% on two benchmark datasets, demonstrating absolute improvements of
10.74% and 13.85% over state-of-the-art baselines. In false-positive control
evaluation at fixed FPR levels (0.0001, 0.001, 0.01), our approach delivers
4.82, 5.91, and 2.53 higher TPR respectively compared to the best-performing
baseline. These results highlight the effectiveness of LLM-based deobfuscation
and underscore the importance of modeling cluster-level relationships in
detecting malicious code.

</details>


### [13] [DoS Attacks and Defense Technologies in Blockchain Systems: A Hierarchical Analysis](https://arxiv.org/abs/2507.22611)
*Chunyi Zhang,Fengjiao Dou,Xiaoqi Li*

Main category: cs.CR

TL;DR: This paper addresses the misconception that blockchain provides absolute security, focusing on categorizing DoS attacks at the contract and consensus layers, and analyzing detection/defense methods to improve blockchain security and stability.


<details>
  <summary>Details</summary>
Motivation: Participants often overlook blockchain's security limitations, and DoS attacks are a critical threat due to their ease and impact. The paper aims to clarify these issues to enhance system resilience.

Method: Categorizes existing DoS attacks according to blockchain architecture, examines principles/methods of contract and consensus layer attacks, and compares detection/defense technologies.

Result: Comprehensive classification of DoS attacks and an analysis of methods to strengthen security, enabling better innovation and application of blockchain systems.

Conclusion: The study highlights necessary security improvements against DoS attacks, offering insights for robust blockchain development and deployment.

Abstract: Blockchain technology is widely used in various fields due to its ability to
provide decentralization and trustless security. This is a fundamental
understanding held by many advocates, but it is misunderstood, leading
participants to fail to recognize the limitations of the security that
blockchain can provide. Among all current network attacks, Denial of Service
(DoS) attacks pose significant threats due to their ease of execution and
destructive potential. This paper, based on the blockchain architecture
hierarchy, categorizes and organizes existing DoS attacks, with a focus on
explaining the principles and methods of contract layer and consensus layer DoS
attacks. Furthermore, this paper comprehensively analyzes and compares commonly
used detection methods and defense technologies, which will contribute to
strengthening the security and stability of blockchain systems and promoting
further innovation and application of blockchain systems.

</details>


### [14] [Hate in Plain Sight: On the Risks of Moderating AI-Generated Hateful Illusions](https://arxiv.org/abs/2507.22617)
*Yiting Qu,Ziqing Yang,Yihan Ma,Michael Backes,Savvas Zannettou,Yang Zhang*

Main category: cs.CR

TL;DR: The paper investigates risks of text-to-image diffusion models for generating scalable, concealable hate messages in optical illusions (hateful illusions). A dataset of 1,571 such illusions was created to expose content moderation model vulnerabilities, revealing detection failures (accuracies <24.5% for classifiers, <10.2% for VLMs) due to vision encoders neglecting secondary hidden messages.


<details>
  <summary>Details</summary>
Motivation: Text-to-image AI systems like Stable Diffusion now enable mass creation of optical illusions. This poses risks as adversaries could encode harmful hate messages into seemingly benign images to evade content moderation, threatening online information security and public discourse.

Method: Generated 1,860 optical illusions using 62 hate messages with Stable Diffusion and ControlNet. Systematically evaluated 6 content moderation classifiers and 9 vision language models, quantifying their failure to detect hidden layers through accuracy measurements and vision encoder analysis.

Result: Moderation models fail catastrophically at detecting hateful illusions (24.5% accuracy threshold or lower). Vision language models perform worse (10.2% accuracy cutoff) since their vision encoders prioritize surface-level details over hidden secondary information.

Conclusion: Current content detection systems are highly vulnerable to optical illusion-based misinformation. Mitigation requires addressing vision encoder limitations through image transformation techniques and training strategy modifications to identify concealed messages without compromising creative capabilities.

Abstract: Recent advances in text-to-image diffusion models have enabled the creation
of a new form of digital art: optical illusions--visual tricks that create
different perceptions of reality. However, adversaries may misuse such
techniques to generate hateful illusions, which embed specific hate messages
into harmless scenes and disseminate them across web communities. In this work,
we take the first step toward investigating the risks of scalable hateful
illusion generation and the potential for bypassing current content moderation
models. Specifically, we generate 1,860 optical illusions using Stable
Diffusion and ControlNet, conditioned on 62 hate messages. Of these, 1,571 are
hateful illusions that successfully embed hate messages, either overtly or
subtly, forming the Hateful Illusion dataset. Using this dataset, we evaluate
the performance of six moderation classifiers and nine vision language models
(VLMs) in identifying hateful illusions. Experimental results reveal
significant vulnerabilities in existing moderation models: the detection
accuracy falls below 0.245 for moderation classifiers and below 0.102 for VLMs.
We further identify a critical limitation in their vision encoders, which
mainly focus on surface-level image details while overlooking the secondary
layer of information, i.e., hidden messages. To address this risk, we explore
preliminary mitigation measures and identify the most effective approaches from
the perspectives of image transformations and training-level strategies.

</details>


### [15] [Cryptanalysis of LC-MUME: A Lightweight Certificateless Multi-User Matchmaking Encryption for Mobile Devices](https://arxiv.org/abs/2507.22674)
*Ramprasad Sarkar*

Main category: cs.CR

TL;DR: A cryptanalysis of Yang et al.'s LC-MUME scheme reveals a critical security flaw violating EUF-CMA claims, along with proposed security enhancement strategies.


<details>
  <summary>Details</summary>
Motivation: This paper addresses the security gap in Yang et al.'s lightweight certificateless encryption framework by exposing vulnerabilities and proposing improvements for mobile devices.

Method: The authors conducted theoretical analysis of the LC-MUME scheme's security proofs and implemented practical attacks demonstrating ciphertext forgery by Type-I adversaries without requiring full private keys. They then developed modification strategies to remediate these weaknesses.

Result: Cryptanalytic results validate the attack feasibility with negligible computational cost. The proposed modification strategies resolve the security issues while maintaining the framework's lightweight characteristics.

Conclusion: The study demonstrates vulnerabilities in certificateless encryption schemes' security claims, establishes the need for rigorous cryptographic validation, and contributes practical countermeasures to secure mobile computing environments through improved encryption protocols.

Abstract: Yang et al. proposed a lightweight certificateless multiuser matchmaking
encryption (LC-MUME) scheme for mobile devices, published in IEEE Transactions
on Information Forensics and Security (TIFS) (DOI: 10.1109/TIFS.2023.3321961).
Their construction aims to reduce computational and communication overhead
within a one-to-many certificateless cryptographic framework. The authors claim
that their scheme satisfies existential unforgeability under chosen-message
attacks (EUF-CMA) in the random oracle model. However, our cryptanalytic study
demonstrates that the scheme fails to meet this critical security requirement.
In particular, we show that a Type-I adversary can successfully forge a valid
ciphertext without possessing the complete private key of the sender. Both
theoretical analysis and practical implementation confirm that this attack can
be mounted with minimal computational cost. To address these weaknesses, we
propose a modification strategy to strengthen the security of matchmaking
encryption schemes in mobile computing environments.

</details>


### [16] [Empirical Evaluation of Concept Drift in ML-Based Android Malware Detection](https://arxiv.org/abs/2507.22772)
*Ahmed Sabbah,Radi Jarrar,Samer Zein,David Mohaisen*

Main category: cs.CR

TL;DR: The paper investigates concept drift in Android malware detection by comparing two datasets, nine algorithms (ML, DL, LLMs), and five feature types, revealing its widespread impact and the insufficiency of balancing techniques for long-term mitigation.


<details>
  <summary>Details</summary>
Motivation: Existing machine learning models for Android malware detection face challenges with concept drift due to rapidly evolving malware characteristics, necessitating strategies to maintain detection effectiveness.

Method: Evaluated concept drift across two datasets and nine algorithms (including LLMs) using static, dynamic, hybrid, semantic, and image-based features, employing default configurations without hyperparameter tuning.

Result: Concept drift significantly affects model performance, influenced by feature types and data environments. Balancing algorithms partially address class imbalance but fail to resolve drift, with LLMs showing modest promise.

Conclusion: Concept drift remains a critical challenge due to dynamic malware evolution, unrelated to algorithm type but requiring deeper exploration of strategies, including better hyperparameter tuning and LLMs' potential.

Abstract: Despite outstanding results, machine learning-based Android malware detection
models struggle with concept drift, where rapidly evolving malware
characteristics degrade model effectiveness. This study examines the impact of
concept drift on Android malware detection, evaluating two datasets and nine
machine learning and deep learning algorithms, as well as Large Language Models
(LLMs). Various feature types--static, dynamic, hybrid, semantic, and
image-based--were considered. The results showed that concept drift is
widespread and significantly affects model performance. Factors influencing the
drift include feature types, data environments, and detection methods.
Balancing algorithms helped with class imbalance but did not fully address
concept drift, which primarily stems from the dynamic nature of the malware
landscape. No strong link was found between the type of algorithm used and
concept drift, the impact was relatively minor compared to other variables
since hyperparameters were not fine-tuned, and the default algorithm
configurations were used. While LLMs using few-shot learning demonstrated
promising detection performance, they did not fully mitigate concept drift,
highlighting the need for further investigation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [17] [RedCoder: Automated Multi-Turn Red Teaming for Code LLMs](https://arxiv.org/abs/2507.22063)
*Wenjie Jacky Mo,Qin Liu,Xiaofei Wen,Dongwon Jung,Hadi Askari,Wenxuan Zhou,Zhe Zhao,Muhao Chen*

Main category: cs.SE

TL;DR: RedCoder is a red-teaming agent designed to elicit vulnerable code from Code LLMs through multi-turn conversational interactions. It uses a multi-agent gaming process to generate attack strategies and demonstrates superior effectiveness in inducing vulnerabilities compared to existing single/multi-turn methods.


<details>
  <summary>Details</summary>
Motivation: Current red-teaming approaches for code generation require extensive human effort, lack scalability, and ignore the multi-turn interactive nature of real-world AI-assisted programming interactions, necessitating a more automated and context-aware solution.

Method: 1) Simulate adversarial interactions between agents to generate prototype conversations and reusable attack strategies. 2) Fine-tune an LLM on these prototypes as the backbone of RedCoder. 3) Deploy the agent to autonomously engage Code LLMs in multi-turn conversations, dynamically selecting strategies to guide toward vulnerable code generation.

Result: RedCoder outperforms prior red-teaming methods (both single and multi-turn) across multiple Code LLMs in inducing vulnerabilities within code generation outputs, verified through experimental evaluations.

Conclusion: Adopting a multi-turn, strategy-driven conversational approach enhances the effectiveness of red-teaming for code LLMs. RedCoder establishes a scalable framework that exposes security blind spots in generation systems by replicating real-world development workflows.}}

Abstract: Large Language Models (LLMs) for code generation (i.e., Code LLMs) have
demonstrated impressive capabilities in AI-assisted software development and
testing. However, recent studies have shown that these models are prone to
generating vulnerable or even malicious code under adversarial settings.
Existing red-teaming approaches rely on extensive human effort, limiting their
scalability and practicality, and generally overlook the interactive nature of
real-world AI-assisted programming, which often unfolds over multiple turns. To
bridge these gaps, we present RedCoder, a red-teaming agent that engages victim
models in multi-turn conversation to elicit vulnerable code. The pipeline to
construct RedCoder begins with a multi-agent gaming process that simulates
adversarial interactions, yielding a set of prototype conversations and an
arsenal of reusable attack strategies. We then fine-tune an LLM on these
prototype conversations to serve as the backbone of RedCoder. Once deployed,
RedCoder autonomously engages Code LLMs in multi-turn conversations,
dynamically retrieving relevant strategies from the arsenal to steer the
dialogue toward vulnerability-inducing outputs. Experiments across multiple
Code LLMs show that our approach outperforms prior single-turn and multi-turn
red-team methods in inducing vulnerabilities in code generation, offering a
scalable and effective tool for evaluating the security boundaries of modern
code-generation systems.

</details>


### [18] [Machine Learning Experiences: A story of learning AI for use in enterprise software testing that can be used by anyone](https://arxiv.org/abs/2507.22064)
*Michael Cohoon,Debbie Furman*

Main category: cs.SE

TL;DR: The paper outlines a machine learning workflow for software testing, detailing steps from data collection to model evaluation, providing a structured CRISP-DM-like process for applying ML techniques effectively.


<details>
  <summary>Details</summary>
Motivation: The authors aim to provide a clear, replicable workflow for individuals applying ML to software testing, addressing the challenges of data preparation, model selection, and evaluation in a domain-specific context.

Method: The method follows an 8-step ML workflow: data gathering, cleaning, feature engineering, data splitting for training/testing, model selection, training, testing, and performance evaluation. The process is inspired by CRISP-DM but tailored to software testing applications.

Result: The paper demonstrates a complete ML workflow application in software testing, achieving a structured approach but does not specify quantitative results or performance metrics.

Conclusion: The structured workflow effectively guides software testing projects using ML, and its adaptability to other domains suggests broader applicability beyond the specific use case presented.

Abstract: This paper details the machine learning (ML) journey of a group of people
focused on software testing. It tells the story of how this group progressed
through a ML workflow (similar to the CRISP-DM process). This workflow consists
of the following steps and can be used by anyone applying ML techniques to a
project: gather the data; clean the data; perform feature engineering on the
data; splitting the data into two sets, one for training and one for testing;
choosing a machine learning model; training the model; testing the model and
evaluating the model performance. By following this workflow, anyone can
effectively apply ML to any project that they are doing.

</details>


### [19] [Fuzzing: Randomness? Reasoning! Efficient Directed Fuzzing via Large Language Models](https://arxiv.org/abs/2507.22065)
*Xiaotao Feng,Xiaogang Zhu,Kun Hu,Jincheng Wang,Yingjie Cao,Guang Gong,Jianfeng Pan*

Main category: cs.SE

TL;DR: RandLuzz integrates large language models (LLMs) with directed fuzzing to eliminate randomness in seeds and reduce randomness in mutators, achieving significant speedups in bug exposure compared to existing fuzzers.


<details>
  <summary>Details</summary>
Motivation: Randomness in fuzzing reduces efficiency, requiring extensive time to expose bugs. Directed fuzzing mitigates but does not resolve this issue, necessitating improvements in seed and mutator design to address the randomness dilemma.

Method: The paper proposes RandLuzz, which uses LLMs to generate bug-targeted seeds for specific code locations and creates bug-specific mutators by analyzing bug causes and mutation suggestions. LLMs construct mutation code to expedite bug detection.

Result: RandLuzz demonstrates average speedups of 2.1–4.8× over state-of-the-art directed fuzzers using handcrafted seeds, 2.7× speedup for individual bugs, and exposes 8 bugs within 60 seconds during evaluations against AFLGo, Beacon, WindRanger, and SelectFuzz.

Conclusion: RandLuzz effectively improves the quality of seeds and mutators in directed fuzzing by leveraging LLM capabilities, achieving substantial performance gains in bug detection efficiency and reducing time-to-exposure for critical vulnerabilities.

Abstract: Fuzzing is highly effective in detecting bugs due to the key contribution of
randomness. However, randomness significantly reduces the efficiency of
fuzzing, causing it to cost days or weeks to expose bugs. Even though directed
fuzzing reduces randomness by guiding fuzzing towards target buggy locations,
the dilemma of randomness still challenges directed fuzzers. Two critical
components, which are seeds and mutators, contain randomness and are closely
tied to the conditions required for triggering bugs. Therefore, to address the
challenge of randomness, we propose to use large language models (LLMs) to
remove the randomness in seeds and reduce the randomness in mutators. With
their strong reasoning and code generation capabilities, LLMs can be used to
generate reachable seeds that target pre-determined locations and to construct
bug-specific mutators tailored for specific bugs. We propose RandLuzz, which
integrates LLMs and directed fuzzing, to improve the quality of seeds and
mutators, resulting in efficient bug exposure. RandLuzz analyzes function call
chain or functionality to guide LLMs in generating reachable seeds. To
construct bug-specific mutators, RandLuzz uses LLMs to perform bug analysis,
obtaining information such as bug causes and mutation suggestions, which
further help generate code that performs bug-specific mutations. We evaluate
RandLuzz by comparing it with four state-of-the-art directed fuzzers, AFLGo,
Beacon, WindRanger, and SelectFuzz. With RandLuzz-generated seeds, the fuzzers
achieve an average speedup ranging from 2.1$\times$ to 4.8$\times$ compared to
using widely-used initial seeds. Additionally, when evaluated on individual
bugs, RandLuzz achieves up to a 2.7$\times$ speedup compared to the
second-fastest exposure. On 8 bugs, RandLuzz can even expose them within 60
seconds.

</details>


### [20] [CodableLLM: Automating Decompiled and Source Code Mapping for LLM Dataset Generation](https://arxiv.org/abs/2507.22066)
*Dylan Manuel,Paul Rad*

Main category: cs.SE

TL;DR: CodableLLM is a Python framework automating dataset generation for code-focused large language models by aligning decompiled binaries with their original source code, enabling multi-level code understanding and generation.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of generating large, high-quality datasets for code understanding and generation, particularly the alignment between decompiled binaries and their original source code to train robust LLMs.

Method: CodableLLM automates dataset creation by mapping decompiled functions to corresponding source functions using existing decompilers and parsers, supporting multiple programming languages.

Result: Evaluation demonstrates that CodableLLM outperforms existing tools in generating aligned datasets for code-focused LLMs, offering robustness and efficiency.

Conclusion: CodableLLM provides a reliable solution for generating multi-language code datasets across abstraction levels, advancing the development of code-understanding LLMs through automated alignment of binaries and source code.

Abstract: The generation of large, high-quality datasets for code understanding and
generation remains a significant challenge, particularly when aligning
decompiled binaries with their original source code. To address this, we
present CodableLLM, a Python framework designed to automate the creation and
curation of datasets by mapping decompiled functions to their corresponding
source functions. This process enhances the alignment between decompiled and
source code representations, facilitating the development of large language
models (LLMs) capable of understanding and generating code across multiple
abstraction levels. CodableLLM supports multiple programming languages and
integrates with existing decompilers and parsers to streamline dataset
generation. This paper presents the design and implementation of CodableLLM,
evaluates its performance in dataset creation, and compares it to existing
tools in the field. The results demonstrate that CodableLLM offers a robust and
efficient solution for generating datasets tailored for code-focused LLMS.

</details>


### [21] [Automated Test Data Generation for Enterprise Protobuf Systems: A Metaclass-Enhanced Statistical Approach](https://arxiv.org/abs/2507.22070)
*Y. Du*

Main category: cs.SE

TL;DR: A novel framework uses Python metaclasses and statistical log analysis to generate test data for complex Protobuf schemas in enterprise systems, achieving 95% faster preparation and 80% better coverage than existing methods. Handles up to 15-level nesting and generates >100k test cases rapidly.


<details>
  <summary>Details</summary>
Motivation: Enterprise systems with complex nested Protobuf interfaces face inadequate traditional test data generation methods that fail to handle hierarchical/graph-like structures effectively, hindering performance testing of business interfaces.

Method: Leverages Python metaclass system for dynamic type enhancement combined with production log statistical analysis for value domain extraction. Uses schema introspection, value distribution analysis, and recursive descent algorithms to manage nested structures.

Result: 95% reduction in test data preparation time (vs. existing approaches) and 80% coverage improvement on three real-world enterprise systems; successfully handles 15-level Protobuf nesting and generates >100k test cases in seconds.

Conclusion: The framework demonstrates significant improvements in efficiency and coverage for testing enterprise Protobuf interfaces with complex nested structures, providing a scalable solution through production-based analysis and programmable schema manipulation.

Abstract: Large-scale enterprise systems utilizing Protocol Buffers (protobuf) present
significant challenges for performance testing, particularly when targeting
intermediate business interfaces with complex nested data structures.
Traditional test data generation approaches are inadequate for handling the
intricate hierarchical and graph-like structures inherent in enterprise
protobuf schemas. This paper presents a novel test data generation framework
that leverages Python's metaclass system for dynamic type enhancement and
statistical analysis of production logs for realistic value domain extraction.
Our approach combines automatic schema introspection, statistical value
distribution analysis, and recursive descent algorithms for handling deeply
nested structures. Experimental evaluation on three real-world enterprise
systems demonstrates up to 95\% reduction in test data preparation time and
80\% improvement in test coverage compared to existing approaches. The
framework successfully handles protobuf structures with up to 15 levels of
nesting and generates comprehensive test suites containing over 100,000 test
cases within seconds.

</details>


### [22] [Analyzing and Evaluating the Behavior of Git Diff and Merge](https://arxiv.org/abs/2507.22071)
*Niels Glodny*

Main category: cs.SE

TL;DR: This paper reveals critical yet overlooked flaws in Git's diff and merge algorithms, showing they have pathological behaviors, ambiguous outputs, and scalability issues. It systematically documents Git's core mechanisms and their unintended consequences.


<details>
  <summary>Details</summary>
Motivation: Git algorithms enable collaboration but their inner workings are poorly understood, limiting their potential for broader applications. Understanding their exact behavior is crucial to address reliability issues in version control systems.

Method: The analysis reverses engineered Git's diff (histogram algorithm) and merge (ort strategy) processes through implementation studies and algorithmic modeling. Experiments demonstrated extreme cases like exponential time complexity and non-commutative operations between merges and rebases.

Result: 1. Histogram diff shows a single-line change can mark entire files as modified. 2. Ort merger exhibits exponential execution time with commit numbers. 3. Non-commutativity between merges and rebases disrupts expected behavior. 4. Arbitrarily ordered merging of non-conflicting changes violates predictability assumptions.

Conclusion: Git's algorithmic weaknesses, including severe performance issues and ambiguous merge semantics, create hidden collaboration risks. These findings demand rigorous algorithmic improvements to ensure reliable version control at scale, particularly for critical software development workflows.

Abstract: Despite being widely used, the algorithms that enable collaboration with Git
are not well understood. The diff and merge algorithms are particularly
interesting, as they could be applied in other contexts. In this thesis, I
document the main functionalities of Git: how diffs are computed, how they are
used to run merges, and how merges enable more complex operations. In the
process, I show multiple unexpected behaviors in Git, including the following:
The histogram diff algorithm has pathological cases where a single-line change
can cause the entire rest of the file to be marked as changed. The default
merge strategy (ort) can result in merges requiring exponential time in the
number of commits in the history. Merges and rebases are not commutative, and
even when merges do not result in a conflict, the result is not specified but
depends on the diff algorithm used. And finally, sometimes when two sides of a
merge add different lines at the same position, the result is not a conflict,
but a merge containing both changes after each other, in arbitrary order.

</details>


### [23] [CodeEvo: Interaction-Driven Synthesis of Code-centric Data through Hybrid and Iterative Feedback](https://arxiv.org/abs/2507.22080)
*Qiushi Sun,Jinyang Gong,Lei Li,Qipeng Guo,Fei Yuan*

Main category: cs.SE

TL;DR: This paper introduces CodeEvo, a framework for iterative code data synthesis using two LLM agents (Coder and Reviewer) with a hybrid feedback mechanism that combines compiler determinism and generative flexibility, resulting in high-quality instruction-code pairs that improve code generation model performance.


<details>
  <summary>Details</summary>
Motivation: Manually curated code generation data is expensive and limited in scale, while existing synthetic methods produce ungrounded, repetitive, or simplistic code lacking rigorous validation without reliable quality assurance processes.

Method: Develops CodeEvo framework with (1) Agent Collaboration: Coder agent generates candidate code/test cases, Reviewer agent proposes instructions/feedback to refine the coding process and (2) Hybrid Feedback: Merges compiler-determined correctness with agent-suggested instructions to provide automated data quality validation across iterations.

Result: Models fine-tuned on CodeEvo-synthesized data achieve significant performance improvements over established baselines across code generation benchmarks with varying difficulties, out-scoring existing methods in multiple evaluation metrics.

Conclusion: CodeEvo demonstrates that combining LLM agent collaboration with deterministic verification mechanisms provides a scalable solution for generating grounded, diverse, and complex code training data, offering actionable insights for optimizing code-centric data synthesis pipelines.

Abstract: Acquiring high-quality instruction-code pairs is essential for training Large
Language Models (LLMs) for code generation. Manually curated data is expensive
and inherently limited in scale, motivating the development of code-centric
synthesis methods. Yet, current approaches either focus on augmenting existing
code or rely on predefined heuristics, both lacking rigorous data validation,
which results in synthetic data that is ungrounded, repetitive, or overly
simplistic. Inspired by collaborative programming practices, we propose
CodeEvo, a framework that synthesizes code data through iterative interactions
between two LLM agents: a Coder, which generates candidate code and test cases
based on given instructions, and a Reviewer, which guides the synthesis process
by producing new instructions and feedback. We further introduce a hybrid
feedback mechanism that combines compiler determinism with the generative
flexibility of agents, enabling automatic quality control throughout synthesis.
Extensive experiments demonstrate that models fine-tuned on CodeEvo data
significantly outperform established baselines across code generation
benchmarks with various difficulties. In-depth analyses further provide
insights from multiple perspectives into effective code-centric data synthesis.

</details>


### [24] [BOOP: Write Right Code](https://arxiv.org/abs/2507.22085)
*Vaani Goenka,Aalok D. Thakkar*

Main category: cs.SE

TL;DR: The paper introduces BOOP, a structured programming education framework that shifts focus from trial-and-error code execution to systematic reasoning about correctness, implemented via a VS Code extension with promising results in improving student skills.


<details>
  <summary>Details</summary>
Motivation: Novice programmers often use syntax-first and test-case-driven approaches, leading to ineffective learning and conceptual misunderstandings. AI coding tools may compound this by offering syntactically correct but logically flawed solutions.

Method: A four-phase framework (Blueprint: formal specification, Operations: language-agnostic algorithm design, Implementation: code writing, Proof: correctness validation) enforced by a VS Code extension and preprocessor that restricts counterproductive patterns.

Result: Initial evaluation shows improved algorithmic reasoning and problem decomposition in students, reduced trial-and-error debugging, with instructors noting stronger foundational skills despite initial student feedback about the framework's verbosity.

Conclusion: BOOP's structured approach successfully redirects learning focus toward understanding correctness principles, demonstrating effectiveness in mitigating dependence on trial-and-error and AI-generated syntactically correct but conceptually flawed solutions.

Abstract: Novice programmers frequently adopt a syntax-specific and test-case-driven
approach, writing code first and adjusting until programs compile and test
cases pass, rather than developing correct solutions through systematic
reasoning. AI coding tools exacerbate this challenge by providing syntactically
correct but conceptually flawed solutions. In this paper, we introduce BOOP
(Blueprint, Operations, OCaml, Proof), a structured framework requiring four
mandatory phases: formal specification, language-agnostic algorithm
development, implementation, and correctness proof. This shifts focus from
``making code work'' to understanding why code is correct.
  BOOP was implemented at our institution using a VS Code extension and
preprocessor that enforces constraints and identifies counterproductive
patterns. Initial evaluation shows improved algorithmic reasoning and reduced
trial-and-error debugging. Students reported better edge case understanding and
problem decomposition, though some initially found the format verbose.
Instructors observed stronger foundational skills compared to traditional
approaches.

</details>


### [25] [TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories](https://arxiv.org/abs/2507.22086)
*Honghua Dong,Jiacheng Yang,Xun Deng,Yuhe Jiang,Gennady Pekhimenko,Fan Long,Xujie Si*

Main category: cs.SE

TL;DR: TypyBench is a benchmark for evaluating type inference in dynamic languages using LLMs, revealing that while models perform well on TypeSim, they struggle with type consistency and nested types.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the underexplored effectiveness of large language models (LLMs) in type inference for dynamic languages like Python, highlighting persistent challenges in software engineering and the need for repository-level evaluation benchmarks.

Method: TypyBench introduces two metrics: TypeSim (measuring semantic relationships between predicted/ground-truth types) and TypeCheck (assessing type consistency across codebases). Models were tested on a dataset of 50 high-quality Python repositories.

Result: LLMs achieved decent TypeSim scores but exhibited poor performance on complex nested types and large inconsistencies across codebases.

Conclusion: The work establishes TypyBench as a foundational framework for prioritizing repository-level consistency over type similarity in future LLM research, offering insights into performance across varying type complexities.

Abstract: Type inference for dynamic languages like Python is a persistent challenge in
software engineering. While large language models (LLMs) have shown promise in
code understanding, their type inference capabilities remain underexplored. We
introduce TypyBench, a benchmark designed to evaluate LLMs' type inference
across entire Python repositories. TypyBench features two novel metrics:
TypeSim, which captures nuanced semantic relationships between predicted and
ground truth types, and TypeCheck, which assesses type consistency across
codebases. Our evaluation of various LLMs on a curated dataset of 50
high-quality Python repositories reveals that, although LLMs achieve decent
TypeSim scores, they struggle with complex nested types and exhibit significant
type consistency errors. These findings suggest that future research should
shift focus from improving type similarity to addressing repository-level
consistency. TypyBench provides a foundation for this new direction, offering
insights into model performance across different type complexities and usage
contexts. Our code and data are available at
https://github.com/typybench/typybench.

</details>


### [26] [Secure coding for web applications: Frameworks, challenges, and the role of LLMs](https://arxiv.org/abs/2507.22223)
*Kiana Kiashemshaki,Mohammad Jalili Torkamani,Negin Mahmoudi*

Main category: cs.SE

TL;DR: This paper reviews secure coding practices across frameworks and domains, introduces a threat categorization aligned with OWASP Top 10, and explores the role of LLMs in secure code evaluation through a case study.


<details>
  <summary>Details</summary>
Motivation: Secure coding is critical but underadopted due to organizational, educational, and technical barriers, necessitating actionable research and frameworks for integration.

Method: Comprehensive review of secure coding practices in web development, DevSecOps, and cloud security; structured framework comparison; OWASP-based threat categorization; reproducible LLM case study across four vulnerability types.

Result: The LLM case study demonstrates promising potential for automating secure coding evaluation and recommendations, while framework comparisons highlight existing adoption challenges.

Conclusion: The paper provides practical pathways for integrating secure coding into development workflows and establishes LLMs as viable tools for improving security practices through reproducible analysis.

Abstract: Secure coding is a critical yet often overlooked practice in software
development. Despite extensive awareness efforts, real-world adoption remains
inconsistent due to organizational, educational, and technical barriers. This
paper provides a comprehensive review of secure coding practices across major
frameworks and domains, including web development, DevSecOps, and cloud
security. It introduces a structured framework comparison and categorizes
threats aligned with the OWASP Top 10. Additionally, we explore the rising role
of Large Language Models (LLMs) in evaluating and recommending secure code,
presenting a reproducible case study across four major vulnerability types.
This paper offers practical insights for researchers, developers, and educators
on integrating secure coding into real-world development processes.

</details>


### [27] [From Articles to Code: On-Demand Generation of Core Algorithms from Scientific Publications](https://arxiv.org/abs/2507.22324)
*Cameron S. Movassaghi,Amanda Momenzadeh,Jesse G. Meyer*

Main category: cs.SE

TL;DR: Current large language models can generate code from scientific method descriptions in papers, reducing maintenance costs of traditional libraries.


<details>
  <summary>Details</summary>
Motivation: Addressing high maintenance costs of software packages through automated code generation.

Method: Benchmarked state-of-the-art LLMs (GPT-o4-mini-high, Gemini Pro 2.5, Claude Sonnet 4) for implementing core algorithms from original scientific publications.

Result: LLMs reliably reproduced package functionality with performance comparable to conventional libraries across diverse algorithms.

Conclusion: This shift towards on-demand code generation using scientific publications as context may eliminate static package maintenance overhead.

Abstract: Maintaining software packages imposes significant costs due to dependency
management, bug fixes, and versioning. We show that rich method descriptions in
scientific publications can serve as standalone specifications for modern large
language models (LLMs), enabling on-demand code generation that could supplant
human-maintained libraries. We benchmark state-of-the-art models
(GPT-o4-mini-high, Gemini Pro 2.5, Claude Sonnet 4) by tasking them with
implementing a diverse set of core algorithms drawn from original publications.
Our results demonstrate that current LLMs can reliably reproduce package
functionality with performance indistinguishable from conventional libraries.
These findings foreshadow a paradigm shift toward flexible, on-demand code
generation and away from static, human-maintained packages, which will result
in reduced maintenance overhead by leveraging published articles as sufficient
context for the automated implementation of analytical workflows.

</details>


### [28] [AutoCodeSherpa: Symbolic Explanations in AI Coding Agents](https://arxiv.org/abs/2507.22414)
*Sungmin Kang,Haifeng Ruan,Abhik Roychoudhury*

Main category: cs.SE

TL;DR: This paper introduces a workflow for LLM agents in software engineering where bugs are explained via symbolic formulae (input/output/infection conditions) as executable property-based tests, enhancing both human comprehension and automated evaluation of suggested patches.


<details>
  <summary>Details</summary>
Motivation: LLM agents for software tasks require higher confidence and explainability in their outputs to be used in production environments, where decisions must be evidence-backed and transparent.

Method: The proposed workflow generates executable explanations using symbolic formulae structured as input conditions, infection conditions, and output conditions. These are implemented as property-based tests (PBT) and program-internal symbolic expressions, enabling automation and aiding developer understanding.

Result: The explanations can be directly used to generate concrete test cases for issues, support automated patch acceptance/rejection, and improve the quality of LLM-based repair techniques by providing verifiable reasoning.

Conclusion: Program analysis-driven symbolic explanations for LLM agents not only clarify repair decisions but also enable fully automated workflows and cross-agent improvements, addressing critical gaps in reliability and interpretability for production deployment.

Abstract: Large Language Model (LLM) agents autonomously use external tools on top of
one or more LLMs to accomplish specific tasks. Lately LLM agents for software
engineering tasks have become popular. These agents can benefit from the use of
program analysis tools working on program representations. This is demonstrated
by existing agentic AI solutions such as AutoCodeRover or SpecRover which
perform automated program repair. Specifically the goal of these works is to
use program analysis to improve the patch quality. These agents are currently
being used to automatically fix static analysis issues from the widely used
SonarQube static analyzer.
  Nevertheless, for the agents to be deployed in a production environment,
agents need to suggest software artifacts, such as patches, with evidence and
with high confidence. In this work, we provide a workflow where an agent
provides explanations of the bug in the form of symbolic formulae. The
explanations are in the form of input conditions, infection conditions and
output conditions, implemented as property based tests (PBT) and
program-internal symbolic expressions. These can help in human developer
cognition of the agent outputs as well as in achieving completely automated
agentic workflows for software. The human developer can benefit from the input
condition, represented as a PBT, to generate various concrete inputs showing a
given issue. Furthermore, since the PBTs are executable, our explanations are
executable as well. We can thus also use the explanations in a completely
automated issue resolution environment for accepting or rejecting the patches
that are suggested by patching agents such as AutoCodeRover. Finally, as
agentic AI approaches continue to develop, the program analysis driven
explanations can be provided to other LLM-based repair techniques such as
Agentless to improve their output.

</details>


### [29] [Ensemble Fuzzing with Dynamic Resource Scheduling and Multidimensional Seed Evaluation](https://arxiv.org/abs/2507.22442)
*Yukai Zhao,Shaohua Wang,Jue Wang,Xing Hu,Xin Xia*

Main category: cs.SE

TL;DR: Legion is a new ensemble fuzzing framework that dynamically allocates resources using an upper confidence bound algorithm and multidimensional seed evaluation, enhancing bug detection efficiency and effectiveness.


<details>
  <summary>Details</summary>
Motivation: Existing ensemble fuzzing techniques waste resources due to suboptimal scheduling and lack comprehensive performance evaluation methods.

Method: We introduce Legion with: 1) a dynamic resource scheduling algorithm based on the upper confidence bound (UCB) for adaptive fuzzer prioritization, and 2) a multidimensional seed evaluation strategy considering coverage, mutation utility, and edge frequency.

Result: Legion detected 20 total vulnerabilities (with 5 novel bugs and 3 CVEs) in real-world open-source projects, outperforming state-of-the-art base/ensemble fuzzing approaches across benchmark tests.

Conclusion: Legion demonstrates significant improvements in ensemble fuzzing by addressing resource scheduling inefficiencies and enabling fine-grained performance evaluation through novel algorithmic contributions.

Abstract: Fuzzing is widely used for detecting bugs and vulnerabilities, with various
techniques proposed to enhance its effectiveness. To combine the advantages of
multiple technologies, researchers proposed ensemble fuzzing, which integrates
multiple base fuzzers. Despite promising results, state-of-the-art ensemble
fuzzing techniques face limitations in resource scheduling and performance
evaluation, leading to unnecessary resource waste. In this paper, we propose
Legion, a novel ensemble fuzzing framework that dynamically schedules resources
during the ensemble fuzzing campaign. We designed a novel resource scheduling
algorithm based on the upper confidence bound algorithm to reduce the resource
consumption of ineffective base fuzzers. Additionally, we introduce a
multidimensional seed evaluation strategy, which considers multiple metrics to
achieve more comprehensive fine-grained performance evaluation. We implemented
Legion as a prototype tool and evaluated its effectiveness on Google's
fuzzer-test-suite as well as real-world open-source projects. Results show that
Legion outperforms existing state-of-the-art base fuzzers and ensemble fuzzing
techniques, detecting 20 vulnerabilities in real-world open-source
projects-five previously unknown and three classified as CVEs.

</details>


### [30] [Inside madupite: Technical Design and Performance](https://arxiv.org/abs/2507.22538)
*Matilde Gargiani,Robin Sieber,Philip Pawlowsky,John Lygeros*

Main category: cs.SE

TL;DR: Madupite is a distributed solver for large-scale MDPs enabling exact solutions without function approximation.


<details>
  <summary>Details</summary>
Motivation: Existing solvers struggle with large-scale discounted infinite-horizon MDPs, especially in near-undiscounted settings that require exact solutions beyond memory capacity of standard hardware.

Method: The solver uses mathematical optimization methods with distributed computing architecture to leverage high-performance clusters, and offers algorithm customization to exploit problem-specific structures for faster convergence.

Result: Madupite outperforms current methods in scalability and efficiency across applications in epidemiology and control, solving MDPs exceeding typical laptop memory limits.

Conclusion: Madupite establishes a new standard as the first distributed MDP solver capable of exact large-scale solutions with customizable algorithms, significantly advancing MDP computation capabilities.

Abstract: In this work, we introduce and benchmark madupite, a newly proposed
high-performance solver designed for large-scale discounted infinite-horizon
Markov decision processes with finite state and action spaces. After a brief
overview of the class of mathematical optimization methods on which madupite
relies, we provide details on implementation choices, technical design and
deployment. We then demonstrate its scalability and efficiency by showcasing
its performance on the solution of Markov decision processes arising from
different application areas, including epidemiology and classical control.
Madupite sets a new standard as, to the best of our knowledge, it is the only
solver capable of efficiently computing exact solutions for large-scale Markov
decision processes, even when these exceed the memory capacity of modern
laptops and operate in near-undiscounted settings. This is possible as madupite
can work in a fully distributed manner and therefore leverage the memory
storage and computation capabilities of modern high-performance computing
clusters. This key feature enables the solver to efficiently handle problems of
medium to large size in an exact manner instead of necessarily resorting to
function approximations. Moreover, madupite is unique in allowing users to
customize the solution algorithm to better exploit the specific structure of
their problem, significantly accelerating convergence especially in
large-discount factor settings. Overall, madupite represents a significant
advancement, offering unmatched scalability and flexibility in solving
large-scale Markov decision processes.

</details>


### [31] [RePaCA: Leveraging Reasoning Large Language Models for Static Automated Patch Correctness Assessment](https://arxiv.org/abs/2507.22580)
*Marcos Fuster-Pena,David de-Fitero-Dominguez,Antonio Garcia-Cabot,Eva Garcia-Lopez*

Main category: cs.SE

TL;DR: RePaCA introduces an LLM-based static APCA technique enhanced through reinforcement learning, achieving SoTA performance with 83.1% accuracy and 84.8% F1-score by addressing overfitting patch issues in APR tools. It improves accuracy, generalization, and explainability.


<details>
  <summary>Details</summary>
Motivation: Existing APR tools produce overfitting patches that pass tests without fixing root causes. Current static APCA approaches lack reliability, flexibility, and transparency in assessing patch correctness.

Method: RePaCA utilizes LLMs specialized in reasoning tasks. The model is prompted with buggy/fixed code pairs and trained via Group Relative Policy Optimization (Reinforcement Learning) to generate a correctness classification and Chain of Thought analysis.

Result: RePaCA achieves 83.1% accuracy and 84.8% F1-score on Defects4J benchmark, demonstrating superior generalization across datasets compared to leading techniques. Generates explainable reasoning for assessments.

Conclusion: Finetuned reasoning LLMs significantly enhance static APCA capabilities, offering improvements in accuracy, cross-dataset generalization, and patch assessment explainability compared to existing methods.

Abstract: Automated Program Repair (APR) seeks to automatically correct software bugs
without requiring human intervention. However, existing tools tend to generate
patches that satisfy test cases without fixing the underlying bug, those are
known as overfitting patches. To address this issue, Automated Patch
Correctness Assessment (APCA) attempts to identify overfitting patches
generated by APR tools. It can be solved as a static approach, meaning that no
additional information is needed beyond the original and fixed code snippets.
Current static techniques often struggle with reliability, flexibility and
transparency. To address these issues, we introduce RePaCA, a novel static APCA
technique that leverages Large Language Models (LLMs) specialized in thinking
tasks. Our model is prompted with both buggy and fixed code snippets and guided
to generate a Chain of Thought that analyses code differences, reasons about
how the patch addresses the root cause, and ultimately provides a binary
classification: correct or overfitting. To enhance these reasoning capabilities
for the APCA task specifically, the LLM is finetuned using Reinforcement
Learning with the Group Relative Policy Optimization algorithm. When evaluated
on a standard Defects4J-derived test, our approach achieves state-of-the-art
performance, with 83.1% accuracy and an 84.8% F1-score. Furthermore, our model
demonstrates superior generalization capabilities when trained on different
datasets, outperforming the leading technique. This reasoning capability also
provides enhanced explainability for the patch assessment. These findings
underscore the considerable promise of finetuned, reasoning LLMs to advance
static APCA by enhancing accuracy, generalization, and explainability.

</details>


### [32] [Metamorphic Testing of Deep Code Models: A Systematic Literature Review](https://arxiv.org/abs/2507.22610)
*Ali Asgari,Milan de Koning,Pouria Derakhshanfar,Annibale Panichella*

Main category: cs.SE

TL;DR: This paper provides a systematic literature review analyzing metamorphic testing approaches for evaluating robustness of deep-code models, summarizing 45 studies on their transformations, techniques, datasets, and metrics while identifying key challenges.


<details>
  <summary>Details</summary>
Motivation: Deep-code models, despite high task accuracy, require robustness evaluation against adversarial inputs (e.g., variable renaming) to ensure reliability in real-world software engineering applications.

Method: Conducted a systematic literature review of 45 primary papers, examining metamorphic testing methodologies involving semantic-preserving program transformations and model output stability analysis.

Result: Summarized current landscape of deep-code robustness testing: identified commonly evaluated models (e.g., transformers), tasks (completion, detection), datasets, target languages, and metrics. Highlighted gaps in transformation diversity and evaluation criteria.

Conclusion: Metamorphic testing remains the dominant approach but requires improvements in canonicalization handling, cross-lingual evaluation, and integrating multiple test strategies to strengthen deep-code model robustness guarantees.

Abstract: Large language models and deep learning models designed for code intelligence
have revolutionized the software engineering field due to their ability to
perform various code-related tasks. These models can process source code and
software artifacts with high accuracy in tasks such as code completion, defect
detection, and code summarization; therefore, they can potentially become an
integral part of modern software engineering practices. Despite these
capabilities, robustness remains a critical quality attribute for deep-code
models as they may produce different results under varied and adversarial
conditions (e.g., variable renaming). Metamorphic testing has become a widely
used approach to evaluate models' robustness by applying semantic-preserving
transformations to input programs and analyzing the stability of model outputs.
While prior research has explored testing deep learning models, this systematic
literature review focuses specifically on metamorphic testing for deep code
models. By studying 45 primary papers, we analyze the transformations,
techniques, and evaluation methods used to assess robustness. Our review
summarizes the current landscape, identifying frequently evaluated models,
programming tasks, datasets, target languages, and evaluation metrics, and
highlights key challenges and future directions for advancing the field.

</details>


### [33] [A Systematic Literature Review on Detecting Software Vulnerabilities with Large Language Models](https://arxiv.org/abs/2507.22659)
*Sabrina Kaniewski,Fabian Schmidt,Markus Enzweiler,Michael Menth,Tobias Heer*

Main category: cs.SE

TL;DR: This paper provides a systematic literature review analyzing 227 studies on LLM-based software vulnerability detection (2020-2025), addressing fragmentation in the field by offering a structured taxonomy, dataset analysis, and future research directions.


<details>
  <summary>Details</summary>
Motivation: The rapid development of LLM-based software vulnerability detection has led to a fragmented research landscape with inconsistent methodologies and datasets, hindering comparison and understanding of state-of-the-art techniques.

Method: A comprehensive SLR was conducted, categorizing 227 studies by task formulation, input representation, system architecture, and adaptation techniques. Dataset characteristics, coverage, and diversity were also systematically analyzed.

Result: The paper presents: (1) a fine-grained taxonomy of vulnerability detection approaches, (2) analysis of input/dataset limitations, (3) identification of key research gaps, (4) actionable future research opportunities, and (5) publicly released artifacts and a living repository.

Conclusion: The review establishes a structured framework for LLM-based vulnerability detection research, improves transparency in the field, and provides practical resources to enable more comparable and reproducible future work.

Abstract: The increasing adoption of Large Language Models (LLMs) in software
engineering has sparked interest in their use for software vulnerability
detection. However, the rapid development of this field has resulted in a
fragmented research landscape, with diverse studies that are difficult to
compare due to differences in, e.g., system designs and dataset usage. This
fragmentation makes it difficult to obtain a clear overview of the
state-of-the-art or compare and categorize studies meaningfully. In this work,
we present a comprehensive systematic literature review (SLR) of LLM-based
software vulnerability detection. We analyze 227 studies published between
January 2020 and June 2025, categorizing them by task formulation, input
representation, system architecture, and adaptation techniques. Further, we
analyze the datasets used, including their characteristics, vulnerability
coverage, and diversity. We present a fine-grained taxonomy of vulnerability
detection approaches, identify key limitations, and outline actionable future
research opportunities. By providing a structured overview of the field, this
review improves transparency and serves as a practical guide for researchers
and practitioners aiming to conduct more comparable and reproducible research.
We publicly release all artifacts and maintain a living repository of LLM-based
software vulnerability detection studies.

</details>


### [34] [RobEthiChor: Automated Context-aware Ethics-based Negotiation for Autonomous Robots](https://arxiv.org/abs/2507.22664)
*Mashal Afzal Memon,Gianluca Filippone,Gian Luca Scoccia,Marco Autili,Paola Inverardi*

Main category: cs.SE

TL;DR: RobEthiChor proposes an ethics-based negotiation framework for autonomous systems to incorporate individual user ethics and contextual factors, enhancing trust and decision-making personalization. A ROS implementation (RobEthiChor-Ros) and experiments with real robots demonstrate its feasibility and scalability.


<details>
  <summary>Details</summary>
Motivation: Autonomous systems currently lack personalization in ethical decision-making, leading to reduced user trust and inability to align with diverse moral beliefs. Negotiation mechanisms are crucial when multiple systems with conflicting ethical preferences interact.

Method: The paper introduces a domain-agnostic reference architecture (RobEthiChor) and implements it in ROS (RobEthiChor-Ros) to enable ethics-based negotiation. Real-robot experiments with resource contention scenarios validate the approach.

Result: RobEthiChor achieved 73%+ agreement rates in scenarios, with average negotiation time of 0.67s. Results confirm the system's feasibility, effectiveness, and scalability in handling ethical negotiations among autonomous systems.

Conclusion: RobEthiChor successfully demonstrates that ethics-based negotiation can be implemented in autonomous systems to respect user moral beliefs while maintaining scalability through its domain-agnostic architecture and ROS integration.

Abstract: The presence of autonomous systems is growing at a fast pace and it is
impacting many aspects of our lives. Designed to learn and act independently,
these systems operate and perform decision-making without human intervention.
However, they lack the ability to incorporate users' ethical preferences, which
are unique for each individual in society and are required to personalize the
decision-making processes. This reduces user trust and prevents autonomous
systems from behaving according to the moral beliefs of their end-users. When
multiple systems interact with differing ethical preferences, they must
negotiate to reach an agreement that satisfies the ethical beliefs of all the
parties involved and adjust their behavior consequently. To address this
challenge, this paper proposes RobEthiChor, an approach that enables autonomous
systems to incorporate user ethical preferences and contextual factors into
their decision-making through ethics-based negotiation. RobEthiChor features a
domain-agnostic reference architecture for designing autonomous systems capable
of ethic-based negotiating. The paper also presents RobEthiChor-Ros, an
implementation of RobEthiChor within the Robot Operating System (ROS), which
can be deployed on robots to provide them with ethics-based negotiation
capabilities. To evaluate our approach, we deployed RobEthiChor-Ros on real
robots and ran scenarios where a pair of robots negotiate upon resource
contention. Experimental results demonstrate the feasibility and effectiveness
of the system in realizing ethics-based negotiation. RobEthiChor allowed robots
to reach an agreement in more than 73\% of the scenarios with an acceptable
negotiation time (0.67s on average). Experiments also demonstrate that the
negotiation approach implemented in RobEthiChor is scalable.

</details>


### [35] [The Multi-Agent Fault Localization System Based on Monte Carlo Tree Search Approach](https://arxiv.org/abs/2507.22800)
*Rui Ren*

Main category: cs.SE

TL;DR: KnowledgeMind is an LLM-based RCA framework addressing limitations in existing methods by reducing context window burden and mitigating hallucinations through Monte Carlo Tree Search and a rule-based reward mechanism, achieving 49.29%-128.35% improved localization accuracy.


<details>
  <summary>Details</summary>
Motivation: Highly decoupled microservices increase system reliability challenges and incident frequency, demanding better RCA methods. Existing LLM approaches suffer from hallucination errors and struggle with context window constraints when processing large anomalous data volumes.

Method: Proposes KnowledgeMind: A multi-agent system using Monte Carlo Tree Search for service-by-service exploration combined with a rule-based, real-time knowledge base reward mechanism to guide reasoning and suppress hallucinations.

Result: Reduces context window requirements to 1/10th of existing methods while improving root cause localization accuracy by 49.29% to 128.35% compared to SOTA LLM-based RCA frameworks.

Conclusion: KnowledgeMind's structured exploration approach effectively addresses key limitations (hallucination, context window constraints) in LLM-based RCA through reward-guided reasoning, significantly outperforming current state-of-the-art methods.

Abstract: In real-world scenarios, due to the highly decoupled and flexible nature of
microservices, it poses greater challenges to system reliability. The more
frequent occurrence of incidents has created a demand for Root Cause
Analysis(RCA) methods that enable rapid identification and recovery of
incidents. Large language model (LLM) provides a new path for quickly locating
and recovering from incidents by leveraging their powerful generalization
ability combined with expert experience. Current LLM for RCA frameworks are
based on ideas like ReAct and Chain-of-Thought, but the hallucination of LLM
and the propagation nature of anomalies often lead to incorrect localization
results. Moreover, the massive amount of anomalous information generated in
large, complex systems presents a huge challenge for the context window length
of LLMs. To address these challenges, we propose KnowledgeMind, an innovative
LLM multi-agent system based on Monte Carlo Tree Search and a knowledge base
reward mechanism for standardized service-by-service reasoning. Compared to
State-Of-The-Art(SOTA) LLM for RCA methods, our service-by-service exploration
approach significantly reduces the burden on the maximum context window length,
requiring only one-tenth of its size. Additionally, by incorporating a
rule-based real-time reward mechanism, our method effectively mitigates
hallucinations during the inference process. Compared to the SOTA LLM for RCA
framework, our method achieves a 49.29% to 128.35% improvement in root cause
localization accuracy.

</details>


### [36] [Repair-R1: Better Test Before Repair](https://arxiv.org/abs/2507.22853)
*Haichuan Hu,Xiaochen Xie,Quanjun Zhang*

Main category: cs.SE

TL;DR: Repair-R1 is an Automated Program Repair approach that integrates test cases during model training and prioritizes test generation before repair, using reinforcement learning to improve repair effectiveness compared to conventional LLM-based APR techniques.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based APR methods use test cases only during inference phase and after repair generation, missing opportunities to leverage test cases in training or as a pre-repair analysis step. Prior testing can better locate defects and understand their root causes.

Method: Repair-R1 generates discriminative test cases to identify defective behaviors before performing repairs. The model is trained with three backbone architectures using reinforcement learning to co-optimize test generation and bug repair processes.

Result: Repair-R1 achieves 2.68%-48.29% higher repair success rate than vanilla models, 16.38%-53.28% improvement in test generation success rate, and 0.78%-53.96% increase in test coverage across four benchmark datasets. Code and weights are publicly available.

Conclusion: Integrating test cases into training and shifting test generation to precede repair significantly enhances APR effectiveness, with Repair-R1 outperforming existing methods in multiple metrics. Published resources enable replication and further research.

Abstract: APR (Automated Program Repair) aims to automatically locate program defects,
generate patches and validate the repairs. Existing techniques for APR are
often combined with LLMs (Large Language Models), which leverages the
code-related knowledge of LLMs to improve repair effectiveness. Current
LLM-based APR methods typically utilize test cases only during the inference
stage, adopting an iterative approach that performs repair first and validates
it through test execution afterward. This conventional paradigm neglects two
important aspects: the potential contribution of test cases in the training
phase, and the possibility of leveraging testing prior to repair. To address
this, we propose Repair-R1, which introduces test cases into the model's
training phase and shifts test generation to precede repair. The model is
required to first generate discriminative test cases that can distinguish
defective behaviors, and then perform repair based on these tests. This enables
the model to better locate defects and understand the underlying causes of
defects, thereby improving repair effectiveness. We implement Repair-R1 with
three different backbone models, using RL (reinforcement learning) to
co-optimize test generation and bug repair. Experimental results on four widely
adopted benchmarks demonstrate the superiority of Repair-R1. Specially,
compared to vanilla models, Repair-R1 improves repair success rate by 2.68\% to
48.29\%, test generation success rate by 16.38\% to 53.28\%, and test coverage
by 0.78\% to 53.96\%. We publish the code and weights at
https://github.com/Tomsawyerhu/APR-RL and
https://huggingface.co/tomhu/Qwen3-4B-RL-5000-step.

</details>


### [37] [Tracking research software outputs in the UK](https://arxiv.org/abs/2507.22871)
*Domhnall Carlin,Austen Rainer*

Main category: cs.SE

TL;DR: This study analyzes the storage and registration of research software in UK academic institutions by examining the UKRI Gateway to Research (GtR) metadata. It finds that publicly funded research software constitutes a low proportion of reported outputs, with significant issues in artifact sharing (e.g., 25% lack links and 45% have invalid URLs). GitHub hosts 18% of valid software artifacts, and the findings highlight risks for software sustainability if sharing practices remain inadequate.


<details>
  <summary>Details</summary>
Motivation: The growth of Open Science emphasizes accessibility of research artifacts (data/code), but practical methods for recognizing and tracking research software as a critical output remain inconsistent. UK institutions need evaluation to address traceability gaps in funding bodies' reports.

Method: The authors examined where UK academic institutions store and register research software by querying metadata in the UKRI's Gateway to Research (GtR) database, analyzing publicly funded software listings, validating URLs, and comparing findings with 2023 data.

Result: Research software represents a low proportion of reported outputs in GtR. Twenty-five percent have no links, and 45% of URLs are missing or erroneous. Among valid URLs, public commercial code repositories dominate (18% on GitHub). Artifact sharing practices remain suboptimal compared to 2023 benchmarks.

Conclusion: Inadequate artifact sharing for research software threatens its long-term value and sustainability. Without improved dissemination, software may become transient tools rather than lasting scientific resources, undermining the goals of Open Science in preserving traceable, reusable outputs.

Abstract: Research software is crucial in the research process and the growth of Open
Science underscores the importance of accessing research artifacts, like data
and code, raising traceability challenges among outputs. While it is a clear
principle that research code, along with other essential outputs, should be
recognised as artifacts of the research process, the how of this principle
remains variable. This study examines where UK academic institutions store and
register software as a unique research output, searching the UKRI's Gateway to
Research (GtR) metadata for publicly funded research software in the UK. The
quantity of software reported as research outcomes remains low in proportion to
other categories. Artifact sharing appears low, with one-quarter of the
reported software having no links and 45% having either a missing or erroneous
URL. Of the valid URLs, we find the single largest category is Public
Commercial Code Repository, with GitHub being the host of 18% of all publicly
funded research software listed. These observations are contrasted with past
findings from 2023 and finally, we discuss the lack of artifact sharing in UK
research, with resulting implications for the maintenance and evolution of
research software. Without dissemination, research software risks demotion to a
transient artifact, useful only to meet short term research demands but
ultimately lost to the broader enterprise of science.

</details>
