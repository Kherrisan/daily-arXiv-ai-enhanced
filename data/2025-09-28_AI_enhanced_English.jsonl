{"id": "2509.20382", "categories": ["cs.CR", "cs.AI", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.20382", "abs": "https://arxiv.org/abs/2509.20382", "authors": ["Dilli Hang Rai", "Sabin Kafley"], "title": "Lightweight MobileNetV1+GRU for ECG Biometric Authentication: Federated and Adversarial Evaluation", "comment": "5 pages, 7 figures, 5 tables", "summary": "ECG biometrics offer a unique, secure authentication method, yet their\ndeployment on wearable devices faces real-time processing, privacy, and\nspoofing vulnerability challenges. This paper proposes a lightweight deep\nlearning model (MobileNetV1+GRU) for ECG-based authentication, injection of\n20dB Gaussian noise & custom preprocessing. We simulate wearable conditions and\nedge deployment using the ECGID, MIT-BIH, CYBHi, and PTB datasets, achieving\naccuracies of 99.34%, 99.31%, 91.74%, and 98.49%, F1-scores of 0.9869, 0.9923,\n0.9125, and 0.9771, Precision of 0.9866, 0.9924, 0.9180 and 0.9845, Recall of\n0.9878, 0.9923, 0.9129, and 0.9756, equal error rates (EER) of 0.0009, 0.00013,\n0.0091, and 0.0009, and ROC-AUC values of 0.9999, 0.9999, 0.9985, and 0.9998,\nwhile under FGSM adversarial attacks, accuracy drops from 96.82% to as low as\n0.80%. This paper highlights federated learning, adversarial testing, and the\nneed for diverse wearable physiological datasets to ensure secure and scalable\nbiometrics.", "AI": {"tldr": "The paper proposes a lightweight ECG-based authentication model (MobileNetV1 + GRU) for wearables, addressing real-time processing, privacy, and spoofing risks. It achieves high accuracy but shows vulnerabilities under adversarial attacks, emphasizing federated learning and diverse datasets for reliable deployment.", "motivation": "ECG biometrics on wearables face challenges in real-time performance, privacy leakage, and spoofing. Lightweight models are needed to balance efficiency and security.", "method": "Combined MobileNetV1 and GRU with 20dB Gaussian noise injection and custom preprocessing. Evaluated on ECGID, MIT-BIH, CYBHi, and PTB datasets under wearable-edge conditions, federated learning, and FGSM adversarial attacks.", "result": "Achieved 98.7%-99.34%-level accuracy across datasets, with EER rates <1%. Under FGSM attacks, accuracy dropped from 96.82% to 0.80%, revealing security vulnerabilities.", "conclusion": "The model demonstrates strong baseline performance but highlights critical vulnerabilities under adversarial attacks. Federated learning and diverse physiological datasets are essential for secure, scalable ECG biometrics in wearables."}}
{"id": "2509.20383", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20383", "abs": "https://arxiv.org/abs/2509.20383", "authors": ["Wei Wan", "Yuxuan Ning", "Zhicong Huang", "Cheng Hong", "Shengshan Hu", "Ziqi Zhou", "Yechao Zhang", "Tianqing Zhu", "Wanlei Zhou", "Leo Yu Zhang"], "title": "MARS: A Malignity-Aware Backdoor Defense in Federated Learning", "comment": "NeurIPS 2025", "summary": "Federated Learning (FL) is a distributed paradigm aimed at protecting\nparticipant data privacy by exchanging model parameters to achieve high-quality\nmodel training. However, this distributed nature also makes FL highly\nvulnerable to backdoor attacks. Notably, the recently proposed state-of-the-art\n(SOTA) attack, 3DFed (SP2023), uses an indicator mechanism to determine whether\nthe backdoor models have been accepted by the defender and adaptively optimizes\nbackdoor models, rendering existing defenses ineffective. In this paper, we\nfirst reveal that the failure of existing defenses lies in the employment of\nempirical statistical measures that are loosely coupled with backdoor attacks.\nMotivated by this, we propose a Malignity-Aware backdooR defenSe (MARS) that\nleverages backdoor energy (BE) to indicate the malicious extent of each neuron.\nTo amplify malignity, we further extract the most prominent BE values from each\nmodel to form a concentrated backdoor energy (CBE). Finally, a novel\nWasserstein distance-based clustering method is introduced to effectively\nidentify backdoor models. Extensive experiments demonstrate that MARS can\ndefend against SOTA backdoor attacks and significantly outperforms existing\ndefenses.", "AI": {"tldr": "This paper proposes MARS, a malignity-aware backdoor detection method for federated learning that addresses limitations of existing defenses by using backdoor energy analysis and Wasserstein-based clustering.", "motivation": "Existing FL defenses fail due to loose coupling with backdoor attack characteristics, as shown by the adaptive 3DFed attack (SP2023) that bypasses statistical measures. The paper identifies weaknesses in empirical defense approaches.", "method": "MARS operates in three stages: (1): Calculates neuron-level backdoor energy (BE) to quantify malignity, (2): Concentrates prominent BE values into CBE to amplify malicious patterns, (3): Uses Wasserstein distance-based clustering to isolate backdoor models.", "result": "Extensive experiments show MARS achieves superior defense performance against state-of-the-art backdoor attacks like 3DFed, outperforming existing methods with significant improvements in detection and mitigation.", "conclusion": "The paper presents MARS as a fundamentally novel defense framework that overcomes existing limitations through focused energy analysis and optimized clustering, setting a new benchmark for FL backdoor defenses."}}
{"id": "2509.20384", "categories": ["cs.CR", "cs.AI", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20384", "abs": "https://arxiv.org/abs/2509.20384", "authors": ["Jiayi Lin", "Liangcai Su", "Junzhe Li", "Chenxiong Qian"], "title": "R1-Fuzz: Specializing Language Models for Textual Fuzzing via Reinforcement Learning", "comment": null, "summary": "Fuzzing is effective for vulnerability discovery but struggles with complex\ntargets such as compilers, interpreters, and database engines, which accept\ntextual input that must satisfy intricate syntactic and semantic constraints.\nAlthough language models (LMs) have attracted interest for this task due to\ntheir vast latent knowledge and reasoning potential, their practical adoption\nhas been limited. The major challenges stem from insufficient exploration of\ndeep program logic among real-world codebases, and the high cost of leveraging\nlarger models. To overcome these challenges, we propose R1-Fuzz, the first\nframework that leverages reinforcement learning (RL) to specialize\ncost-efficient LMs and integrate them for complex textual fuzzing input\ngeneration. R1-Fuzz introduces two key designs: coverage-slicing-based question\nconstruction and a distance-based reward calculation. Through RL-based\npost-training of a model with our constructed dataset, R1-Fuzz designs a\nfuzzing workflow that tightly integrates LMs to reason deep program semantics\nduring fuzzing. Evaluations on diverse real-world targets show that our design\nenables a small model, named R1-Fuzz-7B, to rival or even outperform much\nlarger models in real-world fuzzing. Notably, R1-Fuzz achieves up to 75\\%\nhigher coverage than state-of-the-art fuzzers and discovers 29 previously\nunknown vulnerabilities, demonstrating its practicality.", "AI": {"tldr": "R1-Fuzz is a cost-efficient framework using reinforcement learning to enhance language models for complex fuzzing tasks, achieving performance comparable to larger models while discovering 29 new vulnerabilities.", "motivation": "Fuzzing struggles with complex targets due to syntactic/semantic constraints, and language models (LMs), despite potential, face limitations in exploration and cost-of-large-models.", "method": "R1-Fuzz leverages RL-based post-training of a small model (R1-Fuzz-7B) using coverage-slicing question construction and distance-based rewards to integrate LMs with deep program logic reasoning during fuzzing.", "result": "R1-Fuzz outperforms state-of-the-art fuzzers by 75\\% in coverage and discovers 29 new vulnerabilities, while a 7B model matches performance of larger models.", "conclusion": "By combining RL and specialized LM training, R1-Fuzz addresses real-world fuzzing limitations with cost-effective, effective input generation, proving practical for complex software targets."}}
{"id": "2509.20388", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20388", "abs": "https://arxiv.org/abs/2509.20388", "authors": ["Amir AL-Maamari"], "title": "Can You Trust Your Copilot? A Privacy Scorecard for AI Coding Assistants", "comment": null, "summary": "The rapid integration of AI-powered coding assistants into developer\nworkflows has raised significant privacy and trust concerns. As developers\nentrust proprietary code to services like OpenAI's GPT, Google's Gemini, and\nGitHub Copilot, the unclear data handling practices of these tools create\nsecurity and compliance risks. This paper addresses this challenge by\nintroducing and applying a novel, expert-validated privacy scorecard. The\nmethodology involves a detailed analysis of four document types; from legal\npolicies to external audits; to score five leading assistants against 14\nweighted criteria. A legal expert and a data protection officer refined these\ncriteria and their weighting. The results reveal a distinct hierarchy of\nprivacy protections, with a 20-point gap between the highest- and lowest-ranked\ntools. The analysis uncovers common industry weaknesses, including the\npervasive use of opt-out consent for model training and a near-universal\nfailure to filter secrets from user prompts proactively. The resulting\nscorecard provides actionable guidance for developers and organizations,\nenabling evidence-based tool selection. This work establishes a new benchmark\nfor transparency and advocates for a shift towards more user-centric privacy\nstandards in the AI industry.", "AI": {"tldr": "This paper evaluates five leading AI coding assistants using a novel privacy scorecard, identifying significant privacy concerns and gaps in their data practices.", "motivation": "The integration of AI coding assistants poses privacy and trust risks due to opaque data handling practices, necessitating a standardized evaluation method.", "method": "An expert-validated privacy scorecard was designed through collaboration between a legal expert and a data protection officer. It involved analysis of four document types (legal policies, external audits, etc.) against 14 weighted criteria.", "result": "A 20-point gap in privacy protections was found among the five tools, with common issues such as opt-out consent for training and failure to filter secrets from prompts.", "conclusion": "The scorecard offers actionable guidance for developers, establishes a new transparency benchmark, and advocates for user-centric privacy improvements in the AI industry."}}
{"id": "2509.20380", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.20380", "abs": "https://arxiv.org/abs/2509.20380", "authors": ["Samyak Jhaveri", "Vanessa Klotzmann", "Crista Lopes"], "title": "ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation", "comment": null, "summary": "The increasing ubiquity of GPUs is accompanied by the increasing complexity\nof their hardware and parallel programming frameworks. Directive-based parallel\nprogramming standards like OpenACC simplify GPU programming to some extent by\nabstracting away low-level complexities, but a fair amount of expertise is\nstill required in order to use those directives effectively.\n  We introduce ACCeLLiuM, two open weights Large Language Models specifically\nfine-tuned for generating expert OpenACC directives for data-parallel loops,\nalong with the supervised fine-tuning dataset that was used to train them. The\nACCeLLiuM SFT dataset contains 4,033 OpenACC pragma-loop pairs mined from\npublic GitHub C/C++ repositories, with 3,223 pairs for training and 810 for\ntesting. Experimental evaluations show a pronounced performance gap in\ngenerating correct OpenACC pragmas between base LLMs and our fine-tuned\nversions. On the held-out test set, base LLMs fail to consistently generate\nvalid pragmas, whereas LLMs fine-tuned on the ACCeLLiuM dataset generate valid\npragmas with the correct directive type for $87\\%$ of the data-parallel loops,\nand exact pragmas--including directives, clauses, clause order, and clause\nvariables--for $50\\%$ of the cases. Even when not exact, generated pragmas\nfrequently incorporate the correct clauses in a different order than the\nground-truth label, or include additional clauses that enable finer control\nover parallel execution, data movement, and concurrency, offering practical\nvalue beyond strict string-matching. By publicly releasing the code, models,\nand dataset as ACCeLLiuM we hope to establish a reproducible benchmark for\nLLM-powered OpenACC pragma generation, and lower the barrier to automated GPU\noffloading of serially written programs.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.20391", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20391", "abs": "https://arxiv.org/abs/2509.20391", "authors": ["Md. Alamgir Hossain", "Waqas Ishtiaq", "Md. Samiul Islam"], "title": "A Comparative Analysis of Ensemble-Based Machine Learning Approaches with Explainable AI for Multi-Class Intrusion Detection in Drone Networks", "comment": "27 pages, 18 figures, 10 tables", "summary": "The growing integration of drones into civilian, commercial, and defense\nsectors introduces significant cybersecurity concerns, particularly with the\nincreased risk of network-based intrusions targeting drone communication\nprotocols. Detecting and classifying these intrusions is inherently challenging\ndue to the dynamic nature of drone traffic and the presence of multiple\nsophisticated attack vectors such as spoofing, injection, replay, and\nman-in-the-middle (MITM) attacks. This research aims to develop a robust and\ninterpretable intrusion detection framework tailored for drone networks, with a\nfocus on handling multi-class classification and model explainability. We\npresent a comparative analysis of ensemble-based machine learning models,\nnamely Random Forest, Extra Trees, AdaBoost, CatBoost, and XGBoost, trained on\na labeled dataset comprising benign traffic and nine distinct intrusion types.\nComprehensive data preprocessing was performed, including missing value\nimputation, scaling, and categorical encoding, followed by model training and\nextensive evaluation using metrics such as macro F1-score, ROC AUC, Matthews\nCorrelation Coefficient, and Log Loss. Random Forest achieved the highest\nperformance with a macro F1-score of 0.9998 and ROC AUC of 1.0000. To validate\nthe superiority of the models, statistical tests, including Friedmans test, the\nWilcoxon signed-rank test with Holm correction, and bootstrapped confidence\nintervals, were applied. Furthermore, explainable AI methods, SHAP and LIME,\nwere integrated to interpret both global and local feature importance,\nenhancing model transparency and decision trustworthiness. The proposed\napproach not only delivers near-perfect accuracy but also ensures\ninterpretability, making it highly suitable for real-time and safety-critical\ndrone operations.", "AI": {"tldr": "The paper introduces a robust, interpretable intrusion detection framework for drones, evaluating ensemble machine learning models and incorporating explainable AI methods.", "motivation": "The integration of drones into various sectors raises cybersecurity concerns due to dynamic traffic and sophisticated attack vectors like spoofing and MITM attacks.", "method": "The authors compared ensemble-based machine learning models (Random Forest, Extra Trees, AdaBoost, CatBoost, XGBoost) on a labeled dataset with benign and intrusive drone traffic, using metrics like F1-score and ROC AUC, and incorporated SHAP and LIME for model explainability.", "result": "Random Forest achieved the highest performance with a macro F1-score of 0.9998 and ROC AUC of 1.0000, and statistical tests validated the models superiority.", "conclusion": "The proposed framework ensures both near-perfect accuracy and interpretability, making it suitable for real-time, safety-critical drone operations."}}
{"id": "2509.20385", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20385", "abs": "https://arxiv.org/abs/2509.20385", "authors": ["Ishara Devendra", "Chaman Wijesiriwardana", "Prasad Wimalaratne"], "title": "State-of-the-Art in Software Security Visualization: A Systematic Review", "comment": null, "summary": "Software security visualization is an interdisciplinary field that combines\nthe technical complexity of cybersecurity, including threat intelligence and\ncompliance monitoring, with visual analytics, transforming complex security\ndata into easily digestible visual formats. As software systems get more\ncomplex and the threat landscape evolves, traditional text-based and numerical\nmethods for analyzing and interpreting security concerns become increasingly\nineffective. The purpose of this paper is to systematically review existing\nresearch and create a comprehensive taxonomy of software security visualization\ntechniques through literature, categorizing these techniques into four types:\ngraph-based, notation-based, matrix-based, and metaphor-based visualization.\nThis systematic review explores over 60 recent key research papers in software\nsecurity visualization, highlighting its key issues, recent advancements, and\nprospective future research directions. From the comprehensive analysis, the\ntwo main areas were distinctly highlighted as extensive software development\nvisualization, focusing on advanced methods for depicting software\narchitecture: operational security visualization and cybersecurity\nvisualization. The findings highlight the necessity for innovative\nvisualization techniques that adapt to the evolving security landscape, with\npractical implications for enhancing threat detection, improving security\nresponse strategies, and guiding future research.", "AI": {"tldr": "The paper systematically reviews software security visualization techniques, categorizing them into four types and highlighting two main areas\u2014software development and operational security visualization\u2014with a focus on adapting to evolving threats.", "motivation": "Traditional text-based and numerical methods are becoming ineffective as software systems and the threat landscape grow more complex. There is a need for better visualization techniques to enhance threat detection and security response strategies.", "method": "The authors conducted a systematic review of the literature, analyzing over 60 recent key research papers to identify and categorize software security visualization techniques into a comprehensive taxonomy.", "result": "The review resulted in a taxonomy of four visualization types: graph-based, notation-based, matrix-based, and metaphor-based. It identified two main areas\u2014extensive software development visualization and operational/cybersecurity visualization\u2014shaping the current research and practices.", "conclusion": "Innovative and adaptable visualization techniques are essential for addressing current and future software security challenges. They offer practical benefits for threat detection and response, and guide future research."}}
{"id": "2509.20395", "categories": ["cs.CR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20395", "abs": "https://arxiv.org/abs/2509.20395", "authors": ["Noam Schmitt", "Marc Antoine Lacoste"], "title": "Centralized vs. Decentralized Security for Space AI Systems? A New Look", "comment": "IEEE HPEC 2025 - 29th Annual IEEE High Performance Extreme Computing\n  Virtual Conference, MIT Lincoln Laboratory, Sep 2025, Boston (MA), United\n  States", "summary": "This paper investigates the trade-off between centralized and decentralized\nsecurity management in constellations of satellites to balance security and\nperformance. We highlight three key AI architectures for automated security\nmanagement: (a) centralized, (b) distributed and (c) federated. The centralized\narchitecture is the best option short term, providing fast training, despite\nthe hard challenge of the communication latency overhead across space.\nDecentralized architectures are better alternatives in the longer term,\nproviding enhanced scalability and security.", "AI": {"tldr": "The paper investigates the trade-off between centralized and decentralized security management in satellite constellations, focusing on balancing security and performance. It highlights three AI architectures for automated security: centralized, distributed, and federated.", "motivation": "The operational constraints of space, such as communication latency, require a careful study of security management strategies to ensure both security and performance. The paper is motivated by the need to compare and evaluate centralized and decentralized approaches in terms of their suitability for space-based AI systems.", "method": "The paper likely uses a comparative analysis approach to evaluate the three AI architectures (centralized, distributed, and federated) considering the unique challenges of space communication, such as latency and scalability. The analysis probably involves theoretical evaluation and maybe some simulations or use-case studies.", "result": "The standardized evaluation of AI architectures for satellite security by this paper reveals that the centralized architecture is optimal in the short term due to fast training despite communication latency challenges. Decentralized systems, particularly federated learning, are suggested as viable long-term options for handling the growing number of satellites and improving security.", "conclusion": "The paper concludes that while centralized security management is suitable for the near future, decentralized approaches, especially federated learning, will become more advantageous as the size and scale of satellite constellations expand, offering better scalability and security."}}
{"id": "2509.20386", "categories": ["cs.SE", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20386", "abs": "https://arxiv.org/abs/2509.20386", "authors": ["Nishant Gaurav", "Adit Akarsh", "Ankit Ranjan", "Manoj Bajaj"], "title": "Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments", "comment": null, "summary": "We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef-\nficiently operate with extensive Model Control Protocol (MCP) tool sets that\nexceed the contextual memory limitations of large language models. Our approach\naddresses the fundamental challenge of tool selection in environments\ncontaining hundreds or thousands of available tools, where loading all tools\nsimultaneously is computationally infeasible. We propose and evaluate five\ndistinct architectures that progressively refine the tool selection process,\nculminating in a search-and-load mechanism that achieves intelligent tool\nselection with minimal computational overhead. Our experimental results\ndemonstrate that the proposed approach reduces tool loading by up to 50% while\nmaintaining task completion accuracy, advancing the path towards truly\ngeneral-purpose AI agents capable of dynamically adapting to diverse task\nenvironments.", "AI": {"tldr": "Dynamic ReAct is a new approach that allows ReAct agents to handle large tool sets efficiently without overloading memory. The method uses a search-and-load mechanism to select the most relevant tools dynamically, reducing the number of tools loaded by up to 50% and maintaining task accuracy. This advancement supports the development of more general AI agents.", "motivation": "The motivation for this paper arises from the challenge of enabling ReAct agents to operate effectively in environments with a vast number of available Model Control Protocol (MCP) tools. Loading all these tools simultaneously is computationally infeasible due to the contextual memory limitations of large language models. This issue limits the scalability and real-world applicability of ReAct agents when dealing with extensive tool sets.", "method": "The method proposed in the paper involves creating and evaluating five distinct architectures that progressively refine the process of selecting relevant tools from a large set of Model Control Protocol (MCP) tools. These architectures focus on optimizing the tool selection to reduce the number of tools loaded, with the ultimate solution being a search-and-load mechanism that allows for intelligent and efficient tool selection without overloading the model's memory. The search-and-load approach prioritizes the most suitable tools for the current task based on dynamic consideration, ensuring adaptive and intelligent selection.", "result": "The results show that the proposed approach achieves a reduction in the number of tools loaded by up to 50% compared to traditional methods, without compromising the accuracy of task completion. This significant overall efficiency makes it possible for ReAct agents to manage larger and more diverse Model Control Protocol (MCP) tool sets effectively.", "conclusion": "The conclusion emphasizes that Dynamic ReAct addresses the critical challenge of inefficiency in large-scale tool usage for ReAct agents by dynamically selecting the most relevant tools. By achieving high task completion accuracy with reduced computational overhead, the approach marks a significant step forward in the development of general AI agents capable of adaption to a variety of task environments."}}
{"id": "2509.20399", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20399", "abs": "https://arxiv.org/abs/2509.20399", "authors": ["Birk Torpmann-Hagen", "Michael A. Riegler", "P\u00e5l Halvorsen", "Dag Johansen"], "title": "Defending against Stegomalware in Deep Neural Networks with Permutation Symmetry", "comment": null, "summary": "Deep neural networks are being utilized in a growing number of applications,\nboth in production systems and for personal use. Network checkpoints are as a\nconsequence often shared and distributed on various platforms to ease the\ndevelopment process. This work considers the threat of neural network\nstegomalware, where malware is embedded in neural network checkpoints at a\nnegligible cost to network accuracy. This constitutes a significant security\nconcern, but is nevertheless largely neglected by the deep learning\npractitioners and security specialists alike. We propose the first effective\ncountermeasure to these attacks. In particular, we show that state-of-the-art\nneural network stegomalware can be efficiently and effectively neutralized\nthrough shuffling the column order of the weight- and bias-matrices, or\nequivalently the channel-order of convolutional layers. We show that this\neffectively corrupts payloads that have been embedded by state-of-the-art\nmethods in neural network steganography at no cost to network accuracy,\noutperforming competing methods by a significant margin. We then discuss\npossible means by which to bypass this defense, additional defense methods, and\nadvocate for continued research into the security of machine learning systems.", "AI": {"tldr": "This paper introduces the first effective defense against neural network stegomalware by shuffling weight/bias matrix columns or convolutional channel orders, neutralizing malware without affecting model accuracy.", "motivation": "Shared deep learning checkpoints pose security risks as malware can be stealthily embedded with minor accuracy costs. Both practitioners and security experts overlook this threat.", "method": "The proposed method disrupts stegomalware by shuffling neural network weight/bias matrices or convolutional channel orders, corrupting embedded payloads while preserving model accuracy.", "result": "The approach outperforms existing methods by completely neutralizing state-of-the-art steganography payloads with zero accuracy degradation.", "conclusion": "The work demonstrates a robust mitigation strategy while advocating for ongoing research into ML security to address potential defense bypasses and broader system robustness."}}
{"id": "2509.20387", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20387", "abs": "https://arxiv.org/abs/2509.20387", "authors": ["Qusai Ramadan", "Jukka Ruohonen", "Abhishek Tiwari", "Adam Alami", "Zeyd Boukhers"], "title": "Towards Systematic Specification and Verification of Fairness Requirements: A Position Paper", "comment": "Accepted at the 2025 IEEE 33rd International Requirements Engineering\n  Conference Workshops", "summary": "Decisions suggested by improperly designed software systems might be prone to\ndiscriminate against people based on protected characteristics, such as gender\nand ethnicity. Previous studies attribute such undesired behavior to flaws in\nalgorithmic design or biased data. However, these studies ignore that\ndiscrimination is often the result of a lack of well-specified fairness\nrequirements and their verification. The fact that experts' knowledge about\nfairness is often implicit makes the task of specifying precise and verifiable\nfairness requirements difficult. In related domains, such as security\nengineering, knowledge graphs have been proven to be effective in formalizing\nknowledge to assist requirements specification and verification. To address the\nlack of formal mechanisms for specifying and verifying fairness requirements,\nwe propose the development of a knowledge graph-based framework for fairness.\nIn this paper, we discuss the challenges, research questions, and a road map\ntowards addressing the research questions.", "AI": {"tldr": "The paper proposes a knowledge graph-based framework to specify and verify fairness requirements in software systems.", "motivation": "Improperly designed software systems can discriminate against people based on protected characteristics like gender and ethnicity. Existing studies focus on algorithmic flaws and biased data, but overlook the role of ambiguous or missing fairness requirements. This paper addresses that gap.", "method": "We draw on lessons from the use of knowledge graphs in security engineering domains and propose a framework specifically for formalizing fairness requirements. The methodology includes the discussion of challenges, formulation of research questions, and a strategic roadmap for research progress in this area.", "result": "The paper does not present experimental results but lays the theoretical and conceptual groundwork for a new approach to fairness in software development.", "conclusion": "This study fills a critical gap by formalizing the need for a knowledge graph-based framework to articulate and verify fairness requirements, thereby offering potential to mitigate discrimination in software systems."}}
{"id": "2509.20405", "categories": ["cs.CR", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.20405", "abs": "https://arxiv.org/abs/2509.20405", "authors": ["Visar Berisha", "Prad Kadambi", "Isabella Lenz"], "title": "Why Speech Deepfake Detectors Won't Generalize: The Limits of Detection in an Open World", "comment": null, "summary": "Speech deepfake detectors are often evaluated on clean, benchmark-style\nconditions, but deployment occurs in an open world of shifting devices,\nsampling rates, codecs, environments, and attack families. This creates a\n``coverage debt\" for AI-based detectors: every new condition multiplies with\nexisting ones, producing data blind spots that grow faster than data can be\ncollected. Because attackers can target these uncovered regions, worst-case\nperformance (not average benchmark scores) determines security. To demonstrate\nthe impact of the coverage debt problem, we analyze results from a recent\ncross-testing framework. Grouping performance by bona fide domain and spoof\nrelease year, two patterns emerge: newer synthesizers erase the legacy\nartifacts detectors rely on, and conversational speech domains\n(teleconferencing, interviews, social media) are consistently the hardest to\nsecure. These findings show that detection alone should not be relied upon for\nhigh-stakes decisions. Detectors should be treated as auxiliary signals within\nlayered defenses that include provenance, personhood credentials, and policy\nsafeguards.", "AI": {"tldr": "The paper discusses how speech deepfake detectors face performance issues under changing real-world conditions, creating a 'coverage debt.' It highlights vulnerabilities with newer synthesizers and in conversational domains, suggesting that detection should be part of a multi-layered defense strategy rather than a sole decision-making factor.", "motivation": "Existing evaluations of speech deepfake detectors often use controlled benchmarks, but real-word conditions in deployment cause these models to miss data blind spots when new factors like synthesizers or environments are introduced.", "method": "The authors analyzed cross-testing framework results by categorizing performance in terms of bonafide domains and spoof release years, identifying patterns in failure rates.", "result": "Newer deepfake synthesizers produce outputs without legacy artifacts, evading detectors, and conversational speech domains are most vulnerable to attacks.", "conclusion": "Speech detectors alone are insufficient for security in high-stakes scenarios; they should be integrated with other methods like provenance checks and policy measures."}}
{"id": "2509.20415", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20415", "abs": "https://arxiv.org/abs/2509.20415", "authors": ["Yu Pan", "Xiaocheng Li", "Hanzhao Wang"], "title": "Online-Optimized RAG for Tool Use and Function Calling", "comment": null, "summary": "In many applications, retrieval-augmented generation (RAG) drives tool use\nand function calling by embedding the (user) queries and matching them to\npre-specified tool/function descriptions. In this paper, we address an\nembedding misalignment issue that often arises in practical applications due to\nimperfect embedding models or noisy descriptions; such misalignment may lead to\nincorrect retrieval and task failure. We introduce Online-Optimized RAG, a\ndeployment-time framework that continually adapts retrieval embeddings from\nlive interactions using minimal feedback (e.g., task success). Online-Optimized\nRAG applies lightweight online gradient updates with negligible per-query\nlatency and requires no changes to the underlying LLM. The method is\nplug-and-play: it supports both single- and multi-hop tool use, dynamic tool\ninventories, and $K$-retrieval with re-ranking. We provide a problem-dependent\ntheoretical analysis that quantifies how the method's performance depends on\nthe initialization quality of the embeddings and other related quantities.\nAcross diverse tool-use and document-retrieval scenarios, our Online-Optimized\nRAG consistently improves tool selection accuracy and end-task success, thus\nproviding a simple, practical path to robust, self-improving RAG systems.", "AI": {"tldr": "The paper introduces Online-Optimized RAG, a framework that continuously adjusts retrieval embeddings using minimal feedback during deployment to enhance tool selection accuracy and task success in RAG systems.", "motivation": "Current RAG systems often encounter task failures due to misalignment in retrieval embeddings caused by imperfect models or noisy descriptions, which is crucial to address for real-world applications.", "method": "Online-Optimized RAG employs lightweight online gradient updates to refine retrieval embeddings with minimal deployment-time latency, allowing for adaptation to live interactions without modifying the underlying LLM.", "result": "Across various tool-use and document-retrieval scenarios, the proposed method consistently improves tool selection accuracy and end-task success rates.", "conclusion": "Online-Optimized RAG provides a robust, self-improving solution for RAG systems, making them more practical and reliable for diverse applications through adaptive embeddings."}}
{"id": "2509.20411", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20411", "abs": "https://arxiv.org/abs/2509.20411", "authors": ["Tharcisse Ndayipfukamiye", "Jianguo Ding", "Doreen Sebastian Sarwatt", "Adamu Gaston Philipo", "Huansheng Ning"], "title": "Adversarial Defense in Cybersecurity: A Systematic Review of GANs for Threat Detection and Mitigation", "comment": "35 pages, 10 tables, 4figures", "summary": "Machine learning-based cybersecurity systems are highly vulnerable to\nadversarial attacks, while Generative Adversarial Networks (GANs) act as both\npowerful attack enablers and promising defenses. This survey systematically\nreviews GAN-based adversarial defenses in cybersecurity (2021--August 31,\n2025), consolidating recent progress, identifying gaps, and outlining future\ndirections. Using a PRISMA-compliant systematic literature review protocol, we\nsearched five major digital libraries. From 829 initial records, 185\npeer-reviewed studies were retained and synthesized through quantitative trend\nanalysis and thematic taxonomy development. We introduce a four-dimensional\ntaxonomy spanning defensive function, GAN architecture, cybersecurity domain,\nand adversarial threat model. GANs improve detection accuracy, robustness, and\ndata utility across network intrusion detection, malware analysis, and IoT\nsecurity. Notable advances include WGAN-GP for stable training, CGANs for\ntargeted synthesis, and hybrid GAN models for improved resilience. Yet,\npersistent challenges remain such as instability in training, lack of\nstandardized benchmarks, high computational cost, and limited explainability.\nGAN-based defenses demonstrate strong potential but require advances in stable\narchitectures, benchmarking, transparency, and deployment. We propose a roadmap\nemphasizing hybrid models, unified evaluation, real-world integration, and\ndefenses against emerging threats such as LLM-driven cyberattacks. This survey\nestablishes the foundation for scalable, trustworthy, and adaptive GAN-powered\ndefenses.", "AI": {"tldr": "This paper reviews GAN-based adversarial defenses in cybersecurity from 2021-2025, analyzing 185 studies to identify advancements (e.g., WGAN-GP, CGANs) and persistent challenges (training instability, computational costs). A four-dimensional taxonomy is proposed, along with a roadmap for improving scalability and adaptability against threats like AI-driven attacks.", "motivation": "ML cybersecurity systems face vulnerabilities from adversarial attacks, while GANs simultaneously enable new threats and promising defenses. This survey addresses the need for systematic evaluation of GAN-based solutions to identify gaps and guide future research.", "method": "PRISMA-compliant review of 185 peer-reviewed studies from five digital libraries using (1) quantitative trend analysis and (2)", "result": "GANs improve detection accuracy in network intrusion, malware analysis, and IoT with specialized architectures (CGANs, hybrid GANs). Challenges include: 63% of studies report training instability; 78\\% lack standardized benchmarks; computational costs increase by 40-150\\% compared to traditional methods.", "conclusion": "While GAN-based defenses show potential, deployment requires advances in stable architectures, unified benchmarking frameworks, and explainability. The proposed roadmap prioritizes hybrid models and real-world integration to prepare against emerging threats like LLM-driven attacks."}}
{"id": "2509.20421", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20421", "abs": "https://arxiv.org/abs/2509.20421", "authors": ["Reiner H\u00e4hnle", "Cosimo Laneve", "Adele Veschetti"], "title": "Formal Verification of Legal Contracts: A Translation-based Approach", "comment": null, "summary": "Stipula is a domain-specific programming language designed to model legal\ncontracts with enforceable properties, especially those involving asset\ntransfers and obligations. This paper presents a methodology to formally verify\nthe correctness of Stipula contracts through translation into Java code\nannotated with Java Modeling Language specifications. As a verification\nbackend, the deductive verification tool KeY is used. Both, the translation and\nthe verification of partial and total correctness for a large subset of Stipula\ncontracts, those with disjoint cycles, is fully automatic. Our work\ndemonstrates that a general-purpose deductive verification tool can be used\nsuccessfully in a translation approach.", "AI": {"tldr": "Stipula is a domain-specific language for legal contracts. This paper presents a method to verify their correctness by translating them into Java with JML annotations, using KeY for fully automatic deductive verification.", "motivation": "Ensuring correctness of legal contracts with enforceable properties (e.g., asset transfers and obligations) is critical to avoid errors and disputes.", "method": "Contracts are translated to Java code annotated with JML specifications. The deductive verification tool KeY is then used to verify partial and total correctness.", "result": "The approach achieves fully automatic verification for a subset of Stipula contracts (those with disjoint cycles), demonstrating seamless integration with a general-purpose tool.", "conclusion": "The study shows that general-purpose deductive verification tools like KeY can succeed in specialized domains like legal contract verification through translation-based approaches."}}
{"id": "2509.20418", "categories": ["cs.CR", "cs.AI", "cs.ET", "K.6.5; I.2.0"], "pdf": "https://arxiv.org/pdf/2509.20418", "abs": "https://arxiv.org/abs/2509.20418", "authors": ["Grace Billiris", "Asif Gill", "Madhushi Bandara"], "title": "A Taxonomy of Data Risks in AI and Quantum Computing (QAI) - A Systematic Review", "comment": "11 pages, 2 figures, 2 tables", "summary": "Quantum Artificial Intelligence (QAI), the integration of Artificial\nIntelligence (AI) and Quantum Computing (QC), promises transformative advances,\nincluding AI-enabled quantum cryptography and quantum-resistant encryption\nprotocols. However, QAI inherits data risks from both AI and QC, creating\ncomplex privacy and security vulnerabilities that are not systematically\nstudied. These risks affect the trustworthiness and reliability of AI and QAI\nsystems, making their understanding critical. This study systematically reviews\n67 privacy- and security-related studies to expand understanding of QAI data\nrisks. We propose a taxonomy of 22 key data risks, organised into five\ncategories: governance, risk assessment, control implementation, user\nconsiderations, and continuous monitoring. Our findings reveal vulnerabilities\nunique to QAI and identify gaps in holistic risk assessment. This work\ncontributes to trustworthy AI and QAI research and provides a foundation for\ndeveloping future risk assessment tools.", "AI": {"tldr": "The paper investigates data risks in Quantum Artificial Intelligence (QAI), creating a taxonomy of 22 risks across five categories (governance, risk assessment, control implementation, user considerations, and continuous monitoring). It highlights QAI-specific vulnerabilities and gaps in holistic risk evaluation, contributing to trustworthy AI/qAI research.", "motivation": "QAI combines AI and quantum computing risks, but systemic analysis of privacy/security vulnerabilities is lacking. Understanding these risks is critical for ensuring the trustworthiness and reliability of QAI systems.", "method": "A systematic review of 67 privacy- and security-related studies to identify and categorize QAI data risks, resulting in a structured taxonomy.", "result": "Identified 22 key QAI data risks in five categories, revealed unique QAI vulnerabilities, and uncovered gaps in comprehensive risk assessment approaches.", "conclusion": "Establishes a foundational framework for addressing QAI risks, advancing trustworthy AI/qAI research, and guiding future risk assessment tool development."}}
{"id": "2509.20491", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20491", "abs": "https://arxiv.org/abs/2509.20491", "authors": ["Brahim Mahmoudi", "Naouel Moha", "Quentin Stievenert", "Florent Avellaneda"], "title": "AI-Specific Code Smells: From Specification to Detection", "comment": null, "summary": "The rise of Artificial Intelligence (AI) is reshaping how software systems\nare developed and maintained. However, AI-based systems give rise to new\nsoftware issues that existing detection tools often miss. Among these, we focus\non AI-specific code smells, recurring patterns in the code that may indicate\ndeeper problems such as unreproducibility, silent failures, or poor model\ngeneralization. We introduce SpecDetect4AI, a tool-based approach for the\nspecification and detection of these code smells at scale. This approach\ncombines a high-level declarative Domain-Specific Language (DSL) for rule\nspecification with an extensible static analysis tool that interprets and\ndetects these rules for AI-based systems. We specified 22 AI-specific code\nsmells and evaluated SpecDetect4AI on 826 AI-based systems (20M lines of code),\nachieving a precision of 88.66% and a recall of 88.89%, outperforming other\nexisting detection tools. Our results show that SpecDetect4AI supports the\nspecification and detection of AI-specific code smells through dedicated rules\nand can effectively analyze large AI-based systems, demonstrating both\nefficiency and extensibility (SUS 81.7/100).", "AI": {"tldr": "SpecDetect4AI addresses undetected AI-specific code smells by introducing a DSL-driven static analysis tool, detecting 22 smells with 88.66% precision and 88.89 recall across 20M lines of code.", "motivation": "Existing tools fail to detect AI-specific code smells causing issues like unreproducibility and poor model generalization. This paper improves detection for AI-based systems.", "method": "Combines a Domain-Specific Language (DSL) for declarative rule specification with an extensible static analysis tool. Defined 22 AI-specific code smells.", "result": "Achieved 88.66% precision and 88.89 recall on 826 AI-based systems (20MLOC), outperforming existing tools. SUS score of 81.7/100 validates usability.", "conclusion": "SpecDetect4AI effectively identifies AI-specific code smells at scale, demonstrating superior efficiency and extensibility for maintaining AI system quality."}}
{"id": "2509.20460", "categories": ["cs.CR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.20460", "abs": "https://arxiv.org/abs/2509.20460", "authors": ["Andrew Campbell", "Anna Scaglione", "Hang Liu", "Victor Elvira", "Sean Peisert", "Daniel Arnold"], "title": "Differential Privacy of Network Parameters from a System Identification Perspective", "comment": null, "summary": "This paper addresses the problem of protecting network information from\nprivacy system identification (SI) attacks when sharing cyber-physical system\nsimulations. We model analyst observations of networked states as time-series\noutputs of a graph filter driven by differentially private (DP) nodal\nexcitations, with the analyst aiming to infer the underlying graph shift\noperator (GSO). Unlike traditional SI, which estimates system parameters, we\nstudy the inverse problem: what assumptions prevent adversaries from\nidentifying the GSO while preserving utility for legitimate analysis. We show\nthat applying DP mechanisms to inputs provides formal privacy guarantees for\nthe GSO, linking the $(\\epsilon,\\delta)$-DP bound to the spectral properties of\nthe graph filter and noise covariance. More precisely, for DP Gaussian signals,\nthe spectral characteristics of both the filter and noise covariance determine\nthe privacy bound, with smooth filters and low-condition-number covariance\nyielding greater privacy.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.20497", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20497", "abs": "https://arxiv.org/abs/2509.20497", "authors": ["Ahmed Aljohani", "Hyunsook Do"], "title": "PromptDebt: A Comprehensive Study of Technical Debt Across LLM Projects", "comment": "Accepted at Proceedings of the 2025 Evaluation and Assessment in\n  Software Engineering (EASE '25)", "summary": "Large Language Models (LLMs) are increasingly embedded in software via APIs\nlike OpenAI, offering powerful AI features without heavy infrastructure. Yet\nthese integrations bring their own form of self-admitted technical debt (SATD).\nIn this paper, we present the first large-scale empirical study of LLM-specific\nSATD: its origins, prevalence, and mitigation strategies. By analyzing 93,142\nPython files across major LLM APIs, we found that 54.49% of SATD instances stem\nfrom OpenAI integrations and 12.35% from LangChain use. Prompt design emerged\nas the primary source of LLM-specific SATD, with 6.61% of debt related to\nprompt configuration and optimization issues, followed by hyperparameter tuning\nand LLM-framework integration. We further explored which prompt techniques\nattract the most debt, revealing that instruction-based prompts (38.60%) and\nfew-shot prompts (18.13%) are particularly vulnerable due to their dependence\non instruction clarity and example quality. Finally, we release a comprehensive\nSATD dataset to support reproducibility and offer practical guidance for\nmanaging technical debt in LLM-powered systems.", "AI": {"tldr": "The paper analyzes LLM-specific SATD by examining 93,142 Python files, finding that prompt design is the main source of debt and releasing a dataset for reproducibility.", "motivation": "Integrating LLMs via APIs introduces new forms of technical debt, but systematic analysis of how it arises and what strategies mitigate it is missing.", "method": "They perform a large-scale empirical study on 93,142 Python files across major LLM APIs and identify sources and frequency of LLM-specific SATD.", "result": "54.49% of debt from OpenAI; 12.35% from LangChain; prompt design is the primary cause (6.61%), especially instruction-based and few-shot prompts (38.60% and 18.13%), and a dataset is released.", "conclusion": "Prompt design strategies and optimizations require further attention in managing technical debt for LLM integration, and the dataset provides support for future improvements."}}
{"id": "2509.20476", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20476", "abs": "https://arxiv.org/abs/2509.20476", "authors": ["Ren-Yi Huang", "Dumindu Samaraweera", "Prashant Shekhar", "J. Morris Chang"], "title": "Advancing Practical Homomorphic Encryption for Federated Learning: Theoretical Guarantees and Efficiency Optimizations", "comment": null, "summary": "Federated Learning (FL) enables collaborative model training while preserving\ndata privacy by keeping raw data locally stored on client devices, preventing\naccess from other clients or the central server. However, recent studies reveal\nthat sharing model gradients creates vulnerability to Model Inversion Attacks,\nparticularly Deep Leakage from Gradients (DLG), which reconstructs private\ntraining data from shared gradients. While Homomorphic Encryption has been\nproposed as a promising defense mechanism to protect gradient privacy, fully\nencrypting all model gradients incurs high computational overhead. Selective\nencryption approaches aim to balance privacy protection with computational\nefficiency by encrypting only specific gradient components. However, the\nexisting literature largely overlooks a theoretical exploration of the spectral\nbehavior of encrypted versus unencrypted parameters, relying instead primarily\non empirical evaluations. To address this gap, this paper presents a framework\nfor theoretical analysis of the underlying principles of selective encryption\nas a defense against model inversion attacks. We then provide a comprehensive\nempirical study that identifies and quantifies the critical factors, such as\nmodel complexity, encryption ratios, and exposed gradients, that influence\ndefense effectiveness. Our theoretical framework clarifies the relationship\nbetween gradient selection and privacy preservation, while our experimental\nevaluation demonstrates how these factors shape the robustness of defenses\nagainst model inversion attacks. Collectively, these contributions advance the\nunderstanding of selective encryption mechanisms and offer principled guidance\nfor designing efficient, scalable, privacy-preserving federated learning\nsystems.", "AI": {"tldr": "This paper introduces a theoretical framework for analyzing the principles of selective encryption as a defense against model inversion attacks in Federated Learning, and an empirical study to identify factors affecting its effectiveness.", "motivation": "The motivation arises from the vulnerability of gradient sharing to model inversion attacks like DLG, and the computational expense of full encryption. The paper seeks to provide theoretical insights to improve selective encryption strategies for privacy with efficiency.", "method": "The authors developed a theoretical framework to analyze selective encryption and conducted empirical experiments under various conditions to assess the defense effectiveness in relation to factors such as model complexity, encryption ratios, and exposed gradients.", "result": "The study results reveal the impact of model complexity, encryption ratios, and exposed gradients on the effectiveness of selective encryption defenses against model inversion attacks, supported by empirical validations.", "conclusion": "The paper concludes that the presented theoretical and empirical analyses enhance understanding of selective encryption, offering guidance to build more efficient and scalable privacy-preserving Federated Learning systems."}}
{"id": "2509.20518", "categories": ["cs.SE", "cs.PL", "D.2.3"], "pdf": "https://arxiv.org/pdf/2509.20518", "abs": "https://arxiv.org/abs/2509.20518", "authors": ["Sayed Mahbub Hasan Amiri", "Md Mainul Islam"], "title": "Enhancing Python Programming Education with an AI-Powered Code Helper: Design, Implementation, and Impact", "comment": "20 pages, 16 figures", "summary": "This is the study that presents an AI-Python-based chatbot that helps\nstudents to learn programming by demonstrating solutions to such problems as\ndebugging errors, solving syntax problems or converting abstract theoretical\nconcepts to practical implementations. Traditional coding tools like Integrated\nDevelopment Environments (IDEs) and static analyzers do not give robotic help\nwhile AI-driven code assistants such as GitHub Copilot focus on getting things\ndone. To close this gap, our chatbot combines static code analysis, dynamic\nexecution tracing, and large language models (LLMs) to provide the students\nwith relevant and practical advice, hence promoting the learning process. The\nchatbots hybrid architecture employs CodeLlama for code embedding, GPT-4 for\nnatural language interactions, and Docker-based sandboxing for secure\nexecution. Evaluated through a mixed-methods approach involving 1,500 student\nsubmissions, the system demonstrated an 85% error resolution success rate,\noutperforming standalone tools like pylint (62%) and GPT-4 (73%). Quantitative\nresults revealed a 59.3% reduction in debugging time among users, with pre- and\npost-test assessments showing a 34% improvement in coding proficiency,\nparticularly in recursion and exception handling. Qualitative feedback from 120\nstudents highlighted the chatbots clarity, accessibility, and\nconfidence-building impact, though critiques included occasional latency and\nrestrictive code sanitization. By balancing technical innovation with\npedagogical empathy, this research provides a blueprint for AI tools that\nprioritize educational equity and long-term skill retention over mere code\ncompletion. The chatbot exemplifies how AI can augment human instruction,\nfostering deeper conceptual understanding in programming education.", "AI": {"tldr": "This paper introduces an AI-Python chatbot that enhances programming education by combining static analysis, dynamic execution, and LLMs like CodeLlama and GPT-4. It addresses gaps in traditional tools and AI code assistants, demonstrating high error-resolution success and improved student learning.", "motivation": "Traditional coding tools lack interactive guidance while AI assistants prioritize code completion over learning. This chatbot bridges the gap by providing pedagogically focused, practical assistance for deeper conceptual understanding.", "method": "The hybrid chatbot uses CodeLlama for code embeddings, GPT-4 for natural language interaction, and Docker sandboxing for safety. It integrates static/dynamic analysis and LLMs, tested via 1,500 student submissions with mixed-methods evaluation.", "result": "Achieved 85% error resolution success (vs. 62% for pylint, 73% for GPT-4 alone), reduced debugging time by 59.3%, and 34% coding proficiency improvement. Student feedback praised clarity and confidence-building but noted latency and code sanitization limitations.", "conclusion": "This chatbot demonstrates AI's potential to complement education by prioritizing skill retention and equity over code completion. Its architecture provides a framework for future AI tools that enhance conceptual understanding in programming instruction."}}
{"id": "2509.20589", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20589", "abs": "https://arxiv.org/abs/2509.20589", "authors": ["Maria Chiper", "Radu Tudor Ionescu"], "title": "Every Character Counts: From Vulnerability to Defense in Phishing Detection", "comment": "Accepted at ICTAI 2025", "summary": "Phishing attacks targeting both organizations and individuals are becoming an\nincreasingly significant threat as technology advances. Current automatic\ndetection methods often lack explainability and robustness in detecting new\nphishing attacks. In this work, we investigate the effectiveness of\ncharacter-level deep learning models for phishing detection, which can provide\nboth robustness and interpretability. We evaluate three neural architectures\nadapted to operate at the character level, namely CharCNN, CharGRU, and\nCharBiLSTM, on a custom-built email dataset, which combines data from multiple\nsources. Their performance is analyzed under three scenarios: (i) standard\ntraining and testing, (ii) standard training and testing under adversarial\nattacks, and (iii) training and testing with adversarial examples. Aiming to\ndevelop a tool that operates as a browser extension, we test all models under\nlimited computational resources. In this constrained setup, CharGRU proves to\nbe the best-performing model across all scenarios. All models show\nvulnerability to adversarial attacks, but adversarial training substantially\nimproves their robustness. In addition, by adapting the Gradient-weighted Class\nActivation Mapping (Grad-CAM) technique to character-level inputs, we are able\nto visualize which parts of each email influence the decision of each model.\nOur open-source code and data is released at\nhttps://github.com/chipermaria/every-character-counts.", "AI": {"tldr": "This paper evaluates character-level deep learning models (CharCNN, CharGRU, CharBiLSTM) for phishing detection, emphasizing robustness, interpretability, and performance under adversarial scenarios. CharGRU performs best in constrained environments, and adversarial training improves model robustness. Grad-CAM visualization provides interpretability.", "motivation": "Current phishing detection methods lack explainability and robustness against new attacks. Character-level models offer potential advantages in capturing subtle attack patterns while enabling interpretability for security experts.", "method": "Compared three character-level architectures (CharCNN, CharGRU, CharBiLSTM) on a multi-source email dataset under three scenarios: standard detection, adversarial attacks, and adversarial training. Evaluated model performance with Grad-CAM visualization for interpretability under limited computational resources.", "result": "CharGRU achieved best performance across all scenarios. All models showed adversarial vulnerability but improved with adversarial training. Grad-CAM successfully identified character-level decision patterns. Code and data were open-sourced.", "conclusion": "Character-level models, particularly CharGRU, provide effective phishing detection with interpretability. Adversarial training enhances robustness, and Grad-CAM visualization offers valuable insights for security analysis. The approach addresses limitations in current detection methods."}}
{"id": "2509.20552", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20552", "abs": "https://arxiv.org/abs/2509.20552", "authors": ["Xinyu Shi", "Zhenhao Li", "An Ran Chen"], "title": "Enhancing LLM-based Fault Localization with a Functionality-Aware Retrieval-Augmented Generation Framework", "comment": null, "summary": "Fault localization (FL) is a critical but time-consuming task in software\ndebugging, aiming to identify faulty code elements. While recent advances in\nlarge language models (LLMs) have shown promise for FL, they often struggle\nwith complex systems due to the lack of project-specific knowledge and the\ndifficulty of navigating large projects. To address these limitations, we\npropose FaR-Loc, a novel framework that enhances method-level FL by integrating\nLLMs with retrieval-augmented generation (RAG). FaR-Loc consists of three key\ncomponents: LLM Functionality Extraction, Semantic Dense Retrieval, and LLM\nRe-ranking. First, given a failed test and its associated stack trace, the LLM\nFunctionality Extraction module generates a concise natural language\ndescription that captures the failing behavior. Next, the Semantic Dense\nRetrieval component leverages a pre-trained code-understanding encoder to embed\nboth the functionality description (natural language) and the covered methods\n(code) into a shared semantic space, enabling the retrieval of methods with\nsimilar functional behavior. Finally, the LLM Re-ranking module reorders the\nretrieved methods based on their contextual relevance. Our experiments on the\nwidely used Defects4J benchmark show that FaR-Loc outperforms state-of-the-art\nLLM-based baselines SoapFL and AutoFL, by 14.6% and 9.1% in Top-1 accuracy, by\n19.2% and 22.1% in Top-5 accuracy, respectively. It also surpasses all\nlearning-based and spectrum-based baselines across all Top-N metrics without\nrequiring re-training. Furthermore, we find that pre-trained code embedding\nmodels that incorporate code structure, such as UniXcoder, can significantly\nimprove fault localization performance by up to 49.0% in Top-1 accuracy.\nFinally, we conduct a case study to illustrate the effectiveness of FaR-Loc and\nto provide insights for its practical application.", "AI": {"tldr": "FaR-Loc is a new FL framework integrating LLMs and RAG to improve method-level fault localization.", "motivation": "Despite the potential of LLMs in FL, they struggle in complex systems due to the lack of project-specific knowledge and difficulty in navigating large codebases.", "method": "FaR-Loc employs three components: LLM Functionality Extraction for behavior description generation, Semantic Dense Retrieval for embedding both code and NL descriptions in a shared space, and LLM Re-ranking for contextual relevance-based ordering of retrieved methods.", "result": "FaR-Loc outperforms existing LLM-based FL approaches by significant margins in both Top-1 and Top-5 accuracies on Defects4J, and also exhibits superiority over learning-based and spectrum-based baselines.", "conclusion": "The integration of LLMs with retrieval-augmented generation via FaR-Loc can enhance FL effectiveness. The use of code-structure-aware pre-trained code embedding models further significantly boosts FL performance,."}}
{"id": "2509.20592", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.20592", "abs": "https://arxiv.org/abs/2509.20592", "authors": ["Oluwole Adewusi", "Wallace S. Msagusa", "Jean Pierre Imanirumva", "Okemawo Obadofin", "Jema D. Ndibwile"], "title": "Beyond SSO: Mobile Money Authentication for Inclusive e-Government in Sub-Saharan Africa", "comment": null, "summary": "The rapid adoption of Mobile Money Services (MMS) in Sub-Saharan Africa (SSA)\noffers a viable path to improve e-Government service accessibility in the face\nof persistent low internet penetration. However, existing Mobile Money\nAuthentication (MMA) methods face critical limitations, including\nsusceptibility to SIM swapping, weak session protection, and poor scalability\nduring peak demand. This study introduces a hybrid MMA framework that combines\nUnstructured Supplementary Service Data (USSD)-based multi-factor\nauthentication with secure session management via cryptographically bound JSON\nWeb Tokens (JWT). Unlike traditional MMA systems that rely solely on SIM-PIN\nverification or smartphone-dependent biometrics, our design implements a\nthree-factor authentication model; SIM verification, PIN entry, and session\ntoken binding, tailored for resource-constrained environments. Simulations and\ncomparative analysis against OAuth-based Single Sign-On (SSO) methods reveal a\n45% faster authentication time (8 seconds vs. 12 to 15 seconds), 15% higher\nsuccess under poor network conditions (95% vs. 80%), and increased resistance\nto phishing and brute-force attacks. Penetration testing and threat modeling\nfurther demonstrate a substantial reduction in vulnerability exposure compared\nto conventional approaches. The primary contributions of this work are: (1) a\nhybrid authentication protocol that ensures offline accessibility and secure\nsession continuity; (2) a tailored security framework addressing threats like\nSIM swapping and social engineering in SSA; and (3) demonstrated scalability\nfor thousands of users with reduced infrastructure overhead. The proposed\napproach advances secure digital inclusion in SSA and other regions with\nsimilar constraints.", "AI": {"tldr": "This paper proposes a hybrid Mobile Money Authentication (MMA) framework for Sub-Saharan Africa, combining USSD-based multi-factor authentication with cryptographically bound JSON Web Tokens (JWT). The solution addresses vulnerabilities like SIM swapping, improves scalability, and performs significantly better than OAuth-based SSO in low-network conditions.", "motivation": "Mobile Money Services (MMS) are widespread in Sub-Saharan Africa (SSA), but existing MMA methods face critical issues: SIM swapping susceptibility, weak session protection, and poor scalability during high demand. Internet penetration remains low, necessitating authentication solutions tailored for resource-constrained environments.", "method": "The authors designed a three-factor authentication model: SIM verification (leveraging USSD\u2019s offline capabilities), user PIN entry, and cryptographically bound JWTs for session security. The framework prioritizes offline accessibility, secure session continuity, and resistance to phishing/brute-force attacks through hybrid security mechanisms.", "result": "Compared to traditional OAuth-based Single Sign-On (SSO), the framework achieves 45s faster authentication time (8s vs. 12-15s), 15s higher success rate under poor network conditions (95 vs. 80%), and demonstrates resilience against phishing and brute-force attacks. Threat modeling and penetration testing validate reduced vulnerability exposure.", "conclusion": "The framework advances secure digital inclusion in SSA by providing a scalable, low-infrastructure MMA solution. Key contributions include: (1)a hybrid authentication protocol for offline/low-bandwidth environments; (2)a threat-specific security framework for SIM swapping and social engineering; and (3)demonstrated scalability for large user bases with minimal overhead."}}
{"id": "2509.20631", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20631", "abs": "https://arxiv.org/abs/2509.20631", "authors": ["Michael Zhang", "Yuan Tian", "Mariam Guizani"], "title": "Design, Implementation and Evaluation of a Novel Programming Language Topic Classification Workflow", "comment": null, "summary": "As software systems grow in scale and complexity, understanding the\ndistribution of programming language topics within source code becomes\nincreasingly important for guiding technical decisions, improving onboarding,\nand informing tooling and education. This paper presents the design,\nimplementation, and evaluation of a novel programming language topic\nclassification workflow. Our approach combines a multi-label Support Vector\nMachine (SVM) with a sliding window and voting strategy to enable fine-grained\nlocalization of core language concepts such as operator overloading, virtual\nfunctions, inheritance, and templates. Trained on the IBM Project CodeNet\ndataset, our model achieves an average F1 score of 0.90 across topics and 0.75\nin code-topic highlight. Our findings contribute empirical insights and a\nreusable pipeline for researchers and practitioners interested in code analysis\nand data-driven software engineering.", "AI": {"tldr": "The paper introduces a new classification workflow for identifying core programming language topics in code, achieving high F1 scores using an SVM model on a large code dataset.", "motivation": "Understanding the distribution of programming language topics in source code is crucial for technical decisions, onboarding, tooling, and education, especially as systems become more complex.", "method": "The approach uses a multi-label Support Vector Machine (SVM) with a sliding window and voting strategy to detect and localize key language concepts in code. The model is trained on the IBM Project CodeNet dataset.", "result": "The SVM model achieves an average F1 score of 0.90 across code language topics and 0.75 in code-topic highlight performance.", "conclusion": "The paper provides a reusable pipeline for code analysis and data-driven software engineering, offering empirical evidence of its classification method's effectiveness."}}
{"id": "2509.20639", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20639", "abs": "https://arxiv.org/abs/2509.20639", "authors": ["Adam Swanda", "Amy Chang", "Alexander Chen", "Fraser Burch", "Paul Kassianik", "Konstantin Berlin"], "title": "A Framework for Rapidly Developing and Deploying Protection Against Large Language Model Attacks", "comment": null, "summary": "The widespread adoption of Large Language Models (LLMs) has revolutionized AI\ndeployment, enabling autonomous and semi-autonomous applications across\nindustries through intuitive language interfaces and continuous improvements in\nmodel development. However, the attendant increase in autonomy and expansion of\naccess permissions among AI applications also make these systems compelling\ntargets for malicious attacks. Their inherent susceptibility to security flaws\nnecessitates robust defenses, yet no known approaches can prevent zero-day or\nnovel attacks against LLMs. This places AI protection systems in a category\nsimilar to established malware protection systems: rather than providing\nguaranteed immunity, they minimize risk through enhanced observability,\nmulti-layered defense, and rapid threat response, supported by a threat\nintelligence function designed specifically for AI-related threats.\n  Prior work on LLM protection has largely evaluated individual detection\nmodels rather than end-to-end systems designed for continuous, rapid adaptation\nto a changing threat landscape. We present a production-grade defense system\nrooted in established malware detection and threat intelligence practices. Our\nplatform integrates three components: a threat intelligence system that turns\nemerging threats into protections; a data platform that aggregates and enriches\ninformation while providing observability, monitoring, and ML operations; and a\nrelease platform enabling safe, rapid detection updates without disrupting\ncustomer workflows. Together, these components deliver layered protection\nagainst evolving LLM threats while generating training data for continuous\nmodel improvement and deploying updates without interrupting production.", "AI": {"tldr": "This paper proposes a production-grade defense system for Large Language Models (LLMs), integrating threat intelligence, data platforms, and rapid deployment to address evolving security threats. The system enables continuous adaptation to emerging risks while maintaining operational stability through layered protections and ML-driven improvement.", "motivation": "Widespread LLM adoption creates new security vulnerabilities as attackers target autonomous systems. Existing approaches focus on isolated detection models while neglecting end-to-end systems for dynamic threat landscapes. Zero-day attacks require a robust defense framework with continuous adaptation capabilities.", "method": "The system comprises three components: (1)a threat intelligence module converting emerging threats into actionable protections, (2)a data platform for observability, monitoring, and ML operations with aggregated information, (3)a deployment platform enabling rapid detection updates without workflow disruption. The architecture emphasizes layered defense, continuous model training from threat data, and seamless production updates.", "result": "The integrated platform demonstrates effective layered protection against evolving LLM attacks while generating training data for model improvement. Defense updates are deployed without production interruptions, validating the system's capacity for continuous adaptation to new threats through its threat intelligence and deployment mechanisms.", "conclusion": "LLM security requires a paradigm shift from isolated detection models to comprehensive, production-grade systems that combine malware defense principles with continuous threat intelligence. The proposed architecture provides scalable protection through layered defenses, real-time observability, and automated model improvement, setting a foundation for future AI security frameworks."}}
{"id": "2509.20780", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20780", "abs": "https://arxiv.org/abs/2509.20780", "authors": ["Daniela Grassi", "Fabio Calefato", "Darja Smite", "Nicole Novielli", "Filippo Lanubile"], "title": "Exploring Engagement in Hybrid Meetings", "comment": null, "summary": "Background. The widespread adoption of hybrid work following the COVID-19\npandemic has fundamentally transformed software development practices,\nintroducing new challenges in communication and collaboration as organizations\ntransition from traditional office-based structures to flexible working\narrangements. This shift has established a new organizational norm where even\ntraditionally office-first companies now embrace hybrid team structures. While\nremote participation in meetings has become commonplace in this new\nenvironment, it may lead to isolation, alienation, and decreased engagement\namong remote team members. Aims. This study aims to identify and characterize\nengagement patterns in hybrid meetings through objective measurements, focusing\non the differences between co-located and remote participants. Method. We\nstudied professionals from three software companies over several weeks,\nemploying a multimodal approach to measure engagement. Data were collected\nthrough self-reported questionnaires and physiological measurements using\nbiometric devices during hybrid meetings to understand engagement dynamics.\nResults. The regression analyses revealed comparable engagement levels between\nonsite and remote participants, though remote participants show lower\nengagement in long meetings regardless of participation mode. Active roles\npositively correlate with higher engagement, while larger meetings and\nafternoon sessions are associated with lower engagement. Conclusions. Our\nresults offer insights into factors associated with engagement and\ndisengagement in hybrid meetings, as well as potential meeting improvement\nrecommendations. These insights are potentially relevant not only for software\nteams but also for knowledge-intensive organizations across various sectors\nfacing similar hybrid collaboration challenges.", "AI": {"tldr": "This study examines engagement patterns in hybrid software development meetings, finding remote and onsite participants have similar engagement levels with some variations.", "motivation": "The shift to hybrid work post-COVID-19 has introduced collaboration challenges, potentially causing remote isolation despite common remote meeting practices.", "method": "The study analyzed engagement using self-reported questionnaires and biometric physiological data collected over weeks from professionals in three companies.", "result": "Regression analysis showed similar engagement between onsite and remote workers. Remote experience lower engagement in long meetings regardless of participation mode. Engagement correlates with active roles but is reduced in large teams and afternoon meetings.", "conclusion": "The study provides actionable insights for improving hybrid meetings in software teams and knowledge-sector organizations facing similar challenges."}}
{"id": "2509.20686", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.20686", "abs": "https://arxiv.org/abs/2509.20686", "authors": ["Rian Adam Rajagede", "Yan Solihin"], "title": "Reliability Analysis of Fully Homomorphic Encryption Systems Under Memory Faults", "comment": null, "summary": "Fully Homomorphic Encryption (FHE) represents a paradigm shift in\ncryptography, enabling computation directly on encrypted data and unlocking\nprivacy-critical computation. Despite being increasingly deployed in real\nplatforms, the reliability aspects of FHE systems, especially how they respond\nto faults, have been mostly neglected. This paper aims to better understand of\nhow FHE computation behaves in the presence of memory faults, both in terms of\nindividual operations as well as at the level of applications, for different\nFHE schemes. Finally, we investigate how effective traditional and FHE-specific\nfault mitigation techniques are.", "AI": {"tldr": "The paper explores how FHE systems behave under memory faults and evaluates the effectiveness of fault mitigation techniques.", "motivation": "FHE allows computations on encrypted data, which is crucial for privacy, but the reliability, particularly fault tolerance, has not been thoroughly studied.", "method": "The study analyzes the impact of memory faults on FHE computation at both the operation level and application level, comparing different FHE schemes.", "result": "The paper presents findings on the behavior of FHE under faults and assesses mitigation techniques, though specific results are not detailed in the abstract.", "conclusion": "Understanding fault tolerance in FHE systems is essential, and the paper highlights the need for reliable fault mitigation strategies."}}
{"id": "2509.20837", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20837", "abs": "https://arxiv.org/abs/2509.20837", "authors": ["Srishti Gureja", "Elena Tommasone", "Jingyi He", "Sara Hooker", "Matthias Gall\u00e9", "Marzieh Fadaee"], "title": "Verification Limits Code LLM Training", "comment": null, "summary": "Large language models for code generation increasingly rely on synthetic\ndata, where both problem solutions and verification tests are generated by\nmodels. While this enables scalable data creation, it introduces a previously\nunexplored bottleneck: the verification ceiling, in which the quality and\ndiversity of training data are fundamentally constrained by the capabilities of\nsynthetic verifiers. In this work, we systematically study how verification\ndesign and strategies influence model performance. We investigate (i) what we\nverify by analyzing the impact of test complexity and quantity: richer test\nsuites improve code generation capabilities (on average +3 pass@1), while\nquantity alone yields diminishing returns, (ii) how we verify by exploring\nrelaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By\nallowing for relaxed thresholds or incorporating LLM-based soft verification,\nwe can recover valuable training data, leading to a 2-4 point improvement in\npass@1 performance. However, this benefit is contingent upon the strength and\ndiversity of the test cases used, and (iii) why verification remains necessary\nthrough controlled comparisons of formally correct versus incorrect solutions\nand human evaluation: retaining diverse correct solutions per problem yields\nconsistent generalization gains. Our results show that Verification as\ncurrently practiced is too rigid, filtering out valuable diversity. But it\ncannot be discarded, only recalibrated. By combining calibrated verification\nwith diverse, challenging problem-solution pairs, we outline a path to break\nthe verification ceiling and unlock stronger code generation models.", "AI": {"tldr": "This paper examines the 'verification ceiling' limitation in code generation models using synthetic data, proposing that relaxing verification thresholds and enhancing test diversity can improve model performance while retaining verification's necessity.", "motivation": "Synthetic data for code generation is constrained by verification ceiling issues where rigid verification standards limit training data quality and diversity, hindering model performance.", "method": "The study analyzes three verification aspects: test suite complexity/quantity, relaxed pass thresholds (e.g., LLM-based soft verification), and the importance of retaining correct solutions through formal correctness comparisons and human evaluation.", "result": "Richer test suites improved pass@1 by 3 points, relaxed thresholds with high-quality tests yielded 2-4 point gains, and problem-solution diversity enhanced generalization. Overly strict verification risks discarding valuable diverse data.", "conclusion": "Verification must be recalibrated\u2014not eliminated\u2014to balance quality and diversity. Combining calibrated verification with challenging problem-solution pairs is proposed to break the verification ceiling and advance code generation models."}}
{"id": "2509.20714", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20714", "abs": "https://arxiv.org/abs/2509.20714", "authors": ["Anh Tu Ngo", "Anupam Chattopadhyay", "Subhamoy Maitra"], "title": "Cryptographic Backdoor for Neural Networks: Boon and Bane", "comment": "Preprint", "summary": "In this paper we show that cryptographic backdoors in a neural network (NN)\ncan be highly effective in two directions, namely mounting the attacks as well\nas in presenting the defenses as well. On the attack side, a carefully planted\ncryptographic backdoor enables powerful and invisible attack on the NN.\nConsidering the defense, we present applications: first, a provably robust NN\nwatermarking scheme; second, a protocol for guaranteeing user authentication;\nand third, a protocol for tracking unauthorized sharing of the NN intellectual\nproperty (IP). From a broader theoretical perspective, borrowing the ideas from\nGoldwasser et. al. [FOCS 2022], our main contribution is to show that all these\ninstantiated practical protocol implementations are provably robust. The\nprotocols for watermarking, authentication and IP tracking resist an adversary\nwith black-box access to the NN, whereas the backdoor-enabled adversarial\nattack is impossible to prevent under the standard assumptions. While the\ntheoretical tools used for our attack is mostly in line with the Goldwasser et.\nal. ideas, the proofs related to the defense need further studies. Finally, all\nthese protocols are implemented on state-of-the-art NN architectures with\nempirical results corroborating the theoretical claims. Further, one can\nutilize post-quantum primitives for implementing the cryptographic backdoors,\nlaying out foundations for quantum-era applications in machine learning (ML).", "AI": {"tldr": "This paper explores the use of cryptographic backdoors in neural networks (NNs) for both attacking and defending against adversaries. It presents three practical protocols for watermarking, user authentication, and IP tracking, all with provable robustness under black-box assumptions. Empirical results are provided, and post-quantum applications are suggested.", "motivation": "The paper is motivated by the need to address vulnerabilities in NNs through cryptographic backdoors that can be used for attacks and defenses, leading to robust and secure ML systems, particularly for future quantum-era applications.", "method": "The authors utilize cryptographic principles, inspired by Goldwasser et. al. [FOCS 2022], to design a backdoor that is integrated within a NN. These backdoors enable adversarial attacks. For defense, three protocols are proposed (watermarking, authentication, and tracking of unauthorized IP sharing), and the paper contributes theoretical analysis of their robustness against black-box adversaries.", "result": "The paper successfully demonstrates the implementation of backdoor-based adversarial attack and defense protocols on state-of-the-art NNs. Theoretical robustness is shown for all three protocols, and practical results confirm these claims.", "conclusion": "The study concludes that cryptographic backdoors in NNs are versatile and effective for conducting attacks and enhancing security through watermarking authentication and IP tracking, all with theoretical guarantees. It opens up potential for applying post-quantum cryptography in ML going forward."}}
{"id": "2509.20881", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20881", "abs": "https://arxiv.org/abs/2509.20881", "authors": ["Yixuan Li", "Xinyi Liu", "Weidong Yang", "Ben Fei", "Shuhao Li", "Mingjie Zhou", "Lipeng Ma"], "title": "PseudoBridge: Pseudo Code as the Bridge for Better Semantic and Logic Alignment in Code Retrieval", "comment": null, "summary": "Code search aims to precisely find relevant code snippets that match natural\nlanguage queries within massive codebases, playing a vital role in software\ndevelopment. Recent advances leverage pre-trained language models (PLMs) to\nbridge the semantic gap between unstructured natural language (NL) and\nstructured programming languages (PL), yielding significant improvements over\ntraditional information retrieval and early deep learning approaches. However,\nexisting PLM-based methods still encounter key challenges, including a\nfundamental semantic gap between human intent and machine execution logic, as\nwell as limited robustness to diverse code styles. To address these issues, we\npropose PseudoBridge, a novel code retrieval framework that introduces\npseudo-code as an intermediate, semi-structured modality to better align NL\nsemantics with PL logic. Specifically, PseudoBridge consists of two stages.\nFirst, we employ an advanced large language model (LLM) to synthesize\npseudo-code, enabling explicit alignment between NL queries and pseudo-code.\nSecond, we introduce a logic-invariant code style augmentation strategy and\nemploy the LLM to generate stylistically diverse yet logically equivalent code\nimplementations with pseudo-code, then align the code snippets of different\nstyles with pseudo-code, enhancing model robustness to code style variation. We\nbuild PseudoBridge across 10 different PLMs and evaluate it on 6 mainstream\nprogramming languages. Extensive experiments demonstrate that PseudoBridge\nconsistently outperforms baselines, achieving significant gains in retrieval\naccuracy and generalization, particularly under zero-shot domain transfer\nscenarios such as Solidity and XLCoST datasets. These results demonstrate the\neffectiveness of explicit logical alignment via pseudo-code and highlight\nPseudoBridge's potential as a robust, generalizable solution for code\nretrieval.", "AI": {"tldr": "This paper introduces PseudoBridge, a code retrieval framework utilizing pseudo-code as an intermediate modality to bridge the semantic gap between natural language and programming languages, enhancing PLM-based methods through explicit alignment and code style augmentation.", "motivation": "Existing PLM-based code search methods face a fundamental semantic gap between human intent and machine logic, as well as poor robustness to diverse coding styles, motivating the need for improved alignment and generalization strategies.", "method": "PseudoBridge employs a two-stage approach: (1) uses an LLM to synthesize pseudo-code for explicit NL-PL alignment, and (2) applies a logic-invariant code style augmentation technique to generate stylistically diverse yet equivalent code implementations aligned with pseudo-code, enhancing style robustness.", "result": "PseudoBridge outperforms baseline methods across 6 programming languages, achieving significant improvements in retrieval accuracy and zero-shot domain transfer scenarios (e.g., Solidity, XLCoST), demonstrating robustness and generalization.", "conclusion": "The pseudo-code-mediated semantic alignment and style augmentation strategy in PseudoBridge effectively addresses key challenges in code search, offering a generalizable solution with potential for scalable, domain-agnostic code retrieval."}}
{"id": "2509.20767", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20767", "abs": "https://arxiv.org/abs/2509.20767", "authors": ["Ayush Kumar", "Kar Wai Fok", "Vrizlynn L. L. Thing"], "title": "ExpIDS: A Drift-adaptable Network Intrusion Detection System With Improved Explainability", "comment": null, "summary": "Despite all the advantages associated with Network Intrusion Detection\nSystems (NIDSs) that utilize machine learning (ML) models, there is a\nsignificant reluctance among cyber security experts to implement these models\nin real-world production settings. This is primarily because of their opaque\nnature, meaning it is unclear how and why the models make their decisions. In\nthis work, we design a deep learning-based NIDS, ExpIDS to have high decision\ntree explanation fidelity, i.e., the predictions of decision tree explanation\ncorresponding to ExpIDS should be as close to ExpIDS's predictions as possible.\nExpIDS can also adapt to changes in network traffic distribution (drift). With\nthe help of extensive experiments, we verify that ExpIDS achieves higher\ndecision tree explanation fidelity and a malicious traffic detection\nperformance comparable to state-of-the-art NIDSs for common attacks with\nvarying levels of real-world drift.", "AI": {"tldr": "ExpIDS is a deep learning-based NIDS designed for high decision tree explanation fidelity and adaptability to network traffic drift, achieving performance comparable to state-of-the-art models.", "motivation": "Cyber security experts are hesitant to adopt ML-driven NIDS due to their 'black box' nature; explaining model decisions is critical for real-world deployment.", "method": "ExpIDS uses deep learning to optimize decision tree explanation fidelity (aligning predictions with interpretable decision trees), incorporates mechanisms to adapt to traffic distribution drift, and employs extensive experiments for validation.", "result": "ExpIDS achieves higher decision tree explanation fidelity than alternatives and matches state-of-the-art NIDS in detecting common attacks, even under realistic network traffic shifts.", "conclusion": "ExpIDS bridges the gap between ML effectiveness and explainability in NIDS, demonstrating that high-performance, transparent systems can be both interpretable and robust to real-world conditions."}}
{"id": "2509.21067", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.21067", "abs": "https://arxiv.org/abs/2509.21067", "authors": ["Oka Kurniawan", "Erick Chandra", "Christopher M. Poskitt", "Yannic Noller", "Kenny Tsu Wei Choo", "Cyrille Jegourel"], "title": "Designing for Novice Debuggers: A Pilot Study on an AI-Assisted Debugging Tool", "comment": "Accepted by the 25th Koli Calling International Conference on\n  Computing Education Research (Koli Calling 2025)", "summary": "Debugging is a fundamental skill that novice programmers must develop.\nNumerous tools have been created to assist novice programmers in this process.\nRecently, large language models (LLMs) have been integrated with automated\nprogram repair techniques to generate fixes for students' buggy code. However,\nmany of these tools foster an over-reliance on AI and do not actively engage\nstudents in the debugging process. In this work, we aim to design an intuitive\ndebugging assistant, CodeHinter, that combines traditional debugging tools with\nLLM-based techniques to help novice debuggers fix semantic errors while\npromoting active engagement in the debugging process. We present findings from\nour second design iteration, which we tested with a group of undergraduate\nstudents. Our results indicate that the students found the tool highly\neffective in resolving semantic errors and significantly easier to use than the\nfirst version. Consistent with our previous study, error localization was the\nmost valuable feature. Finally, we conclude that any AI-assisted debugging tool\nshould be personalized based on user profiles to optimize their interactions\nwith students.", "AI": {"tldr": "This paper introduces CodeHinter, an AI debugging assistant designed to help novice programmers fix semantic errors while promoting active engagement. Through a user study, the authors demonstrate improved usability and effectiveness compared to previous versions, emphasizing the value of error localization and personalization in AI-assisted debugging tools.", "motivation": "Existing AI debugging tools often lead to over-reliance on automation, lacking engagement. Novice programmers need tools that balance guidance with active participation to develop debugging skills.", "method": "The authors designed CodeHinter by integrating traditional debugging tools with LLM-based techniques. They conducted a second design iteration and tested it with undergraduate students through a user study to evaluate usability and effectiveness in resolving semantic errors.", "result": "Students found CodeHinter highly effective for semantic error resolution and significantly easier to use than the first version. Error localization was identified as the most valuable feature, consistent with prior findings.", "conclusion": "AI-assisted debugging tools must be personalized based on user profiles to optimize interactions and promote active learning. CodeHinter\u2019s success highlights the importance of balancing automation with student engagement."}}
{"id": "2509.20796", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20796", "abs": "https://arxiv.org/abs/2509.20796", "authors": ["Yongjiao Li", "Liang Zhu", "Yalin Deng", "Qikun Zhang", "Zhenlei Wang", "Zhu Cao"], "title": "Fast Revocable Attribute-Based Encryption with Data Integrity for Internet of Things", "comment": "16 pages, 7 figures", "summary": "Efficient and secure revocable attribute-based encryption (RABE) is vital for\nensuring flexible and fine-grained access control and data sharing in cloud\nstorage and outsourced data environments within the Internet of Things (IoT).\nHowever, current RABE schemes often struggle to achieve an optimal balance\nbetween efficiency, security, dynamic scalability, and other important\nfeatures, which hampers their practical application. To overcome these\nlimitations, we propose a fast RABE scheme with data integrity for IoT that\nachieves adaptive security with multiple challenge ciphertexts. Our scheme\nsupports the revocation of authorized users and transfers the computationally\nheavy revocation processes to the cloud, thereby easing the computational\nburden on IoT devices. Moreover, it consistently guarantees the integrity and\ncorrectness of data. We have demonstrated its adaptive security within the\ndefined security model with multiple challenge ciphertexts and optimized its\nperformance. Experimental results indicate that our scheme provides better\nperformance than existing solutions. Under the same access policy, our scheme\nreduces computational consumption by 7 to 9 times compared to previous schemes.", "AI": {"tldr": "This paper proposes an efficient and secure revocable attribute-based encryption (RABE), designed for IoT environments, emphasizing adaptive security with multiple challenge ciphertexts, supports user revocation, and ensures data integrity by offloading heavy computations to the cloud.", "motivation": "Current RABE schemes struggle with balancing efficiency, security, scalability, and data integrity, limiting their practical use in data-sharing scenarios in IoT.', 'The computational constraints of IoT devices further exacerbate this challenge.", "method": "The scheme transfers computationally intensive revocation processes to the cloud, supports user revocation, and integrates data integrity assurance. Adaptive security is achieved under a defined security model involving multiple challenge ciphertexts.", "result": "The proposed scheme demonstrates 7\u20139\u00d7 reduction in computational consumption compared to existing solutions, while maintaining adaptive security and data integrity. Experimental validation confirms its superior performance and optimization.", "conclusion": "The proposed RABE scheme effectively addresses efficiency, scalability, and security limitations in IoT environments, offering a practical solution for secure data sharing with strong revocation mechanisms and reduced device computational load."}}
{"id": "2509.21068", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21068", "abs": "https://arxiv.org/abs/2509.21068", "authors": ["Nek Dil Khan", "Javed Ali Khan", "Mobashir Husain", "Muhammad Sohail Khan", "Arif Ali Khan", "Muhammad Azeem Akbar", "Shahid Hussain"], "title": "An Improved Quantum Software Challenges Classification Approach using Transfer Learning and Explainable AI", "comment": null, "summary": "Quantum Software Engineering (QSE) is a research area practiced by tech\nfirms. Quantum developers face challenges in optimizing quantum computing and\nQSE concepts. They use Stack Overflow (SO) to discuss challenges and label\nposts with specialized quantum tags, which often refer to technical aspects\nrather than developer posts. Categorizing questions based on quantum concepts\ncan help identify frequent QSE challenges. We conducted studies to classify\nquestions into various challenges. We extracted 2829 questions from Q&A\nplatforms using quantum-related tags. Posts were analyzed to identify frequent\nchallenges and develop a novel grounded theory. Challenges include Tooling,\nTheoretical, Learning, Conceptual, Errors, and API Usage. Through content\nanalysis and grounded theory, discussions were annotated with common challenges\nto develop a ground truth dataset. ChatGPT validated human annotations and\nresolved disagreements. Fine-tuned transformer algorithms, including BERT,\nDistilBERT, and RoBERTa, classified discussions into common challenges. We\nachieved an average accuracy of 95% with BERT DistilBERT, compared to\nfine-tuned Deep and Machine Learning (D&ML) classifiers, including Feedforward\nNeural Networks (FNN), Convolutional Neural Networks (CNN), and Long Short-Term\nMemory networks (LSTM), which achieved accuracies of 89%, 86%, and 84%,\nrespectively. The Transformer-based approach outperforms the D&ML-based\napproach with a 6\\% increase in accuracy by processing actual discussions,\ni.e., without data augmentation. We applied SHAP (SHapley Additive\nexPlanations) for model interpretability, revealing how linguistic features\ndrive predictions and enhancing transparency in classification. These findings\ncan help quantum vendors and forums better organize discussions for improved\naccess and readability. However,empirical evaluation studies with actual\ndevelopers and vendors are needed.", "AI": {"tldr": "This paper studies quantum software engineering (QSE)-related questions on Stack Overflow, categorizing them into challenges (Tooling, Theoretical, etc.) and comparing transformer-based (BERT, RoBERTa, 95% accuracy) vs. traditional ML (FNN, LSTM, 84-89%) classification methods. SHAP analysis enhances model transparency.", "motivation": "Quantum developers face optimization challenges, often documented via SO posts with quantum tags. Existing tags lack specificity for QSE issues, necessitating systematic categorization to improve forums and vendor tools.", "method": "Collected 2829 quantum-tagged questions from Q&As. Applied grounded theory/content analysis for dataset creation. Used ChatGPT to resolve annotation disagreements. Trained/trained bert_distilbert, Feedforward Neural Networks (FNN), CNN, LSTM models and evaluated SHAP-based interpretability.", "result": "Transformer models achieved 95% accuracy, surpassing D/ML models by 6-9%. SHAP analysis clarified linguistic feature impacts on classification. No data augmentation required for transformers, unlike D&ML methods.", "conclusion": "Transfomer algorithms offer superior QSE challenge classification with better accuracy and interpretability. Results support forum organization but require validation via empirical studies with quantum developers/vendors."}}
{"id": "2509.20808", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20808", "abs": "https://arxiv.org/abs/2509.20808", "authors": ["Raghul Saravanan", "Sudipta Paria", "Aritra Dasgupta", "Swarup Bhunia", "Sai Manoj P D"], "title": "Intelligent Graybox Fuzzing via ATPG-Guided Seed Generation and Submodule Analysis", "comment": "7 pages, 6 figures, 4 tables", "summary": "Hardware Fuzzing emerged as one of the crucial techniques for finding\nsecurity flaws in modern hardware designs by testing a wide range of input\nscenarios. One of the main challenges is creating high-quality input seeds that\nmaximize coverage and speed up verification. Coverage-Guided Fuzzing (CGF)\nmethods help explore designs more effectively, but they struggle to focus on\nspecific parts of the hardware. Existing Directed Gray-box Fuzzing (DGF)\ntechniques like DirectFuzz try to solve this by generating targeted tests, but\nit has major drawbacks, such as supporting only limited hardware description\nlanguages, not scaling well to large circuits, and having issues with\nabstraction mismatches. To address these problems, we introduce a novel\nframework, PROFUZZ, that follows the DGF approach and combines fuzzing with\nAutomatic Test Pattern Generation (ATPG) for more efficient fuzzing. By\nleveraging ATPG's structural analysis capabilities, PROFUZZ can generate\nprecise input seeds that target specific design regions more effectively while\nmaintaining high fuzzing throughput. Our experiments show that PROFUZZ scales\n30x better than DirectFuzz when handling multiple target sites, improves\ncoverage by 11.66%, and runs 2.76x faster, highlighting its scalability and\neffectiveness for directed fuzzing in complex hardware systems.", "AI": {"tldr": "This paper introduces PROFUZZ, a scalable directed fuzzing framework that combines ATPG and fuzzing to overcome limitations in existing hardware fuzzing techniques.", "motivation": "Current Directed Gray-box Fuzzing tools like DirectFuzz suffer from limited language support, poor scalability, abstraction mismatches, and inefficiencies in targeted test generation for hardware designs.", "method": "PROFUZZ integrates Automatic Test Pattern Generation (ATPG) with Directed Fuzzing to structurally analyze hardware designs and generate precise input seeds that target specific regions while maintaining high throughput.", "result": "PROFUZZ achieves 30\u00d7 better scalability for multiple targets, 11.66% coverage improvement, and 2.76\u00d7 faster execution compared to DirectFuzz on complex hardware systems.", "conclusion": "PROFUZZ effectively addresses existing DGF limitations through ATPG-based structural analysis, demonstrating superior scalability and efficiency in directed hardware fuzzing."}}
{"id": "2509.21170", "categories": ["cs.SE", "cs.AI", "D.2.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.21170", "abs": "https://arxiv.org/abs/2509.21170", "authors": ["Yongda Yu", "Guohao Shi", "Xianwei Wu", "Haochuan He", "XueMing Gu", "Qianqian Zhao", "Kui Liu", "Qiushi Wang", "Zhao Tian", "Haifeng Shen", "Guoping Rong"], "title": "Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach", "comment": "22 pages", "summary": "Large Language Models (LLMs) have shown great potential in supporting\nautomated code review due to their impressive capabilities in context\nunderstanding and reasoning. However, these capabilities are still limited\ncompared to human-level cognition because they are heavily influenced by the\ntraining data. Recent research has demonstrated significantly improved\nperformance through fine-tuning LLMs with code review data. However, compared\nto human reviewers who often simultaneously analyze multiple dimensions of code\nreview to better identify issues, the full potential of these methods is\nhampered by the limited or vague information used to fine-tune the models. This\npaper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that\ntrains LLMs with an impressive reasoning ability to analyze multiple dimensions\nof code review by harnessing long COT techniques to provide rich structured\ninformation. To address context loss and reasoning logic loss issues that\nfrequently occur when LLMs process long COT prompts, we propose a solution that\ncombines the Maximum Entropy (ME) modeling principle with pre-defined reasoning\npathways in MelcotCR to enable more effective utilization of in-context\nknowledge within long COT prompts while strengthening the logical tightness of\nthe reasoning process. Empirical evaluations on our curated MelcotCR dataset\nand the public CodeReviewer dataset reveal that a low-parameter base model,\nsuch as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art\nmethods in terms of the accuracy of detecting and describing code issues, with\nits performance remarkably on par with that of the 671B DeepSeek-R1 model.", "AI": {"tldr": "MelcotCR is a novel fine-tuning approach for Large Language Models (LLMs) that enhances their reasoning capabilities for multi-dimensional code review analysis. By combining the Maximum Entropy (ME) modeling principle with chain-of-thought (COT) techniques, it mitigates context and reasoning logic loss in long COT prompts, allowing a 14B base model to outperform state-of-the-art methods and rival a 671B model in accuracy for detecting and describing code issues.", "motivation": "Despite the potential of LLMs in code review, their performance is constrained by the limited or vague information used during fine-tuning, which doesn't match the multi-dimensional analysis of human reviewers. Additionally, LLMs suffer from context loss and reasoning logic loss when handling long COT prompts, limiting their effectiveness in code issue detection and description.", "method": "MelcotCR introduces a fine-tuning method that uses chain-of-thought (COT) techniques to train LLMs with rich structured information, considering multiple dimensions of code review. It combines the Maximum Entropy (ME) principle with pre-defined reasoning pathways to enhance in-context knowledge utilization and strengthen the logical coherence in the reasoning process. This addresses the limitations of long COT prompts in LLMs, improving their capacity for comprehensive code analysis.", "result": "Empirical results show that MelcotCR allows a low-parameter base model (14B Qwen2.5) to surpass current leading methods in code issue detection and description accuracy, performing comparably to a high-capacity model (671B DeepSeek-R1) on both the MelcotCR dataset and the public CodeReviewer dataset.", "conclusion": "This work demonstrates that MelcotCR significantly improves the performance of LLMs in code review by addressing context and reasoning loss challenges in long COT prompts, enabling a 14B model to outperform state-of-the-art methods and match the performance of a 671B model. This offers a promising approach for enhancing code review with less resource-intensive models while maintaining high accuracy levels."}}
{"id": "2509.20835", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20835", "abs": "https://arxiv.org/abs/2509.20835", "authors": ["Yu Liu", "Boxiang He", "Fanggang Wang"], "title": "Security-aware Semantic-driven ISAC via Paired Adversarial Residual Networks", "comment": null, "summary": "This paper proposes a novel and flexible security-aware semantic-driven\nintegrated sensing and communication (ISAC) framework, namely security semantic\nISAC (SS-ISAC). Inspired by the positive impact of the adversarial attack, a\npair of pluggable encryption and decryption modules is designed in the proposed\nSS-ISAC framework. The encryption module is installed after the semantic\ntransmitter, adopting a trainable adversarial residual network (ARN) to create\nthe adversarial attack. Correspondingly, the decryption module before the\nsemantic receiver utilizes another trainable ARN to mitigate the adversarial\nattack and noise. These two modules can be flexibly assembled considering the\nsystem security demands, without drastically modifying the hardware\ninfrastructure. To ensure the sensing and communication (SAC) performance while\npreventing the eavesdropping threat, the above ARNs are jointly optimized by\nminimizing a carefully designed loss function that relates to the adversarial\nattack power, SAC performance, as well as the privacy leakage risk. Simulation\nresults validate the effectiveness of the proposed SS-ISAC framework in terms\nof both SAC and eavesdropping prevention performance.", "AI": {"tldr": "The paper introduces SS-ISAC, a security-aware semantic-driven ISAC framework that integrates adversarial residual networks (ARNs) for encryption/decryption to balance SAC performance and eavesdropping prevention without hardware overhauls.", "motivation": "Existing ISAC systems lack robust security against eavesdropping and adversarial attacks. The authors aim to address both communication quality and security demands by leveraging adversarial mechanisms from machine learning.", "method": "1. Implements a dual-ARN architecture with encryption (post-semantic transmitter) and decryption (pre-semantic receiver) modules. 2. Optimizes ARNs using a custom loss function that jointly minimizes adversarial attack power, SAC performance degradation, and privacy leakage risks.", "result": "Simulation results demonstrate SS-ISAC achieves superior SAC performance and eavesdropping suppression compared to baseline methods, validating the effectiveness of the adversarial network design and joint optimization strategy.", "conclusion": "SS-ISAC provides a flexible, hardware-efficient solution for secure ISAC systems by transforming adversarial attacks into features for security enhancement while maintaining communication/sensing performance."}}
{"id": "2509.21292", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21292", "abs": "https://arxiv.org/abs/2509.21292", "authors": ["Ronivaldo Ferreira", "Guilherme da Silva", "Carla Rocha", "Gustavo Pinto"], "title": "Semantic Clustering of Civic Proposals: A Case Study on Brazil's National Participation Platform", "comment": "12 pages, in Portuguese language", "summary": "Promoting participation on digital platforms such as Brasil Participativo has\nemerged as a top priority for governments worldwide. However, due to the sheer\nvolume of contributions, much of this engagement goes underutilized, as\norganizing it presents significant challenges: (1) manual classification is\nunfeasible at scale; (2) expert involvement is required; and (3) alignment with\nofficial taxonomies is necessary. In this paper, we introduce an approach that\ncombines BERTopic with seed words and automatic validation by large language\nmodels. Initial results indicate that the generated topics are coherent and\ninstitutionally aligned, with minimal human effort. This methodology enables\ngovernments to transform large volumes of citizen input into actionable data\nfor public policy.", "AI": {"tldr": "This paper presents a scalable method combining BERTopic, seed words, and LLM validation to organize large volumes of citizen input on digital governance platforms with minimal human effort.", "motivation": "Governments face challenges organizing massive citizen contributions on platforms like Brasil Participativo due to scale, reliance on expert curation, and alignment with official taxonomies.", "method": "The approach integrates BERTopic for topic modeling, leverages seed words to guide topic coherence, and employs large language models for automatic validation against institutional requirements.", "result": "Initial results demonstrate the methodology produces coherent, institutionally aligned topics that significantly reduce manual effort required for classifying citizen input.", "conclusion": "This approach enables governments to efficiently transform unstructured citizen engagement into actionable policy data at scale through automated taxonomy alignment."}}
{"id": "2509.20861", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20861", "abs": "https://arxiv.org/abs/2509.20861", "authors": ["Chao Zha", "Haolin Pan", "Bing Bai", "Jiangxing Wu", "Ruyun Zhang"], "title": "FlowXpert: Context-Aware Flow Embedding for Enhanced Traffic Detection in IoT Network", "comment": null, "summary": "In the Internet of Things (IoT) environment, continuous interaction among a\nlarge number of devices generates complex and dynamic network traffic, which\nposes significant challenges to rule-based detection approaches. Machine\nlearning (ML)-based traffic detection technology, capable of identifying\nanomalous patterns and potential threats within this traffic, serves as a\ncritical component in ensuring network security. This study first identifies a\nsignificant issue with widely adopted feature extraction tools (e.g.,\nCICMeterFlow): the extensive use of time- and length-related features leads to\nhigh sparsity, which adversely affects model convergence. Furthermore, existing\ntraffic detection methods generally lack an embedding mechanism capable of\nefficiently and comprehensively capturing the semantic characteristics of\nnetwork traffic. To address these challenges, we propose a novel feature\nextraction tool that eliminates traditional time and length features in favor\nof context-aware semantic features related to the source host, thus improving\nthe generalizability of the model. In addition, we design an embedding training\nframework that integrates the unsupervised DBSCAN clustering algorithm with a\ncontrastive learning strategy to effectively capture fine-grained semantic\nrepresentations of traffic. Extensive empirical evaluations are conducted on\nthe real-world Mawi data set to validate the proposed method in terms of\ndetection accuracy, robustness, and generalization. Comparative experiments\nagainst several state-of-the-art (SOTA) models demonstrate the superior\nperformance of our approach. Furthermore, we confirm its applicability and\ndeployability in real-time scenarios.", "AI": {"tldr": "This paper addresses the limitations of ML-based network traffic detection in IoT by proposing a feature extraction tool that eliminates time and length features for semantic, context-aware features. It introduces an embedding framework combining DBSCAN clustering and contrastive learning to enhance model performance, validated on the Mawi dataset with promising results.", "motivation": "Continuous interaction among IoT devices generates complex and dynamic network traffic, making rule-based detection difficult. Current ML-based methods rely on tools like CICMeterFlow, which introduce high sparsity due to time- and length-related features and lack mechanisms to fully capture traffic semantics.", "method": "The paper presents a novel feature extraction method that replaces time and length features with semantic features related to the source host. It also develops an embedding training framework combining unsupervised DBSCAN clustering with a contrastive learning strategy to capture fine-grained semantic representations of traffic.", "result": "Extensive experiments on the Mawi dataset show improved detection accuracy, robustness, and generalization compared to state-of-the-art models. The approach also demonstrates applicability in real-time scenarios.", "conclusion": "The paper concludes that its proposed method effectively addresses feature sparsity and semantic representation issues in IoT traffic detection, offering significant improvements in model performance and practical deployment."}}
{"id": "2509.20880", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.20880", "abs": "https://arxiv.org/abs/2509.20880", "authors": ["Cheng Lyu", "Mu Yuan", "Dabin Zheng", "Siwei Sun", "Shun Li"], "title": "A Generalized $\u03c7_n$-Function", "comment": null, "summary": "The mapping $\\chi_n$ from $\\F_{2}^{n}$ to itself defined by $y=\\chi_n(x)$\nwith $y_i=x_i+x_{i+2}(1+x_{i+1})$, where the indices are computed modulo $n$,\nhas been widely studied for its applications in lightweight cryptography.\nHowever, $\\chi_n $ is bijective on $\\F_2^n$ only when $n$ is odd, restricting\nits use to odd-dimensional vector spaces over $\\F_2$. To address this\nlimitation, we introduce and analyze the generalized mapping $\\chi_{n, m}$\ndefined by $y=\\chi_{n,m}(x)$ with $y_i=x_i+x_{i+m} (x_{i+m-1}+1)(x_{i+m-2}+1)\n\\cdots (x_{i+1}+1)$, where $m$ is a fixed integer with $m\\nmid n$. To\ninvestigate such mappings, we further generalize $\\chi_{n,m}$ to $\\theta_{m,\nk}$, where $\\theta_{m, k}$ is given by $y_i=x_{i+mk} \\prod_{\\substack{j=1,\\,\\,\nm \\nmid j}}^{mk-1} \\left(x_{i+j}+1\\right), \\,\\,{\\rm for }\\,\\, i\\in\n\\{0,1,\\ldots,n-1\\}$. We prove that these mappings generate an abelian group\nisomorphic to the group of units in $\\F_2[z]/(z^{\\lfloor n/m\\rfloor +1})$. This\nstructural insight enables us to construct a broad class of permutations over\n$\\F_2^n$ for any positive integer $n$, along with their inverses. We rigorously\nanalyze algebraic properties of these mappings, including their iterations,\nfixed points, and cycle structures. Additionally, we provide a comprehensive\ndatabase of the cryptographic properties for iterates of $\\chi_{n,m}$ for small\nvalues of $n$ and $m$. Finally, we conduct a comparative security and\nimplementation cost analysis among $\\chi_{n,m}$, $\\chi_n$, $\\chi\\chi_n$\n(EUROCRYPT 2025 \\cite{belkheyar2025chi}) and their variants, and prove\nConjecture~1 proposed in~\\cite{belkheyar2025chi} as a by-product of our study.\nOur results lead to generalizations of $\\chi_n$, providing alternatives to\n$\\chi_n$ and $\\chi\\chi_n$.", "AI": {"tldr": "Generalizes \u03c7_n mapping for even n via \u03c7_{n,m} and \u03b8_{m,k}, establishes algebraic structures, proves bijections, analyzes security properties, and resolves prior conjectures.", "motivation": "Original \u03c7_n is non-bijective for even n, limiting cryptography use; prior work lacks structure-based permutation constructions for even dimensions.", "method": "Introduces generalized mappings \u03c7_{n,m} and \u03b8_{m,k} with modular indices, proves abelian group isomorphism to F\u2082[z]/(z^\u230an/m\u230b+1) units, analyzes iterations/fixed points, and provides cryptographic benchmarks.", "result": "Constructs permutations for all n \u2265 1 with explicit inverse mappings, proves group-theoretic properties, confirms security advantages over \u03c7_n and \u03c7\u03c7_n via implementation analysis.", "conclusion": "Solves the parity constraint of \u03c7_n, provides a comprehensive framework for lightweight cryptographic permutations with verified algebraic and security properties."}}
{"id": "2509.21011", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21011", "abs": "https://arxiv.org/abs/2509.21011", "authors": ["Ping He", "Changjiang Li", "Binbin Zhao", "Tianyu Du", "Shouling Ji"], "title": "Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools", "comment": null, "summary": "The remarkable capability of large language models (LLMs) has led to the wide\napplication of LLM-based agents in various domains. To standardize interactions\nbetween LLM-based agents and their environments, model context protocol (MCP)\ntools have become the de facto standard and are now widely integrated into\nthese agents. However, the incorporation of MCP tools introduces the risk of\ntool poisoning attacks, which can manipulate the behavior of LLM-based agents.\nAlthough previous studies have identified such vulnerabilities, their red\nteaming approaches have largely remained at the proof-of-concept stage, leaving\nthe automatic and systematic red teaming of LLM-based agents under the MCP tool\npoisoning paradigm an open question. To bridge this gap, we propose\nAutoMalTool, an automated red teaming framework for LLM-based agents by\ngenerating malicious MCP tools. Our extensive evaluation shows that AutoMalTool\neffectively generates malicious MCP tools capable of manipulating the behavior\nof mainstream LLM-based agents while evading current detection mechanisms,\nthereby revealing new security risks in these agents.", "AI": {"tldr": "The paper introduces AutoMalTool, an automated red teaming framework that generates malicious MCP tools to exploit vulnerabilities in LLM-based agents, demonstrating their susceptibility to tool poisoning attacks while evading detection.", "motivation": "Existing red teaming approaches for LLM agent vulnerabilities remain at the proof-of-concept stage, lacking systematic methods to identify security risks under MCP tool poisoning paradigms.", "method": "Proposes AutoMalTool, an automated framework that generates malicious MCP tools through systematic attack pattern synthesis to test and manipulate mainstream LLM-based agents.", "result": "AutoMalTool successfully generates undetectable malicious MCP tools that manipulate agent behavior, uncovering new security risks in widely adopted LLM-based agent systems.", "conclusion": "Highlights critical security gaps in LLM-based agent systems under MCP tool integration, necessitating advanced defense mechanisms against automated tool poisoning attacks."}}
{"id": "2509.20924", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20924", "abs": "https://arxiv.org/abs/2509.20924", "authors": ["Hanbo Huang", "Yiran Zhang", "Hao Zheng", "Xuan Gong", "Yihan Li", "Lin Liu", "Shiyu Liang"], "title": "RLCracker: Exposing the Vulnerability of LLM Watermarks with Adaptive RL Attacks", "comment": null, "summary": "Large Language Models (LLMs) watermarking has shown promise in detecting\nAI-generated content and mitigating misuse, with prior work claiming robustness\nagainst paraphrasing and text editing. In this paper, we argue that existing\nevaluations are not sufficiently adversarial, obscuring critical\nvulnerabilities and overstating the security. To address this, we introduce\nadaptive robustness radius, a formal metric that quantifies watermark\nresilience against adaptive adversaries. We theoretically prove that optimizing\nthe attack context and model parameters can substantially reduce this radius,\nmaking watermarks highly susceptible to paraphrase attacks. Leveraging this\ninsight, we propose RLCracker, a reinforcement learning (RL)-based adaptive\nattack that erases watermarks while preserving semantic fidelity. RLCracker\nrequires only limited watermarked examples and zero access to the detector.\nDespite weak supervision, it empowers a 3B model to achieve 98.5% removal\nsuccess and an average 0.92 P-SP score on 1,500-token Unigram-marked texts\nafter training on only 100 short samples. This performance dramatically exceeds\n6.75% by GPT-4o and generalizes across five model sizes over ten watermarking\nschemes. Our results confirm that adaptive attacks are broadly effective and\npose a fundamental threat to current watermarking defenses.", "AI": {"tldr": "This paper reveals critical vulnerabilities in LLM watermarking defenses by introducing adaptive robustness radius as a formal metric. It proposes RLCracker, an RL-based paraphrase attack achieving 98.58% watermark removal success with minimal samples, demonstrating severe threats to existing AI watermarking security.", "motivation": "Existing watermarking security claims are overstated due to insufficiently adversarial evaluations, creating a false sense of security while critical vulnerabilities persist.", "method": "1) Theoretical proof showing optimizing attack context/model parameters reduces adaptive robustness radius 2Development of RLCracker: a reinforcement learning attack that erases watermarks while preserving semantic content using limited watermarked examples and no detector access.", "result": "RLCracker achieves 98.58% removal success rate and 0.92 P-SP semantic preservation score on 1500-token texts with just 100 short training samples. Outperforms GPT-4o by 1400% and generalizes across 10 watermarking schemes and 5 model sizes.", "conclusion": "Current LLM watermarking defenses are fundamentally vulnerable to adaptive attacks, necessitating reevaluation of security claims and development of more robust watermarking techniques."}}
{"id": "2509.20943", "categories": ["cs.CR", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.20943", "abs": "https://arxiv.org/abs/2509.20943", "authors": ["Dincy R. Arikkat", "Sneha B. T.", "Serena Nicolazzo", "Antonino Nocera", "Vinod P.", "Rafidha Rehiman K. A.", "Karthika R"], "title": "CTI Dataset Construction from Telegram", "comment": null, "summary": "Cyber Threat Intelligence (CTI) enables organizations to anticipate, detect,\nand mitigate evolving cyber threats. Its effectiveness depends on high-quality\ndatasets, which support model development, training, evaluation, and\nbenchmarking. Building such datasets is crucial, as attack vectors and\nadversary tactics continually evolve. Recently, Telegram has gained prominence\nas a valuable CTI source, offering timely and diverse threat-related\ninformation that can help address these challenges. In this work, we address\nthese challenges by presenting an end-to-end automated pipeline that\nsystematically collects and filters threat-related content from Telegram. The\npipeline identifies relevant Telegram channels and scrapes 145,349 messages\nfrom 12 curated channels out of 150 identified sources. To accurately filter\nthreat intelligence messages from generic content, we employ a BERT-based\nclassifier, achieving an accuracy of 96.64%. From the filtered messages, we\ncompile a dataset of 86,509 malicious Indicators of Compromise, including\ndomains, IPs, URLs, hashes, and CVEs. This approach not only produces a\nlarge-scale, high-fidelity CTI dataset but also establishes a foundation for\nfuture research and operational applications in cyber threat detection.", "AI": {"tldr": "This paper presents an automated pipeline to extract a large-scale Cyber Threat Intelligence (CTI) dataset from Telegram, achieving 96.64% classification accuracy and compiling 86,509 malicious indicators.", "motivation": "High-quality CTI datasets are essential for combating evolving cyber threats, but building such datasets is challenging due to the dynamic nature of attack vectors and the need for timely threat sources like Telegram.", "method": "An end-to-end pipeline identifies relevant Telegram channels, scrapes 145,349 messages from curated channels, and applies a BERT-based classifier to filter threat intelligence messages, extracting malicious indicators (domains, IPs, URLs, hashes, CVEs).", "result": "A dataset of 86,509 high-fidelity malicious indicators with 96.64% classification accuracy, demonstrating scalability and reliability for CTI collection.", "conclusion": "The pipeline establishes a durable framework for generating and maintaining large-scale CTI datasets from Telegram, enabling improved threat detection and serving as a foundation for future research."}}
{"id": "2509.20972", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20972", "abs": "https://arxiv.org/abs/2509.20972", "authors": ["Ibrahim Altan", "Abdulla Bachir", "Yousuf Parbhulkar", "Abdul Muksith Rizvi", "Moshiur Farazi"], "title": "Dual-Path Phishing Detection: Integrating Transformer-Based NLP with Structural URL Analysis", "comment": "Paper accepted for presentation at the ACS/IEEE 22nd International\n  Conference on Computer Systems and Applications (AICCSA 2025)", "summary": "Phishing emails pose a persistent and increasingly sophisticated threat,\nundermining email security through deceptive tactics designed to exploit both\nsemantic and structural vulnerabilities. Traditional detection methods, often\nbased on isolated analysis of email content or embedded URLs, fail to\ncomprehensively address these evolving attacks. In this paper, we propose a\ndual-path phishing detection framework that integrates transformer-based\nnatural language processing (NLP) with classical machine learning to jointly\nanalyze email text and embedded URLs. Our approach leverages the complementary\nstrengths of semantic analysis using fine-tuned transformer architectures\n(e.g., DistilBERT) and structural link analysis via character-level TF-IDF\nvectorization paired with classical classifiers (e.g., Random Forest).\nEmpirical evaluation on representative email and URL datasets demonstrates that\nthis combined approach significantly improves detection accuracy. Specifically,\nthe DistilBERT model achieves a near-optimal balance between accuracy and\ncomputational efficiency for textual phishing detection, while Random Forest\nnotably outperforms other classical classifiers in identifying malicious URLs.\nThe modular design allows flexibility for standalone deployment or ensemble\nintegration, facilitating real-world adoption. Collectively, our results\nhighlight the efficacy and practical value of this dual-path approach,\nestablishing a scalable, accurate, and interpretable solution capable of\nenhancing email security against contemporary phishing threats.", "AI": {"tldr": "This paper proposes a dual-path phishing detection framework combining transformer-based NLP (e.g., DistilBERT) for email text analysis and classical machine learning (e.g., Random Forests) for URL analysis, achieving improved accuracy and flexibility.", "motivation": "Traditional phishing detection methods inadequately address sophisticated attacks by analyzing email content or URLs in isolation, failing to leverage complementary semantic and structural features effectively.", "method": "A dual-path framework integrating fine-tuned transformers (for semantic email text analysis) and character-level TF-IDF vectorization with classical classifiers (for structural URL analysis), enabling joint analysis of phishing components.", "result": "The DistilBERT model achieved optimal accuracy-efficiency tradeoffs for text analysis, while Random Forest classifiers outperformed peers in URL-based detection. Combined, the framework demonstrated significantly improved detection accuracy on representative datasets.", "conclusion": "The dual-path approach establishes a scalable, interpretable phishing detection solution that outperforms single-modality methods, offering practical deployment flexibility and enhanced defenses against evolving phishing threats."}}
{"id": "2509.21057", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21057", "abs": "https://arxiv.org/abs/2509.21057", "authors": ["Jiahao Huo", "Shuliang Liu", "Bin Wang", "Junyan Zhang", "Yibo Yan", "Aiwei Liu", "Xuming Hu", "Mingxun Zhou"], "title": "PMark: Towards Robust and Distortion-free Semantic-level Watermarking with Channel Constraints", "comment": null, "summary": "Semantic-level watermarking (SWM) for large language models (LLMs) enhances\nwatermarking robustness against text modifications and paraphrasing attacks by\ntreating the sentence as the fundamental unit. However, existing methods still\nlack strong theoretical guarantees of robustness, and reject-sampling-based\ngeneration often introduces significant distribution distortions compared with\nunwatermarked outputs. In this work, we introduce a new theoretical framework\non SWM through the concept of proxy functions (PFs) $\\unicode{x2013}$ functions\nthat map sentences to scalar values. Building on this framework, we propose\nPMark, a simple yet powerful SWM method that estimates the PF median for the\nnext sentence dynamically through sampling while enforcing multiple PF\nconstraints (which we call channels) to strengthen watermark evidence. Equipped\nwith solid theoretical guarantees, PMark achieves the desired distortion-free\nproperty and improves the robustness against paraphrasing-style attacks. We\nalso provide an empirically optimized version that further removes the\nrequirement for dynamical median estimation for better sampling efficiency.\nExperimental results show that PMark consistently outperforms existing SWM\nbaselines in both text quality and robustness, offering a more effective\nparadigm for detecting machine-generated text. Our code will be released at\n[this URL](https://github.com/PMark-repo/PMark).", "AI": {"tldr": "The paper introduces PMark, a new semantic-level watermarking method for large language models using proxy functions, which improves robustness and text quality over existing methods.", "motivation": "Important for watermarking models to remain robust against modifications and maintain natural text output. There is a problem with distortion when using reject sampling's-based generation and inadequate theoretical robustness.", "method": "The method consists of leveraging proxy functions (PFs) to map sentences to scalar values, then dynamically estimating the PF median while enforcing multiple PF constraints (channels) to strengthen watermark evidence.", "result": "PMark offers improved robustness against paraphrasing attacks and maintains text quality with minimal distortion. The optimized version is efficient for implementation as it removes the need for dynamic median estimation.", "conclusion": "The proposed PMark method provides a more effective paradigms for watermarking LLMs, which is demonstrated through experimental results improving  current SWM baselines in effectiveness and efficiency."}}
{"id": "2509.21147", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21147", "abs": "https://arxiv.org/abs/2509.21147", "authors": ["Amr Akmal Abouelmagd", "Amr Hilal"], "title": "Emerging Paradigms for Securing Federated Learning Systems", "comment": null, "summary": "Federated Learning (FL) facilitates collaborative model training while\nkeeping raw data decentralized, making it a conduit for leveraging the power of\nIoT devices while maintaining privacy of the locally collected data. However,\nexisting privacy- preserving techniques present notable hurdles. Methods such\nas Multi-Party Computation (MPC), Homomorphic Encryption (HE), and Differential\nPrivacy (DP) often incur high compu- tational costs and suffer from limited\nscalability. This survey examines emerging approaches that hold promise for\nenhancing both privacy and efficiency in FL, including Trusted Execution\nEnvironments (TEEs), Physical Unclonable Functions (PUFs), Quantum Computing\n(QC), Chaos-Based Encryption (CBE), Neuromorphic Computing (NC), and Swarm\nIntelligence (SI). For each paradigm, we assess its relevance to the FL\npipeline, outlining its strengths, limitations, and practical considerations.\nWe conclude by highlighting open challenges and prospective research avenues,\noffering a detailed roadmap for advancing secure and scalable FL systems.", "AI": {"tldr": "This survey explores novel techniques to improve privacy and efficiency in Federated Learning (FL), addressing the limitations of traditional methods like MPC, HE, and DP, and highlights open challenges for future research.", "motivation": "The motivation arises from the limitations of existing privacy-preserving techniques in FL, such as high computational costs and scalability issues, necessitating the exploration of emerging solutions for secure and efficient decentralized learning.", "method": "The method involves a comprehensive survey and analysis of emerging approaches such as Trusted Execution Environments (TEEs), Physical Unclonable Functions (PUFs), Quantum Computing (QC), Chaos-Based Encryption (CBE), Neuromorphic Computing (NC), and Swarm Intelligence (SI), evaluating their relevance to FL by examining strengths, limitations, and practical deployment factors.", "result": "The result is an in-depth assessment of how these emerging techniques can enhance privacy and efficiency in FL, providing a detailed taxonomy of their applications, advantages, and drawbacks within FL frameworks.", "conclusion": "The conclusion emphasizes that while these emerging methods present new opportunities to overcome FL's privacy-efficiency trade-offs, significant challenges remain in their integration and scalability, and the paper outlines a roadmap for future advancements in secure and scalable FL systems."}}
