<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 20]
- [cs.SE](#cs.SE) [Total: 12]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [AI Propaganda factories with language models](https://arxiv.org/abs/2508.20186)
*Lukasz Olejnik*

Main category: cs.CR

TL;DR: Small AI models can create credible political messaging with personas, shifting focus from model identity to persona design. Engagement stress boosts extremism. Defenses must now target conversation patterns over restricting models.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the feasibility of executing end-to-end AI-powered influence operations using commodity hardware, highlighting the need for new defensive strategies against the proliferation of automated influence content.

Method: The study utilized small language models to generate coherent, persona-driven political messages, which were evaluated automatically without human raters. The behaviors emerged through testing how persona design and engagement stress impacts content generation.

Result: Two findings: persona design overwhelmingly dictates model behavior over model identity ('persona-over-model'), and engagement requiring counter-arguments increases ideological adherence and extreme content ('engagement as stressor'). Automated influence content production is viable for actors of all sizes.

Conclusion: The paper concludes that defense strategies against AI-powered influence operations should transition from limiting model access to concentrating on conversation-centric detection and disruption of campaigns. The consistency inherent in these operations serves as a detection cue.

Abstract: AI-powered influence operations can now be executed end-to-end on commodity
hardware. We show that small language models produce coherent, persona-driven
political messaging and can be evaluated automatically without human raters.
Two behavioural findings emerge. First, persona-over-model: persona design
explains behaviour more than model identity. Second, engagement as a stressor:
when replies must counter-arguments, ideological adherence strengthens and the
prevalence of extreme content increases. We demonstrate that fully automated
influence-content production is within reach of both large and small actors.
Consequently, defence should shift from restricting model access towards
conversation-centric detection and disruption of campaigns and coordination
infrastructure. Paradoxically, the very consistency that enables these
operations also provides a detection signature.

</details>


### [2] [FlowMalTrans: Unsupervised Binary Code Translation for Malware Detection Using Flow-Adapter Architecture](https://arxiv.org/abs/2508.20212)
*Minghao Hu,Junzhe Wang,Weisen Zhao,Qiang Zeng,Lannan Luo*

Main category: cs.CR

TL;DR: This paper proposes using NMT and NFs to translate malware across ISAs, allowing a single trained model to detect malware in multiple architectures, significantly reducing data collection efforts.


<details>
  <summary>Details</summary>
Motivation: Collecting and labeling malware samples for each ISA is resource-intensive, which limits the scalability of deep learning-based malware detection to new or less-supported architectures.

Method: The approach leverages Neural Machine Translation (NMT) and Normalizing Flows (NFs) to translate malware from target ISAs to a well-supported ISA (e.g., X86-64), allowing reuse of a pre-trained model.

Result: By translating malware between ISAs, the method reduces dependency on ISA-specific datasets, enabling efficient cross-ISA detection without requiring new training data.

Conclusion: The proposed method effectively enables malware detection across multiple ISAs using a single trained model, thereby reducing the need for labor-intensive data collection for each ISA.

Abstract: Applying deep learning to malware detection has drawn great attention due to
its notable performance. With the increasing prevalence of cyberattacks
targeting IoT devices, there is a parallel rise in the development of malware
across various Instruction Set Architectures (ISAs). It is thus important to
extend malware detection capacity to multiple ISAs. However, training a deep
learning-based malware detection model usually requires a large number of
labeled malware samples. The process of collecting and labeling sufficient
malware samples to build datasets for each ISA is labor-intensive and
time-consuming. To reduce the burden of data collection, we propose to leverage
the ideas of Neural Machine Translation (NMT) and Normalizing Flows (NFs) for
malware detection. Specifically, when dealing with malware in a certain ISA, we
translate it to an ISA with sufficient malware samples (like X86-64). This
allows us to apply a model trained on one ISA to analyze malware from another
ISA. Our approach reduces the data collection effort by enabling malware
detection across multiple ISAs using a model trained on a single ISA.

</details>


### [3] [Robustness Assessment and Enhancement of Text Watermarking for Google's SynthID](https://arxiv.org/abs/2508.20228)
*Xia Han,Qi Li,Jianbing Ni,Mohammad Zulkernine*

Main category: cs.CR

TL;DR: SynGuard introduces a hybrid watermarking framework for LLMs, combining semantic and lexical techniques to outperform existing methods against attacks. Code available at https://github.com/githshine/SynGuard.


<details>
  <summary>Details</summary>
Motivation: Existing LLM watermarking methods (e.g., SynthID-Text) are vulnerable to attacks like paraphrasing and back-translation, undermining watermark detectability and provenance tracking.

Method: SynGuard combines Semantic Information Retrieval (SIR) for semantic alignment with SynthID-Textâ€™s probabilistic watermarking to jointly embed watermarks at lexical and semantic levels.

Result: SynGuard improves watermark recovery by 11.1% in F1 score over SynthID-Text across multiple attack scenarios, showing robustness to real-world tampering.

Conclusion: The study demonstrates the effectiveness of SynGuard in enhancing watermark robustness against meaning-preserving attacks through semantic-aware watermarking, with publicly available resources for further research.

Abstract: Recent advances in LLM watermarking methods such as SynthID-Text by Google
DeepMind offer promising solutions for tracing the provenance of AI-generated
text. However, our robustness assessment reveals that SynthID-Text is
vulnerable to meaning-preserving attacks, such as paraphrasing, copy-paste
modifications, and back-translation, which can significantly degrade watermark
detectability. To address these limitations, we propose SynGuard, a hybrid
framework that combines the semantic alignment strength of Semantic Information
Retrieval (SIR) with the probabilistic watermarking mechanism of SynthID-Text.
Our approach jointly embeds watermarks at both lexical and semantic levels,
enabling robust provenance tracking while preserving the original meaning.
Experimental results across multiple attack scenarios show that SynGuard
improves watermark recovery by an average of 11.1\% in F1 score compared to
SynthID-Text. These findings demonstrate the effectiveness of semantic-aware
watermarking in resisting real-world tampering. All code, datasets, and
evaluation scripts are publicly available at:
https://github.com/githshine/SynGuard.

</details>


### [4] [Network-Level Prompt and Trait Leakage in Local Research Agents](https://arxiv.org/abs/2508.20282)
*Hyejun Jeong,Mohammadreze Teymoorianfard,Abhinav Kumar,Amir Houmansadr,Eugene Badasarian*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We show that Web and Research Agents (WRAs) -- language model-based systems
that investigate complex topics on the Internet -- are vulnerable to inference
attacks by passive network adversaries such as ISPs. These agents could be
deployed \emph{locally} by organizations and individuals for privacy, legal, or
financial purposes. Unlike sporadic web browsing by humans, WRAs visit
$70{-}140$ domains with distinguishable timing correlations, enabling unique
fingerprinting attacks.
  Specifically, we demonstrate a novel prompt and user trait leakage attack
against WRAs that only leverages their network-level metadata (i.e., visited IP
addresses and their timings). We start by building a new dataset of WRA traces
based on user search queries and queries generated by synthetic personas. We
define a behavioral metric (called OBELS) to comprehensively assess similarity
between original and inferred prompts, showing that our attack recovers over
73\% of the functional and domain knowledge of user prompts. Extending to a
multi-session setting, we recover up to 19 of 32 latent traits with high
accuracy. Our attack remains effective under partial observability and noisy
conditions. Finally, we discuss mitigation strategies that constrain domain
diversity or obfuscate traces, showing negligible utility impact while reducing
attack effectiveness by an average of 29\%.

</details>


### [5] [Surveying the Operational Cybersecurity and Supply Chain Threat Landscape when Developing and Deploying AI Systems](https://arxiv.org/abs/2508.20307)
*Michael R Smith,Joe Ingram*

Main category: cs.CR

TL;DR: AI transforms systems but introduces overlooked cyber risks. This paper explores AI lifecycle threats, past exploits, and demands for modern security frameworks to protect AI reliability.


<details>
  <summary>Details</summary>
Motivation: The integration of AI introduces unique cyber threats that traditional security assessments fail to address, such as AI output manipulation, necessitating specialized protective measures.

Method: The study explores operational cybersecurity and supply chain risks across the AI lifecycle, leveraging historical exploitation cases and practical insights to identify vulnerabilities.

Result: Highlights risks like manipulating AI outputs (e.g., slowing performance, generating false positives, degrading accuracy) and advocates for lifecycle-wide security frameworks.

Conclusion: Organizations must adopt tailored security frameworks to address novel cyber threats in AI-driven systems, ensuring reliability and resilience.

Abstract: The rise of AI has transformed the software and hardware landscape, enabling
powerful capabilities through specialized infrastructures, large-scale data
storage, and advanced hardware. However, these innovations introduce unique
attack surfaces and objectives which traditional cybersecurity assessments
often overlook. Cyber attackers are shifting their objectives from conventional
goals like privilege escalation and network pivoting to manipulating AI outputs
to achieve desired system effects, such as slowing system performance, flooding
outputs with false positives, or degrading model accuracy. This paper serves to
raise awareness of the novel cyber threats that are introduced when
incorporating AI into a software system. We explore the operational
cybersecurity and supply chain risks across the AI lifecycle, emphasizing the
need for tailored security frameworks to address evolving threats in the
AI-driven landscape. We highlight previous exploitations and provide insights
from working in this area. By understanding these risks, organizations can
better protect AI systems and ensure their reliability and resilience.

</details>


### [6] [MindGuard: Tracking, Detecting, and Attributing MCP Tool Poisoning Attack via Decision Dependence Graph](https://arxiv.org/abs/2508.20412)
*Zhiqiang Wang,Junyang Zhang,Guanquan Shi,HaoRan Cheng,Yunhao Yao,Kaiwen Guo,Haohua Du,Xiang-Yang Li*

Main category: cs.CR

TL;DR: MindGuard detects and attributes Tool Poisoning Attacks using attention-derived Decision Dependence Graphs, achieving high accuracy and efficiency while bridging traditional security principles with LLM decision-making.


<details>
  <summary>Details</summary>
Motivation: Tool Poisoning Attacks (TPA) exploit poisoned tool metadata to manipulate LLM agents without detectable behavioral traces, rendering behavior-level defenses ineffective. Existing solutions lack decision-specific mechanisms to counteract this threat.

Method: MindGuard employs attention-based Decision Dependence Graphs (DDG) to model LLM decision-making processes, enabling provenance tracking, anomaly detection, and attack attribution without behavioral traces. It adapts classical Program Dependence Graphs for decision-level security.

Result: Experiments show MindGuard achieves 94â€“99% average precision in poisoned invocation detection, 95â€“100% attribution accuracy, sub-second processing times, and zero additional token cost. DDG demonstrates compatibility with classical security policies.

Conclusion: MindGuard addresses the vulnerability of LLM agents to Tool Poisoning Attacks by introducing decision-level safeguards using Decision Dependence Graphs (DDG), achieving robust detection, attribution, and compatibility with traditional security frameworks.

Abstract: The Model Context Protocol (MCP) is increasingly adopted to standardize the
interaction between LLM agents and external tools. However, this trend
introduces a new threat: Tool Poisoning Attacks (TPA), where tool metadata is
poisoned to induce the agent to perform unauthorized operations. Existing
defenses that primarily focus on behavior-level analysis are fundamentally
ineffective against TPA, as poisoned tools need not be executed, leaving no
behavioral trace to monitor.
  Thus, we propose MindGuard, a decision-level guardrail for LLM agents,
providing provenance tracking of call decisions, policy-agnostic detection, and
poisoning source attribution against TPA. While fully explaining LLM decision
remains challenging, our empirical findings uncover a strong correlation
between LLM attention mechanisms and tool invocation decisions. Therefore, we
choose attention as an empirical signal for decision tracking and formalize
this as the Decision Dependence Graph (DDG), which models the LLM's reasoning
process as a weighted, directed graph where vertices represent logical concepts
and edges quantify the attention-based dependencies. We further design robust
DDG construction and graph-based anomaly analysis mechanisms that efficiently
detect and attribute TPA attacks. Extensive experiments on real-world datasets
demonstrate that MindGuard achieves 94\%-99\% average precision in detecting
poisoned invocations, 95\%-100\% attribution accuracy, with processing times
under one second and no additional token cost. Moreover, DDG can be viewed as
an adaptation of the classical Program Dependence Graph (PDG), providing a
solid foundation for applying traditional security policies at the decision
level.

</details>


### [7] [Federated Learning for Large Models in Medical Imaging: A Comprehensive Review](https://arxiv.org/abs/2508.20414)
*Mengyu Sun,Ziyuan Yang,Yongqiang Huang,Hui Yu,Yingyu Chen,Shuren Qi,Andrew Beng Jin Teoh,Yi Zhang*

Main category: cs.CR

TL;DR: This survey explores how Federated Learning overcomes data privacy and centralization barriers in medical imaging AI by enabling decentralized collaboration for reconstruction and diagnosis tasks, while outlining innovations and future research directions.


<details>
  <summary>Details</summary>
Motivation: The development of high-performance AI models in medical imaging is hindered by strict privacy regulations and data-sharing restrictions, limiting model scalability and continuous learning. This motivates the need for privacy-preserving frameworks like FL to enable collaborative training on decentralized datasets.

Method: The paper reviews FL implementations across two stages of the medical imaging pipeline: upstream tasks (CT/MRI reconstruction) and downstream clinical tasks (tumor diagnosis and segmentation). It analyzes innovations in communication efficiency, heterogeneous data alignment, and secure parameter aggregation.

Result: The paper demonstrates that FL facilitates robust, privacy-preserving training for CT/MRI reconstruction across diverse institutions and supports continuous model refinement in clinical tasks (e.g., tumor diagnosis) through local fine-tuning. It also highlights advancements in FL techniques tailored for medical domain challenges.

Conclusion: Federated Learning (FL) is a promising framework for addressing privacy and data fragmentation challenges in medical imaging, enabling collaborative model development and continuous updates without compromising data confidentiality. It serves as a valuable reference for future research directions in the field.

Abstract: Artificial intelligence (AI) has demonstrated considerable potential in the
realm of medical imaging. However, the development of high-performance AI
models typically necessitates training on large-scale, centralized datasets.
This approach is confronted with significant challenges due to strict patient
privacy regulations and legal restrictions on data sharing and utilization.
These limitations hinder the development of large-scale models in medical
domains and impede continuous updates and training with new data. Federated
Learning (FL), a privacy-preserving distributed training framework, offers a
new solution by enabling collaborative model development across fragmented
medical datasets. In this survey, we review FL's contributions at two stages of
the full-stack medical analysis pipeline. First, in upstream tasks such as CT
or MRI reconstruction, FL enables joint training of robust reconstruction
networks on diverse, multi-institutional datasets, alleviating data scarcity
while preserving confidentiality. Second, in downstream clinical tasks like
tumor diagnosis and segmentation, FL supports continuous model updating by
allowing local fine-tuning on new data without centralizing sensitive images.
We comprehensively analyze FL implementations across the medical imaging
pipeline, from physics-informed reconstruction networks to diagnostic AI
systems, highlighting innovations that improve communication efficiency, align
heterogeneous data, and ensure secure parameter aggregation. Meanwhile, this
paper provides an outlook on future research directions, aiming to serve as a
valuable reference for the field's development.

</details>


### [8] [Breaking Diffusion with Cache: Exploiting Approximate Caches in Diffusion Models](https://arxiv.org/abs/2508.20424)
*Desen Sun,Shuncheng Jie,Sihang Liu*

Main category: cs.CR

TL;DR: Caching in AI generators creates remote security holes allowing data exfiltration, prompt theft, and content poisoning - requiring urgent mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: Diffusion models' computational costs lead to adoption of approximate caching, but this breaks user isolation. The work addresses the urgent need to assess security risks introduced by this optimization.

Method: The authors identify and demonstrate three attacks: (1) a covert channel via cached prompts, (2) prompt stealing through cache hit analysis, and (3) poisoning attacks that inject attacker-controlled content into cached prompts.

Result: All attacks were successfully executed remotely through the serving system, proving practical feasibility of exploiting cache-based vulnerabilities to exfiltrate data, steal prompts, and inject malicious content.

Conclusion: Approximate caching in diffusion models introduces severe security vulnerabilities, enabling remote attacks like covert channels, prompt stealing, and poisoning. These findings highlight the need for improved isolation mechanisms in cached generative models.

Abstract: Diffusion models are a powerful class of generative models that produce
content, such as images, from user prompts, but they are computationally
intensive. To mitigate this cost, recent academic and industry work has adopted
approximate caching, which reuses intermediate states from similar prompts in a
cache. While efficient, this optimization introduces new security risks by
breaking isolation among users. This work aims to comprehensively assess new
security vulnerabilities arising from approximate caching. First, we
demonstrate a remote covert channel established with the cache, where a sender
injects prompts with special keywords into the cache and a receiver can recover
that even after days, to exchange information. Second, we introduce a prompt
stealing attack using the cache, where an attacker can recover existing cached
prompts based on cache hit prompts. Finally, we introduce a poisoning attack
that embeds the attacker's logos into the previously stolen prompt, to render
them in future user prompts that hit the cache. These attacks are all performed
remotely through the serving system, which indicates severe security
vulnerabilities in approximate caching.

</details>


### [9] [Ransomware 3.0: Self-Composing and LLM-Orchestrated](https://arxiv.org/abs/2508.20444)
*Md Raz,Meet Udeshi,P. V. Sai Charan,Prashanth Krishnamurthy,Farshad Khorrami,Ramesh Karri*

Main category: cs.CR

TL;DR: LLM-powered ransomware prototype automates attacks via natural language prompts, producing undetected polymorphic variants across devices - signals urgent need for AI defense innovations.


<details>
  <summary>Details</summary>
Motivation: Traditional ransomware detection methods are inadequate against AI-aggregated threats. This work explores the first LLM-orchestrated ransomware model to quantify risks and inform future defenses against adaptive, human-free attack frameworks.

Method: The system synthesizes ransomware code via open-source LLMs using natural language prompts in binary files, enabling closed-loop attack execution with dynamic adaptation across diverse environments through reconnaissance, payload generation, and phased evaluation.

Result: Open-source LLMs successfully generated functional ransomware components across personal, enterprise, and embedded environments; attack campaigns sustained closed-loop execution with polymorphic variants, validated through phase-centric fidelity/cohrence analysis.

Conclusion: The paper presents Ransomware 3.0, a threat model exploiting LLMs to autonomously execute ransomware campaigns, highlighting the existential risk of AI-aggregated threats and the urgent need for advanced defense strategies against polymorphic, AI-driven malware.

Abstract: Using automated reasoning, code synthesis, and contextual decision-making, we
introduce a new threat that exploits large language models (LLMs) to
autonomously plan, adapt, and execute the ransomware attack lifecycle.
Ransomware 3.0 represents the first threat model and research prototype of
LLM-orchestrated ransomware. Unlike conventional malware, the prototype only
requires natural language prompts embedded in the binary; malicious code is
synthesized dynamically by the LLM at runtime, yielding polymorphic variants
that adapt to the execution environment. The system performs reconnaissance,
payload generation, and personalized extortion, in a closed-loop attack
campaign without human involvement. We evaluate this threat across personal,
enterprise, and embedded environments using a phase-centric methodology that
measures quantitative fidelity and qualitative coherence in each attack phase.
We show that open source LLMs can generate functional ransomware components and
sustain closed-loop execution across diverse environments. Finally, we present
behavioral signals and multi-level telemetry of Ransomware 3.0 through a case
study to motivate future development of better defenses and policy enforcements
to address novel AI-enabled ransomware attacks.

</details>


### [10] [Enhancing Resilience for IoE: A Perspective of Networking-Level Safeguard](https://arxiv.org/abs/2508.20504)
*Guan-Yan Yang,Jui-Ning Chen,Farn Wang,Kuo-Hui Yeh*

Main category: cs.CR

TL;DR: This paper proposes a GSL-based framework to secure IoE networks from adversarial attacks by co-optimizing graph structure and node representations, demonstrating superior robustness against cyber threats while addressing challenges in safeguarding critical energy infrastructure.


<details>
  <summary>Details</summary>
Motivation: The Internet of Energy's (IoE) interconnectivity, while enabling efficient energy systems, exposes critical infrastructure to sophisticated cyber threats with significant public safety risks. Traditional safeguards are insufficient against adversarial attacks, necessitating resilient networking-level solutions.

Method: The proposed solution employs Graph Structure Learning (GSL) to jointly optimize graph topology and node representations. This approach inherently resists adversarial network model manipulation through structural adaptability and representation learning.

Result: The framework's robustness is validated through a conceptual analysis, architectural discussion, and a case study using a security dataset. Results show that GSL outperforms representative methods, proving its effectiveness in securing IoE networks against evolving attacks.

Conclusion: This study introduces a GSL-based safeguards framework to enhance the security of IoE networks. The framework demonstrates superior robustness against adversarial attacks, offering a practical solution for protecting critical infrastructure. The authors also outline open challenges and future research directions in this emerging field.

Abstract: The Internet of Energy (IoE) integrates IoT-driven digital communication with
power grids to enable efficient and sustainable energy systems. Still, its
interconnectivity exposes critical infrastructure to sophisticated cyber
threats, including adversarial attacks designed to bypass traditional
safeguards. Unlike general IoT risks, IoE threats have heightened public safety
consequences, demanding resilient solutions. From the networking-level
safeguard perspective, we propose a Graph Structure Learning (GSL)-based
safeguards framework that jointly optimizes graph topology and node
representations to resist adversarial network model manipulation inherently.
Through a conceptual overview, architectural discussion, and case study on a
security dataset, we demonstrate GSL's superior robustness over representative
methods, offering practitioners a viable path to secure IoE networks against
evolving attacks. This work highlights the potential of GSL to enhance the
resilience and reliability of future IoE networks for practitioners managing
critical infrastructure. Lastly, we identify key open challenges and propose
future research directions in this novel research area.

</details>


### [11] [Characterizing Trust Boundary Vulnerabilities in TEE Containers](https://arxiv.org/abs/2508.20962)
*Weijie Liu,Hongbo Chen,Shuo Huai,Zhen Xu,Wenhao Wang,Zhi Li,Zheli Liu*

Main category: cs.CR

TL;DR: This paper examines security vulnerabilities in TEE containers and proposes an automated analyzer to detect flaws that could lead to information leakage and other attacks.


<details>
  <summary>Details</summary>
Motivation: Trusted Execution Environments (TEEs) are crucial for secure application execution, yet existing TEE containerization methods introduce potential vulnerabilities that need systematic analysis.

Method: The authors designed an automated analysis tool to evaluate the isolation boundary of TEE container interfaces.

Result: The analysis identified critical vulnerabilities like information leakage, rollback attacks, denial-of-service, and Iago attacks in some TEE containers.

Conclusion: The findings highlight the need for improved design and implementation of TEE container isolation boundaries. The paper shares lessons for safer containerization and trends in the field.

Abstract: Trusted Execution Environments (TEEs) have emerged as a cornerstone of
confidential computing, garnering significant attention from both academia and
industry. To enable the secure development, execution, and deployment, of
applications on TEE platforms, TEE containers have been introduced as
middleware solutions. These containers aim to shield applications from
potentially malicious operating systems and orchestration interfaces while
maintaining usability and reliability. In this paper, we analyze the isolation
strategies employed by existing TEE containers to protect secure applications.
To address the challenges in analyzing these interfaces, we designed an
automated analyzer to precisely identify and evaluate their isolation
boundaries. We observed that some TEE containers fail to achieve their intended
goals due to critical design and implementation flaws, such as information
leakage, rollback attacks, denial-of-service, and Iago attacks, which pose
significant security risks. Drawing from our findings, we share key lessons to
guide the development of more secure container solutions and discuss emerging
trends in TEE containerization design.

</details>


### [12] [BridgeShield: Enhancing Security for Cross-chain Bridge Applications via Heterogeneous Graph Mining](https://arxiv.org/abs/2508.20517)
*Dan Lin,Shunfeng Lu,Ziyan Liu,Jiajing Wu,Junyuan Fang,Kaixin Lin,Bowen Song,Zibin Zheng*

Main category: cs.CR

TL;DR: BridgeShield addresses cross-chain security gaps by modeling bridges as unified heterogeneous graphs, achieving state-of-the-art performance with 92.58% F1-score through novel graph attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Cross-chain bridges are vulnerable to attacks due to design flaws and their high-value holdings, existing detection methods fail to capture cross-chain semantics comprehensively.

Method: BridgeShield employs heterogeneous graph attention networks with intra-meta-path and inter-meta-path attention mechanisms to model cross-chain behaviors comprehensively.

Result: BridgeShield achieves an average F1-score of 92.58%, showing a 24.39% improvement over state-of-the-art baselines on 51 real-world attack events.

Conclusion: BridgeShield proves to be an effective solution for securing cross-chain bridges, enhancing the resilience of multi-chain ecosystems.

Abstract: Cross-chain bridges play a vital role in enabling blockchain
interoperability. However, due to the inherent design flaws and the enormous
value they hold, they have become prime targets for hacker attacks. Existing
detection methods show progress yet remain limited, as they mainly address
single-chain behaviors and fail to capture cross-chain semantics. To address
this gap, we leverage heterogeneous graph attention networks, which are
well-suited for modeling multi-typed entities and relations, to capture the
complex execution semantics of cross-chain behaviors. We propose BridgeShield,
a detection framework that jointly models the source chain, off-chain
coordination, and destination chain within a unified heterogeneous graph
representation. BridgeShield incorporates intra-meta-path attention to learn
fine-grained dependencies within cross-chain paths and inter-meta-path
attention to highlight discriminative cross-chain patterns, thereby enabling
precise identification of attack behaviors. Extensive experiments on 51
real-world cross-chain attack events demonstrate that BridgeShield achieves an
average F1-score of 92.58%, representing a 24.39% improvement over
state-of-the-art baselines. These results validate the effectiveness of
BridgeShield as a practical solution for securing cross-chain bridges and
enhancing the resilience of multi-chain ecosystems.

</details>


### [13] [Bitcoin as an Interplanetary Monetary Standard with Proof-of-Transit Timestamping](https://arxiv.org/abs/2508.20591)
*Jose E. Puente,Carlos Puente*

Main category: cs.CR

TL;DR: This paper designs a decentralized framework to enable Bitcoin as a shared monetary system between Earth and Mars, introducing PoTT to manage high-latency interplanetary communications and proposing a hybrid architecture using DTN, federations, and commit chains.


<details>
  <summary>Details</summary>
Motivation: Bitcoin's current architecture cannot address interplanetary communication challenges such as high latency and intermittent connectivity between Earth and Mars. The paper addresses this by developing a system maintaining cryptographic integrity without altering Bitcoin's core consensus or monetary properties.

Method: Introduces Proof-of-Transit Timestamping (PoTT) for secure audit trails across high-latency links, combines DTN and LEO mesh networking, implements header-first replication, long-horizon Lightning channels with planetary watchtowers, and utilizes federated sidechains or blind-merge-mined (BMM) commit chains for settlement.

Result: Demonstrates PoTT's formalization and security analysis, showing it improves reliability and accountability across planetary networks. Proposes near-term federated solutions for Mars and long-term BMM commit chains, ensuring Earth's monetary base remains unchanged while enabling pegged local systems on Mars.

Conclusion: The paper proposes an architectural framework for using Bitcoin as a shared monetary standard between Earth and Mars, leveraging cryptographic primitives like PoTT to handle interplanetary communication constraints. It concludes that strong federations and potential blind-merge-mined commit chains provide viable solutions for Mars-based transactions while preserving Earth's Bitcoin monetary base.

Abstract: We explore the feasibility of deploying Bitcoin as the shared monetary
standard between Earth and Mars, accounting for physical constraints of
interplanetary communication. We introduce a novel primitive, Proof-of-Transit
Timestamping (PoTT), to provide cryptographic, tamper-evident audit trails for
Bitcoin data across high-latency, intermittently-connected links. Leveraging
Delay/Disruption-Tolerant Networking (DTN) and optical low-Earth-orbit (LEO)
mesh constellations, we propose an architecture for header-first replication,
long-horizon Lightning channels with planetary watchtowers, and secure
settlement through federated sidechains or blind-merge-mined (BMM) commit
chains. We formalize PoTT, analyze its security model, and show how it
measurably improves reliability and accountability without altering Bitcoin
consensus or its monetary base. Near-term deployments favor strong federations
for local settlement; longer-term, blind-merge-mined commit chains (if adopted)
provide an alternative. The Earth L1 monetary base remains unchanged, while
Mars can operate a pegged commit chain or strong federation with 1:1 pegged
assets for local block production. For transparency, if both time-beacon
regimes are simultaneously compromised, PoTT-M2 (and PoTT generally) reduces to
administrative assertions rather than cryptographic time-anchoring.

</details>


### [14] [CyberSleuth: Autonomous Blue-Team LLM Agent for Web Attack Forensics](https://arxiv.org/abs/2508.20643)
*Stefano Fumero,Kai Huang,Matteo Boffa,Danilo Giordano,Marco Mellia,Zied Ben Houidi,Dario Rossi*

Main category: cs.CR

TL;DR: This paper introduces CyberSleuth, a leading LLM-agent for cybersecurity forensics that accurately identifies attack vulnerabilities and performs well in human evaluations, while providing an open benchmark to advance defensive AI research.


<details>
  <summary>Details</summary>
Motivation: While LLM agents have been extensively studied for offensive cybersecurity tasks, their defensive applications in incident response and forensics remain underexplored. This work addresses this gap by systematically studying LLM-agent design for forensic investigations of web application attacks.

Method: The authors propose CyberSleuth, an autonomous agent that analyzes packet-level traces and application logs to identify attack details. They evaluate four agent architectures and six LLM backends across 20 incident scenarios, validate performance on 2025 incidents, and conduct a human study with 22 experts.

Result: CyberSleuth achieves 80% accuracy in identifying exact CVEs in attacks and receives positive ratings for report quality from cybersecurity experts. The DeepSeek R1 open-source LLM shows particular effectiveness. The research releases a benchmark and platform for reproducible evaluation.

Conclusion: The study concludes that CyberSleuth is an effective LLM-agent design for forensic investigations, providing a reproducible benchmark and platform to advance defensive LLM research in cybersecurity.

Abstract: Large Language Model (LLM) agents are powerful tools for automating complex
tasks. In cybersecurity, researchers have primarily explored their use in
red-team operations such as vulnerability discovery and penetration tests.
Defensive uses for incident response and forensics have received comparatively
less attention and remain at an early stage. This work presents a systematic
study of LLM-agent design for the forensic investigation of realistic web
application attacks. We propose CyberSleuth, an autonomous agent that processes
packet-level traces and application logs to identify the targeted service, the
exploited vulnerability (CVE), and attack success. We evaluate the consequences
of core design decisions - spanning tool integration and agent architecture -
and provide interpretable guidance for practitioners. We benchmark four agent
architectures and six LLM backends on 20 incident scenarios of increasing
complexity, identifying CyberSleuth as the best-performing design. In a
separate set of 10 incidents from 2025, CyberSleuth correctly identifies the
exact CVE in 80% of cases. At last, we conduct a human study with 22 experts,
which rated the reports of CyberSleuth as complete, useful, and coherent. They
also expressed a slight preference for DeepSeek R1, a good news for open source
LLM. To foster progress in defensive LLM research, we release both our
benchmark and the CyberSleuth platform as a foundation for fair, reproducible
evaluation of forensic agents.

</details>


### [15] [Multi-Agent Penetration Testing AI for the Web](https://arxiv.org/abs/2508.20816)
*Isaac David,Arthur Gervais*

Main category: cs.CR

TL;DR: MAPTA is a cost-effective multi-agent system for AI-driven web app security audits, achieving high success rates and efficient resource use (76.9% success, $3.67 avg cost), uncovering critical vulnerabilities in popular repositories.


<details>
  <summary>Details</summary>
Motivation: The democratization of software development through AI has led to a surge in vulnerable code (up to 40%), outpacing traditional security auditing capabilities, necessitating scalable and cost-effective solutions.

Method: MAPTA employs a multi-agent system integrating large language model orchestration, tool-grounded execution, and end-to-end exploit validation for autonomous security assessments.

Result: MAPTA achieves 76.9% overall success on the XBOW benchmark, with perfect performance on SSRF/misconfigurations, discovers critical vulnerabilities in real-world repositories, and operates at an average cost of $3.67 per assessment.

Conclusion: MAPTA effectively addresses the scalability issue in AI-generated code security auditing by combining multi-agent systems with efficient resource use, achieving strong success rates and low costs, making it practical for widespread adoption.

Abstract: AI-powered development platforms are making software creation accessible to a
broader audience, but this democratization has triggered a scalability crisis
in security auditing. With studies showing that up to 40% of AI-generated code
contains vulnerabilities, the pace of development now vastly outstrips the
capacity for thorough security assessment.
  We present MAPTA, a multi-agent system for autonomous web application
security assessment that combines large language model orchestration with
tool-grounded execution and end-to-end exploit validation. On the 104-challenge
XBOW benchmark, MAPTA achieves 76.9% overall success with perfect performance
on SSRF and misconfiguration vulnerabilities, 83% success on broken
authorization, and strong results on injection attacks including server-side
template injection (85%) and SQL injection (83%). Cross-site scripting (57%)
and blind SQL injection (0%) remain challenging. Our comprehensive cost
analysis across all challenges totals $21.38 with a median cost of $0.073 for
successful attempts versus $0.357 for failures. Success correlates strongly
with resource efficiency, enabling practical early-stopping thresholds at
approximately 40 tool calls or $0.30 per challenge.
  MAPTA's real-world findings are impactful given both the popularity of the
respective scanned GitHub repositories (8K-70K stars) and MAPTA's low average
operating cost of $3.67 per open-source assessment: MAPTA discovered critical
vulnerabilities including RCEs, command injections, secret exposure, and
arbitrary file write vulnerabilities. Findings are responsibly disclosed, 10
findings are under CVE review.

</details>


### [16] [JADES: A Universal Framework for Jailbreak Assessment via Decompositional Scoring](https://arxiv.org/abs/2508.20848)
*Junjie Chu,Mingjie Li,Ziqing Yang,Ye Leng,Chenhao Lin,Chao Shen,Michael Backes,Yun Shen,Yang Zhang*

Main category: cs.CR

TL;DR: JADES is a decomposition-based jailbreak evaluation framework that achieves 98.5% human-level agreement by scoring sub-questions and detecting hallucinations, significantly outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing jailbreak evaluation methods are inconsistent and misaligned with human perception due to reliance on proxy indicators or simplistic judgments, necessitating a more accurate and universal assessment framework.

Method: JADES employs a decompositional scoring framework that automatically breaks down harmful questions into weighted sub-questions, scores sub-answers using a weight-aggregation process, and integrates an optional fact-checking module to detect hallucinations.

Result: JADES achieves 98.5% agreement with human evaluators on the JailbreakQR benchmark, outperforming baselines by >9%. Re-evaluation revealed substantial overestimation of attack success rates (e.g., LAA's GPT-3.5-Turbo success rate drops from 93% to 69%).

Conclusion: JADES provides accurate, consistent, and interpretable evaluations for jailbreak attempts, serving as a reliable basis for future assessments.

Abstract: Accurately determining whether a jailbreak attempt has succeeded is a
fundamental yet unresolved challenge. Existing evaluation methods rely on
misaligned proxy indicators or naive holistic judgments. They frequently
misinterpret model responses, leading to inconsistent and subjective
assessments that misalign with human perception. To address this gap, we
introduce JADES (Jailbreak Assessment via Decompositional Scoring), a universal
jailbreak evaluation framework. Its key mechanism is to automatically decompose
an input harmful question into a set of weighted sub-questions, score each
sub-answer, and weight-aggregate the sub-scores into a final decision. JADES
also incorporates an optional fact-checking module to strengthen the detection
of hallucinations in jailbreak responses. We validate JADES on JailbreakQR, a
newly introduced benchmark proposed in this work, consisting of 400 pairs of
jailbreak prompts and responses, each meticulously annotated by humans. In a
binary setting (success/failure), JADES achieves 98.5% agreement with human
evaluators, outperforming strong baselines by over 9%. Re-evaluating five
popular attacks on four LLMs reveals substantial overestimation (e.g., LAA's
attack success rate on GPT-3.5-Turbo drops from 93% to 69%). Our results show
that JADES could deliver accurate, consistent, and interpretable evaluations,
providing a reliable basis for measuring future jailbreak attacks.

</details>


### [17] [Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review](https://arxiv.org/abs/2508.20863)
*Matteo Gioele Collu,Umberto Salviati,Roberto Confalonieri,Mauro Conti,Giovanni Apruzzese*

Main category: cs.CR

TL;DR: This paper demonstrates that adversarial text manipulation in academic papers can reliably deceive LLM-based peer-review systems through invisible prompt injection attacks, even when attacks lack malicious intent, highlighting urgent security risks in automated academic evaluation processes.


<details>
  <summary>Details</summary>
Motivation: The increasing use of LLMs in scientific peer-review necessitates understanding their reliability and vulnerabilities to manipulation, particularly through hidden attacks that could compromise review integrity without requiring malicious intent.

Method: The researchers formalized three threat models to represent varying attacker motivations, developed adversarial prompts undetectable by humans, conducted a user study with scholars to derive peer-review prompts, and systematically evaluated adversarial effectiveness across multiple reviewing prompts, LLM systems, and papers. They also tested detection evasion techniques against automated checks.

Result: Adversarial prompts successfully misled LLMs across diverse configurations, sometimes impacting 'honest-but-lazy' reviewers. The prompts demonstrated high reliability in manipulating outputs while remaining undetectable, with efficacy maintained across different LLM-based systems and academic papers.

Conclusion: The study highlights the critical need for developing robust detection and prevention mechanisms to mitigate adversarial prompt attacks in automated peer-review systems, emphasizing the importance of secure implementation of LLMs in academic processes.

Abstract: Large Language Models (LLMs) are increasingly being integrated into the
scientific peer-review process, raising new questions about their reliability
and resilience to manipulation. In this work, we investigate the potential for
hidden prompt injection attacks, where authors embed adversarial text within a
paper's PDF to influence the LLM-generated review. We begin by formalising
three distinct threat models that envision attackers with different motivations
-- not all of which implying malicious intent. For each threat model, we design
adversarial prompts that remain invisible to human readers yet can steer an
LLM's output toward the author's desired outcome. Using a user study with
domain scholars, we derive four representative reviewing prompts used to elicit
peer reviews from LLMs. We then evaluate the robustness of our adversarial
prompts across (i) different reviewing prompts, (ii) different commercial
LLM-based systems, and (iii) different peer-reviewed papers. Our results show
that adversarial prompts can reliably mislead the LLM, sometimes in ways that
adversely affect a "honest-but-lazy" reviewer. Finally, we propose and
empirically assess methods to reduce detectability of adversarial prompts under
automated content checks.

</details>


### [18] [AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning](https://arxiv.org/abs/2508.20866)
*Amine Lbath,Massih-Reza Amini,Aurelien Delaitre,Vadim Okun*

Main category: cs.CR

TL;DR: This paper proposes an AI-agent-driven framework to generate realistic vulnerability datasets via RAG and low-rank fine-tuning, achieving 89â€“95% accuracy across 116 C/C++ code samples, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with scalability and accuracy, while AI approaches depend on high-quality training data. Generating realistic vulnerability datasets is critical for improving automated detection systems.

Method: A novel framework coordinating multiple AI agents (simulating expert reasoning) and traditional tools, using Retrieval-Augmented Generation (RAG) for contextual grounding and Low-Rank approximation for model fine-tuning.

Result: Experimental validation on 116 code samples from three benchmarks showed the framework outperforms existing techniques in dataset accuracy, with 89â€“95% success rates in function-level vulnerability injection.

Conclusion: The proposed framework effectively addresses the limitations of traditional and AI-driven vulnerability detection methods by automatically generating realistic datasets with high accuracy, demonstrating success rates between 89% and 95% in injecting vulnerabilities into C/C++ codebases.

Abstract: The increasing complexity of software systems and the sophistication of
cyber-attacks have underscored the critical need for effective automated
vulnerability detection and repair systems. Traditional methods, such as static
program analysis, face significant challenges related to scalability,
adaptability, and high false-positive and false-negative rates. AI-driven
approaches, particularly those using machine learning and deep learning models,
show promise but are heavily reliant on the quality and quantity of training
data. This paper introduces a novel framework designed to automatically
introduce realistic, category-specific vulnerabilities into secure C/C++
codebases to generate datasets. The proposed approach coordinates multiple AI
agents that simulate expert reasoning, along with function agents and
traditional code analysis tools. It leverages Retrieval-Augmented Generation
for contextual grounding and employs Low-Rank approximation of weights for
efficient model fine-tuning. Our experimental study on 116 code samples from
three different benchmarks suggests that our approach outperforms other
techniques with regard to dataset accuracy, achieving between 89\% and 95\%
success rates in injecting vulnerabilities at function level.

</details>


### [19] [PromptSleuth: Detecting Prompt Injection via Semantic Intent Invariance](https://arxiv.org/abs/2508.20890)
*Mengxiao Wang,Yuxuan Zhang,Guofei Gu*

Main category: cs.CR

TL;DR: This paper introduces a new benchmark for advanced prompt injection attacks and proposes PromptSleuth, an intent-based defense that outperforms existing solutions by analyzing task-level semantics instead of surface patterns.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to capture emerging prompt injection threats as adversaries evolve with paraphrased, obfuscated, and multi-task attacks, necessitating improved evaluation methods and defenses.

Method: The paper introduces a new comprehensive benchmark expanding prior datasets with advanced injection techniques and multi-task scenarios, then proposes PromptSleuth, a semantic-oriented defense framework that detects injection by analyzing task-level intent rather than surface features.

Result: Experimental results show that established defenses falter under the new benchmark, while PromptSleuth outperforms existing methods on multiple state-of-the-art benchmarks without compromising runtime or cost efficiency.

Conclusion: The study concludes that intent-based semantic reasoning through PromptSleuth provides a robust, efficient, and generalizable defense against evolving prompt injection threats in LLMs.

Abstract: Large Language Models (LLMs) are increasingly integrated into real-world
applications, from virtual assistants to autonomous agents. However, their
flexibility also introduces new attack vectors-particularly Prompt Injection
(PI), where adversaries manipulate model behavior through crafted inputs. As
attackers continuously evolve with paraphrased, obfuscated, and even multi-task
injection strategies, existing benchmarks are no longer sufficient to capture
the full spectrum of emerging threats.
  To address this gap, we construct a new benchmark that systematically extends
prior efforts. Our benchmark subsumes the two widely-used existing ones while
introducing new manipulation techniques and multi-task scenarios, thereby
providing a more comprehensive evaluation setting. We find that existing
defenses, though effective on their original benchmarks, show clear weaknesses
under our benchmark, underscoring the need for more robust solutions. Our key
insight is that while attack forms may vary, the adversary's intent-injecting
an unauthorized task-remains invariant. Building on this observation, we
propose PromptSleuth, a semantic-oriented defense framework that detects prompt
injection by reasoning over task-level intent rather than surface features.
Evaluated across state-of-the-art benchmarks, PromptSleuth consistently
outperforms existing defense while maintaining comparable runtime and cost
efficiency. These results demonstrate that intent-based semantic reasoning
offers a robust, efficient, and generalizable strategy for defending LLMs
against evolving prompt injection threats.

</details>


### [20] [Guarding Against Malicious Biased Threats (GAMBiT) Experiments: Revealing Cognitive Bias in Human-Subjects Red-Team Cyber Range Operations](https://arxiv.org/abs/2508.20963)
*Brandon Beltz,Jim Doty,Yvonne Fonken,Nikolos Gurney,Brett Israelsen,Nathan Lau,Stacy Marsella,Rachelle Thomas,Stoney Trent,Peggy Wu,Ya-Ting Yang,Quanyan Zhu*

Main category: cs.CR

TL;DR: GAMBiT project releases three large human-subjects cyber attack datasets (July 2024-March 2025) capturing skilled attackers' operations in a simulated network, enabling research on behavior modeling and bias-aware cybersecurity analytics.


<details>
  <summary>Details</summary>
Motivation: To provide large-scale, empirically grounded datasets needed for studying real-world attacker behaviors, biases, and their operational patterns in a controlled cyber range environment.

Method: Three 8-hour human-subjects experiments involving 19-20 skilled attackers each, using the SimSpace Cyber Force Platform and collecting multi-modal data including self-reports, operational records, terminal logs, PCAPs, and NIDS alerts.

Result: Release of three datasets with multi-modal attacker behavior data, derivative logs, and labels, supporting research in behavior modeling, bias analysis, and defensive method benchmarking.

Conclusion: The datasets contribute to advancing research in attacker behavior modeling, bias-aware analytics, and benchmarking methods by providing large-scale empirical data from skilled attackers in controlled simulations.

Abstract: We present three large-scale human-subjects red-team cyber range datasets
from the Guarding Against Malicious Biased Threats (GAMBiT) project. Across
Experiments 1-3 (July 2024-March 2025), 19-20 skilled attackers per experiment
conducted two 8-hour days of self-paced operations in a simulated enterprise
network (SimSpace Cyber Force Platform) while we captured multi-modal data:
self-reports (background, demographics, psychometrics), operational notes,
terminal histories, keylogs, network packet captures (PCAP), and NIDS alerts
(Suricata). Each participant began from a standardized Kali Linux VM and
pursued realistic objectives (e.g., target discovery and data exfiltration)
under controlled constraints. Derivative curated logs and labels are included.
The combined release supports research on attacker behavior modeling,
bias-aware analytics, and method benchmarking. Data are available via IEEE
Dataport entries for Experiments 1-3.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [21] [Evaluating LLMs on microservice-based applications: how complex is your specification?](https://arxiv.org/abs/2508.20119)
*Daniel M. Yellin*

Main category: cs.SE

TL;DR: This paper evaluates LLM code generation for microservices, finding that while models perform adequately on basic tasks, they fail at complex real-world scenarios due to insufficient handling of advanced infrastructure and business logic requirements.


<details>
  <summary>Details</summary>
Motivation: The study investigates the limitations of LLMs in generating production-level microservice code, addressing gaps in understanding their performance beyond simple benchmarks to inform practical deployment.

Method: The authors created a framework for automated unit testing of LLM-synthesized microservice code, along with a metric to evaluate specification difficulty based on factors like complexity and external dependencies.

Result: Strong LLMs like GPT-3o-mini achieve good results on medium-complexity specifications but show poor performance on high-complexity ones involving databases, external services, and security requirements.

Conclusion: LLMs struggle with complex real-world code generation due to intricate business logic and non-functional requirements, necessitating future research to address these challenges.

Abstract: In this paper we evaluate how far LLMs have advanced in generating code for
real-world problems. Specifically, we explore code synthesis for
microservice-based applications, a widely used architecture pattern. We define
a standard template for specifying these applications, and we propose a metric
for judging the difficulty level of a specification. The higher the score, the
more difficult it is to generate code for the specification. We develop a
framework to automate the process of testing LLM-synthesized code for a
microservice using unit tests. Our experimental results show that strong LLMs
(like GPT-3o-mini) do fairly well on medium difficulty specifications but do
very poorly on those of higher difficulty levels. This is due to more intricate
business logic, a greater use of external services, database integration and
inclusion of non-functional capabilities such as authentication. We analyzed
the errors in LLM-synthesized code and report on the key challenges LLMs face
in generating code for these specifications thereby suggesting future research
directions to improve code synthesis for real-world problems.

</details>


### [22] [Towards Better Correctness and Efficiency in Code Generation](https://arxiv.org/abs/2508.20124)
*Yunlong Feng,Yang Xu,Xiao Xu,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While code large language models have demonstrated remarkable progress in
code generation, the generated code often exhibits poor runtime efficiency,
limiting its practical application in performance-sensitive scenarios. To
address this limitation, we propose an efficiency-oriented reinforcement
learning framework guided by a novel performance reward. Based on this
framework, we take a deeper dive into the code efficiency problem, identifying
then proposing methods to overcome key bottlenecks: (1) Dynamic exploration
overcomes the static data constraints of offline fine-tuning, enabling the
discovery of more efficient code implementations. (2) The error-insensitive
reinforcement learning method and high-contrast efficiency signals are crucial
for mitigating systematic errors and achieving effective optimization. (3)
Online exploration is most effective when starting from a high-correctness
baseline, as this allows for efficiency improvements without sacrificing
accuracy. With these discoveries, we finally propose a two-stage tuning method,
which achieves high and balanced performance across correctness and efficiency.
The results of experiments show the effectiveness of the method, which improves
code correctness by 10.18\% and runtime efficiency by 7.75\% on a 7B model,
achieving performance comparable to much larger model.

</details>


### [23] [Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators](https://arxiv.org/abs/2508.20340)
*Maolin Sun,Yibiao Yang,Yuming Zhou*

Main category: cs.SE

TL;DR: Chimera enhances LLM-based SMT solver testing by synthesizing grammar-compliant term generators, reducing syntax errors and computational overhead. It identified 43 bugs in Z3/cvc5, with 40 resolved, showcasing its efficacy and efficiency in solver fuzzing.


<details>
  <summary>Details</summary>
Motivation: Traditional testing techniques for SMT solvers lag due to rapid feature evolution, while LLM-based approaches face two critical challenges: high syntactic error rates in generated formulas and high computational overhead from iterative LLM interactions. This necessitates a robust alternative that ensures syntactic correctness and scalability.

Method: Chimera employs LLMs to (1) automatically extract context-free grammars (CFGs) for SMT theories from documentation and solver-specific extensions, and (2) synthesize composable Boolean term generators that adhere to these grammars. It leverages these generators to populate structural skeletons derived from existing formulas during fuzzing, ensuring syntactic validity and semantic diversity while minimizing runtime LLM interactions.

Result: Chimera uncovered 43 confirmed bugs in the Z3 and cvc5 SMT solvers, with 40 of these actively fixed by developers. Its design achieves syntactic validity, semantic diversity, and significant computational efficiency gains by reducing LLM interactions to a one-time process.

Conclusion: Chimera effectively addresses the limitations of existing LLM-based SMT solver testing methods by ensuring syntactic validity and drastically reducing runtime overhead through grammar-driven term generator synthesis. It demonstrated success in detecting 43 confirmed bugs in Z3 and cvc5, with 40 of them already resolved.

Abstract: Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems
and programming languages research, providing the foundation for tasks like
symbolic execution and automated verification. Because these solvers sit on the
critical path, their correctness is essential, and high-quality test formulas
are key to uncovering bugs. However, while prior testing techniques performed
well on earlier solver versions, they struggle to keep pace with rapidly
evolving features. Recent approaches based on Large Language Models (LLMs) show
promise in exploring advanced solver capabilities, but two obstacles remain:
nearly half of the generated formulas are syntactically invalid, and iterative
interactions with the LLMs introduce substantial computational overhead. In
this study, we present Chimera, a novel LLM-assisted fuzzing framework that
addresses both issues by shifting from direct formula generation to the
synthesis of reusable term (i.e., logical expression) generators. Particularly,
Chimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for
SMT theories, including solver-specific extensions, from documentation, and (2)
synthesize composable Boolean term generators that adhere to these grammars.
During fuzzing, Chimera populates structural skeletons derived from existing
formulas with the terms iteratively produced by the LLM-synthesized generators.
This design ensures syntactic validity while promoting semantic diversity.
Notably, Chimera requires only one-time LLM interaction investment,
dramatically reducing runtime cost. We evaluated Chimera on two leading SMT
solvers: Z3 and cvc5. Our experiments show that Chimera has identified 43
confirmed bugs, 40 of which have already been fixed by developers.

</details>


### [24] [Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought](https://arxiv.org/abs/2508.20370)
*Lingzhe Zhang,Tong Jia,Kangjin Wang,Weijie Hong,Chiming Duan,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: This paper introduces RCLAgent, a multi-agent framework for microservice root cause localization that mimics SRE reasoning patterns. It achieves state-of-the-art results by using a novel recursion-of-thought strategy and requires significantly less data (single request) compared to prior methods.


<details>
  <summary>Details</summary>
Motivation: Contemporary microservice systems face frequent failures requiring accurate root cause localization. Existing methods either rely on rigid pre-defined schemas or lack interpretability, necessitating an adaptive and explainable solution aligned with human SRE practices.

Method: RCLAgent employs a multi-agent recursion-of-thought framework combining a novel recursion-of-thought strategy, data integration from multiple agents, and tool-assisted analysis to guide the LLM's reasoning process towards root cause identification.

Result: Experimental evaluations show RCLAgent outperforms state-of-the-art methods by accurately localizing root causes using only a single request, rather than requiring aggregated data from multiple requests.

Conclusion: RCLAgent is an effective adaptive root cause localization method for microservice systems, improving efficiency and precision by leveraging a multi-agent recursion-of-thought framework and achieving superior performance with fewer requests.

Abstract: As contemporary microservice systems become increasingly popular and
complex-often comprising hundreds or even thousands of fine-grained,
interdependent subsystems-they are facing more frequent failures. Ensuring
system reliability thus demands accurate root cause localization. While traces
and metrics have proven to be effective data sources for this task, existing
methods either heavily rely on pre-defined schemas, which struggle to adapt to
evolving operational contexts, or lack interpretability in their reasoning
process, thereby leaving Site Reliability Engineers (SREs) confused. In this
paper, we conduct a comprehensive study on how SREs localize the root cause of
failures, drawing insights from multiple professional SREs across different
organizations. Our investigation reveals that human root cause analysis
exhibits three key characteristics: recursiveness, multi-dimensional expansion,
and cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,
an adaptive root cause localization method for microservice systems that
leverages a multi-agent recursion-of-thought framework. RCLAgent employs a
novel recursion-of-thought strategy to guide the LLM's reasoning process,
effectively integrating data from multiple agents and tool-assisted analysis to
accurately pinpoint the root cause. Experimental evaluations on various public
datasets demonstrate that RCLAgent achieves superior performance by localizing
the root cause using only a single request-outperforming state-of-the-art
methods that depend on aggregating multiple requests. These results underscore
the effectiveness of RCLAgent in enhancing the efficiency and precision of root
cause localization in complex microservice environments.

</details>


### [25] [AI and Agile Software Development: A Research Roadmap from the XP2025 Workshop](https://arxiv.org/abs/2508.20563)
*Zheying Zhang,Tomas Herda,Victoria Pichler,Pekka Abrahamsson,Geir K. Hanssen,Joshua Kerievsky,Alex Polyakov,Mohit Chandna,Marius Irgens,Kai-Kristian Kemell,Ayman Asad Khan,Crystal Kwok,Evan Leybourn,Munish Malik,Dorota Mleczko,Morteza Moalagh,Christopher Morales,Yuliia Pieskova,Daniel PlanÃ¶tscher,Mika Saari,Anastasiia Tkalich,Karl Josef Gstettner,Xiaofeng Wang*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper synthesizes the key findings from a full-day XP2025 workshop on
"AI and Agile: From Frustration to Success", held in Brugg-Windisch,
Switzerland. The workshop brought together over 30 interdisciplinary academic
researchers and industry practitioners to tackle the concrete challenges and
emerging opportunities at the intersection of Generative Artificial
Intelligence (GenAI) and agile software development. Through structured,
interactive breakout sessions, participants identified shared pain points like
tool fragmentation, governance, data quality, and critical skills gaps in AI
literacy and prompt engineering. These issues were further analyzed, revealing
underlying causes and cross-cutting concerns. The workshop concluded by
collaboratively co-creating a multi-thematic research roadmap, articulating
both short-term, implementable actions and visionary, long-term research
directions. This cohesive agenda aims to guide future investigation and drive
the responsible, human-centered integration of GenAI into agile practices.

</details>


### [26] [Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol](https://arxiv.org/abs/2508.20737)
*Wei Ma,Yixiao Yang,Qiang Hu,Shi Ying,Zhi Jin,Bo Du,Zhenchang Xing,Tianlin Li,Junjie Shi,Yang Liu,Linxiao Jiang*

Main category: cs.SE

TL;DR: This paper investigates challenges in LLM application testing by analyzing architecture layers, comparing software/AI testing methods, and proposes strategies + AICL protocol for standardized, trustworthy quality assurance in complex LLM systems.


<details>
  <summary>Details</summary>
Motivation: As LLM applications grow in complexity with non-determinism and context dependence, traditional software testing methods face limitations. The paper aims to address structural disconnects between software engineering and AI communities in testing methodologies.

Method: The paper decomposes LLM applications into three architecture layers (System Shell, Prompt Orchestration, LLM Inference Core) and analyzes traditional testing methods' applicability. It compares Testing AI methods with AI safety techniques, identifies four fundamental differences, and proposes four collaborative strategies (Retain, Translate, Integrate, Runtime) to address six core challenges.

Result: The study proposes collaborative strategies for quality assurance, a closed-loop trustworthy framework with pre-deployment validation and runtime monitoring, practical testing guidance, and a standardized protocol (AICL) for AI agent communication.

Conclusion: The paper concludes that standardizing LLM application testing requires collaborative strategies and a framework integrating software engineering and AI safety practices. It proposes Agent Interaction Communication Language (AICL) for standardization and tooling support.

Abstract: Applications of Large Language Models~(LLMs) have evolved from simple text
generators into complex software systems that integrate retrieval augmentation,
tool invocation, and multi-turn interactions. Their inherent non-determinism,
dynamism, and context dependence pose fundamental challenges for quality
assurance. This paper decomposes LLM applications into a three-layer
architecture: \textbf{\textit{System Shell Layer}}, \textbf{\textit{Prompt
Orchestration Layer}}, and \textbf{\textit{LLM Inference Core}}. We then assess
the applicability of traditional software testing methods in each layer:
directly applicable at the shell layer, requiring semantic reinterpretation at
the orchestration layer, and necessitating paradigm shifts at the inference
core. A comparative analysis of Testing AI methods from the software
engineering community and safety analysis techniques from the AI community
reveals structural disconnects in testing unit abstraction, evaluation metrics,
and lifecycle management. We identify four fundamental differences that
underlie 6 core challenges. To address these, we propose four types of
collaborative strategies (\emph{Retain}, \emph{Translate}, \emph{Integrate},
and \emph{Runtime}) and explore a closed-loop, trustworthy quality assurance
framework that combines pre-deployment validation with runtime monitoring.
Based on these strategies, we offer practical guidance and a protocol proposal
to support the standardization and tooling of LLM application testing. We
propose a protocol \textbf{\textit{Agent Interaction Communication Language}}
(AICL) that is used to communicate between AI agents. AICL has the
test-oriented features and is easily integrated in the current agent framework.

</details>


### [27] [From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of LLM-Generated Behavioural Specifications from Food-Safety Regulations](https://arxiv.org/abs/2508.20744)
*Shabnam Hassani,Mehrdad Sabetzadeh,Daniel Amyot*

Main category: cs.SE

TL;DR: LLMs produce high-quality Gherkin specifications from legal texts with minimal quality issues, significantly reducing manual compliance work for developers.


<details>
  <summary>Details</summary>
Motivation: Legal texts are challenging for engineers to translate into compliance artifacts manually due to labor-intensive processes requiring domain expertise. Generative AI offers automation potential for these tasks.

Method: A quasi-experimental study with ten participants evaluating 60 Gherkin specifications generated by Claude and Llama from food-safety regulations. Each specification was assessed by two participants across five criteria (Relevance, Clarity, Completeness, Singularity, Time Savings) through 120 assessments.

Result: 75-90% of specifications achieved highest ratings across criteria. Llama outperformed Claude slightly in Clarity/Completeness/Time Savings, while Claude had stronger Singularity. No 'lowest' ratings occurred, though hallucinations/omissions were noted in feedback.

Conclusion: LLMs can generate high-quality Gherkin specifications from legal texts, reducing manual effort and providing structured artifacts useful for implementation, assurance, and test generation.

Abstract: Context: Laws and regulations increasingly affect software design and quality
assurance, but legal texts are written in technology-neutral language. This
creates challenges for engineers who must develop compliance artifacts such as
requirements and acceptance criteria. Manual creation is labor-intensive,
error-prone, and requires domain expertise. Advances in Generative AI (GenAI),
especially Large Language Models (LLMs), offer a way to automate deriving such
artifacts.
  Objective: We present the first systematic human-subject study of LLMs'
ability to derive behavioral specifications from legal texts using a
quasi-experimental design. These specifications translate legal requirements
into a developer-friendly form.
  Methods: Ten participants evaluated specifications generated from food-safety
regulations by Claude and Llama. Using Gherkin, a structured BDD language, 60
specifications were produced. Each participant assessed 12 across five
criteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each
specification was reviewed by two participants, yielding 120 assessments.
  Results: For Relevance, 75% of ratings were highest and 20% second-highest.
Clarity reached 90% highest. Completeness: 75% highest, 19% second.
Singularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No
lowest ratings occurred. Mann-Whitney U tests showed no significant differences
across participants or models. Llama slightly outperformed Claude in Clarity,
Completeness, and Time Savings, while Claude was stronger in Singularity.
Feedback noted hallucinations and omissions but confirmed the utility of the
specifications.
  Conclusion: LLMs can generate high-quality Gherkin specifications from legal
texts, reducing manual effort and providing structured artifacts useful for
implementation, assurance, and test generation.

</details>


### [28] [Towards an Architectural Perspective for Sustainability: Bundle the Needs from Industry](https://arxiv.org/abs/2508.20774)
*Markus Funke,Patricia Lago*

Main category: cs.SE

TL;DR: This paper introduces a sustainability perspective vision for software architecture, validated through literature snowballing and expert focus groups, offering framework-independent guidance to address sustainability concerns.


<details>
  <summary>Details</summary>
Motivation: Sustainability is an emerging quality property in software-intensive systems, but architects lack structured guidance to address it effectively during the design phase.

Method: The authors formulated the sustainability perspective vision using evidence from snowballing seminal literature and conducting a focus group with field experts.

Result: Findings confirm the relevance of perspective elements in practice and highlight implications for shaping a sustainability perspective that meets industrial needs.

Conclusion: The paper concludes that a sustainability perspective vision offers a structured approach to address sustainability concerns in software-intensive systems, independent of architecture frameworks and industry contexts.

Abstract: Sustainability is increasingly recognized as an emerging quality property in
software-intensive systems, yet architects lack structured guidance to address
it effectively throughout the software design phase. Architectural
perspectives-an architectural knowledge artifact composed of concerns,
activities, tactics, pitfalls, and checklists-offer a promising approach to
tackle such emerging quality properties across architectural views and are also
independent of architecture frameworks and industry contexts. In this paper, we
present a sustainability perspective vision, i.e., a revised notion of
architectural perspective meant to be filled with its own elements to target
sustainability concerns. We formulate our sustainability perspective vision
through evidence from applying snowballing to seminal literature and from
conducting a focus group with experts in the field. Our findings confirm the
relevance of the different perspective elements in practice and highlight
implications for shaping a sustainability perspective that meets industrial
needs.

</details>


### [29] [Automated Test Oracles for Flaky Cyber-Physical System Simulators: Approach and Evaluation](https://arxiv.org/abs/2508.20902)
*Baharin A. Jodat,Khouloud Gaaloul,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.SE

TL;DR: Developed interpretable test oracles for CPS using genetic programming (GP-Ochiai) that outperform existing approaches in accuracy and robustness against simulator flakiness through predicate-based assertions.


<details>
  <summary>Details</summary>
Motivation: Simulation-based testing for cyber-physical systems (CPS) is computationally expensive and vulnerable to simulator flakiness, requiring repeated test executions and increasing validation costs. Automated, interpretable test oracles are needed to improve cost-effectiveness and reliability.

Method: The paper introduces two oracle-generation methods: (1) Genetic Programming (GP) with fitness functions based on spectrum-based fault localization (Ochiai, Tarantula, Naish) and (2) Decision Trees/Decision Rules (DT/DR). The GP approach with Ochiai demonstrated superior performance.

Result: GP oracles with the Ochiai formula achieved the highest accuracy, maintaining effectiveness even with simulator flakiness. They showed only 4% average variation in accuracy across four flaky systems (networking/autonomous driving) compared to significantly lower performing Tarantula/Naish, DT, and DR approaches.

Conclusion: The proposed assertion-based test oracles using genetic programming (GP) with Ochiai significantly outperform alternative approaches in accuracy and robustness against simulator flakiness across aerospace, networking, and autonomous driving domains.

Abstract: Simulation-based testing of cyber-physical systems (CPS) is costly due to the
time-consuming execution of CPS simulators. In addition, CPS simulators may be
flaky, leading to inconsistent test outcomes and requiring repeated test
re-execution for reliable test verdicts. Automated test oracles that do not
require system execution are therefore crucial for reducing testing costs.
Ideally, such test oracles should be interpretable to facilitate human
understanding of test verdicts, and they must be robust against the potential
flakiness of CPS simulators. In this article, we propose assertion-based test
oracles for CPS as sets of logical and arithmetic predicates defined over the
inputs of the system under test. Given a test input, our assertion-based test
oracle determines, without requiring test execution, whether the test passes,
fails, or if the oracle is inconclusive in predicting a verdict. We describe
two methods for generating assertion-based test oracles: one using genetic
programming~(GP) that employs well-known spectrum-based fault localization
(SBFL) ranking formulas, namely Ochiai, Tarantula, and Naish, as fitness
functions; and the other using decision trees (DT) and decision rules (DR). We
evaluate our assertion-based test oracles through case studies in the domains
of aerospace, networking and autonomous driving. We show that test oracles
generated using GP with Ochiai are significantly more accurate than those
obtained using GP with Tarantula and Naish or using DT or DR. Moreover, this
accuracy advantage remains even when accounting for the flakiness of the system
under test. We further show that the assertion-based test oracles generated by
GP with Ochiai are robust against flakiness with only 4% average variation in
their accuracy results across four different network and autonomous driving
systems with flaky behaviours.

</details>


### [30] [Deep Learning Based Concurrency Bug Detection and Localization](https://arxiv.org/abs/2508.20911)
*Zuocheng Feng,Kaiwen Zhang,Miaomiao Wang,Yiming Cheng,Yuandao Cai,Xiaofeng Li,Guanjun Liu*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Concurrency bugs, caused by improper synchronization of shared resources in
multi-threaded or distributed systems, are notoriously hard to detect and thus
compromise software reliability and security. The existing deep learning
methods face three main limitations. First, there is an absence of large and
dedicated datasets of diverse concurrency bugs for them. Second, they lack
sufficient representation of concurrency semantics. Third, binary
classification results fail to provide finer-grained debug information such as
precise bug lines. To address these problems, we propose a novel method for
effective concurrency bug detection as well as localization. We construct a
dedicated concurrency bug dataset to facilitate model training and evaluation.
We then integrate a pre-trained model with a heterogeneous graph neural network
(GNN), by incorporating a new Concurrency-Aware Code Property Graph (CCPG) that
concisely and effectively characterizes concurrency semantics. To further
facilitate debugging, we employ SubgraphX, a GNN-based interpretability method,
which explores the graphs to precisely localize concurrency bugs, mapping them
to specific lines of source code. On average, our method demonstrates an
improvement of 10\% in accuracy and precision and 26\% in recall compared to
state-of-the-art methods across diverse evaluation settings.

</details>


### [31] [ConfLogger: Enhance Systems' Configuration Diagnosability through Configuration Logging](https://arxiv.org/abs/2508.20977)
*Shiwen Shan,Yintong Huo,Yuxin Su,Zhining Wang,Dan Li,Zibin Zheng*

Main category: cs.SE

TL;DR: ConfLogger enhances configuration diagnosability by combining static taint analysis with LLMs for log generation, achieving SOTA results in accuracy and diagnostic speed for solving misconfiguration issues.


<details>
  <summary>Details</summary>
Motivation: Current diagnosability methods focus on post-failure behavior analysis but lack mechanisms to ensure software logs sufficient failure information for effective configuration issue diagnosis, creating a critical gap in addressing misconfigurations and latent bugs.

Method: ConfLogger integrates configuration-aware static taint analysis to identify sensitive code segments and leverages LLM-based analysis to generate diagnostic logs from configuration code contexts, unifying these techniques to enhance logging practices at the source code level.

Result: ConfLogger achieves 100% error localization accuracy in 30 scenarios, 74% log coverage (12-30% better than LLM baselines), 79.3% higher recall, and 26.2% higher F1 score in variable logging. A user study showed 1.25Ã— faster troubleshooting with 251.4% improved accuracy.

Conclusion: ConfLogger effectively enhances software configuration diagnosability through its innovative combination of static taint analysis and LLM-based logging, demonstrating significant improvements in accuracy, coverage, and user efficiency over existing approaches.

Abstract: Modern configurable systems offer customization via intricate configuration
spaces, yet such flexibility introduces pervasive configuration-related issues
such as misconfigurations and latent softwarebugs. Existing diagnosability
supports focus on post-failure analysis of software behavior to identify
configuration issues, but none of these approaches look into whether the
software clue sufficient failure information for diagnosis. To fill in the
blank, we propose the idea of configuration logging to enhance existing logging
practices at the source code level. We develop ConfLogger, the first tool that
unifies configuration-aware static taint analysis with LLM-based log generation
to enhance software configuration diagnosability. Specifically, our method 1)
identifies configuration-sensitive code segments by tracing
configuration-related data flow in the whole project, and 2) generates
diagnostic log statements by analyzing configuration code contexts. Evaluation
results on eight popular software systems demonstrate the effectiveness of
ConfLogger to enhance configuration diagnosability. Specifically,
ConfLogger-enhanced logs successfully aid a log-based misconfiguration
diagnosis tool to achieve 100% accuracy on error localization in 30 silent
misconfiguration scenarios, with 80% directly resolvable through explicit
configuration information exposed. In addition, ConfLogger achieves 74%
coverage of existing logging points, outperforming baseline LLM-based loggers
by 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,
and 26.2% higher in F1 compared to the state-of-the-art baseline in terms of
variable logging while also augmenting diagnostic value. A controlled user
study on 22 cases further validated its utility, speeding up diagnostic time by
1.25x and improving troubleshooting accuracy by 251.4%.

</details>


### [32] [Dynamics of Gender Bias in Software Engineering](https://arxiv.org/abs/2508.21050)
*Thomas J. Misa*

Main category: cs.SE

TL;DR: This paper examines gender bias in software engineering history and finds systemic exclusion of women at ICSE, recommending policy changes to improve diversity.


<details>
  <summary>Details</summary>
Motivation: The paper addresses gender bias in software engineering, which inherits biases from engineering and computer science, aiming to understand its historical development and impact on diversity.

Method: The paper combines a historical survey of software engineering's origins and leadership with quantitative analysis of women's author participation in the ICSE conference from 1976-2010, identifying years of statistical gender exclusion.

Result: Analysis reveals a dozen years (1976-2010) with statistically significant gender exclusion in ICSE authorship, showing systemic underrepresentation of women in software engineering research.

Conclusion: The paper concludes by suggesting policy dimensions for addressing gender bias in computing, emphasizing the need for structured interventions to promote equity in software engineering research.

Abstract: The field of software engineering is embedded in both engineering and
computer science, and may embody gender biases endemic to both. This paper
surveys software engineering's origins and its long-running attention to
engineering professionalism, profiling five leaders; it then examines the
field's recent attention to gender issues and gender bias. It next
quantitatively analyzes women's participation as research authors in the
field's leading International Conference of Software Engineering (1976-2010),
finding a dozen years with statistically significant gender exclusion. Policy
dimensions of research on gender bias in computing are suggested.

</details>
