{"id": "2507.08061", "categories": ["cs.SE", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2507.08061", "abs": "https://arxiv.org/abs/2507.08061", "authors": ["Andrea Morales Coto", "Aditi Verma"], "title": "The State of Computational Science in Fission and Fusion Energy", "comment": null, "summary": "The tools used to engineer something are just as important as the thing that\nis actually being engineered. In fact, in many cases, the tools can indeed\ndetermine what is engineerable. In fusion and fission1 energy engineering,\nsoftware has become the dominant tool for design. For that reason, in 2024, for\nthe first time ever, we asked 103 computational scientists developing the codes\nused in fusion and fission energy about the problems they are attempting to\nsolve with their codes, the tools available to them to solve them, and their\nend to end developer experience with said tools.\n  The results revealed a changing tide in software tools in fusion and fission,\nwith more and more computational scientists preferring modern programming\nlanguages, open-source codes, and modular software. These trends represent a\npeek into what will happen 5 to 10 years in the future of nuclear engineering.\nSince the majority of our respondents belonged to US national labs and\nuniversities, these results hint at the most cutting-edge trends in the\nindustry. The insights included in the State of Computational Science in\nFission and Fusion Energy indicate a dramatic shift toward multiphysics codes,\na drop-off in the use of FORTRAN in favor of more modern languages like Python\nand C++, and ever-rising budgets for code development, at times reaching $50M\nin a single organization.\n  Our survey paints a future of nuclear engineering codes that is modular in\nnature, small in terms of compute, and increasingly prioritized by\norganizations. Access to our results in web form are available online.", "AI": {"tldr": "The 2024 survey of 103 computational scientists in nuclear energy reveals a shift toward modern programming languages (e.g., Python, C++), open-source, modular software, and growing development budgets (up to $50M) in fusion and fission energy engineering.", "motivation": "Software tools are critical in fusion/fission energy engineering, as they shape what is engineerable and remain underexplored in industry trends.", "method": "Survey of 103 computational scientists from US national labs and universities, analyzing their tool preferences and developer experiences.", "result": "Rising adoption of Python/C++ over FORTRAN, increased use of modular/multiphysics codes, open-source software popularity, and substantial development budgets (up to $50M).", "conclusion": "Nuclear energy codes are evolving toward modular, compute-efficient, and industry-prioritized designs, reflecting cutting-edge trends in software-driven engineering."}}
{"id": "2507.08149", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08149", "abs": "https://arxiv.org/abs/2507.08149", "authors": ["Valerie Chen", "Ameet Talwalkar", "Robert Brennan", "Graham Neubig"], "title": "Code with Me or for Me? How Increasing AI Automation Transforms Developer Workflows", "comment": null, "summary": "Developers now have access to a growing array of increasingly autonomous AI\ntools to support software development. While numerous studies have examined\ndeveloper use of copilots, which can provide chat assistance or code\ncompletions, evaluations of coding agents, which can automatically write files\nand run code, still largely rely on static benchmarks without\nhumans-in-the-loop. In this work, we conduct the first academic study to\nexplore developer interactions with coding agents and characterize how more\nautonomous AI tools affect user productivity and experience, compared to\nexisting copilots. We evaluate two leading copilot and agentic coding\nassistants, GitHub Copilot and OpenHands, recruiting participants who regularly\nuse the former. Our results show agents have the potential to assist developers\nin ways that surpass copilots (e.g., completing tasks that humans might not\nhave accomplished before) and reduce the user effort required to complete\ntasks. However, there are challenges involved in enabling their broader\nadoption, including how to ensure users have an adequate understanding of agent\nbehaviors. Our results not only provide insights into how developer workflows\nchange as a result of coding agents but also highlight how user interactions\nwith agents differ from those with existing copilots, motivating a set of\nrecommendations for researchers building new agents. Given the broad set of\ndevelopers who still largely rely on copilot-like systems, our work highlights\nkey challenges of adopting more agentic systems into developer workflows.", "AI": {"tldr": "This paper examines how autonomous coding agents impact developer productivity and experience compared to copilots like GitHub Copilot, identifying both benefits (task completion beyond human capability, reduced effort) and challenges (agent behavior understanding) in their adoption through empirical evaluation involving real developers.", "motivation": "While prior research has explored developer interactions with copilots, autonomous coding agents remain largely unexamined in human-in-the-loop studies. The paper motivates research by highlighting the gap in understanding how agentic AI tools alter developer workflows and how their broader adoption might be facilitated.", "method": "An empirical comparison of two systems: GitHub Copilot (copilot-like assistant) and OpenHands (agentic assistant). Participants regularly using GitHub Copilot were recruited to evaluate both tools, focusing on task completion, productivity, and user experience through direct human interaction rather than static benchmarks.", "result": "Agents outperformed copilots in enabling task completion (e.g., achieving results developers couldn't achieve independently) and reducing human effort. However, challenges in understanding agent behaviors emerged as a critical barrier to adoption. The study characterizes behavioral patterns and identifies fundamental differences in how developers interact with agentic vs. copilot tools.", "conclusion": "The work reveals significant opportunities for coding agents to enhance software development while emphasizing critical adoption challenges that must be addressed. Key insights include the need for better agent transparency, workflow adaptation requirements, and a framework of recommendations for building future agentic systems that bridge current capability-expectation gaps among developers."}}
{"id": "2507.08160", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08160", "abs": "https://arxiv.org/abs/2507.08160", "authors": ["Ot\u00e1vio Cury", "Guilherme Avelino"], "title": "The Impact of Generative AI on Code Expertise Models: An Exploratory Study", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) tools for source code generation\nhave significantly boosted productivity in software development. However, they\nalso raise concerns, particularly the risk that developers may rely heavily on\nthese tools, reducing their understanding of the generated code. We hypothesize\nthat this loss of understanding may be reflected in source code knowledge\nmodels, which are used to identify developer expertise. In this work, we\npresent an exploratory analysis of how a knowledge model and a Truck Factor\nalgorithm built upon it can be affected by GenAI usage. To investigate this, we\ncollected statistical data on the integration of ChatGPT-generated code into\nGitHub projects and simulated various scenarios by adjusting the degree of\nGenAI contribution. Our findings reveal that most scenarios led to measurable\nimpacts, indicating the sensitivity of current expertise metrics. This suggests\nthat as GenAI becomes more integrated into development workflows, the\nreliability of such metrics may decrease.", "AI": {"tldr": "The paper explores how generative AI (GenAI) tools in code generation may reduce developer understanding of code, affecting expertise metrics like knowledge models and the Truck Factor algorithm.", "motivation": "Developers' over-reliance on GenAI tools like ChatGPT poses risks to maintainable software development by creating knowledge gaps, yet the impact on expertise measurement remains understudied.", "method": "The researchers collected GitHub integration data for ChatGPT-generated code and simulated GenAI contribution levels to assess their effects on expertise metrics.", "result": "Simulations showed measurable impacts on current expertise metrics, indicating their sensitivity to decreased code understanding from GenAI reliance.", "conclusion": "As GenAI integration grows, current methods for evaluating developer expertise may become less reliable, requiring updated metrics."}}
{"id": "2507.08250", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08250", "abs": "https://arxiv.org/abs/2507.08250", "authors": ["Yasaman Abedini", "Abbas Heydarnoori"], "title": "Leveraging Large Language Models for Classifying App Users' Feedback", "comment": null, "summary": "In recent years, significant research has been conducted into classifying\napplication (app) user feedback, primarily relying on supervised machine\nlearning algorithms. However, fine-tuning more generalizable classifiers based\non existing labeled datasets remains an important challenge, as creating large\nand accurately labeled datasets often requires considerable time and resources.\nIn this paper, we evaluate the capabilities of four advanced LLMs, including\nGPT-3.5-Turbo, GPT-4, Flan-T5, and Llama3-70b, to enhance user feedback\nclassification and address the challenge of the limited labeled dataset. To\nachieve this, we conduct several experiments on eight datasets that have been\nmeticulously labeled in prior research. These datasets include user reviews\nfrom app stores, posts from the X platform, and discussions from the public\nforums, widely recognized as representative sources of app user feedback. We\nanalyze the performance of various LLMs in identifying both fine-grained and\ncoarse-grained user feedback categories. Given the substantial volume of daily\nuser feedback and the computational limitations of LLMs, we leverage these\nmodels as an annotation tool to augment labeled datasets with general and\napp-specific data. This augmentation aims to enhance the performance of\nstate-of-the-art BERT-based classification models. Our findings indicate that\nLLMs when guided by well-crafted prompts, can effectively classify user\nfeedback into coarse-grained categories. Moreover, augmenting the training\ndataset with datasets labeled using the consensus of LLMs can significantly\nenhance classifier performance.", "AI": {"tldr": "This paper evaluates four large language models (GPT-3.5-Turbo, GPT-4, Flan-T5, Llama3-70b) as annotation tools to augment user feedback datasets and improve BERT-based classification models when labeled data is limited.", "motivation": "Creating large, accurately labeled user feedback datasets for supervised machine learning is time-consuming and resource-intensive, necessitating more generalizable solutions for feedback classification.", "method": "The authors conducted experiments on eight well-labeled user feedback datasets (including app store reviews, X posts, and public forums) to assess LLM performance in coarse-grained and fine-grained classification tasks. LLM-generated labels were used to augment training data for state-of-the-art BERT models due to computational constraints and the high volume of user feedback.", "result": "LLMs demonstrated effectiveness in coarse-grained classification when using well-crafted prompts. Augmenting datasets with LLM-labeled data using consensus improved classifier performance significantly, particularly under limited labeled data conditions.", "conclusion": "Advanced LLMs can serve as viable annotation tools to address dataset limitations in user feedback classification. When leveraged strategically with prompt engineering and consensus-based labeling, they enhance the performance of BERT-based models with reduced dependency on large human-labeled datasets."}}
{"id": "2507.08158", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.08158", "abs": "https://arxiv.org/abs/2507.08158", "authors": ["Marika Swanberg", "Meenatchi Sundaram Muthu Selva Annamalai", "Jamie Hayes", "Borja Balle", "Adam Smith"], "title": "Beyond the Worst Case: Extending Differential Privacy Guarantees to Realistic Adversaries", "comment": null, "summary": "Differential Privacy (DP) is a family of definitions that bound the\nworst-case privacy leakage of a mechanism. One important feature of the\nworst-case DP guarantee is it naturally implies protections against adversaries\nwith less prior information, more sophisticated attack goals, and complex\nmeasures of a successful attack. However, the analytical tradeoffs between the\nadversarial model and the privacy protections conferred by DP are not well\nunderstood thus far. To that end, this work sheds light on what the worst-case\nguarantee of DP implies about the success of attackers that are more\nrepresentative of real-world privacy risks.\n  In this paper, we present a single flexible framework that generalizes and\nextends the patchwork of bounds on DP mechanisms found in prior work. Our\nframework allows us to compute high-probability guarantees for DP mechanisms on\na large family of natural attack settings that previous bounds do not capture.\nOne class of such settings is the approximate reconstruction of multiple\nindividuals' data, such as inferring nearly entire columns of a tabular data\nset from noisy marginals and extracting sensitive information from DP-trained\nlanguage models.\n  We conduct two empirical case studies to illustrate the versatility of our\nbounds and compare them to the success of state-of-the-art attacks.\nSpecifically, we study attacks that extract non-uniform PII from a DP-trained\nlanguage model, as well as multi-column reconstruction attacks where the\nadversary has access to some columns in the clear and attempts to reconstruct\nthe remaining columns for each person's record. We find that the absolute\nprivacy risk of attacking non-uniform data is highly dependent on the\nadversary's prior probability of success. Our high probability bounds give us a\nnuanced understanding of the privacy leakage of DP mechanisms in a variety of\npreviously understudied attack settings.", "AI": {"tldr": "This paper establishes a flexible framework to derive high-probability bounds for differential privacy mechanisms in both unexplored and real-world attack scenarios.", "motivation": "Existing differential privacy analyses focus on worst-case guarantees, but quantitative tradeoffs between adversarial models and privacy protections remain poorly understood.", "method": "Developing a generalized analytical framework through careful reconstruction risk analysis and empirical validation against state-of-the-art attacks in two critical domains", "result": "The framework quantitatively demonstrates how DP protections against multi-column reconstruction attacks and sensitive information extraction from language models depend on adversarial prior probability, with empirical validation showing its effectiveness", "conclusion": "The high-probability bounds enable nuanced privacy risk assessment across diverse attack settings, complementing traditional DP guarantees while addressing practical limitations"}}
{"id": "2507.08467", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08467", "abs": "https://arxiv.org/abs/2507.08467", "authors": ["Youshuai Tan", "Zhanwei Zhang", "Jinfu Chen", "Zishuo Ding", "Jifeng Xuan", "Weiyi Shang"], "title": "Computing Floating-Point Errors by Injecting Perturbations", "comment": "arXiv admin note: text overlap with arXiv:2412.20804", "summary": "Floating-point programs form the foundation of modern science and\nengineering, providing the essential computational framework for a wide range\nof applications, such as safety-critical systems, aerospace engineering, and\nfinancial analysis. Floating-point errors can lead to severe consequences.\nAlthough floating-point errors widely exist, only a subset of inputs may\ntrigger significant errors in floating-point programs. Therefore, it is crucial\nto determine whether a given input could produce such errors. Researchers tend\nto take the results of high-precision floating-point programs as oracles for\ndetecting floating-point errors, which introduces two main limitations: (1)\ndifficulty of implementation and (2) prolonged execution time. The two recent\ntools, ATOMU and FPCC, can partially address these issues. However, ATOMU\nsuffers from false positives; while FPCC, though eliminating false positives,\noperates at a considerably slower speed.\n  To address these two challenges, we propose a novel approach named\nPI-detector to computing floating-point errors effectively and efficiently. Our\napproach is based on the observation that floating-point errors stem from large\ncondition numbers in atomic operations (such as addition and subtraction),\nwhich then propagate and accumulate. PI-detector injects small perturbations\ninto the operands of individual atomic operations within the program and\ncompares the outcomes of the original program with the perturbed version to\ncompute floating-point errors. We evaluate PI-detector with datasets from ATOMU\nand HSED, as well as a complex linear system-solving program. Experimental\nresults demonstrate that PI-detector can perform efficient and accurate\nfloating-point error computation.", "AI": {"tldr": "The paper proposes PI-detector, a new method for efficiently computing floating-point errors in programs by perturbing atomic operations and comparing results, addressing limitations of existing tools like ATOMU and FPCC that face false positives or slow performance.", "motivation": "Floating-point errors in scientific and engineering applications can have severe consequences, yet current error detection methods relying on high-precision oracles are either inaccurate (false positives) or computationally expensive, necessitating a more effective and efficient solution.", "method": "PI-detector injects small perturbations into operands of atomic operations (addition/subtraction) in the program, then computes and compares the program's original output against the perturbed version to identify floating-point errors.", "result": "Experimental evaluations on ATOMU, HSED datasets, and a complex linear system-solving program demonstrate that PI-detector achieves efficient and accurate floating-point error computation without the limitations of previous tools.", "conclusion": "PI-detector effectively addresses floating-point error detection by leveraging operand perturbation in atomic operations, offering a faster and more precise alternative to existing solutions."}}
{"id": "2507.08166", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.08166", "abs": "https://arxiv.org/abs/2507.08166", "authors": ["Chris S. Lin", "Joyce Qu", "Gururaj Saileshwar"], "title": "GPUHammer: Rowhammer Attacks on GPU Memories are Practical", "comment": "20 pages, including appendices. The paper will appear in SEC'25", "summary": "Rowhammer is a read disturbance vulnerability in modern DRAM that causes\nbit-flips, compromising security and reliability. While extensively studied on\nIntel and AMD CPUs with DDR and LPDDR memories, its impact on GPUs using GDDR\nmemories, critical for emerging machine learning applications, remains\nunexplored. Rowhammer attacks on GPUs face unique challenges: (1) proprietary\nmapping of physical memory to GDDR banks and rows, (2) high memory latency and\nfaster refresh rates that hinder effective hammering, and (3) proprietary\nmitigations in GDDR memories, difficult to reverse-engineer without FPGA-based\ntest platforms. We introduce GPUHammer, the first Rowhammer attack on NVIDIA\nGPUs with GDDR6 DRAM. GPUHammer proposes novel techniques to reverse-engineer\nGDDR DRAM row mappings, and employs GPU-specific memory access optimizations to\namplify hammering intensity and bypass mitigations. Thus, we demonstrate the\nfirst successful Rowhammer attack on a discrete GPU, injecting up to 8\nbit-flips across 4 DRAM banks on an NVIDIA A6000 with GDDR6 memory. We also\nshow how an attacker can use these to tamper with ML models, causing\nsignificant accuracy drops (up to 80%).", "AI": {"tldr": "GPUHammer demonstrates the first Rowhammer attack on NVIDIA GDDR6 GPUs, enabling ML model tampering with up to 80% accuracy degradation.", "motivation": "Rowhammer vulnerabilities in CPU DRAMs are well studied, but GPU GDDR memories' security implications for ML applications remain unexplored.", "method": "Reverse-engineered GDDR6 row mappings and optimized GPU memory access to bypass hardware mitigations through proprietary attack techniques.", "result": "Successfully induced 8 bit-flips across 4 banks on NVIDIA A6000 GPUs, proving ML model manipulation feasibility with 80% accuracy drop demonstrations.", "conclusion": "Exposes critical GDDR6 security gaps in GPU architecture, challenging assumptions about proprietary memory protections and emphasizing ML infrastructure vulnerabilities."}}
{"id": "2507.08523", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08523", "abs": "https://arxiv.org/abs/2507.08523", "authors": ["Yilun Wang", "Pengfei Chen", "Haiyu Huang", "Zilong He", "Gou Tan", "Chuanfu Zhang", "Jingkai He", "Zibin Zheng"], "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via ICL-oriented Prefix Caching", "comment": null, "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.", "AI": {"tldr": "InferLog is an LLM inference optimization method for online log parsing that addresses privacy and latency issues in production environments without compromising accuracy.", "motivation": "Commercial LLM log parsers face deployment challenges due to privacy risks and latency/throughput limitations. Existing optimization methods reduce query counts but overlook per-invocation latency exacerbating performance under concurrent workloads.", "method": "1) Prefix-aware ICL Refinement policy to enhance prefix caching efficiency via example refinement and permutation. 2) Meta-learning pipeline for task-specific configuration tuning to find optimal scheduling parameters for dynamic workloads.", "result": "Experiments on Loghub and vLLM show InferLog outperforms existing optimization methods by significantly accelerating state-of-the-art LLM-based parsing while maintaining accuracy.", "conclusion": "InferLog identifies inference efficiency as the critical bottleneck in online log parsing, offering an effective optimization framework that makes LLM deployment practical for high-volume log streams with real-time constraints."}}
{"id": "2507.08286", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.08286", "abs": "https://arxiv.org/abs/2507.08286", "authors": ["Aufa Nasywa Rahman", "Bimo Sunarfri Hantono", "Guntur Dharma Putra"], "title": "TruChain: A Multi-Layer Architecture for Trusted, Verifiable, and Immutable Open Banking Data", "comment": "8 pages, 7 figures. Accepted to IEEE MetaCom 2025", "summary": "Open banking framework enables third party providers to access financial data\nacross banking institutions, leading to unprecedented innovations in the\nfinancial sector. However, some open banking standards remain susceptible to\nsevere technological risks, including unverified data sources, inconsistent\ndata integrity, and lack of immutability. In this paper, we propose a layered\narchitecture that provides assurance in data trustworthiness with three\ndistinct levels of trust, covering source validation, data-level\nauthentication, and tamper-proof storage. The first layer guarantees the source\nlegitimacy using decentralized identity and verifiable presentation, while the\nsecond layer verifies data authenticity and consistency using cryptographic\nsigning. Lastly, the third layer guarantees data immutability through the\nTangle, a directed acyclic graph distributed ledger. We implemented a\nproof-of-concept implementation of our solution to evaluate its performance,\nwhere the results demonstrate that the system scales linearly with a stable\nthroughput, exhibits a 100% validation rate, and utilizes under 35% of CPU and\n350 MiB memory. Compared to a real-world open banking implementation, our\nsolution offers significantly reduced latency and stronger data integrity\nassurance. Overall, our solution offers a practical and efficient system for\nsecure data sharing in financial ecosystems while maintaining regulatory\ncompliance.", "AI": {"tldr": "The paper addresses technological risks in open banking by proposing a three-layered architecture using decentralized identity, cryptographic signing, and Tangle for trustworthiness, demonstrating efficient scalability and strong data integrity.", "motivation": "Current open banking standards face severe risks like unverified data sources, inconsistent integrity, and lack of immutability, threatening secure data sharing and regulatory compliance.", "method": "A layered architecture: Layer 1 ensures source validation via decentralized identity and verifiable presentations; Layer 2 guarantees data authenticity using cryptographic signing; Layer 3 ensures immutability through the Tangle DAG ledger.", "result": "Proof-of-concept shows linear scalability, 100% validation rate, <35% CPU usage, 350 MiB memory consumption, and significantly reduced latency compared to real-world implementations with robust data integrity.", "conclusion": "The proposed solution provides a practical, efficient, and compliant system for secure open banking data sharing, mitigating critical risks while maintaining high performance and trust."}}
{"id": "2507.08594", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.08594", "abs": "https://arxiv.org/abs/2507.08594", "authors": ["Fernando Ayach", "Vitor Lameir\u00e3o", "Raul Le\u00e3o", "Jerfferson Felizardo", "Rafael Sobrinho", "Vanessa Borges", "Patr\u00edcia Matsubara", "Awdren Font\u00e3o"], "title": "Generating Proto-Personas through Prompt Engineering: A Case Study on Efficiency, Effectiveness and Empathy", "comment": "12 pages; 2 figures; Preprint with the original submission accepted\n  for publication at 39th Brazilian Symposium on Software Engineering (SBES)", "summary": "Proto-personas are commonly used during early-stage Product Discovery, such\nas Lean Inception, to guide product definition and stakeholder alignment.\nHowever, the manual creation of proto-personas is often time-consuming,\ncognitively demanding, and prone to bias. In this paper, we propose and\nempirically investigate a prompt engineering-based approach to generate\nproto-personas with the support of Generative AI (GenAI). Our goal is to\nevaluate the approach in terms of efficiency, effectiveness, user acceptance,\nand the empathy elicited by the generated personas. We conducted a case study\nwith 19 participants embedded in a real Lean Inception, employing a qualitative\nand quantitative methods design. The results reveal the approach's efficiency\nby reducing time and effort and improving the quality and reusability of\npersonas in later discovery phases, such as Minimum Viable Product (MVP)\nscoping and feature refinement. While acceptance was generally high, especially\nregarding perceived usefulness and ease of use, participants noted limitations\nrelated to generalization and domain specificity. Furthermore, although\ncognitive empathy was strongly supported, affective and behavioral empathy\nvaried significantly across participants. These results contribute novel\nempirical evidence on how GenAI can be effectively integrated into software\nProduct Discovery practices, while also identifying key challenges to be\naddressed in future iterations of such hybrid design processes.", "AI": {"tldr": "This paper proposes a prompt engineering-based Generative AI (GenAI) approach for creating proto-personas in software Product Discovery, validated through a case study with 19 participants in a real Lean Inception.", "motivation": "Manual proto-persona creation for Product Discovery is time-intensive, cognitively demanding, and prone to bias, motivating the need for a GenAI-supported approach to enhance efficiency and objectivity.", "method": "The authors developed and evaluated a GenAI-based proto-persona generation approach using prompt engineering, employing mixed methods (qualitative and quantitative) in a real-world case study during Lean Inception with 19 participants.", "result": "The GenAI approach reduced creation time/effort, improved persona quality and reusability for downstream tasks (MVP scoping, feature refinement), achieved high user acceptance (usefulness/ease of use), but showed limitations in generalization and domain specificity. Empathy outcomes were mixed, with cognitive empathy strongly supported but affective/behavioral empathy varying.", "conclusion": "GenAI can effectively enhance Product Discovery by generating proto-personas more efficiently, though challenges remain in generalization and balanced empathy elicitation. The study provides empirical guidance for hybrid AI-driven design processes."}}
{"id": "2507.08288", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08288", "abs": "https://arxiv.org/abs/2507.08288", "authors": ["Qingxiao Guo", "Xinjie Zhu", "Yilong Ma", "Hui Jin", "Yunhao Wang", "Weifeng Zhang", "Xiaobing Guo"], "title": "Invariant-based Robust Weights Watermark for Large Language Models", "comment": null, "summary": "Watermarking technology has gained significant attention due to the\nincreasing importance of intellectual property (IP) rights, particularly with\nthe growing deployment of large language models (LLMs) on billions\nresource-constrained edge devices. To counter the potential threats of IP theft\nby malicious users, this paper introduces a robust watermarking scheme without\nretraining or fine-tuning for transformer models. The scheme generates a unique\nkey for each user and derives a stable watermark value by solving linear\nconstraints constructed from model invariants. Moreover, this technology\nutilizes noise mechanism to hide watermark locations in multi-user scenarios\nagainst collusion attack. This paper evaluates the approach on three popular\nmodels (Llama3, Phi3, Gemma), and the experimental results confirm the strong\nrobustness across a range of attack methods (fine-tuning, pruning,\nquantization, permutation, scaling, reversible matrix and collusion attacks).", "AI": {"tldr": "The paper proposes a post-training watermarking method for transformer models (LLMs) that generates per-user watermarks through linear constrained model invariants, offering robustness against IP theft and collusion attacks without requiring retraining.", "motivation": "With billions of edge devices hosting LLMs, protecting intellectual property from malicious IP theft (including collusion attacks among multiple users) has become critical. Existing watermarking techniques often require retraining or fine-tuning, which is computationally expensive and impractical for distributed scenarios.", "method": "1) Generates unique user keys for watermarking\n2) Derives stable watermarks by solving linear constraints based on model invariants\n3) Implements a noise mechanism to obscure watermark locations in multi-user settings\n4) Requires no retraining or model modification", "result": "Demonstrated strong robustness against:\n- Fine-tuning, pruning, and quantization\n- Structural attacks (permutation, scaling, reversible matrix)\n- Collusion attacks\nEvaluated on Llama3, Phi3, and Gemma models showing consistent performance", "conclusion": "This watermarking approach provides a practical solution for securing distributed LLMs with minimal overhead, maintaining watermark integrity under diverse attacks and multi-user collusion scenarios while avoiding the need for model retraining."}}
{"id": "2507.08627", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08627", "abs": "https://arxiv.org/abs/2507.08627", "authors": ["Chi-en Amy Tai", "Pengyu Nie", "Lukasz Golab", "Alexander Wong"], "title": "NL in the Middle: Code Translation with LLMs and Intermediate Representations", "comment": null, "summary": "Studies show that large language models (LLMs) produce buggy code\ntranslations. One avenue to improve translation accuracy is through\nintermediate representations, which could provide structured insights to guide\nthe model's understanding. We explore whether code translation using LLMs can\nbenefit from intermediate representations via natural language (NL) and\nabstract syntax trees (ASTs). Since prompt engineering greatly affects LLM\nperformance, we consider several ways to integrate these representations, from\none-shot to chain-of-thought (CoT) prompting. Using Open Gpt4 8X7B and\nspecialized StarCoder and CodeGen models on popular code translation benchmarks\n(CodeNet and AVATAR), we find that CoT with an intermediate NL summary performs\nbest, with an increase of 13.8% and 6.7%, respectively, in successful\ntranslations for the best-performing model (Open Gpt4 8X7B) compared to the\nzero-shot prompt.", "AI": {"tldr": "The paper investigates leveraging intermediate representations (natural language summaries and abstract syntax trees) in code translation LLMs via prompt engineering. CoT prompting with natural language summaries improved translation success rates by 13.8% and 6.7% on CodeNet and AVATAR benchmarks.", "motivation": "LLMs frequently generate buggy code translations, suggesting a need for architectural or prompting innovations that provide structured guidance during translation.", "method": "Experimental analysis of one-shot vs chain-of-thought (CoT) prompting methods with Open Gpt4 8X7B, StarCoder, and CodeGen models on CodeNet and AVATAR code translation benchmarks.", "result": "CoT with intermediate natural language summaries achieved highest gains, with Open Gpt4 8X7B showing 13.8% and 6.7% increases in successful translations on CodeNet and AVATAR compared to zero-shot baselines.", "conclusion": "Prompt engineering that incorporates intermediate representations, particularly natural language summaries in chain-of-thought format, significantly improves code translation accuracy for large language models."}}
{"id": "2507.08312", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.08312", "abs": "https://arxiv.org/abs/2507.08312", "authors": ["Jesus Lopez", "Viviana Cadena", "Mohammad Saidur Rahman"], "title": "Evaluating Post-Quantum Cryptographic Algorithms on Resource-Constrained Devices", "comment": "8 pages, 4 figures, 4 tables. This paper is accepted at the IEEE\n  Quantum Week 2025 -- IEEE International Conference on Quantum Computing and\n  Engineering (QCE) 2025", "summary": "The rapid advancement of quantum computing poses a critical threat to\nclassical cryptographic algorithms such as RSA and ECC, particularly in\nInternet of Things (IoT) devices, where secure communication is essential but\noften constrained by limited computational resources. This paper investigates\nthe feasibility of deploying post-quantum cryptography (PQC) algorithms on\nresource-constrained devices. In particular, we implement three PQC algorithms\n-- BIKE, CRYSTALS-Kyber, and HQC -- on a lightweight IoT platform built with\nRaspberry Pi devices. Leveraging the Open Quantum Safe (\\texttt{liboqs})\nlibrary in conjunction with \\texttt{mbedTLS}, we develop quantum-secure key\nexchange protocols, and evaluate their performance in terms of computational\noverhead, memory usage, and energy consumption for quantum secure\ncommunication. Experimental results demonstrate that the integration of PQC\nalgorithms on constrained hardware is practical, reinforcing the urgent need\nfor quantum-resilient cryptographic frameworks in next-generation IoT devices.\nThe implementation of this paper is available at\nhttps://iqsec-lab.github.io/PQC-IoT/.", "AI": {"tldr": "This paper evaluates post-quantum cryptography (PQC) algorithms BIKE, CRYSTALS-Kyber, and HQC on Raspberry Pi-based IoT devices, confirming their practical deployment despite constrained resources through performance analysis.", "motivation": "Quantum computing threatens classical cryptographic methods (RSA, ECC) in IoT devices, which rely on secure but resource-constrained communication systems. Transitioning to quantum-secure protocols is urgently needed.", "method": "1. Implemented BIKE, CRYSTALS-Kyber, HQC on lightweight IoT devices using Raspberry Pi hardware. 2. Integrated Open Quantum Safe (liboqs) with mbedTLS. 3. Evaluated computational overhead, memory usage, and energy consumption for key exchange protocols.", "result": "All three PQC algorithms achieved practical performance on constrained hardware: - Computational overhead, memory usage, and energy consumption measurements captured - Protocols demonstrated functional feasibility in real-world IoT environments - Direct comparison of resource requirements between BIKE, Kyber, and HQC", "conclusion": "Quantum-secure cryptographic implementation is achievable in IoT devices despite resource limitations. Next-generation IoT systems require immediate integration of PQC frameworks to ensure long-term security against quantum threats."}}
{"id": "2507.08671", "categories": ["cs.SE", "D.2.3; D.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.08671", "abs": "https://arxiv.org/abs/2507.08671", "authors": ["Hua Ge", "Juan Zhai", "Minxue Pan", "Fusen He", "Ziyue Tan"], "title": "LLMCup: Ranking-Enhanced Comment Updating with LLMs", "comment": "13 pages, 10 figures", "summary": "While comments are essential for enhancing code readability and\nmaintainability in modern software projects, developers are often motivated to\nupdate code but not comments, leading to outdated or inconsistent documentation\nthat hinders future understanding and maintenance. Recent approaches such as\nCUP and HebCup have attempted automatic comment updating using neural\nsequence-to-sequence models and heuristic rules, respectively. However, these\nmethods can miss or misinterpret crucial information during comment updating,\nresulting in inaccurate comments, and they often struggle with complex update\nscenarios. Given these challenges, a promising direction lies in leveraging\nlarge language models (LLMs), which have shown impressive performance in\nsoftware engineering tasks such as comment generation, code synthesis, and\nprogram repair. This suggests their strong potential to capture the logic\nbehind code modifications - an ability that is crucial for the task of comment\nupdating. Nevertheless, selecting an appropriate prompt strategy for an LLM on\neach update case remains challenging. To address this, we propose a novel\ncomment updating framework, LLMCup, which first uses multiple prompt strategies\nto provide diverse candidate updated comments via an LLM, and then employs a\nranking model, CupRank, to select the best candidate as final updated comment.\nExperimental results demonstrate the effectiveness of LLMCup, with improvements\nover state-of-the-art baselines (CUP and HebCup) by 49.0%-116.9% in Accuracy,\n10.8%-20% in BLEU-4, 4.6% in METEOR, 0.9%-1.9% in F1, and 2.1%-3.4% in\nSentenceBert similarity. Furthermore, a user study shows that comments updated\nby LLMCup sometimes surpass human-written updates, highlighting the importance\nof incorporating human evaluation in comment quality assessment.", "AI": {"tldr": "LLMCup is a novel comment updating framework using diverse LLM prompts and a ranking model (CupRank) to improve accuracy and quality, outperforming CUP/HebCup by 49.0%-116.9% in accuracy with human evaluation surpassing in some cases.", "motivation": "Existing comment-updating methods (e.g., CUP, HebCup) often miss/misinterpret code logic during updates, leading to inaccurate documentation for complex software changes.", "method": "1) Generates diverse candidate comments via LLM using multiple prompt strategies 2) Applies CupRank ranking model to select optimal output", "result": "49.0%-116.9% accuracy improvement over SOTA baselines, 10.8%-20% BLEU-4 gain, and user studies showing AI-generated comments outperforming human-written updates in certain cases", "conclusion": "LLMs with strategic prompting can surpass existing methods in comment updating, but require effective diversity/ranking mechanisms to maximize utility while emphasizing the need for human evaluation in quality assessment"}}
{"id": "2507.08331", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.08331", "abs": "https://arxiv.org/abs/2507.08331", "authors": ["Chun-I Fan", "Li-En Chang", "Cheng-Han Shie"], "title": "Qualcomm Trusted Application Emulation for Fuzzing Testing", "comment": "This work is currently under review for presentation at the USENIX\n  Security 2025 poster session", "summary": "In recent years, the increasing awareness of cybersecurity has led to a\nheightened focus on information security within hardware devices and products.\nIncorporating Trusted Execution Environments (TEEs) into product designs has\nbecome a standard practice for safeguarding sensitive user information.\nHowever, vulnerabilities within these components present significant risks, if\nexploited by attackers, these vulnerabilities could lead to the leakage of\nsensitive data, thereby compromising user privacy and security. This research\ncenters on trusted applications (TAs) within the Qualcomm TEE and introduces a\nnovel emulator specifically designed for these applications. Through reverse\nengineering techniques, we thoroughly analyze Qualcomm TAs and develop a\npartial emulation environment that accurately emulates their behavior.\nAdditionally, we integrate fuzzing testing techniques into the emulator to\nsystematically uncover potential vulnerabilities within Qualcomm TAs,\ndemonstrating its practical effectiveness in identifying real-world security\nflaws. This research makes a significant contribution by being the first to\nprovide both the implementation methods and source codes for a Qualcomm TAs\nemulator, offering a valuable reference for future research efforts. Unlike\nprevious approaches that relied on complex and resource-intensive full-system\nsimulations, our approach is lightweight and effective, making security testing\nof TA more convenient.", "AI": {"tldr": "This paper proposes a novel lightweight emulator for Qualcomm Trusted Execution Environment (TEE) applications, combining reverse engineering and fuzzing testing to identify vulnerabilities efficiently.", "motivation": "Vulnerabilities in TEEs pose significant risks to user data and privacy. Existing security testing methods for Qualcomm TAs rely on complex full-system simulations, necessitating a more practical and resource-efficient approach.", "method": "The authors use reverse engineering techniques to analyze Qualcomm TAs and construct a partial emulation environment. They further integrate fuzzing techniques into the emulator to systematically detect vulnerabilities.", "result": "The developed emulator successfully identifies real-world security flaws in Qualcomm TAs and provides implementation details and source code, marking the first such open solution for Qualcomm TEE applications.", "conclusion": "The research establishes a practical, scalable framework for TEE security testing, offering both a reference implementation and a convenient alternative to traditional simulation-based approaches."}}
{"id": "2507.08730", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08730", "abs": "https://arxiv.org/abs/2507.08730", "authors": ["Zezhen Xiang", "Jingzhi Gong", "Tao Chen"], "title": "Dually Hierarchical Drift Adaptation for Online Configuration Performance Learning", "comment": "Accepted by ICSE 2026", "summary": "Modern configurable software systems need to learn models that correlate\nconfiguration and performance. However, when the system operates in dynamic\nenvironments, the workload variations, hardware changes, and system updates\nwill inevitably introduce concept drifts at different levels - global drifts,\nwhich reshape the performance landscape of the entire configuration space; and\nlocal drifts, which only affect certain sub-regions of that space. As such,\nexisting offline and transfer learning approaches can struggle to adapt to\nthese implicit and unpredictable changes in real-time, rendering configuration\nperformance learning challenging. To address this, we propose DHDA, an online\nconfiguration performance learning framework designed to capture and adapt to\nthese drifts at different levels. The key idea is that DHDA adapts to both the\nlocal and global drifts using dually hierarchical adaptation: at the upper\nlevel, we redivide the data into different divisions, within each of which the\nlocal model is retrained, to handle global drifts only when necessary. At the\nlower level, the local models of the divisions can detect local drifts and\nadapt themselves asynchronously. To balance responsiveness and efficiency, DHDA\ncombines incremental updates with periodic full retraining to minimize\nredundant computation when no drifts are detected. Through evaluating eight\nsoftware systems and against state-of-the-art approaches, we show that DHDA\nachieves considerably better accuracy and can effectively adapt to drifts with\nup to 2x improvements, while incurring reasonable overhead and is able to\nimprove different local models in handling concept drift.", "AI": {"tldr": "DHDA is an online configuration performance learning framework that addresses global and local concept drifts in dynamic environments using dually hierarchical adaptation, achieving 2x accuracy improvements over existing methods.", "motivation": "Existing offline/transfer learning approaches struggle with real-time adaptation to global (entire performance landscape) and local (sub-region) concept drifts caused by workload changes, hardware updates, and system dynamics.", "method": "Dually hierarchical framework: upper-level periodically re-divides data into divisions for global drift adaptation, while lower-level local models handle local drifts via asynchronous updates. Combines incremental learning with periodic full retraining to balance efficiency.", "result": "Outperforms state-of-the-art methods on eight software systems by 2x in accuracy, adapts effectively to drifts with reasonable overhead, and demonstrates improved local model handling of concept drift.", "conclusion": "DHDA successfully addresses multi-level concept drifts in configuration performance learning through hierarchical adaptation, offering superior accuracy and adaptability while maintaining computational efficiency in dynamic environments."}}
{"id": "2507.08540", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08540", "abs": "https://arxiv.org/abs/2507.08540", "authors": ["Ioannis Lamprou", "Alexander Shevtsov", "Ioannis Arapakis", "Sotiris Ioannidis"], "title": "White-Basilisk: A Hybrid Model for Code Vulnerability Detection", "comment": null, "summary": "The proliferation of software vulnerabilities presents a significant\nchallenge to cybersecurity, necessitating more effective detection\nmethodologies. We introduce White-Basilisk, a novel approach to vulnerability\ndetection that demonstrates superior performance while challenging prevailing\nassumptions in AI model scaling. Utilizing an innovative architecture that\nintegrates Mamba layers, linear self-attention, and a Mixture of Experts\nframework, White-Basilisk achieves state-of-the-art results in vulnerability\ndetection tasks with a parameter count of only 200M. The model's capacity to\nprocess sequences of unprecedented length enables comprehensive analysis of\nextensive codebases in a single pass, surpassing the context limitations of\ncurrent Large Language Models (LLMs). White-Basilisk exhibits robust\nperformance on imbalanced, real-world datasets, while maintaining computational\nefficiency that facilitates deployment across diverse organizational scales.\nThis research not only establishes new benchmarks in code security but also\nprovides empirical evidence that compact, efficiently designed models can\noutperform larger counterparts in specialized tasks, potentially redefining\noptimization strategies in AI development for domain-specific applications.", "AI": {"tldr": "White-Basilisk is a 200M-parameter vulnerability detection model using Mamba layers, linear self-attention, and Mixture of Experts to process long code sequences efficiently, outperforming current LLMs in specialized tasks.", "motivation": "Software vulnerabilities require improved detection methods. Existing LLMs struggle with long code contexts and have overestimated scaling assumptions.", "method": "Combines Mamba sequence layers, linear self-attention for long-range analysis, and a Mixture of Experts framework, enabling 200M-parameter efficient processing of extensive codebases.", "result": "Achieves state-of-the-art vulnerability detection performance, outperforms LLMs in imbalanced real-world datasets, and processes long code sequences in one pass.", "conclusion": "Demonstrates compact model designs can surpass larger models in domain-specific cybersecurity tasks, providing empirical evidence to rethink AI scaling assumptions for security applications."}}
