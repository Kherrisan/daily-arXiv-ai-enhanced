{"id": "2508.20119", "categories": ["cs.SE", "cs.LG", "68T42", "I.2.6; I.2.2; D.2.2"], "pdf": "https://arxiv.org/pdf/2508.20119", "abs": "https://arxiv.org/abs/2508.20119", "authors": ["Daniel M. Yellin"], "title": "Evaluating LLMs on microservice-based applications: how complex is your specification?", "comment": "20 pages + 7 pages appendices. 7 Figures. 8 Tables", "summary": "In this paper we evaluate how far LLMs have advanced in generating code for\nreal-world problems. Specifically, we explore code synthesis for\nmicroservice-based applications, a widely used architecture pattern. We define\na standard template for specifying these applications, and we propose a metric\nfor judging the difficulty level of a specification. The higher the score, the\nmore difficult it is to generate code for the specification. We develop a\nframework to automate the process of testing LLM-synthesized code for a\nmicroservice using unit tests. Our experimental results show that strong LLMs\n(like GPT-3o-mini) do fairly well on medium difficulty specifications but do\nvery poorly on those of higher difficulty levels. This is due to more intricate\nbusiness logic, a greater use of external services, database integration and\ninclusion of non-functional capabilities such as authentication. We analyzed\nthe errors in LLM-synthesized code and report on the key challenges LLMs face\nin generating code for these specifications thereby suggesting future research\ndirections to improve code synthesis for real-world problems.", "AI": {"tldr": "This paper evaluates LLM code generation for microservices, finding that while models perform adequately on basic tasks, they fail at complex real-world scenarios due to insufficient handling of advanced infrastructure and business logic requirements.", "motivation": "The study investigates the limitations of LLMs in generating production-level microservice code, addressing gaps in understanding their performance beyond simple benchmarks to inform practical deployment.", "method": "The authors created a framework for automated unit testing of LLM-synthesized microservice code, along with a metric to evaluate specification difficulty based on factors like complexity and external dependencies.", "result": "Strong LLMs like GPT-3o-mini achieve good results on medium-complexity specifications but show poor performance on high-complexity ones involving databases, external services, and security requirements.", "conclusion": "LLMs struggle with complex real-world code generation due to intricate business logic and non-functional requirements, necessitating future research to address these challenges."}}
{"id": "2508.20124", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20124", "abs": "https://arxiv.org/abs/2508.20124", "authors": ["Yunlong Feng", "Yang Xu", "Xiao Xu", "Binyuan Hui", "Junyang Lin"], "title": "Towards Better Correctness and Efficiency in Code Generation", "comment": null, "summary": "While code large language models have demonstrated remarkable progress in\ncode generation, the generated code often exhibits poor runtime efficiency,\nlimiting its practical application in performance-sensitive scenarios. To\naddress this limitation, we propose an efficiency-oriented reinforcement\nlearning framework guided by a novel performance reward. Based on this\nframework, we take a deeper dive into the code efficiency problem, identifying\nthen proposing methods to overcome key bottlenecks: (1) Dynamic exploration\novercomes the static data constraints of offline fine-tuning, enabling the\ndiscovery of more efficient code implementations. (2) The error-insensitive\nreinforcement learning method and high-contrast efficiency signals are crucial\nfor mitigating systematic errors and achieving effective optimization. (3)\nOnline exploration is most effective when starting from a high-correctness\nbaseline, as this allows for efficiency improvements without sacrificing\naccuracy. With these discoveries, we finally propose a two-stage tuning method,\nwhich achieves high and balanced performance across correctness and efficiency.\nThe results of experiments show the effectiveness of the method, which improves\ncode correctness by 10.18\\% and runtime efficiency by 7.75\\% on a 7B model,\nachieving performance comparable to much larger model.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.20340", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.20340", "abs": "https://arxiv.org/abs/2508.20340", "authors": ["Maolin Sun", "Yibiao Yang", "Yuming Zhou"], "title": "Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators", "comment": null, "summary": "Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems\nand programming languages research, providing the foundation for tasks like\nsymbolic execution and automated verification. Because these solvers sit on the\ncritical path, their correctness is essential, and high-quality test formulas\nare key to uncovering bugs. However, while prior testing techniques performed\nwell on earlier solver versions, they struggle to keep pace with rapidly\nevolving features. Recent approaches based on Large Language Models (LLMs) show\npromise in exploring advanced solver capabilities, but two obstacles remain:\nnearly half of the generated formulas are syntactically invalid, and iterative\ninteractions with the LLMs introduce substantial computational overhead. In\nthis study, we present Chimera, a novel LLM-assisted fuzzing framework that\naddresses both issues by shifting from direct formula generation to the\nsynthesis of reusable term (i.e., logical expression) generators. Particularly,\nChimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for\nSMT theories, including solver-specific extensions, from documentation, and (2)\nsynthesize composable Boolean term generators that adhere to these grammars.\nDuring fuzzing, Chimera populates structural skeletons derived from existing\nformulas with the terms iteratively produced by the LLM-synthesized generators.\nThis design ensures syntactic validity while promoting semantic diversity.\nNotably, Chimera requires only one-time LLM interaction investment,\ndramatically reducing runtime cost. We evaluated Chimera on two leading SMT\nsolvers: Z3 and cvc5. Our experiments show that Chimera has identified 43\nconfirmed bugs, 40 of which have already been fixed by developers.", "AI": {"tldr": "Chimera enhances LLM-based SMT solver testing by synthesizing grammar-compliant term generators, reducing syntax errors and computational overhead. It identified 43 bugs in Z3/cvc5, with 40 resolved, showcasing its efficacy and efficiency in solver fuzzing.", "motivation": "Traditional testing techniques for SMT solvers lag due to rapid feature evolution, while LLM-based approaches face two critical challenges: high syntactic error rates in generated formulas and high computational overhead from iterative LLM interactions. This necessitates a robust alternative that ensures syntactic correctness and scalability.", "method": "Chimera employs LLMs to (1) automatically extract context-free grammars (CFGs) for SMT theories from documentation and solver-specific extensions, and (2) synthesize composable Boolean term generators that adhere to these grammars. It leverages these generators to populate structural skeletons derived from existing formulas during fuzzing, ensuring syntactic validity and semantic diversity while minimizing runtime LLM interactions.", "result": "Chimera uncovered 43 confirmed bugs in the Z3 and cvc5 SMT solvers, with 40 of these actively fixed by developers. Its design achieves syntactic validity, semantic diversity, and significant computational efficiency gains by reducing LLM interactions to a one-time process.", "conclusion": "Chimera effectively addresses the limitations of existing LLM-based SMT solver testing methods by ensuring syntactic validity and drastically reducing runtime overhead through grammar-driven term generator synthesis. It demonstrated success in detecting 43 confirmed bugs in Z3 and cvc5, with 40 of them already resolved."}}
{"id": "2508.20370", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20370", "abs": "https://arxiv.org/abs/2508.20370", "authors": ["Lingzhe Zhang", "Tong Jia", "Kangjin Wang", "Weijie Hong", "Chiming Duan", "Minghua He", "Ying Li"], "title": "Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought", "comment": null, "summary": "As contemporary microservice systems become increasingly popular and\ncomplex-often comprising hundreds or even thousands of fine-grained,\ninterdependent subsystems-they are facing more frequent failures. Ensuring\nsystem reliability thus demands accurate root cause localization. While traces\nand metrics have proven to be effective data sources for this task, existing\nmethods either heavily rely on pre-defined schemas, which struggle to adapt to\nevolving operational contexts, or lack interpretability in their reasoning\nprocess, thereby leaving Site Reliability Engineers (SREs) confused. In this\npaper, we conduct a comprehensive study on how SREs localize the root cause of\nfailures, drawing insights from multiple professional SREs across different\norganizations. Our investigation reveals that human root cause analysis\nexhibits three key characteristics: recursiveness, multi-dimensional expansion,\nand cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,\nan adaptive root cause localization method for microservice systems that\nleverages a multi-agent recursion-of-thought framework. RCLAgent employs a\nnovel recursion-of-thought strategy to guide the LLM's reasoning process,\neffectively integrating data from multiple agents and tool-assisted analysis to\naccurately pinpoint the root cause. Experimental evaluations on various public\ndatasets demonstrate that RCLAgent achieves superior performance by localizing\nthe root cause using only a single request-outperforming state-of-the-art\nmethods that depend on aggregating multiple requests. These results underscore\nthe effectiveness of RCLAgent in enhancing the efficiency and precision of root\ncause localization in complex microservice environments.", "AI": {"tldr": "This paper introduces RCLAgent, a multi-agent framework for microservice root cause localization that mimics SRE reasoning patterns. It achieves state-of-the-art results by using a novel recursion-of-thought strategy and requires significantly less data (single request) compared to prior methods.", "motivation": "Contemporary microservice systems face frequent failures requiring accurate root cause localization. Existing methods either rely on rigid pre-defined schemas or lack interpretability, necessitating an adaptive and explainable solution aligned with human SRE practices.", "method": "RCLAgent employs a multi-agent recursion-of-thought framework combining a novel recursion-of-thought strategy, data integration from multiple agents, and tool-assisted analysis to guide the LLM's reasoning process towards root cause identification.", "result": "Experimental evaluations show RCLAgent outperforms state-of-the-art methods by accurately localizing root causes using only a single request, rather than requiring aggregated data from multiple requests.", "conclusion": "RCLAgent is an effective adaptive root cause localization method for microservice systems, improving efficiency and precision by leveraging a multi-agent recursion-of-thought framework and achieving superior performance with fewer requests."}}
{"id": "2508.20186", "categories": ["cs.CR", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.20186", "abs": "https://arxiv.org/abs/2508.20186", "authors": ["Lukasz Olejnik"], "title": "AI Propaganda factories with language models", "comment": null, "summary": "AI-powered influence operations can now be executed end-to-end on commodity\nhardware. We show that small language models produce coherent, persona-driven\npolitical messaging and can be evaluated automatically without human raters.\nTwo behavioural findings emerge. First, persona-over-model: persona design\nexplains behaviour more than model identity. Second, engagement as a stressor:\nwhen replies must counter-arguments, ideological adherence strengthens and the\nprevalence of extreme content increases. We demonstrate that fully automated\ninfluence-content production is within reach of both large and small actors.\nConsequently, defence should shift from restricting model access towards\nconversation-centric detection and disruption of campaigns and coordination\ninfrastructure. Paradoxically, the very consistency that enables these\noperations also provides a detection signature.", "AI": {"tldr": "Small AI models can create credible political messaging with personas, shifting focus from model identity to persona design. Engagement stress boosts extremism. Defenses must now target conversation patterns over restricting models.", "motivation": "The paper is motivated by the feasibility of executing end-to-end AI-powered influence operations using commodity hardware, highlighting the need for new defensive strategies against the proliferation of automated influence content.", "method": "The study utilized small language models to generate coherent, persona-driven political messages, which were evaluated automatically without human raters. The behaviors emerged through testing how persona design and engagement stress impacts content generation.", "result": "Two findings: persona design overwhelmingly dictates model behavior over model identity ('persona-over-model'), and engagement requiring counter-arguments increases ideological adherence and extreme content ('engagement as stressor'). Automated influence content production is viable for actors of all sizes.", "conclusion": "The paper concludes that defense strategies against AI-powered influence operations should transition from limiting model access to concentrating on conversation-centric detection and disruption of campaigns. The consistency inherent in these operations serves as a detection cue."}}
{"id": "2508.20563", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20563", "abs": "https://arxiv.org/abs/2508.20563", "authors": ["Zheying Zhang", "Tomas Herda", "Victoria Pichler", "Pekka Abrahamsson", "Geir K. Hanssen", "Joshua Kerievsky", "Alex Polyakov", "Mohit Chandna", "Marius Irgens", "Kai-Kristian Kemell", "Ayman Asad Khan", "Crystal Kwok", "Evan Leybourn", "Munish Malik", "Dorota Mleczko", "Morteza Moalagh", "Christopher Morales", "Yuliia Pieskova", "Daniel Plan\u00f6tscher", "Mika Saari", "Anastasiia Tkalich", "Karl Josef Gstettner", "Xiaofeng Wang"], "title": "AI and Agile Software Development: A Research Roadmap from the XP2025 Workshop", "comment": null, "summary": "This paper synthesizes the key findings from a full-day XP2025 workshop on\n\"AI and Agile: From Frustration to Success\", held in Brugg-Windisch,\nSwitzerland. The workshop brought together over 30 interdisciplinary academic\nresearchers and industry practitioners to tackle the concrete challenges and\nemerging opportunities at the intersection of Generative Artificial\nIntelligence (GenAI) and agile software development. Through structured,\ninteractive breakout sessions, participants identified shared pain points like\ntool fragmentation, governance, data quality, and critical skills gaps in AI\nliteracy and prompt engineering. These issues were further analyzed, revealing\nunderlying causes and cross-cutting concerns. The workshop concluded by\ncollaboratively co-creating a multi-thematic research roadmap, articulating\nboth short-term, implementable actions and visionary, long-term research\ndirections. This cohesive agenda aims to guide future investigation and drive\nthe responsible, human-centered integration of GenAI into agile practices.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.20212", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20212", "abs": "https://arxiv.org/abs/2508.20212", "authors": ["Minghao Hu", "Junzhe Wang", "Weisen Zhao", "Qiang Zeng", "Lannan Luo"], "title": "FlowMalTrans: Unsupervised Binary Code Translation for Malware Detection Using Flow-Adapter Architecture", "comment": "This paper is accepted to EMNLP 2025 Findings", "summary": "Applying deep learning to malware detection has drawn great attention due to\nits notable performance. With the increasing prevalence of cyberattacks\ntargeting IoT devices, there is a parallel rise in the development of malware\nacross various Instruction Set Architectures (ISAs). It is thus important to\nextend malware detection capacity to multiple ISAs. However, training a deep\nlearning-based malware detection model usually requires a large number of\nlabeled malware samples. The process of collecting and labeling sufficient\nmalware samples to build datasets for each ISA is labor-intensive and\ntime-consuming. To reduce the burden of data collection, we propose to leverage\nthe ideas of Neural Machine Translation (NMT) and Normalizing Flows (NFs) for\nmalware detection. Specifically, when dealing with malware in a certain ISA, we\ntranslate it to an ISA with sufficient malware samples (like X86-64). This\nallows us to apply a model trained on one ISA to analyze malware from another\nISA. Our approach reduces the data collection effort by enabling malware\ndetection across multiple ISAs using a model trained on a single ISA.", "AI": {"tldr": "This paper proposes using NMT and NFs to translate malware across ISAs, allowing a single trained model to detect malware in multiple architectures, significantly reducing data collection efforts.", "motivation": "Collecting and labeling malware samples for each ISA is resource-intensive, which limits the scalability of deep learning-based malware detection to new or less-supported architectures.", "method": "The approach leverages Neural Machine Translation (NMT) and Normalizing Flows (NFs) to translate malware from target ISAs to a well-supported ISA (e.g., X86-64), allowing reuse of a pre-trained model.", "result": "By translating malware between ISAs, the method reduces dependency on ISA-specific datasets, enabling efficient cross-ISA detection without requiring new training data.", "conclusion": "The proposed method effectively enables malware detection across multiple ISAs using a single trained model, thereby reducing the need for labor-intensive data collection for each ISA."}}
{"id": "2508.20737", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20737", "abs": "https://arxiv.org/abs/2508.20737", "authors": ["Wei Ma", "Yixiao Yang", "Qiang Hu", "Shi Ying", "Zhi Jin", "Bo Du", "Zhenchang Xing", "Tianlin Li", "Junjie Shi", "Yang Liu", "Linxiao Jiang"], "title": "Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol", "comment": null, "summary": "Applications of Large Language Models~(LLMs) have evolved from simple text\ngenerators into complex software systems that integrate retrieval augmentation,\ntool invocation, and multi-turn interactions. Their inherent non-determinism,\ndynamism, and context dependence pose fundamental challenges for quality\nassurance. This paper decomposes LLM applications into a three-layer\narchitecture: \\textbf{\\textit{System Shell Layer}}, \\textbf{\\textit{Prompt\nOrchestration Layer}}, and \\textbf{\\textit{LLM Inference Core}}. We then assess\nthe applicability of traditional software testing methods in each layer:\ndirectly applicable at the shell layer, requiring semantic reinterpretation at\nthe orchestration layer, and necessitating paradigm shifts at the inference\ncore. A comparative analysis of Testing AI methods from the software\nengineering community and safety analysis techniques from the AI community\nreveals structural disconnects in testing unit abstraction, evaluation metrics,\nand lifecycle management. We identify four fundamental differences that\nunderlie 6 core challenges. To address these, we propose four types of\ncollaborative strategies (\\emph{Retain}, \\emph{Translate}, \\emph{Integrate},\nand \\emph{Runtime}) and explore a closed-loop, trustworthy quality assurance\nframework that combines pre-deployment validation with runtime monitoring.\nBased on these strategies, we offer practical guidance and a protocol proposal\nto support the standardization and tooling of LLM application testing. We\npropose a protocol \\textbf{\\textit{Agent Interaction Communication Language}}\n(AICL) that is used to communicate between AI agents. AICL has the\ntest-oriented features and is easily integrated in the current agent framework.", "AI": {"tldr": "This paper investigates challenges in LLM application testing by analyzing architecture layers, comparing software/AI testing methods, and proposes strategies + AICL protocol for standardized, trustworthy quality assurance in complex LLM systems.", "motivation": "As LLM applications grow in complexity with non-determinism and context dependence, traditional software testing methods face limitations. The paper aims to address structural disconnects between software engineering and AI communities in testing methodologies.", "method": "The paper decomposes LLM applications into three architecture layers (System Shell, Prompt Orchestration, LLM Inference Core) and analyzes traditional testing methods' applicability. It compares Testing AI methods with AI safety techniques, identifies four fundamental differences, and proposes four collaborative strategies (Retain, Translate, Integrate, Runtime) to address six core challenges.", "result": "The study proposes collaborative strategies for quality assurance, a closed-loop trustworthy framework with pre-deployment validation and runtime monitoring, practical testing guidance, and a standardized protocol (AICL) for AI agent communication.", "conclusion": "The paper concludes that standardizing LLM application testing requires collaborative strategies and a framework integrating software engineering and AI safety practices. It proposes Agent Interaction Communication Language (AICL) for standardization and tooling support."}}
{"id": "2508.20228", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20228", "abs": "https://arxiv.org/abs/2508.20228", "authors": ["Xia Han", "Qi Li", "Jianbing Ni", "Mohammad Zulkernine"], "title": "Robustness Assessment and Enhancement of Text Watermarking for Google's SynthID", "comment": "submitted to TrustCom2025", "summary": "Recent advances in LLM watermarking methods such as SynthID-Text by Google\nDeepMind offer promising solutions for tracing the provenance of AI-generated\ntext. However, our robustness assessment reveals that SynthID-Text is\nvulnerable to meaning-preserving attacks, such as paraphrasing, copy-paste\nmodifications, and back-translation, which can significantly degrade watermark\ndetectability. To address these limitations, we propose SynGuard, a hybrid\nframework that combines the semantic alignment strength of Semantic Information\nRetrieval (SIR) with the probabilistic watermarking mechanism of SynthID-Text.\nOur approach jointly embeds watermarks at both lexical and semantic levels,\nenabling robust provenance tracking while preserving the original meaning.\nExperimental results across multiple attack scenarios show that SynGuard\nimproves watermark recovery by an average of 11.1\\% in F1 score compared to\nSynthID-Text. These findings demonstrate the effectiveness of semantic-aware\nwatermarking in resisting real-world tampering. All code, datasets, and\nevaluation scripts are publicly available at:\nhttps://github.com/githshine/SynGuard.", "AI": {"tldr": "SynGuard introduces a hybrid watermarking framework for LLMs, combining semantic and lexical techniques to outperform existing methods against attacks. Code available at https://github.com/githshine/SynGuard.", "motivation": "Existing LLM watermarking methods (e.g., SynthID-Text) are vulnerable to attacks like paraphrasing and back-translation, undermining watermark detectability and provenance tracking.", "method": "SynGuard combines Semantic Information Retrieval (SIR) for semantic alignment with SynthID-Text\u2019s probabilistic watermarking to jointly embed watermarks at lexical and semantic levels.", "result": "SynGuard improves watermark recovery by 11.1% in F1 score over SynthID-Text across multiple attack scenarios, showing robustness to real-world tampering.", "conclusion": "The study demonstrates the effectiveness of SynGuard in enhancing watermark robustness against meaning-preserving attacks through semantic-aware watermarking, with publicly available resources for further research."}}
{"id": "2508.20744", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20744", "abs": "https://arxiv.org/abs/2508.20744", "authors": ["Shabnam Hassani", "Mehrdad Sabetzadeh", "Daniel Amyot"], "title": "From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of LLM-Generated Behavioural Specifications from Food-Safety Regulations", "comment": null, "summary": "Context: Laws and regulations increasingly affect software design and quality\nassurance, but legal texts are written in technology-neutral language. This\ncreates challenges for engineers who must develop compliance artifacts such as\nrequirements and acceptance criteria. Manual creation is labor-intensive,\nerror-prone, and requires domain expertise. Advances in Generative AI (GenAI),\nespecially Large Language Models (LLMs), offer a way to automate deriving such\nartifacts.\n  Objective: We present the first systematic human-subject study of LLMs'\nability to derive behavioral specifications from legal texts using a\nquasi-experimental design. These specifications translate legal requirements\ninto a developer-friendly form.\n  Methods: Ten participants evaluated specifications generated from food-safety\nregulations by Claude and Llama. Using Gherkin, a structured BDD language, 60\nspecifications were produced. Each participant assessed 12 across five\ncriteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each\nspecification was reviewed by two participants, yielding 120 assessments.\n  Results: For Relevance, 75% of ratings were highest and 20% second-highest.\nClarity reached 90% highest. Completeness: 75% highest, 19% second.\nSingularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No\nlowest ratings occurred. Mann-Whitney U tests showed no significant differences\nacross participants or models. Llama slightly outperformed Claude in Clarity,\nCompleteness, and Time Savings, while Claude was stronger in Singularity.\nFeedback noted hallucinations and omissions but confirmed the utility of the\nspecifications.\n  Conclusion: LLMs can generate high-quality Gherkin specifications from legal\ntexts, reducing manual effort and providing structured artifacts useful for\nimplementation, assurance, and test generation.", "AI": {"tldr": "LLMs produce high-quality Gherkin specifications from legal texts with minimal quality issues, significantly reducing manual compliance work for developers.", "motivation": "Legal texts are challenging for engineers to translate into compliance artifacts manually due to labor-intensive processes requiring domain expertise. Generative AI offers automation potential for these tasks.", "method": "A quasi-experimental study with ten participants evaluating 60 Gherkin specifications generated by Claude and Llama from food-safety regulations. Each specification was assessed by two participants across five criteria (Relevance, Clarity, Completeness, Singularity, Time Savings) through 120 assessments.", "result": "75-90% of specifications achieved highest ratings across criteria. Llama outperformed Claude slightly in Clarity/Completeness/Time Savings, while Claude had stronger Singularity. No 'lowest' ratings occurred, though hallucinations/omissions were noted in feedback.", "conclusion": "LLMs can generate high-quality Gherkin specifications from legal texts, reducing manual effort and providing structured artifacts useful for implementation, assurance, and test generation."}}
{"id": "2508.20282", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20282", "abs": "https://arxiv.org/abs/2508.20282", "authors": ["Hyejun Jeong", "Mohammadreze Teymoorianfard", "Abhinav Kumar", "Amir Houmansadr", "Eugene Badasarian"], "title": "Network-Level Prompt and Trait Leakage in Local Research Agents", "comment": "under review", "summary": "We show that Web and Research Agents (WRAs) -- language model-based systems\nthat investigate complex topics on the Internet -- are vulnerable to inference\nattacks by passive network adversaries such as ISPs. These agents could be\ndeployed \\emph{locally} by organizations and individuals for privacy, legal, or\nfinancial purposes. Unlike sporadic web browsing by humans, WRAs visit\n$70{-}140$ domains with distinguishable timing correlations, enabling unique\nfingerprinting attacks.\n  Specifically, we demonstrate a novel prompt and user trait leakage attack\nagainst WRAs that only leverages their network-level metadata (i.e., visited IP\naddresses and their timings). We start by building a new dataset of WRA traces\nbased on user search queries and queries generated by synthetic personas. We\ndefine a behavioral metric (called OBELS) to comprehensively assess similarity\nbetween original and inferred prompts, showing that our attack recovers over\n73\\% of the functional and domain knowledge of user prompts. Extending to a\nmulti-session setting, we recover up to 19 of 32 latent traits with high\naccuracy. Our attack remains effective under partial observability and noisy\nconditions. Finally, we discuss mitigation strategies that constrain domain\ndiversity or obfuscate traces, showing negligible utility impact while reducing\nattack effectiveness by an average of 29\\%.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.20774", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20774", "abs": "https://arxiv.org/abs/2508.20774", "authors": ["Markus Funke", "Patricia Lago"], "title": "Towards an Architectural Perspective for Sustainability: Bundle the Needs from Industry", "comment": null, "summary": "Sustainability is increasingly recognized as an emerging quality property in\nsoftware-intensive systems, yet architects lack structured guidance to address\nit effectively throughout the software design phase. Architectural\nperspectives-an architectural knowledge artifact composed of concerns,\nactivities, tactics, pitfalls, and checklists-offer a promising approach to\ntackle such emerging quality properties across architectural views and are also\nindependent of architecture frameworks and industry contexts. In this paper, we\npresent a sustainability perspective vision, i.e., a revised notion of\narchitectural perspective meant to be filled with its own elements to target\nsustainability concerns. We formulate our sustainability perspective vision\nthrough evidence from applying snowballing to seminal literature and from\nconducting a focus group with experts in the field. Our findings confirm the\nrelevance of the different perspective elements in practice and highlight\nimplications for shaping a sustainability perspective that meets industrial\nneeds.", "AI": {"tldr": "This paper introduces a sustainability perspective vision for software architecture, validated through literature snowballing and expert focus groups, offering framework-independent guidance to address sustainability concerns.", "motivation": "Sustainability is an emerging quality property in software-intensive systems, but architects lack structured guidance to address it effectively during the design phase.", "method": "The authors formulated the sustainability perspective vision using evidence from snowballing seminal literature and conducting a focus group with field experts.", "result": "Findings confirm the relevance of perspective elements in practice and highlight implications for shaping a sustainability perspective that meets industrial needs.", "conclusion": "The paper concludes that a sustainability perspective vision offers a structured approach to address sustainability concerns in software-intensive systems, independent of architecture frameworks and industry contexts."}}
{"id": "2508.20307", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20307", "abs": "https://arxiv.org/abs/2508.20307", "authors": ["Michael R Smith", "Joe Ingram"], "title": "Surveying the Operational Cybersecurity and Supply Chain Threat Landscape when Developing and Deploying AI Systems", "comment": "11 pages, 5 figures", "summary": "The rise of AI has transformed the software and hardware landscape, enabling\npowerful capabilities through specialized infrastructures, large-scale data\nstorage, and advanced hardware. However, these innovations introduce unique\nattack surfaces and objectives which traditional cybersecurity assessments\noften overlook. Cyber attackers are shifting their objectives from conventional\ngoals like privilege escalation and network pivoting to manipulating AI outputs\nto achieve desired system effects, such as slowing system performance, flooding\noutputs with false positives, or degrading model accuracy. This paper serves to\nraise awareness of the novel cyber threats that are introduced when\nincorporating AI into a software system. We explore the operational\ncybersecurity and supply chain risks across the AI lifecycle, emphasizing the\nneed for tailored security frameworks to address evolving threats in the\nAI-driven landscape. We highlight previous exploitations and provide insights\nfrom working in this area. By understanding these risks, organizations can\nbetter protect AI systems and ensure their reliability and resilience.", "AI": {"tldr": "AI transforms systems but introduces overlooked cyber risks. This paper explores AI lifecycle threats, past exploits, and demands for modern security frameworks to protect AI reliability.", "motivation": "The integration of AI introduces unique cyber threats that traditional security assessments fail to address, such as AI output manipulation, necessitating specialized protective measures.", "method": "The study explores operational cybersecurity and supply chain risks across the AI lifecycle, leveraging historical exploitation cases and practical insights to identify vulnerabilities.", "result": "Highlights risks like manipulating AI outputs (e.g., slowing performance, generating false positives, degrading accuracy) and advocates for lifecycle-wide security frameworks.", "conclusion": "Organizations must adopt tailored security frameworks to address novel cyber threats in AI-driven systems, ensuring reliability and resilience."}}
{"id": "2508.20902", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20902", "abs": "https://arxiv.org/abs/2508.20902", "authors": ["Baharin A. Jodat", "Khouloud Gaaloul", "Mehrdad Sabetzadeh", "Shiva Nejati"], "title": "Automated Test Oracles for Flaky Cyber-Physical System Simulators: Approach and Evaluation", "comment": null, "summary": "Simulation-based testing of cyber-physical systems (CPS) is costly due to the\ntime-consuming execution of CPS simulators. In addition, CPS simulators may be\nflaky, leading to inconsistent test outcomes and requiring repeated test\nre-execution for reliable test verdicts. Automated test oracles that do not\nrequire system execution are therefore crucial for reducing testing costs.\nIdeally, such test oracles should be interpretable to facilitate human\nunderstanding of test verdicts, and they must be robust against the potential\nflakiness of CPS simulators. In this article, we propose assertion-based test\noracles for CPS as sets of logical and arithmetic predicates defined over the\ninputs of the system under test. Given a test input, our assertion-based test\noracle determines, without requiring test execution, whether the test passes,\nfails, or if the oracle is inconclusive in predicting a verdict. We describe\ntwo methods for generating assertion-based test oracles: one using genetic\nprogramming~(GP) that employs well-known spectrum-based fault localization\n(SBFL) ranking formulas, namely Ochiai, Tarantula, and Naish, as fitness\nfunctions; and the other using decision trees (DT) and decision rules (DR). We\nevaluate our assertion-based test oracles through case studies in the domains\nof aerospace, networking and autonomous driving. We show that test oracles\ngenerated using GP with Ochiai are significantly more accurate than those\nobtained using GP with Tarantula and Naish or using DT or DR. Moreover, this\naccuracy advantage remains even when accounting for the flakiness of the system\nunder test. We further show that the assertion-based test oracles generated by\nGP with Ochiai are robust against flakiness with only 4% average variation in\ntheir accuracy results across four different network and autonomous driving\nsystems with flaky behaviours.", "AI": {"tldr": "Developed interpretable test oracles for CPS using genetic programming (GP-Ochiai) that outperform existing approaches in accuracy and robustness against simulator flakiness through predicate-based assertions.", "motivation": "Simulation-based testing for cyber-physical systems (CPS) is computationally expensive and vulnerable to simulator flakiness, requiring repeated test executions and increasing validation costs. Automated, interpretable test oracles are needed to improve cost-effectiveness and reliability.", "method": "The paper introduces two oracle-generation methods: (1) Genetic Programming (GP) with fitness functions based on spectrum-based fault localization (Ochiai, Tarantula, Naish) and (2) Decision Trees/Decision Rules (DT/DR). The GP approach with Ochiai demonstrated superior performance.", "result": "GP oracles with the Ochiai formula achieved the highest accuracy, maintaining effectiveness even with simulator flakiness. They showed only 4% average variation in accuracy across four flaky systems (networking/autonomous driving) compared to significantly lower performing Tarantula/Naish, DT, and DR approaches.", "conclusion": "The proposed assertion-based test oracles using genetic programming (GP) with Ochiai significantly outperform alternative approaches in accuracy and robustness against simulator flakiness across aerospace, networking, and autonomous driving domains."}}
{"id": "2508.20412", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20412", "abs": "https://arxiv.org/abs/2508.20412", "authors": ["Zhiqiang Wang", "Junyang Zhang", "Guanquan Shi", "HaoRan Cheng", "Yunhao Yao", "Kaiwen Guo", "Haohua Du", "Xiang-Yang Li"], "title": "MindGuard: Tracking, Detecting, and Attributing MCP Tool Poisoning Attack via Decision Dependence Graph", "comment": null, "summary": "The Model Context Protocol (MCP) is increasingly adopted to standardize the\ninteraction between LLM agents and external tools. However, this trend\nintroduces a new threat: Tool Poisoning Attacks (TPA), where tool metadata is\npoisoned to induce the agent to perform unauthorized operations. Existing\ndefenses that primarily focus on behavior-level analysis are fundamentally\nineffective against TPA, as poisoned tools need not be executed, leaving no\nbehavioral trace to monitor.\n  Thus, we propose MindGuard, a decision-level guardrail for LLM agents,\nproviding provenance tracking of call decisions, policy-agnostic detection, and\npoisoning source attribution against TPA. While fully explaining LLM decision\nremains challenging, our empirical findings uncover a strong correlation\nbetween LLM attention mechanisms and tool invocation decisions. Therefore, we\nchoose attention as an empirical signal for decision tracking and formalize\nthis as the Decision Dependence Graph (DDG), which models the LLM's reasoning\nprocess as a weighted, directed graph where vertices represent logical concepts\nand edges quantify the attention-based dependencies. We further design robust\nDDG construction and graph-based anomaly analysis mechanisms that efficiently\ndetect and attribute TPA attacks. Extensive experiments on real-world datasets\ndemonstrate that MindGuard achieves 94\\%-99\\% average precision in detecting\npoisoned invocations, 95\\%-100\\% attribution accuracy, with processing times\nunder one second and no additional token cost. Moreover, DDG can be viewed as\nan adaptation of the classical Program Dependence Graph (PDG), providing a\nsolid foundation for applying traditional security policies at the decision\nlevel.", "AI": {"tldr": "MindGuard detects and attributes Tool Poisoning Attacks using attention-derived Decision Dependence Graphs, achieving high accuracy and efficiency while bridging traditional security principles with LLM decision-making.", "motivation": "Tool Poisoning Attacks (TPA) exploit poisoned tool metadata to manipulate LLM agents without detectable behavioral traces, rendering behavior-level defenses ineffective. Existing solutions lack decision-specific mechanisms to counteract this threat.", "method": "MindGuard employs attention-based Decision Dependence Graphs (DDG) to model LLM decision-making processes, enabling provenance tracking, anomaly detection, and attack attribution without behavioral traces. It adapts classical Program Dependence Graphs for decision-level security.", "result": "Experiments show MindGuard achieves 94\u201399% average precision in poisoned invocation detection, 95\u2013100% attribution accuracy, sub-second processing times, and zero additional token cost. DDG demonstrates compatibility with classical security policies.", "conclusion": "MindGuard addresses the vulnerability of LLM agents to Tool Poisoning Attacks by introducing decision-level safeguards using Decision Dependence Graphs (DDG), achieving robust detection, attribution, and compatibility with traditional security frameworks."}}
{"id": "2508.20911", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20911", "abs": "https://arxiv.org/abs/2508.20911", "authors": ["Zuocheng Feng", "Kaiwen Zhang", "Miaomiao Wang", "Yiming Cheng", "Yuandao Cai", "Xiaofeng Li", "Guanjun Liu"], "title": "Deep Learning Based Concurrency Bug Detection and Localization", "comment": null, "summary": "Concurrency bugs, caused by improper synchronization of shared resources in\nmulti-threaded or distributed systems, are notoriously hard to detect and thus\ncompromise software reliability and security. The existing deep learning\nmethods face three main limitations. First, there is an absence of large and\ndedicated datasets of diverse concurrency bugs for them. Second, they lack\nsufficient representation of concurrency semantics. Third, binary\nclassification results fail to provide finer-grained debug information such as\nprecise bug lines. To address these problems, we propose a novel method for\neffective concurrency bug detection as well as localization. We construct a\ndedicated concurrency bug dataset to facilitate model training and evaluation.\nWe then integrate a pre-trained model with a heterogeneous graph neural network\n(GNN), by incorporating a new Concurrency-Aware Code Property Graph (CCPG) that\nconcisely and effectively characterizes concurrency semantics. To further\nfacilitate debugging, we employ SubgraphX, a GNN-based interpretability method,\nwhich explores the graphs to precisely localize concurrency bugs, mapping them\nto specific lines of source code. On average, our method demonstrates an\nimprovement of 10\\% in accuracy and precision and 26\\% in recall compared to\nstate-of-the-art methods across diverse evaluation settings.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.20414", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20414", "abs": "https://arxiv.org/abs/2508.20414", "authors": ["Mengyu Sun", "Ziyuan Yang", "Yongqiang Huang", "Hui Yu", "Yingyu Chen", "Shuren Qi", "Andrew Beng Jin Teoh", "Yi Zhang"], "title": "Federated Learning for Large Models in Medical Imaging: A Comprehensive Review", "comment": null, "summary": "Artificial intelligence (AI) has demonstrated considerable potential in the\nrealm of medical imaging. However, the development of high-performance AI\nmodels typically necessitates training on large-scale, centralized datasets.\nThis approach is confronted with significant challenges due to strict patient\nprivacy regulations and legal restrictions on data sharing and utilization.\nThese limitations hinder the development of large-scale models in medical\ndomains and impede continuous updates and training with new data. Federated\nLearning (FL), a privacy-preserving distributed training framework, offers a\nnew solution by enabling collaborative model development across fragmented\nmedical datasets. In this survey, we review FL's contributions at two stages of\nthe full-stack medical analysis pipeline. First, in upstream tasks such as CT\nor MRI reconstruction, FL enables joint training of robust reconstruction\nnetworks on diverse, multi-institutional datasets, alleviating data scarcity\nwhile preserving confidentiality. Second, in downstream clinical tasks like\ntumor diagnosis and segmentation, FL supports continuous model updating by\nallowing local fine-tuning on new data without centralizing sensitive images.\nWe comprehensively analyze FL implementations across the medical imaging\npipeline, from physics-informed reconstruction networks to diagnostic AI\nsystems, highlighting innovations that improve communication efficiency, align\nheterogeneous data, and ensure secure parameter aggregation. Meanwhile, this\npaper provides an outlook on future research directions, aiming to serve as a\nvaluable reference for the field's development.", "AI": {"tldr": "This survey explores how Federated Learning overcomes data privacy and centralization barriers in medical imaging AI by enabling decentralized collaboration for reconstruction and diagnosis tasks, while outlining innovations and future research directions.", "motivation": "The development of high-performance AI models in medical imaging is hindered by strict privacy regulations and data-sharing restrictions, limiting model scalability and continuous learning. This motivates the need for privacy-preserving frameworks like FL to enable collaborative training on decentralized datasets.", "method": "The paper reviews FL implementations across two stages of the medical imaging pipeline: upstream tasks (CT/MRI reconstruction) and downstream clinical tasks (tumor diagnosis and segmentation). It analyzes innovations in communication efficiency, heterogeneous data alignment, and secure parameter aggregation.", "result": "The paper demonstrates that FL facilitates robust, privacy-preserving training for CT/MRI reconstruction across diverse institutions and supports continuous model refinement in clinical tasks (e.g., tumor diagnosis) through local fine-tuning. It also highlights advancements in FL techniques tailored for medical domain challenges.", "conclusion": "Federated Learning (FL) is a promising framework for addressing privacy and data fragmentation challenges in medical imaging, enabling collaborative model development and continuous updates without compromising data confidentiality. It serves as a valuable reference for future research directions in the field."}}
{"id": "2508.20977", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20977", "abs": "https://arxiv.org/abs/2508.20977", "authors": ["Shiwen Shan", "Yintong Huo", "Yuxin Su", "Zhining Wang", "Dan Li", "Zibin Zheng"], "title": "ConfLogger: Enhance Systems' Configuration Diagnosability through Configuration Logging", "comment": "13 pages, 6 figures, accpeted by ICSE '26 (The 48th IEEE/ACM\n  International Conference on Software Engineering)", "summary": "Modern configurable systems offer customization via intricate configuration\nspaces, yet such flexibility introduces pervasive configuration-related issues\nsuch as misconfigurations and latent softwarebugs. Existing diagnosability\nsupports focus on post-failure analysis of software behavior to identify\nconfiguration issues, but none of these approaches look into whether the\nsoftware clue sufficient failure information for diagnosis. To fill in the\nblank, we propose the idea of configuration logging to enhance existing logging\npractices at the source code level. We develop ConfLogger, the first tool that\nunifies configuration-aware static taint analysis with LLM-based log generation\nto enhance software configuration diagnosability. Specifically, our method 1)\nidentifies configuration-sensitive code segments by tracing\nconfiguration-related data flow in the whole project, and 2) generates\ndiagnostic log statements by analyzing configuration code contexts. Evaluation\nresults on eight popular software systems demonstrate the effectiveness of\nConfLogger to enhance configuration diagnosability. Specifically,\nConfLogger-enhanced logs successfully aid a log-based misconfiguration\ndiagnosis tool to achieve 100% accuracy on error localization in 30 silent\nmisconfiguration scenarios, with 80% directly resolvable through explicit\nconfiguration information exposed. In addition, ConfLogger achieves 74%\ncoverage of existing logging points, outperforming baseline LLM-based loggers\nby 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,\nand 26.2% higher in F1 compared to the state-of-the-art baseline in terms of\nvariable logging while also augmenting diagnostic value. A controlled user\nstudy on 22 cases further validated its utility, speeding up diagnostic time by\n1.25x and improving troubleshooting accuracy by 251.4%.", "AI": {"tldr": "ConfLogger enhances configuration diagnosability by combining static taint analysis with LLMs for log generation, achieving SOTA results in accuracy and diagnostic speed for solving misconfiguration issues.", "motivation": "Current diagnosability methods focus on post-failure behavior analysis but lack mechanisms to ensure software logs sufficient failure information for effective configuration issue diagnosis, creating a critical gap in addressing misconfigurations and latent bugs.", "method": "ConfLogger integrates configuration-aware static taint analysis to identify sensitive code segments and leverages LLM-based analysis to generate diagnostic logs from configuration code contexts, unifying these techniques to enhance logging practices at the source code level.", "result": "ConfLogger achieves 100% error localization accuracy in 30 scenarios, 74% log coverage (12-30% better than LLM baselines), 79.3% higher recall, and 26.2% higher F1 score in variable logging. A user study showed 1.25\u00d7 faster troubleshooting with 251.4% improved accuracy.", "conclusion": "ConfLogger effectively enhances software configuration diagnosability through its innovative combination of static taint analysis and LLM-based logging, demonstrating significant improvements in accuracy, coverage, and user efficiency over existing approaches."}}
{"id": "2508.20424", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20424", "abs": "https://arxiv.org/abs/2508.20424", "authors": ["Desen Sun", "Shuncheng Jie", "Sihang Liu"], "title": "Breaking Diffusion with Cache: Exploiting Approximate Caches in Diffusion Models", "comment": null, "summary": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching.", "AI": {"tldr": "Caching in AI generators creates remote security holes allowing data exfiltration, prompt theft, and content poisoning - requiring urgent mitigation strategies.", "motivation": "Diffusion models' computational costs lead to adoption of approximate caching, but this breaks user isolation. The work addresses the urgent need to assess security risks introduced by this optimization.", "method": "The authors identify and demonstrate three attacks: (1) a covert channel via cached prompts, (2) prompt stealing through cache hit analysis, and (3) poisoning attacks that inject attacker-controlled content into cached prompts.", "result": "All attacks were successfully executed remotely through the serving system, proving practical feasibility of exploiting cache-based vulnerabilities to exfiltrate data, steal prompts, and inject malicious content.", "conclusion": "Approximate caching in diffusion models introduces severe security vulnerabilities, enabling remote attacks like covert channels, prompt stealing, and poisoning. These findings highlight the need for improved isolation mechanisms in cached generative models."}}
{"id": "2508.21050", "categories": ["cs.SE", "cs.CY", "K.2; K.6.3; K.4; K.7"], "pdf": "https://arxiv.org/pdf/2508.21050", "abs": "https://arxiv.org/abs/2508.21050", "authors": ["Thomas J. Misa"], "title": "Dynamics of Gender Bias in Software Engineering", "comment": "26 pages, 3 figures", "summary": "The field of software engineering is embedded in both engineering and\ncomputer science, and may embody gender biases endemic to both. This paper\nsurveys software engineering's origins and its long-running attention to\nengineering professionalism, profiling five leaders; it then examines the\nfield's recent attention to gender issues and gender bias. It next\nquantitatively analyzes women's participation as research authors in the\nfield's leading International Conference of Software Engineering (1976-2010),\nfinding a dozen years with statistically significant gender exclusion. Policy\ndimensions of research on gender bias in computing are suggested.", "AI": {"tldr": "This paper examines gender bias in software engineering history and finds systemic exclusion of women at ICSE, recommending policy changes to improve diversity.", "motivation": "The paper addresses gender bias in software engineering, which inherits biases from engineering and computer science, aiming to understand its historical development and impact on diversity.", "method": "The paper combines a historical survey of software engineering's origins and leadership with quantitative analysis of women's author participation in the ICSE conference from 1976-2010, identifying years of statistical gender exclusion.", "result": "Analysis reveals a dozen years (1976-2010) with statistically significant gender exclusion in ICSE authorship, showing systemic underrepresentation of women in software engineering research.", "conclusion": "The paper concludes by suggesting policy dimensions for addressing gender bias in computing, emphasizing the need for structured interventions to promote equity in software engineering research."}}
{"id": "2508.20444", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20444", "abs": "https://arxiv.org/abs/2508.20444", "authors": ["Md Raz", "Meet Udeshi", "P. V. Sai Charan", "Prashanth Krishnamurthy", "Farshad Khorrami", "Ramesh Karri"], "title": "Ransomware 3.0: Self-Composing and LLM-Orchestrated", "comment": null, "summary": "Using automated reasoning, code synthesis, and contextual decision-making, we\nintroduce a new threat that exploits large language models (LLMs) to\nautonomously plan, adapt, and execute the ransomware attack lifecycle.\nRansomware 3.0 represents the first threat model and research prototype of\nLLM-orchestrated ransomware. Unlike conventional malware, the prototype only\nrequires natural language prompts embedded in the binary; malicious code is\nsynthesized dynamically by the LLM at runtime, yielding polymorphic variants\nthat adapt to the execution environment. The system performs reconnaissance,\npayload generation, and personalized extortion, in a closed-loop attack\ncampaign without human involvement. We evaluate this threat across personal,\nenterprise, and embedded environments using a phase-centric methodology that\nmeasures quantitative fidelity and qualitative coherence in each attack phase.\nWe show that open source LLMs can generate functional ransomware components and\nsustain closed-loop execution across diverse environments. Finally, we present\nbehavioral signals and multi-level telemetry of Ransomware 3.0 through a case\nstudy to motivate future development of better defenses and policy enforcements\nto address novel AI-enabled ransomware attacks.", "AI": {"tldr": "LLM-powered ransomware prototype automates attacks via natural language prompts, producing undetected polymorphic variants across devices - signals urgent need for AI defense innovations.", "motivation": "Traditional ransomware detection methods are inadequate against AI-aggregated threats. This work explores the first LLM-orchestrated ransomware model to quantify risks and inform future defenses against adaptive, human-free attack frameworks.", "method": "The system synthesizes ransomware code via open-source LLMs using natural language prompts in binary files, enabling closed-loop attack execution with dynamic adaptation across diverse environments through reconnaissance, payload generation, and phased evaluation.", "result": "Open-source LLMs successfully generated functional ransomware components across personal, enterprise, and embedded environments; attack campaigns sustained closed-loop execution with polymorphic variants, validated through phase-centric fidelity/cohrence analysis.", "conclusion": "The paper presents Ransomware 3.0, a threat model exploiting LLMs to autonomously execute ransomware campaigns, highlighting the existential risk of AI-aggregated threats and the urgent need for advanced defense strategies against polymorphic, AI-driven malware."}}
{"id": "2508.20504", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.20504", "abs": "https://arxiv.org/abs/2508.20504", "authors": ["Guan-Yan Yang", "Jui-Ning Chen", "Farn Wang", "Kuo-Hui Yeh"], "title": "Enhancing Resilience for IoE: A Perspective of Networking-Level Safeguard", "comment": "To be published in IEEE Network Magazine, 2026", "summary": "The Internet of Energy (IoE) integrates IoT-driven digital communication with\npower grids to enable efficient and sustainable energy systems. Still, its\ninterconnectivity exposes critical infrastructure to sophisticated cyber\nthreats, including adversarial attacks designed to bypass traditional\nsafeguards. Unlike general IoT risks, IoE threats have heightened public safety\nconsequences, demanding resilient solutions. From the networking-level\nsafeguard perspective, we propose a Graph Structure Learning (GSL)-based\nsafeguards framework that jointly optimizes graph topology and node\nrepresentations to resist adversarial network model manipulation inherently.\nThrough a conceptual overview, architectural discussion, and case study on a\nsecurity dataset, we demonstrate GSL's superior robustness over representative\nmethods, offering practitioners a viable path to secure IoE networks against\nevolving attacks. This work highlights the potential of GSL to enhance the\nresilience and reliability of future IoE networks for practitioners managing\ncritical infrastructure. Lastly, we identify key open challenges and propose\nfuture research directions in this novel research area.", "AI": {"tldr": "This paper proposes a GSL-based framework to secure IoE networks from adversarial attacks by co-optimizing graph structure and node representations, demonstrating superior robustness against cyber threats while addressing challenges in safeguarding critical energy infrastructure.", "motivation": "The Internet of Energy's (IoE) interconnectivity, while enabling efficient energy systems, exposes critical infrastructure to sophisticated cyber threats with significant public safety risks. Traditional safeguards are insufficient against adversarial attacks, necessitating resilient networking-level solutions.", "method": "The proposed solution employs Graph Structure Learning (GSL) to jointly optimize graph topology and node representations. This approach inherently resists adversarial network model manipulation through structural adaptability and representation learning.", "result": "The framework's robustness is validated through a conceptual analysis, architectural discussion, and a case study using a security dataset. Results show that GSL outperforms representative methods, proving its effectiveness in securing IoE networks against evolving attacks.", "conclusion": "This study introduces a GSL-based safeguards framework to enhance the security of IoE networks. The framework demonstrates superior robustness against adversarial attacks, offering a practical solution for protecting critical infrastructure. The authors also outline open challenges and future research directions in this emerging field."}}
{"id": "2508.20962", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20962", "abs": "https://arxiv.org/abs/2508.20962", "authors": ["Weijie Liu", "Hongbo Chen", "Shuo Huai", "Zhen Xu", "Wenhao Wang", "Zhi Li", "Zheli Liu"], "title": "Characterizing Trust Boundary Vulnerabilities in TEE Containers", "comment": null, "summary": "Trusted Execution Environments (TEEs) have emerged as a cornerstone of\nconfidential computing, garnering significant attention from both academia and\nindustry. To enable the secure development, execution, and deployment, of\napplications on TEE platforms, TEE containers have been introduced as\nmiddleware solutions. These containers aim to shield applications from\npotentially malicious operating systems and orchestration interfaces while\nmaintaining usability and reliability. In this paper, we analyze the isolation\nstrategies employed by existing TEE containers to protect secure applications.\nTo address the challenges in analyzing these interfaces, we designed an\nautomated analyzer to precisely identify and evaluate their isolation\nboundaries. We observed that some TEE containers fail to achieve their intended\ngoals due to critical design and implementation flaws, such as information\nleakage, rollback attacks, denial-of-service, and Iago attacks, which pose\nsignificant security risks. Drawing from our findings, we share key lessons to\nguide the development of more secure container solutions and discuss emerging\ntrends in TEE containerization design.", "AI": {"tldr": "This paper examines security vulnerabilities in TEE containers and proposes an automated analyzer to detect flaws that could lead to information leakage and other attacks.", "motivation": "Trusted Execution Environments (TEEs) are crucial for secure application execution, yet existing TEE containerization methods introduce potential vulnerabilities that need systematic analysis.", "method": "The authors designed an automated analysis tool to evaluate the isolation boundary of TEE container interfaces.", "result": "The analysis identified critical vulnerabilities like information leakage, rollback attacks, denial-of-service, and Iago attacks in some TEE containers.", "conclusion": "The findings highlight the need for improved design and implementation of TEE container isolation boundaries. The paper shares lessons for safer containerization and trends in the field."}}
{"id": "2508.20517", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20517", "abs": "https://arxiv.org/abs/2508.20517", "authors": ["Dan Lin", "Shunfeng Lu", "Ziyan Liu", "Jiajing Wu", "Junyuan Fang", "Kaixin Lin", "Bowen Song", "Zibin Zheng"], "title": "BridgeShield: Enhancing Security for Cross-chain Bridge Applications via Heterogeneous Graph Mining", "comment": null, "summary": "Cross-chain bridges play a vital role in enabling blockchain\ninteroperability. However, due to the inherent design flaws and the enormous\nvalue they hold, they have become prime targets for hacker attacks. Existing\ndetection methods show progress yet remain limited, as they mainly address\nsingle-chain behaviors and fail to capture cross-chain semantics. To address\nthis gap, we leverage heterogeneous graph attention networks, which are\nwell-suited for modeling multi-typed entities and relations, to capture the\ncomplex execution semantics of cross-chain behaviors. We propose BridgeShield,\na detection framework that jointly models the source chain, off-chain\ncoordination, and destination chain within a unified heterogeneous graph\nrepresentation. BridgeShield incorporates intra-meta-path attention to learn\nfine-grained dependencies within cross-chain paths and inter-meta-path\nattention to highlight discriminative cross-chain patterns, thereby enabling\nprecise identification of attack behaviors. Extensive experiments on 51\nreal-world cross-chain attack events demonstrate that BridgeShield achieves an\naverage F1-score of 92.58%, representing a 24.39% improvement over\nstate-of-the-art baselines. These results validate the effectiveness of\nBridgeShield as a practical solution for securing cross-chain bridges and\nenhancing the resilience of multi-chain ecosystems.", "AI": {"tldr": "BridgeShield addresses cross-chain security gaps by modeling bridges as unified heterogeneous graphs, achieving state-of-the-art performance with 92.58% F1-score through novel graph attention mechanisms.", "motivation": "Cross-chain bridges are vulnerable to attacks due to design flaws and their high-value holdings, existing detection methods fail to capture cross-chain semantics comprehensively.", "method": "BridgeShield employs heterogeneous graph attention networks with intra-meta-path and inter-meta-path attention mechanisms to model cross-chain behaviors comprehensively.", "result": "BridgeShield achieves an average F1-score of 92.58%, showing a 24.39% improvement over state-of-the-art baselines on 51 real-world attack events.", "conclusion": "BridgeShield proves to be an effective solution for securing cross-chain bridges, enhancing the resilience of multi-chain ecosystems."}}
{"id": "2508.20591", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20591", "abs": "https://arxiv.org/abs/2508.20591", "authors": ["Jose E. Puente", "Carlos Puente"], "title": "Bitcoin as an Interplanetary Monetary Standard with Proof-of-Transit Timestamping", "comment": null, "summary": "We explore the feasibility of deploying Bitcoin as the shared monetary\nstandard between Earth and Mars, accounting for physical constraints of\ninterplanetary communication. We introduce a novel primitive, Proof-of-Transit\nTimestamping (PoTT), to provide cryptographic, tamper-evident audit trails for\nBitcoin data across high-latency, intermittently-connected links. Leveraging\nDelay/Disruption-Tolerant Networking (DTN) and optical low-Earth-orbit (LEO)\nmesh constellations, we propose an architecture for header-first replication,\nlong-horizon Lightning channels with planetary watchtowers, and secure\nsettlement through federated sidechains or blind-merge-mined (BMM) commit\nchains. We formalize PoTT, analyze its security model, and show how it\nmeasurably improves reliability and accountability without altering Bitcoin\nconsensus or its monetary base. Near-term deployments favor strong federations\nfor local settlement; longer-term, blind-merge-mined commit chains (if adopted)\nprovide an alternative. The Earth L1 monetary base remains unchanged, while\nMars can operate a pegged commit chain or strong federation with 1:1 pegged\nassets for local block production. For transparency, if both time-beacon\nregimes are simultaneously compromised, PoTT-M2 (and PoTT generally) reduces to\nadministrative assertions rather than cryptographic time-anchoring.", "AI": {"tldr": "This paper designs a decentralized framework to enable Bitcoin as a shared monetary system between Earth and Mars, introducing PoTT to manage high-latency interplanetary communications and proposing a hybrid architecture using DTN, federations, and commit chains.", "motivation": "Bitcoin's current architecture cannot address interplanetary communication challenges such as high latency and intermittent connectivity between Earth and Mars. The paper addresses this by developing a system maintaining cryptographic integrity without altering Bitcoin's core consensus or monetary properties.", "method": "Introduces Proof-of-Transit Timestamping (PoTT) for secure audit trails across high-latency links, combines DTN and LEO mesh networking, implements header-first replication, long-horizon Lightning channels with planetary watchtowers, and utilizes federated sidechains or blind-merge-mined (BMM) commit chains for settlement.", "result": "Demonstrates PoTT's formalization and security analysis, showing it improves reliability and accountability across planetary networks. Proposes near-term federated solutions for Mars and long-term BMM commit chains, ensuring Earth's monetary base remains unchanged while enabling pegged local systems on Mars.", "conclusion": "The paper proposes an architectural framework for using Bitcoin as a shared monetary standard between Earth and Mars, leveraging cryptographic primitives like PoTT to handle interplanetary communication constraints. It concludes that strong federations and potential blind-merge-mined commit chains provide viable solutions for Mars-based transactions while preserving Earth's Bitcoin monetary base."}}
{"id": "2508.20643", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20643", "abs": "https://arxiv.org/abs/2508.20643", "authors": ["Stefano Fumero", "Kai Huang", "Matteo Boffa", "Danilo Giordano", "Marco Mellia", "Zied Ben Houidi", "Dario Rossi"], "title": "CyberSleuth: Autonomous Blue-Team LLM Agent for Web Attack Forensics", "comment": "Code:\n  https://github.com/SmartData-Polito/LLM_Agent_Cybersecurity_Forensic", "summary": "Large Language Model (LLM) agents are powerful tools for automating complex\ntasks. In cybersecurity, researchers have primarily explored their use in\nred-team operations such as vulnerability discovery and penetration tests.\nDefensive uses for incident response and forensics have received comparatively\nless attention and remain at an early stage. This work presents a systematic\nstudy of LLM-agent design for the forensic investigation of realistic web\napplication attacks. We propose CyberSleuth, an autonomous agent that processes\npacket-level traces and application logs to identify the targeted service, the\nexploited vulnerability (CVE), and attack success. We evaluate the consequences\nof core design decisions - spanning tool integration and agent architecture -\nand provide interpretable guidance for practitioners. We benchmark four agent\narchitectures and six LLM backends on 20 incident scenarios of increasing\ncomplexity, identifying CyberSleuth as the best-performing design. In a\nseparate set of 10 incidents from 2025, CyberSleuth correctly identifies the\nexact CVE in 80% of cases. At last, we conduct a human study with 22 experts,\nwhich rated the reports of CyberSleuth as complete, useful, and coherent. They\nalso expressed a slight preference for DeepSeek R1, a good news for open source\nLLM. To foster progress in defensive LLM research, we release both our\nbenchmark and the CyberSleuth platform as a foundation for fair, reproducible\nevaluation of forensic agents.", "AI": {"tldr": "This paper introduces CyberSleuth, a leading LLM-agent for cybersecurity forensics that accurately identifies attack vulnerabilities and performs well in human evaluations, while providing an open benchmark to advance defensive AI research.", "motivation": "While LLM agents have been extensively studied for offensive cybersecurity tasks, their defensive applications in incident response and forensics remain underexplored. This work addresses this gap by systematically studying LLM-agent design for forensic investigations of web application attacks.", "method": "The authors propose CyberSleuth, an autonomous agent that analyzes packet-level traces and application logs to identify attack details. They evaluate four agent architectures and six LLM backends across 20 incident scenarios, validate performance on 2025 incidents, and conduct a human study with 22 experts.", "result": "CyberSleuth achieves 80% accuracy in identifying exact CVEs in attacks and receives positive ratings for report quality from cybersecurity experts. The DeepSeek R1 open-source LLM shows particular effectiveness. The research releases a benchmark and platform for reproducible evaluation.", "conclusion": "The study concludes that CyberSleuth is an effective LLM-agent design for forensic investigations, providing a reproducible benchmark and platform to advance defensive LLM research in cybersecurity."}}
{"id": "2508.20816", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20816", "abs": "https://arxiv.org/abs/2508.20816", "authors": ["Isaac David", "Arthur Gervais"], "title": "Multi-Agent Penetration Testing AI for the Web", "comment": null, "summary": "AI-powered development platforms are making software creation accessible to a\nbroader audience, but this democratization has triggered a scalability crisis\nin security auditing. With studies showing that up to 40% of AI-generated code\ncontains vulnerabilities, the pace of development now vastly outstrips the\ncapacity for thorough security assessment.\n  We present MAPTA, a multi-agent system for autonomous web application\nsecurity assessment that combines large language model orchestration with\ntool-grounded execution and end-to-end exploit validation. On the 104-challenge\nXBOW benchmark, MAPTA achieves 76.9% overall success with perfect performance\non SSRF and misconfiguration vulnerabilities, 83% success on broken\nauthorization, and strong results on injection attacks including server-side\ntemplate injection (85%) and SQL injection (83%). Cross-site scripting (57%)\nand blind SQL injection (0%) remain challenging. Our comprehensive cost\nanalysis across all challenges totals $21.38 with a median cost of $0.073 for\nsuccessful attempts versus $0.357 for failures. Success correlates strongly\nwith resource efficiency, enabling practical early-stopping thresholds at\napproximately 40 tool calls or $0.30 per challenge.\n  MAPTA's real-world findings are impactful given both the popularity of the\nrespective scanned GitHub repositories (8K-70K stars) and MAPTA's low average\noperating cost of $3.67 per open-source assessment: MAPTA discovered critical\nvulnerabilities including RCEs, command injections, secret exposure, and\narbitrary file write vulnerabilities. Findings are responsibly disclosed, 10\nfindings are under CVE review.", "AI": {"tldr": "MAPTA is a cost-effective multi-agent system for AI-driven web app security audits, achieving high success rates and efficient resource use (76.9% success, $3.67 avg cost), uncovering critical vulnerabilities in popular repositories.", "motivation": "The democratization of software development through AI has led to a surge in vulnerable code (up to 40%), outpacing traditional security auditing capabilities, necessitating scalable and cost-effective solutions.", "method": "MAPTA employs a multi-agent system integrating large language model orchestration, tool-grounded execution, and end-to-end exploit validation for autonomous security assessments.", "result": "MAPTA achieves 76.9% overall success on the XBOW benchmark, with perfect performance on SSRF/misconfigurations, discovers critical vulnerabilities in real-world repositories, and operates at an average cost of $3.67 per assessment.", "conclusion": "MAPTA effectively addresses the scalability issue in AI-generated code security auditing by combining multi-agent systems with efficient resource use, achieving strong success rates and low costs, making it practical for widespread adoption."}}
{"id": "2508.20848", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20848", "abs": "https://arxiv.org/abs/2508.20848", "authors": ["Junjie Chu", "Mingjie Li", "Ziqing Yang", "Ye Leng", "Chenhao Lin", "Chao Shen", "Michael Backes", "Yun Shen", "Yang Zhang"], "title": "JADES: A Universal Framework for Jailbreak Assessment via Decompositional Scoring", "comment": "17 pages, 5 figures. For the code and data supporting this work, see\n  https://trustairlab.github.io/jades.github.io/", "summary": "Accurately determining whether a jailbreak attempt has succeeded is a\nfundamental yet unresolved challenge. Existing evaluation methods rely on\nmisaligned proxy indicators or naive holistic judgments. They frequently\nmisinterpret model responses, leading to inconsistent and subjective\nassessments that misalign with human perception. To address this gap, we\nintroduce JADES (Jailbreak Assessment via Decompositional Scoring), a universal\njailbreak evaluation framework. Its key mechanism is to automatically decompose\nan input harmful question into a set of weighted sub-questions, score each\nsub-answer, and weight-aggregate the sub-scores into a final decision. JADES\nalso incorporates an optional fact-checking module to strengthen the detection\nof hallucinations in jailbreak responses. We validate JADES on JailbreakQR, a\nnewly introduced benchmark proposed in this work, consisting of 400 pairs of\njailbreak prompts and responses, each meticulously annotated by humans. In a\nbinary setting (success/failure), JADES achieves 98.5% agreement with human\nevaluators, outperforming strong baselines by over 9%. Re-evaluating five\npopular attacks on four LLMs reveals substantial overestimation (e.g., LAA's\nattack success rate on GPT-3.5-Turbo drops from 93% to 69%). Our results show\nthat JADES could deliver accurate, consistent, and interpretable evaluations,\nproviding a reliable basis for measuring future jailbreak attacks.", "AI": {"tldr": "JADES is a decomposition-based jailbreak evaluation framework that achieves 98.5% human-level agreement by scoring sub-questions and detecting hallucinations, significantly outperforming prior methods.", "motivation": "Existing jailbreak evaluation methods are inconsistent and misaligned with human perception due to reliance on proxy indicators or simplistic judgments, necessitating a more accurate and universal assessment framework.", "method": "JADES employs a decompositional scoring framework that automatically breaks down harmful questions into weighted sub-questions, scores sub-answers using a weight-aggregation process, and integrates an optional fact-checking module to detect hallucinations.", "result": "JADES achieves 98.5% agreement with human evaluators on the JailbreakQR benchmark, outperforming baselines by >9%. Re-evaluation revealed substantial overestimation of attack success rates (e.g., LAA's GPT-3.5-Turbo success rate drops from 93% to 69%).", "conclusion": "JADES provides accurate, consistent, and interpretable evaluations for jailbreak attempts, serving as a reliable basis for future assessments."}}
{"id": "2508.20863", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20863", "abs": "https://arxiv.org/abs/2508.20863", "authors": ["Matteo Gioele Collu", "Umberto Salviati", "Roberto Confalonieri", "Mauro Conti", "Giovanni Apruzzese"], "title": "Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review", "comment": null, "summary": "Large Language Models (LLMs) are increasingly being integrated into the\nscientific peer-review process, raising new questions about their reliability\nand resilience to manipulation. In this work, we investigate the potential for\nhidden prompt injection attacks, where authors embed adversarial text within a\npaper's PDF to influence the LLM-generated review. We begin by formalising\nthree distinct threat models that envision attackers with different motivations\n-- not all of which implying malicious intent. For each threat model, we design\nadversarial prompts that remain invisible to human readers yet can steer an\nLLM's output toward the author's desired outcome. Using a user study with\ndomain scholars, we derive four representative reviewing prompts used to elicit\npeer reviews from LLMs. We then evaluate the robustness of our adversarial\nprompts across (i) different reviewing prompts, (ii) different commercial\nLLM-based systems, and (iii) different peer-reviewed papers. Our results show\nthat adversarial prompts can reliably mislead the LLM, sometimes in ways that\nadversely affect a \"honest-but-lazy\" reviewer. Finally, we propose and\nempirically assess methods to reduce detectability of adversarial prompts under\nautomated content checks.", "AI": {"tldr": "This paper demonstrates that adversarial text manipulation in academic papers can reliably deceive LLM-based peer-review systems through invisible prompt injection attacks, even when attacks lack malicious intent, highlighting urgent security risks in automated academic evaluation processes.", "motivation": "The increasing use of LLMs in scientific peer-review necessitates understanding their reliability and vulnerabilities to manipulation, particularly through hidden attacks that could compromise review integrity without requiring malicious intent.", "method": "The researchers formalized three threat models to represent varying attacker motivations, developed adversarial prompts undetectable by humans, conducted a user study with scholars to derive peer-review prompts, and systematically evaluated adversarial effectiveness across multiple reviewing prompts, LLM systems, and papers. They also tested detection evasion techniques against automated checks.", "result": "Adversarial prompts successfully misled LLMs across diverse configurations, sometimes impacting 'honest-but-lazy' reviewers. The prompts demonstrated high reliability in manipulating outputs while remaining undetectable, with efficacy maintained across different LLM-based systems and academic papers.", "conclusion": "The study highlights the critical need for developing robust detection and prevention mechanisms to mitigate adversarial prompt attacks in automated peer-review systems, emphasizing the importance of secure implementation of LLMs in academic processes."}}
{"id": "2508.20866", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20866", "abs": "https://arxiv.org/abs/2508.20866", "authors": ["Amine Lbath", "Massih-Reza Amini", "Aurelien Delaitre", "Vadim Okun"], "title": "AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning", "comment": null, "summary": "The increasing complexity of software systems and the sophistication of\ncyber-attacks have underscored the critical need for effective automated\nvulnerability detection and repair systems. Traditional methods, such as static\nprogram analysis, face significant challenges related to scalability,\nadaptability, and high false-positive and false-negative rates. AI-driven\napproaches, particularly those using machine learning and deep learning models,\nshow promise but are heavily reliant on the quality and quantity of training\ndata. This paper introduces a novel framework designed to automatically\nintroduce realistic, category-specific vulnerabilities into secure C/C++\ncodebases to generate datasets. The proposed approach coordinates multiple AI\nagents that simulate expert reasoning, along with function agents and\ntraditional code analysis tools. It leverages Retrieval-Augmented Generation\nfor contextual grounding and employs Low-Rank approximation of weights for\nefficient model fine-tuning. Our experimental study on 116 code samples from\nthree different benchmarks suggests that our approach outperforms other\ntechniques with regard to dataset accuracy, achieving between 89\\% and 95\\%\nsuccess rates in injecting vulnerabilities at function level.", "AI": {"tldr": "This paper proposes an AI-agent-driven framework to generate realistic vulnerability datasets via RAG and low-rank fine-tuning, achieving 89\u201395% accuracy across 116 C/C++ code samples, outperforming prior methods.", "motivation": "Traditional methods struggle with scalability and accuracy, while AI approaches depend on high-quality training data. Generating realistic vulnerability datasets is critical for improving automated detection systems.", "method": "A novel framework coordinating multiple AI agents (simulating expert reasoning) and traditional tools, using Retrieval-Augmented Generation (RAG) for contextual grounding and Low-Rank approximation for model fine-tuning.", "result": "Experimental validation on 116 code samples from three benchmarks showed the framework outperforms existing techniques in dataset accuracy, with 89\u201395% success rates in function-level vulnerability injection.", "conclusion": "The proposed framework effectively addresses the limitations of traditional and AI-driven vulnerability detection methods by automatically generating realistic datasets with high accuracy, demonstrating success rates between 89% and 95% in injecting vulnerabilities into C/C++ codebases."}}
{"id": "2508.20890", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20890", "abs": "https://arxiv.org/abs/2508.20890", "authors": ["Mengxiao Wang", "Yuxuan Zhang", "Guofei Gu"], "title": "PromptSleuth: Detecting Prompt Injection via Semantic Intent Invariance", "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrated into real-world\napplications, from virtual assistants to autonomous agents. However, their\nflexibility also introduces new attack vectors-particularly Prompt Injection\n(PI), where adversaries manipulate model behavior through crafted inputs. As\nattackers continuously evolve with paraphrased, obfuscated, and even multi-task\ninjection strategies, existing benchmarks are no longer sufficient to capture\nthe full spectrum of emerging threats.\n  To address this gap, we construct a new benchmark that systematically extends\nprior efforts. Our benchmark subsumes the two widely-used existing ones while\nintroducing new manipulation techniques and multi-task scenarios, thereby\nproviding a more comprehensive evaluation setting. We find that existing\ndefenses, though effective on their original benchmarks, show clear weaknesses\nunder our benchmark, underscoring the need for more robust solutions. Our key\ninsight is that while attack forms may vary, the adversary's intent-injecting\nan unauthorized task-remains invariant. Building on this observation, we\npropose PromptSleuth, a semantic-oriented defense framework that detects prompt\ninjection by reasoning over task-level intent rather than surface features.\nEvaluated across state-of-the-art benchmarks, PromptSleuth consistently\noutperforms existing defense while maintaining comparable runtime and cost\nefficiency. These results demonstrate that intent-based semantic reasoning\noffers a robust, efficient, and generalizable strategy for defending LLMs\nagainst evolving prompt injection threats.", "AI": {"tldr": "This paper introduces a new benchmark for advanced prompt injection attacks and proposes PromptSleuth, an intent-based defense that outperforms existing solutions by analyzing task-level semantics instead of surface patterns.", "motivation": "Existing benchmarks fail to capture emerging prompt injection threats as adversaries evolve with paraphrased, obfuscated, and multi-task attacks, necessitating improved evaluation methods and defenses.", "method": "The paper introduces a new comprehensive benchmark expanding prior datasets with advanced injection techniques and multi-task scenarios, then proposes PromptSleuth, a semantic-oriented defense framework that detects injection by analyzing task-level intent rather than surface features.", "result": "Experimental results show that established defenses falter under the new benchmark, while PromptSleuth outperforms existing methods on multiple state-of-the-art benchmarks without compromising runtime or cost efficiency.", "conclusion": "The study concludes that intent-based semantic reasoning through PromptSleuth provides a robust, efficient, and generalizable defense against evolving prompt injection threats in LLMs."}}
{"id": "2508.20963", "categories": ["cs.CR", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.20963", "abs": "https://arxiv.org/abs/2508.20963", "authors": ["Brandon Beltz", "Jim Doty", "Yvonne Fonken", "Nikolos Gurney", "Brett Israelsen", "Nathan Lau", "Stacy Marsella", "Rachelle Thomas", "Stoney Trent", "Peggy Wu", "Ya-Ting Yang", "Quanyan Zhu"], "title": "Guarding Against Malicious Biased Threats (GAMBiT) Experiments: Revealing Cognitive Bias in Human-Subjects Red-Team Cyber Range Operations", "comment": null, "summary": "We present three large-scale human-subjects red-team cyber range datasets\nfrom the Guarding Against Malicious Biased Threats (GAMBiT) project. Across\nExperiments 1-3 (July 2024-March 2025), 19-20 skilled attackers per experiment\nconducted two 8-hour days of self-paced operations in a simulated enterprise\nnetwork (SimSpace Cyber Force Platform) while we captured multi-modal data:\nself-reports (background, demographics, psychometrics), operational notes,\nterminal histories, keylogs, network packet captures (PCAP), and NIDS alerts\n(Suricata). Each participant began from a standardized Kali Linux VM and\npursued realistic objectives (e.g., target discovery and data exfiltration)\nunder controlled constraints. Derivative curated logs and labels are included.\nThe combined release supports research on attacker behavior modeling,\nbias-aware analytics, and method benchmarking. Data are available via IEEE\nDataport entries for Experiments 1-3.", "AI": {"tldr": "GAMBiT project releases three large human-subjects cyber attack datasets (July 2024-March 2025) capturing skilled attackers' operations in a simulated network, enabling research on behavior modeling and bias-aware cybersecurity analytics.", "motivation": "To provide large-scale, empirically grounded datasets needed for studying real-world attacker behaviors, biases, and their operational patterns in a controlled cyber range environment.", "method": "Three 8-hour human-subjects experiments involving 19-20 skilled attackers each, using the SimSpace Cyber Force Platform and collecting multi-modal data including self-reports, operational records, terminal logs, PCAPs, and NIDS alerts.", "result": "Release of three datasets with multi-modal attacker behavior data, derivative logs, and labels, supporting research in behavior modeling, bias analysis, and defensive method benchmarking.", "conclusion": "The datasets contribute to advancing research in attacker behavior modeling, bias-aware analytics, and benchmarking methods by providing large-scale empirical data from skilled attackers in controlled simulations."}}
