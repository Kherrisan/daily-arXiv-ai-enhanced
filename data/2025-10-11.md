<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 21]
- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [PATCH: Mitigating PII Leakage in Language Models with Privacy-Aware Targeted Circuit PatcHing](https://arxiv.org/abs/2510.07452)
*Anthony Hughes,Vasisht Duddu,N. Asokan,Nikolaos Aletras,Ning Ma*

Main category: cs.CR

TL;DR: The paper introduces PATCH, a method that identifies and edits PII leakage circuits in LMs to effectively reduce PII leakage with minimal utility loss, achieving significant privacy improvements over existing defenses like DP.


<details>
  <summary>Details</summary>
Motivation: LMs may memorize and leak PII from their training data, even when using DP. This poses a privacy risk. A more targeted solution is needed to address this issue without large utility drops.

Method: The authors first use circuit discovery to identify the PII leakage circuits in LMs. Once these circuits are identified, they are  directly edited using PATCH to reduce PII leakage, and PATCH can also be combined with DP for further privacy enhancement.

Result: PATCH achieved a 65% reduction in PII leakage recall, and when combined with DP, reduced residual leakage to as low as 0.01%. This indicates a better performance in privacy-utility trade-off compared to existing defenses.

Conclusion: PATCH presents a more effective and efficient solution to mitigate PII leakage from LMs by directly targeting the responsible computational circuits, leading to significant privacy improvements with less utility loss.

Abstract: Language models (LMs) may memorize personally identifiable information (PII)
from training data, enabling adversaries to extract it during inference.
Existing defense mechanisms such as differential privacy (DP) reduce this
leakage, but incur large drops in utility. Based on a comprehensive study using
circuit discovery to identify the computational circuits responsible PII
leakage in LMs, we hypothesize that specific PII leakage circuits in LMs should
be responsible for this behavior. Therefore, we propose PATCH (Privacy-Aware
Targeted Circuit PatcHing), a novel approach that first identifies and
subsequently directly edits PII circuits to reduce leakage. PATCH achieves
better privacy-utility trade-off than existing defenses, e.g., reducing recall
of PII leakage from LMs by up to 65%. Finally, PATCH can be combined with DP to
reduce recall of residual leakage of an LM to as low as 0.01%. Our analysis
shows that PII leakage circuits persist even after the application of existing
defense mechanisms. In contrast, PATCH can effectively mitigate their impact.

</details>


### [2] [Comparison of Fully Homomorphic Encryption and Garbled Circuit Techniques in Privacy-Preserving Machine Learning Inference](https://arxiv.org/abs/2510.07457)
*Kalyan Cheerla,Lotfi Ben Othmane,Kirill Morozov*

Main category: cs.CR

TL;DR: This paper compares FHE and GC for secure neural network inference. GC offers faster, lower-memory execution, while FHE enables non-interactive but slower inference, highlighting a key privacy-performance trade-off in PPML.


<details>
  <summary>Details</summary>
Motivation: Growing privacy concerns in ML applications (e.g., healthcare, finance, NLP) necessitate solutions to protect sensitive data during inference. Privacy-preserving ML, particularly via FHE and GC, is critical but poorly understood in terms of performance trade-offs.

Method: The study implemented a two-layer neural network using CKKS (FHE) via Microsoft SEAL and TinyGarble2.0 (GC) under the semi-honest threat model. Evaluations focused on error, latency, memory, communication overhead, and rounds.

Result: FHE exhibits non-interactive inference but higher memory and slower execution. GC achieves faster runtime and lower memory consumption at the cost of interactivity. Trade-offs exist between privacy guarantees and computational efficiency.

Conclusion: The paper concludes that FHE and GC offer distinct advantages for secure neural network inference. GC provides faster execution and lower memory usage, making it suitable for interactive applications, while FHE enables non-interactive inference, prioritizing privacy over performance.

Abstract: Machine Learning (ML) is making its way into fields such as healthcare,
finance, and Natural Language Processing (NLP), and concerns over data privacy
and model confidentiality continue to grow. Privacy-preserving Machine Learning
(PPML) addresses this challenge by enabling inference on private data without
revealing sensitive inputs or proprietary models. Leveraging Secure Computation
techniques from Cryptography, two widely studied approaches in this domain are
Fully Homomorphic Encryption (FHE) and Garbled Circuits (GC). This work
presents a comparative evaluation of FHE and GC for secure neural network
inference. A two-layer neural network (NN) was implemented using the CKKS
scheme from the Microsoft SEAL library (FHE) and the TinyGarble2.0 framework
(GC) by IntelLabs. Both implementations are evaluated under the semi-honest
threat model, measuring inference output error, round-trip time, peak memory
usage, communication overhead, and communication rounds. Results reveal a
trade-off: modular GC offers faster execution and lower memory consumption,
while FHE supports non-interactive inference.

</details>


### [3] [A Secure Authentication-Driven Protected Data Collection Protocol in Internet of Things](https://arxiv.org/abs/2510.07462)
*Maryam Ataei Nezhad,Hamid Barati,Ali Barati*

Main category: cs.CR

TL;DR: This paper proposes a three-phase IoT security framework with layered encryption and authentication protocols. NS2 simulations demonstrate enhanced performance metrics and robustness against security threats in IoT networks.


<details>
  <summary>Details</summary>
Motivation: Privacy and information security concerns阻碍 IoT adoption due to hacking risks. Existing solutions lack effective multi-layer security mechanisms for IoT networks.

Method: Three-phase approach: 1) Star structure with unique shared keys between parent-child nodes for encryption. 2) Intracluster multi-hop communication using hop-specific encryption keys. 3) Intercluster authentication protocol for cluster heads. Evaluated via NS2 simulations.

Result: Simulation results show improvements in energy consumption, end-to-end delay, flexibility, packet delivery rate, and node survival rate compared to existing methods. NS2 validation confirms security effectiveness.

Conclusion: The proposed method enhances IoT security through three phases: star structure with unique keys, intracluster multi-hop encryption with key updates, and intercluster authentication. It outperforms others in energy, delay, and packet delivery.

Abstract: Internet of Things means connecting different devices through the Internet.
The Internet of things enables humans to remotely manage and control the
objects they use with the Internet infrastructure. After the advent of the
Internet of Things in homes, organizations, and private companies, privacy and
information security are the biggest concern. This issue has challenged the
spread of the Internet of things as news of the users theft of information by
hackers intensified. The proposed method in this paper consists of three
phases. In the first phase, a star structure is constructed within each
cluster, and a unique key is shared between each child and parent to encrypt
and secure subsequent communications. The second phase is for intracluster
communications, in which members of the cluster send their data to the cluster
head in a multi hop manner. Also, in this phase, the data is encrypted with
different keys in each hop, and at the end of each connection, the keys are
updated to ensure data security. The third phase is to improve the security of
inter cluster communications using an authentication protocol. In this way, the
cluster heads are authenticated before sending information to prevent malicious
nodes in the network. The proposed method is also simulated using NS2 software.
The results showed that the proposed method has improved in terms of energy
consumption, end-to-end delay, flexibility, packet delivery rate, and the
number of alive nodes compared to other methods.

</details>


### [4] [MIRANDA: short signatures from a leakage-free full-domain-hash scheme](https://arxiv.org/abs/2510.07479)
*Alain Couvreur,Thomas Debris-Alazard,Philippe Gaborit,Adrien Vinçotte*

Main category: cs.CR

TL;DR: Miranda is the first full-domain-hash signature scheme based on matrix codes, achieving efficient and secure signatures with minimal size (90 bytes) and avoiding rejection sampling. It leverages a simple trapdoor mechanism using subcode decoding and offers strong EUF-CMA security.


<details>
  <summary>Details</summary>
Motivation: The need for post-quantum secure, efficient signature schemes with practical implementations motivates Miranda. Existing GPV-based schemes like Falcon/Wave require complex rejection sampling, increasing implementation complexity and computational overhead.

Method: Miranda uses the GPV paradigm with a trapdoor based on subcodes of decodable matrix codes (e.g., Gabidulin codes). Signatures are generated via unique decoding solutions, and a uniform bit-sampling technique prevents trapdoor leakage without rejection sampling. The instance uses Gabidulin code lattices (matrix spaces).

Result: Achieved 128-bit security with 90-byte signatures and 2.6 MB public keys. Security analysis under EUF-CMA confirms robustness. The design eliminates rejection sampling, simplifying implementation compared to Falcon/Wave.

Conclusion: Miranda provides a practical, secure post-quantum signature alternative with minimal overhead and simplified implementation, enabling real-world adoption for lightweight and high-security applications.

Abstract: We present $\mathsf{Miranda}$, the first family of full-domain-hash
signatures based on matrix codes. This signature scheme fulfils the paradigm of
Gentry, Peikert and Vaikuntanathan ($\mathsf{GPV}$), which gives strong
security guarantees. Our trapdoor is very simple and generic: if we propose it
with matrix codes, it can actually be instantiated in many other ways since it
only involves a subcode of a decodable code (or lattice) in a unique decoding
regime of parameters. Though $\mathsf{Miranda}$ signing algorithm relies on a
decoding task where there is exactly one solution, there are many possible
signatures given a message to sign and we ensure that signatures are not
leaking information on their underlying trapdoor by means of a very simple
procedure involving the drawing of a small number of uniform bits. In
particular $\mathsf{Miranda}$ does not use a rejection sampling procedure which
makes its implementation a very simple task contrary to other
$\mathsf{GPV}$-like signatures schemes such as $\mathsf{Falcon}$ or even
$\mathsf{Wave}$.
  We instantiate $\mathsf{Miranda}$ with the famous family of Gabidulin codes
represented as spaces of matrices and we study thoroughly its security (in the
EUF-CMA security model). For~$128$ bits of classical security, the signature
sizes are as low as~$90$ bytes and the public key sizes are in the order
of~$2.6$ megabytes.

</details>


### [5] [EMPalm: Exfiltrating Palm Biometric Data via Electromagnetic Side-Channels](https://arxiv.org/abs/2510.07533)
*Haowen Xu,Tianya Zhao,Xuyu Wang,Lei Ma,Jun Dai,Alexander Wyglinski,Xiaoyan Sun*

Main category: cs.CR

TL;DR: Researchers developed EMPalm, an EM-leakage attack that covertly reconstructs high-quality palmprint/palmvein images from hardware emissions, achieving 65% spoofing accuracy on commercial systems.


<details>
  <summary>Details</summary>
Motivation: Current palm recognition systems emit EM signals during operation, inadvertently leaking biometric data. The authors aim to expose this under-explored attack vector and quantify risks to justify countermeasure development.

Method: EMPalm separates EM signals of palmprint and palmvein, identifies their frequency bands, reconstructs images, and employs a diffusion model to enhance fidelity. This involves signal processing, domain-specific feature restoration, and adversarial training for improved accuracy.

Result: EMPalm achieves 65.30% spoofing success rate across four models and demonstrates high-reconstruction quality (SSIM=0.79, PSNR=29.88 dB, FID=6.82) on 9 tested devices, proving the practicality of the attack.

Conclusion: The paper demonstrates significant vulnerabilities in palm biometric systems due to electromagnetic emissions, proving that existing hardware is susceptible to EM-based attacks. It underscores the need for hardware-level countermeasures and raises awareness of physical-layer security risks in biometric authentication.

Abstract: Palm recognition has emerged as a dominant biometric authentication
technology in critical infrastructure. These systems operate in either
single-modal form, using palmprint or palmvein individually, or dual-modal
form, fusing the two modalities. Despite this diversity, they share similar
hardware architectures that inadvertently emit electromagnetic (EM) signals
during operation. Our research reveals that these EM emissions leak palm
biometric information, motivating us to develop EMPalm--an attack framework
that covertly recovers both palmprint and palmvein images from eavesdropped EM
signals. Specifically, we first separate the interleaved transmissions of the
two modalities, identify and combine their informative frequency bands, and
reconstruct the images. To further enhance fidelity, we employ a diffusion
model to restore fine-grained biometric features unique to each domain.
Evaluations on seven prototype and two commercial palm acquisition devices show
that EMPalm can recover palm biometric information with high visual fidelity,
achieving SSIM scores up to 0.79, PSNR up to 29.88 dB, and FID scores as low as
6.82 across all tested devices, metrics that collectively demonstrate strong
structural similarity, high signal quality, and low perceptual discrepancy. To
assess the practical implications of the attack, we further evaluate it against
four state-of-the-art palm recognition models, achieving a model-wise average
spoofing success rate of 65.30% over 6,000 samples from 100 distinct users.

</details>


### [6] [A Minrank-based Encryption Scheme à la Alekhnovich-Regev](https://arxiv.org/abs/2510.07584)
*Thomas Debris-Alazard,Philippe Gaborit,Romaric Neveu,Olivier Ruatta*

Main category: cs.CR

TL;DR: This paper presents a new encryption scheme based on the stationary-MinRank problem, offering strong security guarantees similar to Alekhnovich and Regev's schemes but with improved practical efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the long-standing open question of constructing an encryption scheme whose security relies solely on the hardness of the MinRank problem, building on the foundations of earlier schemes based on LWE and decoding random linear codes that led to standardized protocols like HQC and Kyber.

Method: The method involves adapting existing Alekhnovich and Regev encryption schemes by basing their security on a variation of the MinRank problem, termed the stationary-MinRank problem. A key achievement is demonstrating a search-to-decision reduction for this problem, enabling strong security proofs.

Result: The result is a practical and competitive encryption scheme that provides security based solely on the stationary-MinRank problem. The scheme is more efficient than the original Alekhnovich and Regev's schemes and slightly less efficient than FrodoKEM, suggesting potential for further optimization through structural enhancements similar to those used in HQC and Kyber.

Conclusion: The paper concludes that the proposed encryption scheme partially solves the open problem of constructing MinRank-based encryption and meets practical efficiency criteria by drawing on techniques like those in HQC and Kyber for potential improvements.

Abstract: Introduced in 2003 and 2005, Alekhnovich and Regev' schemes were the first
public-key encryptions whose security is only based on the average hardness of
decoding random linear codes and LWE, without other security assumptions. Such
security guarantees made them very popular, being at the origin of the now
standardized HQC or Kyber.
  We present an adaptation of Alekhnovich and Regev' encryption scheme whose
security is only based on the hardness of a slight variation of MinRank, the
so-called stationary-MinRank problem. We succeeded to reach this strong
security guarantee by showing that stationary-MinRank benefits from a
search-to-decision reduction. Our scheme therefore brings a partial answer to
the long-standing open question of building an encryption scheme whose security
relies solely on the hardness of MinRank.
  Finally, we show after a thoroughly security analysis that our scheme is
practical and competitive with other encryption schemes admitting such strong
security guarantees. Our scheme is slightly less efficient than FrodoKEM, but
much more efficient than Alekhnovich and Regev' original schemes, with
possibilities of improvements by considering more structure, in the same way as
HQC and Kyber.

</details>


### [7] [Rethinking Reasoning: A Survey on Reasoning-based Backdoors in LLMs](https://arxiv.org/abs/2510.07697)
*Man Hu,Xinyi Wu,Zuofeng Suo,Jinbo Feng,Linghui Meng,Yanhao Jia,Anh Tuan Luu,Shuai Zhao*

Main category: cs.CR

TL;DR: This paper reviews reasoning-based backdoor attacks in LLMs, proposes a taxonomy, analyzes defenses, and outlines challenges to advance secure LLM research.


<details>
  <summary>Details</summary>
Motivation: Existing surveys on backdoor attacks lack focus on LLM reasoning capabilities, which are exploited for new security risks despite improving task performance.

Method: The paper introduces a taxonomy categorizing reasoning-based backdoor attacks into associative, passive, and active types, analyzes attack mechanisms, and presents defense strategies.

Result: The study systematically reviews reasoning-based backdoor attacks, establishes a unified framework, discusses defenses, and identifies unresolved challenges for future research.

Conclusion: This work provides a novel perspective on reasoning-based backdoor attacks in LLMs, offering a taxonomy, defenses, and future research directions to enhance LLM security.

Abstract: With the rise of advanced reasoning capabilities, large language models
(LLMs) are receiving increasing attention. However, although reasoning improves
LLMs' performance on downstream tasks, it also introduces new security risks,
as adversaries can exploit these capabilities to conduct backdoor attacks.
Existing surveys on backdoor attacks and reasoning security offer comprehensive
overviews but lack in-depth analysis of backdoor attacks and defenses targeting
LLMs' reasoning abilities. In this paper, we take the first step toward
providing a comprehensive review of reasoning-based backdoor attacks in LLMs by
analyzing their underlying mechanisms, methodological frameworks, and
unresolved challenges. Specifically, we introduce a new taxonomy that offers a
unified perspective for summarizing existing approaches, categorizing
reasoning-based backdoor attacks into associative, passive, and active. We also
present defense strategies against such attacks and discuss current challenges
alongside potential directions for future research. This work offers a novel
perspective, paving the way for further exploration of secure and trustworthy
LLM communities.

</details>


### [8] [ANCORA: Accurate Intrusion Recovery for Web Applications](https://arxiv.org/abs/2510.07806)
*Yihao Peng,Biao Ma,Hai Wan,Xibin Zhao*

Main category: cs.CR

TL;DR: ANCORA is a non-invasive system for precise intrusion recovery in web apps, enabling selective removal of attack effects while preserving legitimate data through syscall isolation, provenance graphs, and spatiotemporal anchors.


<details>
  <summary>Details</summary>
Motivation: Coarse-grained rollbacks cause data loss for legitimate users, and existing methods fail to attribute file/database modifications to specific malicious requests in high-concurrency environments.

Method: 1. Isolates attack-triggered syscalls via request-level tracing. 2. Creates provenance graph to track file changes, including exploit-spawned processes. 3. Uses spatiotemporal anchors (network tuple + time window search) to attribute database operations. 4. Performs unified rewind+selective replay to clean state.

Result: 99.9\% recovery accuracy across 10 apps/20 attack scenarios. 19.8\%/17.8\% max latency/QPS overhead. Recovery throughput: 110.7 DB ops/s, 27.2 files/s.

Conclusion: ANCORA achieves high-precision, low-overhead attack recovery without application changes, effectively preserving concurrent legitimate operations and removing attack fingerprints.

Abstract: Modern web application recovery presents a critical dilemma. Coarse-grained
snapshot rollbacks cause unacceptable data loss for legitimate users.
Surgically removing an attack's impact is hindered by a fundamental challenge
in high-concurrency environments: it is difficult to attribute resulting file
and database modifications to a specific attack-related request. We present
ANCORA, a system for precise intrusion recovery in web applications without
invasive instrumentation. ANCORA first isolates the full sequence of syscalls
triggered by a single malicious request. Based on this sequence, ANCORA
addresses file and database modifications separately. To trace file changes, it
builds a provenance graph that reveals all modifications, including those by
exploit-spawned processes. To attribute database operations, a more difficult
challenge due to connection pooling, ANCORA introduces a novel spatiotemporal
anchor. This anchor uses the request's network connection tuple and active time
window to pinpoint exact database operations. With all malicious file and
database operations precisely identified, ANCORA performs a unified rewind and
selective replay recovery. It reverts the system to a clean snapshot taken
before the attack, then selectively re-applies only legitimate operations to
both the file system and database. This completely removes the attack's effects
while preserving concurrent legitimate data. We evaluated ANCORA on 10 web
applications and 20 CVE-based attack scenarios with concurrency up to 150
connections. Experiments demonstrate ANCORA achieves 99.9% recovery accuracy
with manageable overhead: up to 19.8% response latency increase and 17.8% QPS
decrease in worst cases, and recovery throughput of 110.7 database operations
per second and 27.2 affected files per second, effectively preserving
legitimate data.

</details>


### [9] [Effective and Stealthy One-Shot Jailbreaks on Deployed Mobile Vision-Language Agents](https://arxiv.org/abs/2510.07809)
*Renhua Ding,Xiao Yang,Zhengwei Fang,Jun Luo,Kun He,Jun Zhu*

Main category: cs.CR

TL;DR: Researchers develop a stealthy one-shot jailbreak attack against vision-language mobile agents by injecting invisible prompts into legitimate apps, achieving high success rates in hijacking autonomous smartphone operations


<details>
  <summary>Details</summary>
Motivation: Existing UI-based attacks against mobile agents require conspicuous overlays, elevated permissions, or impractical assumptions, limiting their covert operation and real-world applicability. Current autonomous agents using large vision-language models remain under-protected against subtle prompt injection attacks through standard UI interactions.

Method: The attack framework combines three components: (1) low-privilege in-app prompt injection via UI text inputs, (2) touch-based activation that distinguishes agent vs. human interactions based on physical touch attributes, and (3) keyword-level detoxification using a heuristic-guided iterative-deepening search (HG-IDA*) to bypass filters. The method operates via the Android Debug Bridge (ADB) interface.

Result: Evaluation across closed-source and open-source LVLM backends shows high success rates in single-shot attack scenarios: 82.5% planning hijack and 75.0% execution hijack with GPT-4o. The attack demonstrates practicality in standard Android apps without requiring malicious UI overlays or privileged access.

Conclusion: The paper reveals a critical security flaw in mobile agents driven by LVLMs, demonstrating that practical, stealthy one-shot jailbreak attacks can be mounted with minimal permissions and without detectable UI manipulations. This exposes the urgent need for security countermeasures to protect autonomous smartphone operations.

Abstract: Large vision-language models (LVLMs) enable autonomous mobile agents to
operate smartphone user interfaces, yet vulnerabilities to UI-level attacks
remain critically understudied. Existing research often depends on conspicuous
UI overlays, elevated permissions, or impractical threat models, limiting
stealth and real-world applicability. In this paper, we present a practical and
stealthy one-shot jailbreak attack that leverages in-app prompt injections:
malicious applications embed short prompts in UI text that remain inert during
human interaction but are revealed when an agent drives the UI via ADB (Android
Debug Bridge). Our framework comprises three crucial components: (1)
low-privilege perception-chain targeting, which injects payloads into malicious
apps as the agent's visual inputs; (2) stealthy user-invisible activation, a
touch-based trigger that discriminates agent from human touches using physical
touch attributes and exposes the payload only during agent operation; and (3)
one-shot prompt efficacy, a heuristic-guided, character-level
iterative-deepening search algorithm (HG-IDA*) that performs one-shot,
keyword-level detoxification to evade on-device safety filters. We evaluate
across multiple LVLM backends, including closed-source services and
representative open-source models within three Android applications, and we
observe high planning and execution hijack rates in single-shot scenarios
(e.g., GPT-4o: 82.5% planning / 75.0% execution). These findings expose a
fundamental security vulnerability in current mobile agents with immediate
implications for autonomous smartphone operation.

</details>


### [10] [Decentralised Blockchain Management Through Digital Twins](https://arxiv.org/abs/2510.07901)
*Georgios Diamantopoulos,Nikos Tziritas,Rami Bahsoon,Georgios Theodoropoulos*

Main category: cs.CR

TL;DR: This paper proposes a decentralized blockchain management framework using multiple stakeholder-controlled digital twins organized in a secondary blockchain to enable dynamic governance without centralised control.


<details>
  <summary>Details</summary>
Motivation: Existing blockchain systems face a control vs decentralization trade-off in governance, limiting dynamic system management capabilities.

Method: Develops a dual-layer architecture where stakeholder-managed digital twins in a secondary blockchain coordinate consensus and decision propagation to the primary blockchain through decentralised orchestration.

Result: Simulation evaluations show the mechanism achieves rapid consensus formation and efficient blockchain reconfiguration with minimal operational overhead.

Conclusion: The digital twin-based approach successfully demonstrates decentralized blockchain management capabilities that resolve the traditional control-decentralization trade-off, enabling dynamic governance while maintaining decentralisation.

Abstract: The necessity of blockchain systems to remain decentralised limits current
solutions to blockchain governance and dynamic management, forcing a trade-off
between control and decentralisation. In light of the above, this work proposes
a dynamic and decentralised blockchain management mechanism based on digital
twins. To ensure decentralisation, the proposed mechanism utilises multiple
digital twins that the system's stakeholders control. To facilitate
decentralised decision-making, the twins are organised in a secondary
blockchain system that orchestrates agreement on, and propagation of decisions
to the managed blockchain. This enables the management of blockchain systems
without centralised control. A preliminary evaluation of the performance and
impact of the overheads introduced by the proposed mechanism is conducted
through simulation. The results demonstrate the proposed mechanism's ability to
reach consensus on decisions quickly and reconfigure the primary blockchain
with minimal overhead.

</details>


### [11] [From Defender to Devil? Unintended Risk Interactions Induced by LLM Defenses](https://arxiv.org/abs/2510.07968)
*Xiangtao Meng,Tianshuo Cong,Li Wang,Wenyu Chen,Zheng Li,Shanqing Guo,Xiaoyun Wang*

Main category: cs.CR

TL;DR: This work reveals problematic cross-dimensional interactions among LLM defense strategies, showing how safety, fairness, and privacy defenses can harm each other's effectiveness through neuron-level conflicts. The introduced CrossRiskEval framework highlights the need for holistic defense evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing defense strategies for LLMs are evaluated in isolation, overlooking their cross-dimensional impacts on safety, fairness, and privacy. This paper aims to investigate unintended interactions between defenses and risk dimensions to advance more effective risk mitigation in sensitive domains.

Method: The authors propose CrossRiskEval, a comprehensive evaluation framework to assess how defenses deployed in LLMs inadvertently affect other risk dimensions (safety, fairness, privacy). They conduct extensive empirical studies on 14 defense-deployed LLMs with 12 strategies and perform fine-grained neuron-level analysis to uncover mechanisms of unintended interactions.

Result: Three key findings are identified: 1) Safety defenses may amplify indirect privacy leakage and bias despite suppressing direct sensitive responses; 2) Fairness defenses increase misuse and privacy leakage risks; 3) Privacy defenses impair safety and exacerbate bias. Neuron-level analysis reveals conflict-entangled neurons that cause these unintended interactions.

Conclusion: The paper calls for a paradigm shift in LLM risk evaluation toward holistic, interaction-aware assessment of defense strategies, emphasizing the need to recognize and address conflict-entangled neurons that mediate unintended behaviors.

Abstract: Large Language Models (LLMs) have shown remarkable performance across various
applications, but their deployment in sensitive domains raises significant
concerns. To mitigate these risks, numerous defense strategies have been
proposed. However, most existing studies assess these defenses in isolation,
overlooking their broader impacts across other risk dimensions. In this work,
we take the first step in investigating unintended interactions caused by
defenses in LLMs, focusing on the complex interplay between safety, fairness,
and privacy. Specifically, we propose CrossRiskEval, a comprehensive evaluation
framework to assess whether deploying a defense targeting one risk
inadvertently affects others. Through extensive empirical studies on 14
defense-deployed LLMs, covering 12 distinct defense strategies, we reveal
several alarming side effects: 1) safety defenses may suppress direct responses
to sensitive queries related to bias or privacy, yet still amplify indirect
privacy leakage or biased outputs; 2) fairness defenses increase the risk of
misuse and privacy leakage; 3) privacy defenses often impair safety and
exacerbate bias. We further conduct a fine-grained neuron-level analysis to
uncover the underlying mechanisms of these phenomena. Our analysis reveals the
existence of conflict-entangled neurons in LLMs that exhibit opposing
sensitivities across multiple risk dimensions. Further trend consistency
analysis at both task and neuron levels confirms that these neurons play a key
role in mediating the emergence of unintended behaviors following defense
deployment. We call for a paradigm shift in LLM risk evaluation, toward
holistic, interaction-aware assessment of defense strategies.

</details>


### [12] [Composition Law of Conjugate Observables in Random Permutation Sorting Systems](https://arxiv.org/abs/2510.08013)
*Yurang R. Kuang*

Main category: cs.CR

TL;DR: This paper introduces a theoretically grounded, universally applicable method for generating high-purity randomness from computation using novel mathematical properties of permutation sorting dynamics.


<details>
  <summary>Details</summary>
Motivation: Current methods for extracting randomness from microarchitectural timing fluctuations lack guarantees of uniformity. The paper aims to address this by establishing a principled, platform-independent mechanism for achieving cryptographic-grade randomness.

Method: The authors derive a mathematical relationship between discrete permutation counts and continuous time variables via characteristic and probability generating functions, enabling entropy purification through geometric convergence. They prove convergence theorems with explicit bounds and validate the framework experimentally.

Result: Experimental results demonstrate Shannon entropy exceeding 7.9998 bits per byte and chi-square uniformity across diverse platforms, proving the method's effectiveness.

Conclusion: The paper presents a universal framework for generating provably uniform cryptographic randomness from general-purpose computational processes by leveraging a fundamental composition law in the Random Permutation Sorting System (RPSS).

Abstract: We present the discovery of a fundamental composition law governing conjugate
observables in the Random Permutation Sorting System (RPSS). The law links the
discrete permutation count Np and the continuous elapsed time T through a
functional relation connecting the characteristic function of timing
distributions to the probability generating function of permutation counts.
This framework enables entropy purification, transforming microarchitectural
timing fluctuations into uniform randomness via geometric convergence. We
establish convergence theorems with explicit bounds and validate the results
experimentally, achieving Shannon entropy above 7.9998 bits per byte and
chi-square uniformity across diverse platforms. The composition law provides a
universal foundation for generating provably uniform randomness from
general-purpose computation, securing cryptographic purity from emergent
computational dynamics.

</details>


### [13] [A Novel Ensemble Learning Approach for Enhanced IoT Attack Detection: Redefining Security Paradigms in Connected Systems](https://arxiv.org/abs/2510.08084)
*Hikmat A. M. Abdeljaber,Md. Alamgir Hossain,Sultan Ahmad,Ahmed Alsanad,Md Alimul Haque,Sudan Jha,Jabeen Nazeer*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The rapid expansion of Internet of Things (IoT) devices has transformed
industries and daily life by enabling widespread connectivity and data
exchange. However, this increased interconnection has introduced serious
security vulnerabilities, making IoT systems more exposed to sophisticated
cyber attacks. This study presents a novel ensemble learning architecture
designed to improve IoT attack detection. The proposed approach applies
advanced machine learning techniques, specifically the Extra Trees Classifier,
along with thorough preprocessing and hyperparameter optimization. It is
evaluated on several benchmark datasets including CICIoT2023, IoTID20,
BotNeTIoT L01, ToN IoT, N BaIoT, and BoT IoT. The results show excellent
performance, achieving high recall, accuracy, and precision with very low error
rates. These outcomes demonstrate the model efficiency and superiority compared
to existing approaches, providing an effective and scalable method for securing
IoT environments. This research establishes a solid foundation for future
progress in protecting connected devices from evolving cyber threats.

</details>


### [14] [LLM-Assisted Web Measurements](https://arxiv.org/abs/2510.08101)
*Simone Bozzolan,Stefano Calzavara,Lorenzo Cazzaro*

Main category: cs.CR

TL;DR: This paper explores leveraging Large Language Models (LLMs for targeted web measurement studies by utilizing their semantic understanding to classify websites, demonstrating that LLMs can effectively support security and privacy research.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in the lack of labeled semantic information in traditional top website lists, hindering targeted measurements. Researchers must resort to ad-hoc methods for dataset biasing, creating a need for systematic LLM-based classification and validation approaches.

Method: The authors (1 identify key website classification tasks relevant to web measurements, (2 construct datasets to evaluate LLM performance on these tasks, and (3 conduct LLM-assisted web measurement studies to validate research inferences using rigorous assessment.

Result: LLMs achieved strong performance across classification scenarios, and LLM-assisted measurement studies successfully validated research inferences, demonstrating their practicality for analyzing Web security/privacy trends.

Conclusion: LLMs can serve as a reliable tool for targeted web measurements by providing semantic classification capabilities, enabling researchers to draw valid inferences about security and privacy trends with minimal manual effort.

Abstract: Web measurements are a well-established methodology for assessing the
security and privacy landscape of the Internet. However, existing top lists of
popular websites commonly used as measurement targets are unlabeled and lack
semantic information about the nature of the sites they include. This
limitation makes targeted measurements challenging, as researchers often need
to rely on ad-hoc techniques to bias their datasets toward specific categories
of interest. In this paper, we investigate the use of Large Language Models
(LLMs) as a means to enable targeted web measurement studies through their
semantic understanding capabilities. Building on prior literature, we identify
key website classification tasks relevant to web measurements and construct
datasets to systematically evaluate the performance of different LLMs on these
tasks. Our results demonstrate that LLMs may achieve strong performance across
multiple classification scenarios. We then conduct LLM-assisted web measurement
studies inspired by prior work and rigorously assess the validity of the
resulting research inferences. Our results demonstrate that LLMs can serve as a
practical tool for analyzing security and privacy trends on the Web.

</details>


### [15] [TracE2E: Easily Deployable Middleware for Decentralized Data Traceability](https://arxiv.org/abs/2510.08225)
*Daniel Pressensé,Elisavet Kozyri*

Main category: cs.CR

TL;DR: TracE2E is a Rust-based middleware enabling data explainability and compliance across distributed systems through provenance tracking and policy enforcement without requiring significant application modifications.


<details>
  <summary>Details</summary>
Motivation: Existing solutions require substantial changes to applications to implement data-protection and compliance policies. There is a need for a lightweight, integration-friendly approach that maintains data provenance and policy enforcement across distributed nodes.

Method: TracE2E mediates input/output processes to record provenance information and enforce data protection policies (confidentiality/integrity). It achieves integration through a wrapper of the Rust standard library’s IO module, avoiding deep application modifications.

Result: The paper demonstrates consistent provenance recording across nodes and validates the compliance layer’s ability to enforce multiple data-protection policies effectively.

Conclusion: TracE2E provides a practical and minimally invasive solution for ensuring data explainability and regulatory compliance in distributed systems, addressing limitations in prior approaches.

Abstract: This paper presents TracE2E, a middleware written in Rust, that can provide
both data explainability and compliance across multiple nodes. By mediating
inputs and outputs of processes, TracE2E records provenance information and
enforces data-protection policies (e.g., confidentiality, integrity) that
depend on the recorded provenance. Unlike existing approaches that necessitate
substantial application modifications, TracE2E is designed for easy integration
into existing and future applications through a wrapper of the Rust standard
library's IO module. We describe how TracE2E consistently records provenance
information across nodes, and we demonstrate how the compliance layer of
TracE2E can accommodate the enforcement of multiple policies.

</details>


### [16] [Systematic Assessment of Cache Timing Vulnerabilities on RISC-V Processors](https://arxiv.org/abs/2510.08272)
*Cédrick Austa,Jan Tobias Mühlberg,Jean-Michel Dricot*

Main category: cs.CR

TL;DR: The authors ported an Intel x86-64 benchmark suite for cache-based timing vulnerabilities to RISC-V and evaluated three RISC-V processors, finding varying levels of vulnerability to microarchitectural attacks.


<details>
  <summary>Details</summary>
Motivation: As RISC-V gains popularity, there is a need for tools to evaluate the security of its processor implementations, particularly for microarchitectural side-channel vulnerabilities. Existing tools focus on established architectures like x86-64 and ARM, but not RISC-V.

Method: The authors ported a benchmark suite used for Intel x86-64 to RISC-V and used it to evaluate the cache-based timing vulnerabilities of three commercial RISC-V processors: the T-Head C910, and the SiFive U54 and U74 cores.

Result: The C910 processor showed more distinct timing types, suggesting a higher risk of vulnerability compared to the SiFive cores. Additionally, 37.5% of the vulnerabilities were present in all three processors, while only 6.8% were completely absent.

Conclusion: The ported benchmark and vulnerability assessment can help RISC-V designers identify and address microarchitectural leakage sources earlier in the design process, improving the architecture's security posture.

Abstract: While interest in the open RISC-V instruction set architecture is growing,
tools to assess the security of concrete processor implementations are lacking.
There are dedicated tools and benchmarks for common microarchitectural
side-channel vulnerabilities for popular processor families such as Intel
x86-64 or ARM, but not for RISC-V. In this paper we describe our efforts in
porting an Intel x86-64 benchmark suite for cache-based timing vulnerabilities
to RISC-V. We then use this benchmark to evaluate the security of three
commercially available RISC-V processors, the T-Head C910 and the SiFive U54
and U74 cores. We observe that the C910 processor exhibits more distinct timing
types than the other processors, leading to the assumption that code running on
the C910 would be exposed to more microarchitectural vulnerability sources. In
addition, our evaluation reveals that $37.5\%$ of the vulnerabilities covered
by the benchmark exist in all processors, while only $6.8\%$ are absent from
all cores. Our work, in particular the ported benchmark, aims to support RISC-V
processor designers to identify leakage sources early in their designs and to
support the development of countermeasures.

</details>


### [17] [New Machine Learning Approaches for Intrusion Detection in ADS-B](https://arxiv.org/abs/2510.08333)
*Mikaëla Ngamboé,Jean-Simon Marrocco,Jean-Yves Ouattara,José M. Fernandez,Gabriela Nicolescu*

Main category: cs.CR

TL;DR: This paper proposes novel AI-based intrusion detection systems (IDSs) for securing the vulnerable ADS-B protocol in air traffic management, comparing transformer and xLSTM architectures. While the xLSTM model demonstrates superior attack detection accuracy (98.9 F1-score), it introduces latency that must be balanced against operational requirements.


<details>
  <summary>Details</summary>
Motivation: The widespread adoption of the security-critical yet inherently vulnerable ADS-B protocol in air traffic management necessitates advanced intrusion detection solutions to counter threats requiring real-time detection of subtle, progressive attacks.

Method: The study evaluates two deep learning IDS implementations (transformer-based and xLSTM-based) using a transfer learning strategy: pre-training on benign ADS-B messages followed by fine-tuning on labeled hybrid datasets containing attack instances. This represents the first xLSTM application to ADS-B security.

Result: The xLSTM-based IDS achieves 98.9% F1-score vs. 94.3% for the transformer model while maintaining generalizability to unseen attacks. However, it introduces 7.26s latency (vs. 2.1s for transformers), constrained by Secondary Surveillance Radar refresh intervals.

Conclusion: While xLSTM outperforms in detection accuracy and attack generalization, its latency suggests suitability for SSR-interval compatible applications rather than time-critical operations, highlighting a critical performance-latency tradeoff in ADS-B IDS deployment.

Abstract: With the growing reliance on the vulnerable Automatic Dependent
Surveillance-Broadcast (ADS-B) protocol in air traffic management (ATM),
ensuring security is critical. This study investigates emerging machine
learning models and training strategies to improve AI-based intrusion detection
systems (IDS) for ADS-B. Focusing on ground-based ATM systems, we evaluate two
deep learning IDS implementations: one using a transformer encoder and the
other an extended Long Short-Term Memory (xLSTM) network, marking the first
xLSTM-based IDS for ADS-B. A transfer learning strategy was employed, involving
pre-training on benign ADS-B messages and fine-tuning with labeled data
containing instances of tampered messages. Results show this approach
outperforms existing methods, particularly in identifying subtle attacks that
progressively undermine situational awareness. The xLSTM-based IDS achieves an
F1-score of 98.9%, surpassing the transformer-based model at 94.3%. Tests on
unseen attacks validated the generalization ability of the xLSTM model.
Inference latency analysis shows that the 7.26-second delay introduced by the
xLSTM-based IDS fits within the Secondary Surveillance Radar (SSR) refresh
interval (5-12 s), although it may be restrictive for time-critical operations.
While the transformer-based IDS achieves a 2.1-second latency, it does so at
the cost of lower detection performance.

</details>


### [18] [A Haskell to FHE Transpiler](https://arxiv.org/abs/2510.08343)
*Anne Müller,Mohd Kashif,Nico Döttling*

Main category: cs.CR

TL;DR: This paper introduces a Haskell transpiler for circuit generation compatible with Fully Homomorphic Encryption (FHE), along with an automatic parallelization method for these circuits, tested on PIR and AES with varying performance outcomes compared to manual methods.


<details>
  <summary>Details</summary>
Motivation: Fully Homomorphic Encryption (FHE) allows computations on encrypted data but requires programs to be expressed as circuits, making application development cumbersome. Current transpilers are limited to a few languages, and manual parallelization is necessary, which is labor-intensive.

Method: The authors developed a Haskell transpiler that converts high-level Haskell programs into Boolean circuits suitable for FHE. They implemented an evaluator that parallelizes the circuits by executing layer-wise gates, making the process automatic rather than manual.

Result: The paper demonstrates successful transpilation and parallelization of PIR and AES applications. For AES, the automated parallelization achieved evaluation times of 28 seconds with 16 threads and 8 seconds with 100 threads, outperforming some manual parallelizations.

Conclusion: While the automated parallelization is effective, manual parallelization still holds advantages in certain scenarios. The Haskell transpiler and automatic circuit parallelization reduce the burden of FHE application development, opening new opportunities but also highlighting the current limitations in performance.

Abstract: Fully Homomorphic Encryption (FHE) enables the evaluation of programs
directly on encrypted data. However, because only basic operations can be
performed on ciphertexts, programs must be expressed as boolean or arithmetic
circuits. This low-level representation makes implementing applications for FHE
significantly more cumbersome than writing code in a high-level language. To
reduce this burden, several transpilers have been developed that translate
high-level code into circuit representations. In this work, we extend the range
of high-level languages that can target FHE by introducing a transpiler for
Haskell, which converts Haskell programs into Boolean circuits suitable for
homomorphic evaluation. Our second contribution is the automatic
parallelization of these generated circuits. We implement an evaluator that
executes gates in parallel by parallelizing each layer of the circuit. We
demonstrate the effectiveness of our approach on two key applications: Private
Information Retrieval (PIR) and the AES encryption standard. Prior work has
parallelized AES encryption manually. We demonstrate that the automated method
outperforms some but not all manual parallelizations of AES evaluations under
FHE. We achieve an evaluation time of 28 seconds for a parallel execution with
16 threads and an evaluation time of 8 seconds for a parallel execution with
100 threads

</details>


### [19] [ExPrESSO: Zero-Knowledge backed Extensive Privacy Preserving Single Sign-on](https://arxiv.org/abs/2510.08355)
*Kaustabh Barman,Fabian Piper,Sanjeet Raj Pandey,Axel Kuepper*

Main category: cs.CR

TL;DR: This paper presents a privacy-preserving SSO solution using zk-SNARKs to hide service provider identities during authentication, achieving strong security and efficiency for real-world deployment.


<details>
  <summary>Details</summary>
Motivation: Standardized SSO systems like OIDC fail to protect user privacy as identity providers can track activities. This motivates the need for privacy-preserving authentication mechanisms.

Method: The system employs Groth's zk-SNARK to prove membership of subscribed service providers without revealing identities, coupled with a decentralized and verifiable setup to ensure trust and security.

Result: The approach satisfies stringent security requirements while maintaining low storage and latency overheads, validating its feasibility for production use.

Conclusion: The proposed zero-knowledge-based mechanism enhances user privacy in Single-Sign On (SSO) systems by integrating with OIDC, enabling secure authentication without exposing service provider information. It achieves high security with minimal performance costs, demonstrating practical deployment potential.

Abstract: User authentication is one of the most important aspects for secure
communication between services and end-users over the Internet. Service
providers leverage Single-Sign On (SSO) to make it easier for their users to
authenticate themselves. However, standardized systems for SSO, such as OIDC,
do not guarantee user privacy as identity providers can track user activities.
We propose a zero-knowledge-based mechanism that integrates with OIDC to let
users authenticate through SSO without revealing information about the service
provider. Our system leverages Groth's zk-SNARK to prove membership of
subscribed service providers without revealing their identity. We adopt a
decentralized and verifiable approach to set up the prerequisites of our
construction that further secures and establishes trust in the system. We set
up high security targets and achieve them with minimal storage and latency
cost, proving that our research can be adopted for production.

</details>


### [20] [Rethinking Provenance Completeness with a Learning-Based Linux Scheduler](https://arxiv.org/abs/2510.08479)
*Jinsong Mao,Benjamin E. Ujcich,Shiqing Ma*

Main category: cs.CR

TL;DR: This paper introduces Venus, a reinforcement learning-based kernel scheduler for Linux that mitigates the 'super producer threat' by improving provenance collection completeness and efficiency. It outperforms traditional schedulers while maintaining low overheads.


<details>
  <summary>Details</summary>
Motivation: Existing provenance systems risk missing security-relevant events under high load (via the 'super producer threat'), and current solutions like resource isolation fail to address hardware and performance limitations effectively.

Method: Venus is a learned scheduler using reinforcement learning to dynamically optimize resource allocation for provenance tasks. It learns task behaviors and integrates into Linux's kernel scheduler to ensure completeness requirements.

Result: Venus improves both provenance completeness and efficiency compared to traditional scheduling. It maintains reasonable overheads and, in some cases, enhances overall runtime versus the default Linux scheduler.

Conclusion: Venus demonstrates that kernel-level machine learning can mitigate the super producer threat, offering a practical solution to maintain secure, complete provenance while adapting to system constraints.

Abstract: Provenance plays a critical role in maintaining traceability of a system's
actions for root cause analysis of security threats and impacts. Provenance
collection is often incorporated into the reference monitor of systems to
ensure that an audit trail exists of all events, that events are completely
captured, and that logging of such events cannot be bypassed. However, recent
research has questioned whether existing state-of-the-art provenance collection
systems fail to ensure the security guarantees of a true reference monitor due
to the 'super producer threat' in which provenance generation can overload a
system to force the system to drop security-relevant events and allow an
attacker to hide their actions. One approach towards solving this threat is to
enforce resource isolation, but that does not fully solve the problems
resulting from hardware dependencies and performance limitations.
  In this paper, we show how an operating system's kernel scheduler can
mitigate this threat, and we introduce Venus, a learned scheduler for Linux
specifically designed for provenance. Unlike conventional schedulers that
ignore provenance completeness requirements, Venus leverages reinforcement
learning to learn provenance task behavior and to dynamically optimize resource
allocation. We evaluate Venus's efficacy and show that Venus significantly
improves both the completeness and efficiency of provenance collection systems
compared to traditional scheduling, while maintaining reasonable overheads and
even improving overall runtime in certain cases compared to the default Linux
scheduler.

</details>


### [21] [AI-Driven Post-Quantum Cryptography for Cyber-Resilient V2X Communication in Transportation Cyber-Physical Systems](https://arxiv.org/abs/2510.08496)
*Akid Abrar,Sagar Dasgupta,Mizanur Rahman,Ahmad Alsharif*

Main category: cs.CR

TL;DR: The chapter explores the integration of AI-enhanced Post-Quantum Cryptography (PQC) into Transportation Cyber-Physical Systems (TCPS) to address security vulnerabilities in the era of quantum computing while maintaining system performance.


<details>
  <summary>Details</summary>
Motivation: Quantum computing poses a significant threat to traditional cryptographic methods used in TCPS, necessitating the shift to PQC and the use of AI to enhance these algorithms for secure and resilient communications.

Method: The paper analyzes current TCPS communication protocols, identifies their vulnerabilities to cyber-attacks, illustrates the limitations of traditional cryptographic methods against quantum threats, and then discusses how AI can be leveraged to optimize PQC implementations.

Result: The analysis shows that the combination of AI and PQC can optimize algorithm selection and resource allocation, adapt to evolving threats, and improve the efficiency and effectiveness of secure communications.

Conclusion: Integrating AI-driven Post-Quantum Cryptography into TCPS is critical for ensuring robust and future-proof security against quantum threats while preserving system performance.

Abstract: Transportation Cyber-Physical Systems (TCPS) integrate physical elements,
such as transportation infrastructure and vehicles, with cyber elements via
advanced communication technologies, allowing them to interact seamlessly. This
integration enhances the efficiency, safety, and sustainability of
transportation systems. TCPS rely heavily on cryptographic security to protect
sensitive information transmitted between vehicles, transportation
infrastructure, and other entities within the transportation ecosystem,
ensuring data integrity, confidentiality, and authenticity. Traditional
cryptographic methods have been employed to secure TCPS communications, but the
advent of quantum computing presents a significant threat to these existing
security measures. Therefore, integrating Post-Quantum Cryptography (PQC) into
TCPS is essential to maintain secure and resilient communications. While PQC
offers a promising approach to developing cryptographic algorithms resistant to
quantum attacks, artificial intelligence (AI) can enhance PQC by optimizing
algorithm selection, resource allocation, and adapting to evolving threats in
real-time. AI-driven PQC approaches can improve the efficiency and
effectiveness of PQC implementations, ensuring robust security without
compromising system performance. This chapter introduces TCPS communication
protocols, discusses the vulnerabilities of corresponding communications to
cyber-attacks, and explores the limitations of existing cryptographic methods
in the quantum era. By examining how AI can strengthen PQC solutions, the
chapter presents cyber-resilient communication strategies for TCPS.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [22] [Modeling Developer Burnout with GenAI Adoption](https://arxiv.org/abs/2510.07435)
*Zixuan Feng,Sadia Afroz,Anita Sarma*

Main category: cs.SE

TL;DR: GenAI boosts developer productivity but risks burnout by raising demands. This study finds that investing in job resources and fostering positive GenAI attitudes can transform this risk into an opportunity for healthier, more effective workflows.


<details>
  <summary>Details</summary>
Motivation: Prior research focused on productivity gains from GenAI, but this paper addresses a critical gap by investigating how GenAI adoption may harm developers' well-being through burnout, exploring contextual factors like job demands and resources.

Method: The paper employs a concurrent embedded mixed-methods design, combining a survey of 442 developers with statistical analyses (PLS-SEM and regression) and qualitative analysis of open-ended responses, grounded in the Job Demands--Resources (JD-R) model.

Result: GenAI adoption is shown to increase job demands and burnout risks; however, sufficient job resources and positive developer perceptions of GenAI reduce these effects, revealing a path to mitigate harm.

Conclusion: The study concludes that while GenAI adoption can heighten developer burnout by increasing job demands, the negative impacts can be mitigated through adequate job resources and positive perceptions of GenAI, reframing adoption as an opportunity for organizations to support balanced workflows.

Abstract: Generative AI (GenAI) is rapidly reshaping software development workflows.
While prior studies emphasize productivity gains, the adoption of GenAI also
introduces new pressures that may harm developers' well-being. In this paper,
we investigate the relationship between the adoption of GenAI and developers'
burnout. We utilized the Job Demands--Resources (JD--R) model as the analytic
lens in our empirical study. We employed a concurrent embedded mixed-methods
research design, integrating quantitative and qualitative evidence. We first
surveyed 442 developers across diverse organizations, roles, and levels of
experience. We then employed Partial Least Squares--Structural Equation
Modeling (PLS-SEM) and regression to model the relationships among job demands,
job resources, and burnout, complemented by a qualitative analysis of
open-ended responses to contextualize the quantitative findings. Our results
show that GenAI adoption heightens burnout by increasing job demands, while job
resources and positive perceptions of GenAI mitigate these effects, reframing
adoption as an opportunity.

</details>


### [23] [HotBugs.jar: A Benchmark of Hot Fixes for Time-Critical Bugs](https://arxiv.org/abs/2510.07529)
*Carol Hanna,Federica Sarro,Mark Harman,Justyna Petke*

Main category: cs.SE

TL;DR: Presents HotBugs.jar, the first benchamark of real-world hot fixes from 10 Apache projects containing 679 validated patches (110 reproducible), enabling evaluation of tools for rapid debugging and automated repair.


<details>
  <summary>Details</summary>
Motivation: Hot fixes are critical yet lack dedicated research benchmarks, preventing evaluation of approaches for rapid debugging and automated repair in production systems.

Method: Collected 10 Apache projects with 190k commits/150k issues, identified 746 potential hot fixes via mining, manually validated 679 cases, and used Jira data/internal control-flow analysis to package 110 reproducible fixes with test suites and metadata extending the Bugs.jar framework.

Result: Released HotBugs.jar with 679 validated hot fixes (110 reproducible), adopted as SBSE conference challenge dataset, providing reproduceable evaluation criteria for production-critical software tools.

Conclusion: First benchmark enables systematic study of tools for hot-fix scenarios, filling critical gap in evaluating automated repair and resilience techniques for production systems, with metadata to enable future research directions.

Abstract: Hot fixes are urgent, unplanned changes deployed to production systems to
address time-critical issues. Despite their importance, no existing evaluation
benchmark focuses specifically on hot fixes. We present HotBugs$.$jar, the
first dataset dedicated to real-world hot fixes. From an initial mining of 10
active Apache projects totaling over 190K commits and 150K issue reports, we
identified 746 software patches that met our hot-fix criteria. After manual
evaluation, 679 were confirmed as genuine hot fixes, of which 110 are
reproducible using a test suite. Building upon the Bugs$.$jar framework,
HotBugs$.$jar integrates these 110 reproducible cases and makes available all
679 manually validated hot fixes, each enriched with comprehensive metadata to
support future research. Each hot fix was systematically identified using Jira
issue data, validated by independent reviewers, and packaged in a reproducible
format with buggy and fixed versions, test suites, and metadata. HotBugs$.$jar
has already been adopted as the official challenge dataset for the Search-Based
Software Engineering (SBSE) Conference Challenge Track, demonstrating its
immediate impact. This benchmark enables the study and evaluation of tools for
rapid debugging, automated repair, and production-grade resilience in modern
software systems to drive research in this essential area forward.

</details>


### [24] [RustAssure: Differential Symbolic Testing for LLM-Transpiled C-to-Rust Code](https://arxiv.org/abs/2510.07604)
*Yubo Bai,Tapti Palit*

Main category: cs.SE

TL;DR: Motivated by memory safety concerns in legacy C code, RustAssure uses LLMs and differential symbolic testing to transpile C to Rust, achieving 89.8% compilability and 69.9% semantic equivalence in real-world codebases.


<details>
  <summary>Details</summary>
Motivation: Legacy C codebases are vulnerable to memory safety issues; this work addresses the challenge of automatically converting such code to memory-safe Rust while preserving correctness.

Method: RustAssure leverages Large Language Models (LLMs) and prompt engineering for transpilation, followed by differential symbolic testing to verify semantic equivalence between C and Rust code.

Result: The system achieved 89.8% compilable Rust function conversion from C, with 69.9% of those passing symbolic equivalence tests across five real-world applications.

Conclusion: RustAssure demonstrates that combining LLMs with prompt engineering and differential symbolic testing can effectively transpile C code to Rust while maintaining semantic similarity, offering a promising approach for enhancing software security.

Abstract: Rust is a memory-safe programming language that significantly improves
software security. Existing codebases written in unsafe memory languages, such
as C, must first be transpiled to Rust to take advantage of Rust's improved
safety guarantees. RustAssure presents a system that uses Large Language Models
(LLMs) to automatically transpile existing C codebases to Rust. RustAssure uses
prompt engineering techniques to maximize the chances of the LLM generating
idiomatic and safe Rust code. Moreover, because LLMs often generate code with
subtle bugs that can be missed under traditional unit or fuzz testing,
RustAssure performs differential symbolic testing to establish the semantic
similarity between the original C and LLM-transpiled Rust code. We evaluated
RustAssure with five real-world applications and libraries, and showed that our
system is able to generate compilable Rust functions for 89.8% of all C
functions, of which 69.9% produced equivalent symbolic return values for both
the C and Rust functions.

</details>


### [25] [AppForge: From Assistant to Independent Developer -- Are GPTs Ready for Software Development?](https://arxiv.org/abs/2510.07740)
*Dezhi Ran,Yuan Cao,Mengzhou Wu,Simin Chen,Yuzhe Guo,Jun Ren,Zihe Song,Hao Yu,Jialei Wei,Linyi Li,Wei Yang,Baishakhi Ray,Tao Xie*

Main category: cs.SE

TL;DR: The paper introduces APPFORGE, a benchmark for LLMs to build full Android apps from specs, revealing their current struggles in complex software development.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on code generation at the function level, while real-world app development involves system-wide reasoning about components, states, and lifecycles. The authors find a gap in evaluating LLMs' ability to manage complex software systems.\n

Method: APPFORGE consists of 101 real-world Android app development tasks derived from app documents, and a multi-agent system is used to automatically generate test cases for validation. A manual verification process is followed to ensure accuracy, and an automated evaluation framework is set up for reproducibility.\n

Result: Across 12 LLMs, the effectiveness is low. GPT-5, the best model, produces correct app implementations in only 18.8% of the cases.\n

Conclusion: The findings emphasize that current LLMs still lack the competence to handle software engineering challenges involving complex, multi-component development tasks for Android. APPFORGE is made available for advancing future research in this area.\n

Abstract: Large language models (LLMs) have demonstrated remarkable capability in
function-level code generation tasks. Unlike isolated functions, real-world
applications demand reasoning over the entire software system: developers must
orchestrate how different components interact, maintain consistency across
states over time, and ensure the application behaves correctly within the
lifecycle and framework constraints. Yet, no existing benchmark adequately
evaluates whether LLMs can bridge this gap and construct entire software
systems from scratch. To address this gap, we propose APPFORGE, a benchmark
consisting of 101 software development problems drawn from real-world Android
apps. Given a natural language specification detailing the app functionality, a
language model is tasked with implementing the functionality into an Android
app from scratch. Developing an Android app from scratch requires understanding
and coordinating app states, lifecycle management, and asynchronous operations,
calling for LLMs to generate context-aware, robust, and maintainable code. To
construct APPFORGE, we design a multi-agent system to automatically summarize
the main functionalities from app documents and navigate the app to synthesize
test cases validating the functional correctness of app implementation.
Following rigorous manual verification by Android development experts, APPFORGE
incorporates the test cases within an automated evaluation framework that
enables reproducible assessment without human intervention, making it easily
adoptable for future research. Our evaluation on 12 flagship LLMs show that all
evaluated models achieve low effectiveness, with the best-performing model
(GPT-5) developing only 18.8% functionally correct applications, highlighting
fundamental limitations in current models' ability to handle complex,
multi-component software engineering challenges.

</details>


### [26] [Interleaved Learning and Exploration: A Self-Adaptive Fuzz Testing Framework for MLIR](https://arxiv.org/abs/2510.07815)
*Zeyu Sun,Jingjing Liang,Weiyi Wang,Chenyao Suo,Junjie Chen,Fanjiang Xu*

Main category: cs.SE

TL;DR: FLEX, a self-adaptive MLIR fuzzer, discovers 80 bugs in 30 days using neural program generation and diversity-driven strategies.


<details>
  <summary>Details</summary>
Motivation: Existing MLIR fuzzing methods fail to generate sufficiently diverse and semantically valid test cases, limiting their ability to expose deep-seated bugs in MLIR's complex codebase.

Method: FLEX employs neural networks for program generation, perturbed sampling for diversity, and a feedback-driven augmentation loop using both crashing and non-crashing test cases to iteratively enhance its model.

Result: FLEX discovers 80 new bugs in 30 days (including 53 in 24 hours with 3.5x more bugs than the best baseline) and achieves 28.2% code coverage (42% better than the next-best tool).

Conclusion: FLEX significantly outperforms existing fuzzing tools in finding bugs and improving code coverage for MLIR, demonstrating the effectiveness of its self-adaptive framework.

Abstract: MLIR (Multi-Level Intermediate Representation) has rapidly become a
foundational technology for modern compiler frameworks, enabling extensibility
across diverse domains. However, ensuring the correctness and robustness of
MLIR itself remains challenging. Existing fuzzing approaches-based on manually
crafted templates or rule-based mutations-struggle to generate sufficiently
diverse and semantically valid test cases, making it difficult to expose subtle
or deep-seated bugs within MLIR's complex and evolving code space. In this
paper, we present FLEX, a novel self-adaptive fuzzing framework for MLIR. FLEX
leverages neural networks for program generation, a perturbed sampling strategy
to encourage diversity, and a feedback-driven augmentation loop that
iteratively improves its model using both crashing and non-crashing test cases.
Starting from a limited seed corpus, FLEX progressively learns valid syntax and
semantics and autonomously produces high-quality test inputs. We evaluate FLEX
on the upstream MLIR compiler against four state-of-the-art fuzzers. In a
30-day campaign, FLEX discovers 80 previously unknown bugs-including multiple
new root causes and parser bugs-while in 24-hour fixed-revision comparisons, it
detects 53 bugs (over 3.5x as many as the best baseline) and achieves 28.2%
code coverage, outperforming the next-best tool by 42%. Ablation studies
further confirm the critical role of both perturbed generation and diversity
augmentation in FLEX's effectiveness.

</details>


### [27] [Bug Histories as Sources of Compiler Fuzzing Mutators](https://arxiv.org/abs/2510.07834)
*Lingjun Liu,Feiran Qin,Owolabi Legunsen,Marcelo d'Amorim*

Main category: cs.SE

TL;DR: IssueMut extracts mutators from compiler bug histories to enhance fuzzing, discovering 28-37 new bugs in GCC/LLVM that existing tools missed, proving historical data can improve compiler testing.


<details>
  <summary>Details</summary>
Motivation: Prior compiler fuzzers lack methods to leverage historical bug data, despite the potential of such information to guide fuzzers toward high-impact mutations. This gap limits the effectiveness of existing mutators.

Method: IssueMut proposes an automated approach to mine mutators from compiler bug reports by analyzing program elements linked to past bugs. It retrofits these data-driven mutators into existing fuzzers, extracting 587 mutators from 1,760 GCC and LLVM bug reports.

Result: IssueMut discovered 28 new bugs in GCC and 37 in LLVM, with 60 confirmed or fixed, significantly outperforming state-of-the-art mutational fuzzers which missed these vulnerabilities.

Conclusion: Bug histories are a rich source of information for compiler fuzzing, and IssueMut demonstrates that leveraging these histories can uncover new bugs missed by existing tools, suggesting developers should incorporate historical data into fuzzing strategies.

Abstract: Bugs in compilers, which are critical infrastructure today, can have outsized
negative impacts. Mutational fuzzers aid compiler bug detection by
systematically mutating compiler inputs, i.e., programs. Their effectiveness
depends on the quality of the mutators used. Yet, no prior work used compiler
bug histories as a source of mutators. We propose IssueMut, the first approach
for extracting compiler fuzzing mutators from bug histories. Our insight is
that bug reports contain hints about program elements that induced compiler
bugs; they can guide fuzzers towards similar bugs. IssueMut uses an automated
method to mine mutators from bug reports and retrofit such mutators into
existing mutational compiler fuzzers. Using IssueMut, we mine 587 mutators from
1760 GCC and LLVM bug reports. Then, we run IssueMut on these compilers, with
all their test inputs as seed corpora. We find that "bug history" mutators are
effective: they find new bugs that a state-of-the-art mutational compiler
fuzzer misses-28 in GCC and 37 in LLVM. Of these, 60 were confirmed or fixed,
validating our idea that bug histories have rich information that compiler
fuzzers should leverage.

</details>


### [28] [An AUTOSAR-Aligned Architectural Study of Vulnerabilities in Automotive SoC Software](https://arxiv.org/abs/2510.07941)
*Srijita Basu,Haraldsson Bengt,Miroslaw Staron,Christian Berger,Jennifer Horkoff,Magnus Almgren*

Main category: cs.SE

TL;DR: This paper analyzes 180 automotive SoC vulnerabilities in AUTOSAR-aligned architectures, identifies 16 root causes and 56 affected modules, and provides security strategies for automotive CPS platforms.


<details>
  <summary>Details</summary>
Motivation: The complexity of CCAM systems and the lack of systematic analysis of SoC vulnerabilities in AUTOSAR-aligned architectures motivate this study to understand root causes and mitigation delays in safety-critical environments.

Method: The authors mapped 180 publicly reported vulnerabilities to a representative SoC architecture model aligned with AUTOSAR principles, identifying root causes, affected modules, and mitigation delays by CWE categories and architectural layers.

Result: 16 root causes and 56 affected modules were identified, revealing dominant vulnerability patterns and critical modules with prolonged patch delays, alongside actionable security strategies for automotive CPS platforms.

Conclusion: The study highlights priority areas for securing automotive SoCs under AUTOSAR, emphasizing detection, prioritization, and localization strategies to address systemic vulnerability challenges in layered CPS architectures.

Abstract: Cooperative, Connected and Automated Mobility (CCAM) are complex
cyber-physical systems (CPS) that integrate computation, communication, and
control in safety-critical environments. At their core, System-on-Chip (SoC)
platforms consolidate processing units, communication interfaces, AI
accelerators, and security modules into a single chip. AUTOSAR (AUTomotive Open
System ARchitecture) standard was developed in the automotive domain to better
manage this complexity, defining layered software structures and interfaces to
facilitate reuse of HW/SW components. However, in practice, this integrated SoC
software architecture still poses security challenges, particularly in
real-time, safety-critical environments. Recent reports highlight a surge in
SoC-related vulnerabilities, yet systematic analysis of their root causes and
impact within AUTOSAR-aligned architectures is lacking. This study fills that
gap by analyzing 180 publicly reported automotive SoC vulnerabilities, mapped
to a representative SoC software architecture model that is aligned with
AUTOSAR principles for layered abstraction and service orientation. We identify
16 root causes and 56 affected software modules, and examine mitigation delays
across Common Weakness Enumeration (CWE) categories and architectural layers.
We uncover dominant vulnerability patterns and critical modules with prolonged
patch delays, and provide actionable insights for securing automotive CPS
platforms, including guides for improved detection, prioritization, and
localization strategies for SoC software architectures in SoC-based vehicle
platforms.

</details>


### [29] [Past, Present, and Future of Bug Tracking in the Generative AI Era](https://arxiv.org/abs/2510.08005)
*Utku Boran Torun,Mehmet Taha Demircan,Mahmut Furkan Gön,Eray Tüzün*

Main category: cs.SE

TL;DR: This paper proposes an AI-powered bug tracking framework using LLMs to reduce manual processes, accelerate response times, and improve collaboration between non-technical users and technical teams.


<details>
  <summary>Details</summary>
Motivation: Traditional systems are slow, fragmented, and inefficient due to manual coordination, communication gaps, and asynchronous workflows, delaying resolutions and frustrating users.

Method: The framework integrates LLMs for natural language bug reporting, report refinement, classification, automated invalid bug fixes, issue localization, patch generation, and human-in-the-loop validation across all bug-tracking phases.

Result: The design aims to minimize human overhead, reduce time-to-fix, and enhance software maintenance through intelligent automation, though evaluation details or metrics are not specified in the abstract.

Conclusion: By evolving from paper-based to AI-augmented systems, the framework presents a transformative approach to modern bug tracking, fostering efficiency, user-centricity, and seamless collaboration.

Abstract: Traditional bug tracking systems rely heavily on manual reporting,
reproduction, triaging, and resolution, each carried out by different
stakeholders such as end users, customer support, developers, and testers. This
division of responsibilities requires significant coordination and widens the
communication gap between non-technical users and technical teams, slowing the
process from bug discovery to resolution. Moreover, current systems are highly
asynchronous; users often wait hours or days for a first response, delaying
fixes and contributing to frustration. This paper examines the evolution of bug
tracking, from early paper-based reporting to today's web-based and SaaS
platforms. Building on this trajectory, we propose an AI-powered bug tracking
framework that augments existing tools with intelligent, large language model
(LLM)-driven automation. Our framework addresses two main challenges: reducing
time-to-fix and minimizing human overhead. Users report issues in natural
language, while AI agents refine reports, attempt reproduction, and request
missing details. Reports are then classified, invalid ones resolved through
no-code fixes, and valid ones localized and assigned to developers. LLMs also
generate candidate patches, with human oversight ensuring correctness. By
integrating automation into each phase, our framework accelerates response
times, improves collaboration, and strengthens software maintenance practices
for a more efficient, user-centric future.

</details>


### [30] [Building Whitespace-Sensitive Languages Using Whitespace-Insensitive Components](https://arxiv.org/abs/2510.08200)
*Alexander Hellwig,Nico Jansen,Bernhard Rumpe*

Main category: cs.SE

TL;DR: This paper proposes a pre-processing method to reuse whitespace-insensitive language modules for whitespace-sensitive languages, reducing development time and improving language quality.


<details>
  <summary>Details</summary>
Motivation: The motivation addresses the lack of consistent integration methods between whitespace-sensitive and whitespace-insensitive languages, which hinders reusability and forces redundant development of whitespace-sensitive languages.

Method: The method involves pre-processing language artifacts to adapt whitespace-insensitive modules for constructing whitespace-sensitive languages, as demonstrated by reconstructing a simplified Python version.

Result: The approach successfully reconstructs a simplified Python implementation, validating its effectiveness in reusing whitespace-insensitive components to build whitespace-sensitive languages.

Conclusion: The paper concludes that the proposed pre-processing technique bridges the gap between whitespace-sensitive and whitespace-insensitive languages, enabling modular reuse of language components and enhancing software language development efficiency and quality.

Abstract: In Software Language Engineering, there is a trend towards reusability by
composing modular language components. However, this reusability is severely
inhibited by a gap in integrating whitespace-sensitive and
whitespace-insensitive languages. There is currently no consistent procedure
for seamlessly reusing such language components in both cases, such that
libraries often cannot be reused, and whitespacesensitive languages are
developed from scratch. This paper presents a technique for using modular,
whitespaceinsensitive language modules to construct whitespace sensitive
languages by pre-processing language artifacts before parsing. The approach is
evaluated by reconstructing a simplified version of the programming language
Python. Our solution aims to increase the reusability of existing language
components to reduce development time and increase the overall quality of
software languages.

</details>
