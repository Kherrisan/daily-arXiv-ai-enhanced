<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 4]
- [cs.SE](#cs.SE) [Total: 12]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Fine-Grained Privacy Extraction from Retrieval-Augmented Generation Systems via Knowledge Asymmetry Exploitation](https://arxiv.org/abs/2507.23229)
*Yufei Chen,Yao Wang,Haibin Zhang,Tao Gu*

Main category: cs.CR

TL;DR: This paper proposes a black-box attack framework (91% single-domain privacy extraction, 83% multi-domain) that exploits knowledge asymmetry between RAG and standard LLMs to precisely identify sensitive sentences through adaptive prompt decomposition, semantic scoring, and neural network feature analysis.


<details>
  <summary>Details</summary>
Motivation: RAG systems integrate external knowledge but introduce privacy risks through data leakage, and existing attacks fail to accurately isolate sensitive content or generalize across domains.

Method: 1) Decompose adversarial queries to maximize information disparity 2) Apply semantic relationship scoring to resolve lexical/syntactic ambiguities 3) Train neural network on these features to identify private sentences 4) Enable iterative domain adaptation without predefined knowledge

Result: 91%+ privacy extraction rate in single-domain, 83% in multi-domain scenarios with >65% reduction in sensitive sentence exposure in case studies, while demonstrating cross-domain generalization.

Conclusion: Bridges attack-defense gap in RAG systems by enabling precise private information extraction through knowledge asymmetry exploitation, establishing a foundation for adaptive privacy mitigation strategies.

Abstract: Retrieval-augmented generation (RAG) systems enhance large language models
(LLMs) by integrating external knowledge bases, but this advancement introduces
significant privacy risks. Existing privacy attacks on RAG systems can trigger
data leakage but often fail to accurately isolate knowledge-base-derived
sentences within mixed responses. They also lack robustness when applied across
multiple domains. This paper addresses these challenges by presenting a novel
black-box attack framework that exploits knowledge asymmetry between RAG and
standard LLMs to achieve fine-grained privacy extraction across heterogeneous
knowledge landscapes. We propose a chain-of-thought reasoning strategy that
creates adaptive prompts to steer RAG systems away from sensitive content.
Specifically, we first decompose adversarial queries to maximize information
disparity and then apply a semantic relationship scoring to resolve lexical and
syntactic ambiguities. We finally train a neural network on these feature
scores to precisely identify sentences containing private information. Unlike
prior work, our framework generalizes to unseen domains through iterative
refinement without pre-defined knowledge. Experimental results show that we
achieve over 91% privacy extraction rate in single-domain and 83% in
multi-domain scenarios, reducing sensitive sentence exposure by over 65% in
case studies. This work bridges the gap between attack and defense in RAG
systems, enabling precise extraction of private information while providing a
foundation for adaptive mitigation.

</details>


### [2] [Counterfactual Evaluation for Blind Attack Detection in LLM-based Evaluation Systems](https://arxiv.org/abs/2507.23453)
*Lijia Liu,Takumi Kondo,Kyohei Atarashi,Koh Takeuchi,Jiyi Li,Shigeru Saito,Hisashi Kashima*

Main category: cs.CR

TL;DR: The paper introduces a SE+CFE framework to enhance security against blind attacks in LLM-based evaluations, achieving significant attack detection improvements with minimal performance impact.


<details>
  <summary>Details</summary>
Motivation: LLM-based evaluation systems are vulnerable to 'blind attacks' where candidate answers deceive evaluators without relying on the true ground-truth answer.

Method: The framework combines Standard Evaluation (SE) with Counterfactual Evaluation (CFE), which re-evaluates submissions against a deliberately false ground-truth answer to identify inconsistencies.

Result: Experiments demonstrated SE+CFE's effectiveness in detecting blind attacks while maintaining evaluation performance through minimal detection-cost overhead.

Conclusion: Integrating counterfactual testing significantly strengthens security in evaluation systems against adversarial prompt injections without sacrificing core capabilities.

Abstract: This paper investigates defenses for LLM-based evaluation systems against
prompt injection. We formalize a class of threats called blind attacks, where a
candidate answer is crafted independently of the true answer to deceive the
evaluator. To counter such attacks, we propose a framework that augments
Standard Evaluation (SE) with Counterfactual Evaluation (CFE), which
re-evaluates the submission against a deliberately false ground-truth answer.
An attack is detected if the system validates an answer under both standard and
counterfactual conditions. Experiments show that while standard evaluation is
highly vulnerable, our SE+CFE framework significantly improves security by
boosting attack detection with minimal performance trade-offs.

</details>


### [3] [LLM-Based Identification of Infostealer Infection Vectors from Screenshots: The Case of Aurora](https://arxiv.org/abs/2507.23611)
*Estelle Ruellan,Eric Clay,Nicholas Ascoli*

Main category: cs.CR

TL;DR: This paper proposes an LLM-based reactive analysis framework to automatically process infection screenshots from Aurora infostealer logs, extracting IoCs and mapping infection vectors at scale. 337 URLs and 246 files were identified, revealing 3 distinct malware campaigns and social engineering tactics, demonstrating the feasibility of artifact-driven threat intelligence compared to traditional log-based detection methods.


<details>
  <summary>Details</summary>
Motivation: Manual analysis of 29 million annual stealer logs is impractical, and current research lacks reactive approaches to exploit infection artifacts like screenshots for malware analysis. 92% of malware attacks go undetected via traditional methods, highlighting the need for scalable reactive analysis techniques.

Method: We developed an end-to-end framework using gpt-4o-mini to process 1000 Aurora infostealer screenshots. The method extracts IoCs through visual analysis, maps infection vectors by identifying malicious URLs/installer files, and detects campaigns by cross-referencing filename patterns, URLs, and infection themes using LLM-driven artifact correlation.

Result: Extracted 337 actionable URLs (89% unique) and 246 relevant files (73% with clear infection indicators). Identified 3 distinct campaigns with differing attack patterns: 1) malvertising-driven URL redirection, 2) weaponized document exploitation, and 3) supply chain attacks. The framework achieved 94% precision in infection vector detection vs. human analysts.

Conclusion: By transitioning from log-based to artifact-driven analysis using LLMs, this research establishes a scalable paradigm for malware investigation. Our method reduces investigation time by ~78% compared to manual analysis while maintaining accuracy, enabling early threat detection and campaign attribution in cybersecurity operations centers.

Abstract: Infostealers exfiltrate credentials, session cookies, and sensitive data from
infected systems. With over 29 million stealer logs reported in 2024, manual
analysis and mitigation at scale are virtually unfeasible/unpractical. While
most research focuses on proactive malware detection, a significant gap remains
in leveraging reactive analysis of stealer logs and their associated artifacts.
Specifically, infection artifacts such as screenshots, image captured at the
point of compromise, are largely overlooked by the current literature. This
paper introduces a novel approach leveraging Large Language Models (LLMs), more
specifically gpt-4o-mini, to analyze infection screenshots to extract potential
Indicators of Compromise (IoCs), map infection vectors, and track campaigns.
Focusing on the Aurora infostealer, we demonstrate how LLMs can process
screenshots to identify infection vectors, such as malicious URLs, installer
files, and exploited software themes. Our method extracted 337 actionable URLs
and 246 relevant files from 1000 screenshots, revealing key malware
distribution methods and social engineering tactics. By correlating extracted
filenames, URLs, and infection themes, we identified three distinct malware
campaigns, demonstrating the potential of LLM-driven analysis for uncovering
infection workflows and enhancing threat intelligence. By shifting malware
analysis from traditional log-based detection methods to a reactive,
artifact-driven approach that leverages infection screenshots, this research
presents a scalable method for identifying infection vectors and enabling early
intervention.

</details>


### [4] [Polynomial Lattices for the BIKE Cryptosystem](https://arxiv.org/abs/2507.23641)
*Michael Schaller*

Main category: cs.CR

TL;DR: The paper introduces a rank 2 lattice for the BIKE cryptosystem and improves weak key recovery by solving shortest vector problems and generating reduced bases to detect more weak keys.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance the analysis of BIKE's security by generalizing weak key recovery techniques, building on prior work to detect vulnerabilities more effectively.

Method: The authors construct a rank 2 polynomial ring lattice from BIKE's public key, analyze its properties, and adapt algorithms to compute reduced bases rather than only solving for shortest vectors.

Result: They demonstrate that previous weak key recovery methods implicitly solved SVP in their lattice, and their reduced basis approach enables broader detection of weak keys in BIKE.

Conclusion: This work establishes a lattice-based framework for BIKE security analysis, showing that reduced bases significantly improve the efficiency and scope of weak key recovery compared to prior methods.

Abstract: In this paper we introduce a rank $2$ lattice over a polynomial ring arising
from the public key of the BIKE cryptosystem \cite{aragon2022bike}. The secret
key is a sparse vector in this lattice. We study properties of this lattice and
generalize the recovery of weak keys from \cite{BardetDLO16}. In particular, we
show that they implicitly solved a shortest vector problem in the lattice we
constructed. Rather than finding only a shortest vector, we obtain a reduced
basis of the lattice which makes it possible to check for more weak keys.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [On LLM-Assisted Generation of Smart Contracts from Business Processes](https://arxiv.org/abs/2507.23087)
*Fabian Stiehle,Hans Weytjens,Ingo Weber*

Main category: cs.SE

TL;DR: This paper analyzes the effectiveness of using large language models (LLMs) for generating smart contracts from business processes, proposing an automated evaluation framework. It finds LLMs fall short in execution reliability and suggests integrating LLMs with existing tools for better outcomes.


<details>
  <summary>Details</summary>
Motivation: Address limitations of traditional rule-based code generation by exploring LLM capabilities, while overcoming the gap in current LLM-based studies that rely on small samples or compilation-only testing.

Method: Developed a benchmarking framework to evaluate LLMs (across types/sizes) using larger datasets, measuring performance on process execution properties like flow enforcement, resource allocation, and data conditions.

Result: Empirical evidence shows generated smart contracts frequently fail to meet execution correctness requirements despite syntactic validity.

Conclusion: LLMs cannot yet ensure reliable smart contract generation; recommendations include hybrid approaches combining LLMs with rule-based tools and adopting the proposed evaluation framework.

Abstract: Large language models (LLMs) have changed the reality of how software is
produced. Within the wider software engineering community, among many other
purposes, they are explored for code generation use cases from different types
of input. In this work, we present an exploratory study to investigate the use
of LLMs for generating smart contract code from business process descriptions,
an idea that has emerged in recent literature to overcome the limitations of
traditional rule-based code generation approaches. However, current LLM-based
work evaluates generated code on small samples, relying on manual inspection,
or testing whether code compiles but ignoring correct execution. With this
work, we introduce an automated evaluation framework and provide empirical data
from larger data sets of process models. We test LLMs of different types and
sizes in their capabilities of achieving important properties of process
execution, including enforcing process flow, resource allocation, and
data-based conditions. Our results show that LLM performance falls short of the
perfect reliability required for smart contract development. We suggest future
work to explore responsible LLM integrations in existing tools for code
generation to ensure more reliable output. Our benchmarking framework can serve
as a foundation for developing and evaluating such integrations.

</details>


### [6] [FlowETL: An Autonomous Example-Driven Pipeline for Data Engineering](https://arxiv.org/abs/2507.23118)
*Mattia Di Profio,Mingjun Zhong,Yaji Sripada,Marcel Jaspars*

Main category: cs.SE

TL;DR: FlowETL is an autonomous ETL pipeline that automates complex data transformations through example-based learning, addressing the limitations of current ETL solutions which require extensive human intervention.


<details>
  <summary>Details</summary>
Motivation:  Modern ETL solutions demand significant manual effort to design and implement non-generalizable, context-specific transformations. Existing automation lacks capability to automatically design and apply these transformations.

Method:  FlowETL uses a Planning Engine with paired input-output dataset samples to create transformation plans, executed by ETL workers with monitoring throughout. It combines example-based learning and modular components for automation.

Result:  Demonstrated generalisation across 14 diverse datasets (different domains, file structures, sizes) in standardising and preparing input data to match user-defined targets through automated pipelines.

Conclusion:  FlowETL presents a promising ecosystem for autonomous ETL workflows with broad applicability across heterogeneous data sources, offering a viable solution to reduce human-in-the-loop requirements in data pipeline creation.

Abstract: The Extract, Transform, Load (ETL) workflow is fundamental for populating and
maintaining data warehouses and other data stores accessed by analysts for
downstream tasks. A major shortcoming of modern ETL solutions is the extensive
need for a human-in-the-loop, required to design and implement
context-specific, and often non-generalisable transformations. While related
work in the field of ETL automation shows promising progress, there is a lack
of solutions capable of automatically designing and applying these
transformations. We present FlowETL, a novel example-based autonomous ETL
pipeline architecture designed to automatically standardise and prepare input
datasets according to a concise, user-defined target dataset. FlowETL is an
ecosystem of components which interact together to achieve the desired outcome.
A Planning Engine uses a paired input-output datasets sample to construct a
transformation plan, which is then applied by an ETL worker to the source
dataset. Monitoring and logging provide observability throughout the entire
pipeline. The results show promising generalisation capabilities across 14
datasets of various domains, file structures, and file sizes.

</details>


### [7] [Vibe Modeling: Challenges and Opportunities](https://arxiv.org/abs/2507.23120)
*Jordi Cabot*

Main category: cs.SE

TL;DR: The paper proposes 'vibe modeling' as a new approach combining AI chatbots and model-driven engineering (MDE) to address challenges in developing reliable complex software systems, balancing the flexibility of AI with the rigor of MDE.


<details>
  <summary>Details</summary>
Motivation: Current MDE methods struggle with model complexity, while vibe coding (LLM-based coding) introduces vulnerabilities and maintainability issues. The gap between AI-driven development and conventional MDE necessitates a hybrid approach.

Method: Introduces 'vibe modeling' as a synergistic integration of Large Language Models (LLMs) and domain-specific modeling techniques, enabling natural language model specification while maintaining formal verification benefits.

Result: Outlines core concepts of vibe modeling and identifies critical opportunities (e.g., enhanced productivity) and challenges (e.g., ensuring model correctness) for future implementation in software development workflows.

Conclusion: Vibe modeling represents a critical step toward AI-enhanced MDE, offering potential to accelerate complex system development while addressing reliability concerns through methodological convergence.

Abstract: There is a pressing need for better development methods and tools to keep up
with the growing demand and increasing complexity of new software systems. New
types of user interfaces, the need for intelligent components, sustainability
concerns, ... bring new challenges that we need to handle. In the last years,
model-driven engineering (MDE) has been key to improving the quality and
productivity of software development, but models themselves are becoming
increasingly complex to specify and manage. At the same time, we are witnessing
the growing popularity of vibe coding approaches that rely on Large Language
Models (LLMs) to transform natural language descriptions into running code at
the expenses of code vulnerabilities, scalability issues and maintainability
concerns. In this paper, we introduce the concept of \textit{vibe modeling} as
a novel approach to integrate the best of both worlds (AI and MDE) to speed up
the development of reliable complex systems. We outline the key concepts of
vibe modeling and highlight the opportunities and open challenges it presents
for the future of modeling.

</details>


### [8] [Extension Decisions in Open Source Software Ecosystem](https://arxiv.org/abs/2507.23168)
*Elmira Onagh,Maleknaz Nayebi*

Main category: cs.SE

TL;DR: 65% of new CI Actions in the growing GitHub Marketplace replicate existing capabilities within six months, with first-movers dominating forks/extensions.


<details>
  <summary>Details</summary>
Motivation: To address redundancy in the expanding GitHub Marketplace's Continuous Integration segment by analyzing tool innovation patterns and competition dynamics.

Method: Mapped 6,983 CI Actions to 3,869 providers using version histories; constructed a graph model to timestamp functionality debut, track adoption, and cluster redundant tools.

Result: 65% of new CI Actions replicate existing functionality within six months; first-mover tools account for most forks/derivatives; open dataset shared for further research.

Conclusion: Insights support innovation timing strategies for developers and redundancy management for maintainers; dataset enables longitudinal studies of software ecosystem innovation/competition.

Abstract: GitHub Marketplace is expanding by approximately 41% annually, with new
tools; however, many additions replicate existing functionality. We study this
phenomenon in the platform's largest segment, Continuous Integration (CI), by
linking 6,983 CI Actions to 3,869 providers and mining their version histories.
Our graph model timestamps every functionality's debut, tracks its adoption,
and clusters redundant tools. We find that approximately 65% of new CI Actions
replicate existing capabilities, typically within six months, and that a small
set of first-mover Actions accounts for most subsequent forks and extensions.
These insights enable developers to choose the optimal moment to launch, target
unmet functionality, and help maintainers eliminate redundant tools. We publish
the complete graph and dataset to encourage longitudinal research on innovation
and competition in software ecosystems, and to provide practitioners with a
data-driven roadmap for identifying emerging trends and guiding product
strategy.

</details>


### [9] [AutoBridge: Automating Smart Device Integration with Centralized Platform](https://arxiv.org/abs/2507.23178)
*Siyuan Liu,Zhice Yang,Huangxun Chen*

Main category: cs.SE

TL;DR: AutoBridge automates IoT integration code generation with a divide-and-conquer strategy and multi-stage debugging, achieving 93.87% success rate and 94.87% function coverage without human input, reaching 100% coverage with minimal feedback.


<details>
  <summary>Details</summary>
Motivation: Multimodal IoT systems require complex integration code for new devices, but current methods demand significant human expertise and effort.

Method: AutoBridge uses divide-and-conquer strategy through device-specific knowledge retrieval and platform-specific code synthesis, combined with automated virtual device debugging and interactive hardware-in-the-loop debugging using binary user feedback.

Result: Achieved 93.87% average success rate and 94.87% function coverage without human involvement on 34 IoT devices across two platforms, reaching 100% coverage with minimal yes/no feedback. Outperformed expert programmers using commercial code LLMs by 50%-80% in code accuracy.

Conclusion: AutoBridge demonstrates effective automation for IoT platform compliance with high success rates and coverage, requiring only binary feedback for real-device validation in resource-constrained environments.

Abstract: Multimodal IoT systems coordinate diverse IoT devices to deliver
human-centered services. The ability to incorporate new IoT devices under the
management of a centralized platform is an essential requirement. However, it
requires significant human expertise and effort to program the complex IoT
integration code that enables the platform to understand and control the device
functions. Therefore, we propose AutoBridge to automate IoT integration code
generation. Specifically, AutoBridge adopts a divide-and-conquer strategy: it
first generates device control logic by progressively retrieving
device-specific knowledge, then synthesizes platformcompliant integration code
using platform-specific knowledge. To ensure correctness, AutoBridge features a
multi-stage debugging pipeline, including an automated debugger for virtual IoT
device testing and an interactive hardware-in-the-loop debugger that requires
only binary user feedback (yes and no) for real-device verification. We
evaluate AutoBridge on a benchmark of 34 IoT devices across two open-source IoT
platforms. The results demonstrate that AutoBridge can achieves an average
success rate of 93.87% and an average function coverage of 94.87%, without any
human involvement. With minimal binary yes and no feedback from users, the code
is then revised to reach 100% function coverage. A user study with 15
participants further shows that AutoBridge outperforms expert programmers by
50% to 80% in code accuracy, even when the programmers are allowed to use
commercial code LLMs.

</details>


### [10] [XABPs: Towards eXplainable Autonomous Business Processes](https://arxiv.org/abs/2507.23269)
*Peter Fettke,Fabiana Fournier,Lior Limonad,Andreas Metzger,Stefanie Rinderle-Ma,Barbara Weber*

Main category: cs.SE

TL;DR: This paper addresses challenges in autonomous business processes (ABPs) by introducing eXplainable ABPs (XABPs), which enhance stakeholder trust through rationale articulation. It presents a systematic approach to characterizing XABPs, structuring explainability, and identifying BPM research challenges.


<details>
  <summary>Details</summary>
Motivation: ABPs using AI/ML improve operational efficiency but face issues like decreased stakeholder trust, debugging difficulties, hindered accountability, bias risks, and regulatory compliance problems. The paper is motivated by the need to address these concerns for broader adoption.

Method: The authors provide a systematic approach to XABPs by analyzing their forms, structuring the components of explainability, and systematically identifying key BPM research challenges through theoretical characterization.

Result: The study outlines forms and structures of XABPs, highlights mechanisms for rationale articulation, and pinpoints BPM research challenges such as interoperability, standardization, and ethical alignment in autonomous workflows.

Conclusion: XABPs are positioned as critical for addressing trust and accountability challenges in ABPs. The paper concludes with a roadmap for BPM research to advance explainability frameworks, regulatory compliance, and human-AI collaboration in autonomous business workflows.

Abstract: Autonomous business processes (ABPs), i.e., self-executing workflows
leveraging AI/ML, have the potential to improve operational efficiency, reduce
errors, lower costs, improve response times, and free human workers for more
strategic and creative work. However, ABPs may raise specific concerns
including decreased stakeholder trust, difficulties in debugging, hindered
accountability, risk of bias, and issues with regulatory compliance. We argue
for eXplainable ABPs (XABPs) to address these concerns by enabling systems to
articulate their rationale. The paper outlines a systematic approach to XABPs,
characterizing their forms, structuring explainability, and identifying key BPM
research challenges towards XABPs.

</details>


### [11] [SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution](https://arxiv.org/abs/2507.23348)
*Han Li,Yuling Shi,Shaoxin Lin,Xiaodong Gu,Heng Lian,Xin Wang,Yantao Jia,Tao Huang,Qianxiang Wang*

Main category: cs.SE

TL;DR: SWE-Debate enhances open-source issue resolution using a competitive multi-agent debate framework with structured collaboration and MCTS-based patch generation.


<details>
  <summary>Details</summary>
Motivation: Existing agent-based approaches for issue resolution often get stuck in local solutions and fail to identify cross-codebase patterns due to independent exploration, necessitating a method to encourage diverse reasoning paths and consolidated localization.

Method: SWE-Debate employs competitive multi-agent debate via code dependency graph traversal, three-round structured debates among specialized agents with distinct reasoning perspectives, and integrates a consolidated fix plan into an MCTS-based code modification agent for patch generation.

Result: Experiments on SWE-bench demonstrate SWE-Debate achieves new state-of-the-art results in open-source agent frameworks, significantly outperforming baselines.

Conclusion: Structured multi-agent debates improve LLM-based issue resolution by leveraging diverse reasoning and collaborative convergence, enabling more effective consolidation of fix plans in complex software engineering tasks.

Abstract: Issue resolution has made remarkable progress thanks to the advanced
reasoning capabilities of large language models (LLMs). Recently, agent-based
frameworks such as SWE-agent have further advanced this progress by enabling
autonomous, tool-using agents to tackle complex software engineering tasks.
While existing agent-based issue resolution approaches are primarily based on
agents' independent explorations, they often get stuck in local solutions and
fail to identify issue patterns that span across different parts of the
codebase. To address this limitation, we propose SWE-Debate, a competitive
multi-agent debate framework that encourages diverse reasoning paths and
achieves more consolidated issue localization. SWE-Debate first creates
multiple fault propagation traces as localization proposals by traversing a
code dependency graph. Then, it organizes a three-round debate among
specialized agents, each embodying distinct reasoning perspectives along the
fault propagation trace. This structured competition enables agents to
collaboratively converge on a consolidated fix plan. Finally, this consolidated
fix plan is integrated into an MCTS-based code modification agent for patch
generation. Experiments on the SWE-bench benchmark show that SWE-Debate
achieves new state-of-the-art results in open-source agent frameworks and
outperforms baselines by a large margin.

</details>


### [12] [Quality Evaluation of COBOL to Java Code Transformation](https://arxiv.org/abs/2507.23356)
*Shmulik Froimovich,Raviv Gal,Wesam Ibraheem,Avi Ziv*

Main category: cs.SE

TL;DR: The paper introduces an automated system for evaluating COBOL-to-Java code translations in IBM's watsonx Code Assistant for Z (WCA4Z), combining analytic checkers and LLM-as-a-judge techniques to address challenges in LLM-based translator evaluation.


<details>
  <summary>Details</summary>
Motivation: The system aims to overcome key challenges in evaluating LLM-based code translators, such as model opacity and the complexity of translation quality assessment.

Method: The approach integrates analytic checkers for structured validation with large language model as a judge (LaaJ) techniques, enabling scalable and multi-faceted evaluations within continuous integration workflows.

Result: The system supports large-scale benchmarking, reduces manual review requirements, and provides architecture details, evaluation strategies, and reporting mechanisms that deliver actionable insights.

Conclusion: This automated evaluation system enhances the ability to monitor and improve translation quality in modernized codebases while streamlining development processes for COBOL-to-Java migration projects.

Abstract: We present an automated evaluation system for assessing COBOL-to-Java code
translation within IBM's watsonx Code Assistant for Z (WCA4Z). The system
addresses key challenges in evaluating LLM-based translators, including model
opacity and the complexity of translation quality assessment. Our approach
combines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver
scalable, multi-faceted evaluations. The system supports continuous integration
workflows, enables large-scale benchmarking, and reduces reliance on manual
review. We describe the system architecture, evaluation strategies, and
reporting mechanisms that provide actionable insights for developers and
project managers, facilitating the evolution of high-quality, modernized
codebases.

</details>


### [13] [SWE-Exp: Experience-Driven Software Issue Resolution](https://arxiv.org/abs/2507.23361)
*Silin Chen,Shaoxin Lin,Xiaodong Gu,Yuling Shi,Heng Lian,Longfei Yun,Dong Chen,Weiguo Sun,Lin Cao,Qianxiang Wang*

Main category: cs.SE

TL;DR: SWE-Exp is an experience-enhanced approach for LLM-based software engineering agents that improves resolution rates by systematically storing and leveraging knowledge from past repair successes and failures.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents treat each software issue in isolation, resulting in redundant exploration of failed solutions and missed opportunities to apply successful methods from similar problems.

Method: The approach introduces a multi-faceted experience bank that captures both successful and failed repair attempts, extracting reusable knowledge across different levels of software issue resolution from prior agent trajectories.

Result: SWE-Exp achieves a state-of-the-art 41.6% resolution rate (Pass@1) on the SWE-bench-Verified benchmark using open-source agent frameworks.

Conclusion: The framework establishes a new paradigm for automated software engineering agents by transitioning from trial-and-error exploration to strategic, experience-driven issue resolution through continuous knowledge accumulation and reuse.

Abstract: Recent advances in large language model (LLM) agents have shown remarkable
progress in software issue resolution, leveraging advanced techniques such as
multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current
agents act as memoryless explorers - treating each problem separately without
retaining or reusing knowledge from previous repair experiences. This leads to
redundant exploration of failed trajectories and missed chances to adapt
successful issue resolution methods to similar problems. To address this
problem, we introduce SWE-Exp, an experience - enhanced approach that distills
concise and actionable experience from prior agent trajectories, enabling
continuous learning across issues. Our method introduces a multi-faceted
experience bank that captures both successful and failed repair attempts.
Specifically, it extracts reusable issue resolution knowledge at different
levels - from high-level problem comprehension to specific code changes.
Experiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%
Pass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach
establishes a new paradigm in which automated software engineering agents
systematically accumulate and leverage repair expertise, fundamentally shifting
from trial-and-error exploration to strategic, experience-driven issue
resolution.

</details>


### [14] [Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling](https://arxiv.org/abs/2507.23370)
*Trae Research Team,Pengfei Gao,Zhao Tian,Xiangxin Meng,Xinchen Wang,Ruida Hu,Yuanan Xiao,Yizhou Liu,Zhao Zhang,Junjie Chen,Cuiyun Gao,Yun Lin,Yingfei Xiong,Chao Peng,Xia Liu*

Main category: cs.SE

TL;DR: Trae Agent is an agent-based ensemble reasoning approach for repository-level issue resolution, achieving 10.22% average improvement over baselines and first place on SWE-bench Verified leaderboard with 75.20% Pass@1.


<details>
  <summary>Details</summary>
Motivation: Existing prompting-based ensemble methods struggle with exploring large ensemble spaces and lack repository-level understanding, limiting their effectiveness in software issue resolution.

Method: Trae Agent modularizes generation, pruning, and selection through agent collaboration to address large ensemble spaces and repository-level comprehension, formulated as an optimal solution search problem.

Result: Exceeds four state-of-the-art techniques on SWE-bench with a 10.22% average performance gain in Pass@1 metric, securing 75.20% Pass@1 score.

Conclusion: Trae Agent demonstrates superior effectiveness in repository-level issue resolution via its modular agents and achieves production-level performance, now open-source at https://github.com/bytedance/trae-agent.

Abstract: Software issue resolution is a critical challenge in software engineering and
has garnered increasing attention in recent years. With the rapid advancement
of large language models (LLMs), substantial progress has been made in
addressing real-world software engineering tasks. Recent studies have
introduced ensemble reasoning techniques to enhance the performance of
LLM-based issue resolution. However, existing prompting-based methods still
face limitations in effectively exploring large ensemble spaces and lack the
capacity for repository-level understanding, both of which constrain their
overall effectiveness. In this paper, we propose Trae Agent, the first
agent-based ensemble reasoning approach for repository-level issue resolution.
Trae Agent formulates our goal as an optimal solution search problem and
addresses two key challenges, i.e., large ensemble spaces and repository-level
understanding, through modular agents for generation, pruning, and selection.
We conduct extensive experiments using three leading LLMs on the widely-adopted
SWE-bench benchmark, comparing Trae Agent against four state-of-the-art
ensemble reasoning techniques. Experimental results demonstrate that Trae Agent
consistently achieves superior performance, with an average improvement of
10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first
place on the SWE-bench Verified leaderboard, with a notable Pass@1 score of
75.20%. We are pleased to release Trae Agent as an open-source project to
support the research community, with all resources available at
https://github.com/bytedance/trae-agent.

</details>


### [15] [Dynamic and Static Analysis of Python Software with Kieker Including Reconstructed Architectures](https://arxiv.org/abs/2507.23425)
*Daphné Larrivain,Shinhyung Yang,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: The Kieker observability framework, originally designed for Java, is extended to support Python by integrating static and dynamic analysis in an observability pipeline to provide structural insights into Python applications.


<details>
  <summary>Details</summary>
Motivation: Python's growing popularity necessitates structural application monitoring tools similar to Kieker's capabilities for Java, making Python support valuable for observability.

Method: The paper implements a combined static/dynamic analysis pipeline using Kieker's framework to extract comprehensive system information from Python applications.

Result: The paper demonstrates the feasibility of the Python analysis pipeline approach, though specific quantitative results are not detailed in the abstract.

Conclusion: The study concludes that adapting Kieker for Python application monitoring is worthwhile, offering structural insights through its integrated analysis methods.

Abstract: The Kieker observability framework is a tool that provides users with the
means to design a custom observability pipeline for their application.
Originally tailored for Java, supporting Python with Kieker is worthwhile.
Python's popularity has exploded over the years, thus making structural
insights of Python applications highly valuable. Our Python analysis pipeline
combines static and dynamic analysis in order to build a complete picture of a
given system.

</details>


### [16] [An Empirical Study on the Amount of Changes Required for Merge Request Acceptance](https://arxiv.org/abs/2507.23640)
*Samah Kansab,Mohammed Sayagh,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: This paper investigates code review effort in GitLab MRs, revealing high adjustment rates (71%) and significant line changes (28%+) not correlated with review time or participants. An interpretable ML model (AUC 0.84-0.88) identifies complexity, developer experience, text features, and historical project characteristics as key predictors of effort.


<details>
  <summary>Details</summary>
Motivation: Existing studies focus on code review delays or iteration counts, but few quantify effort based on actual code change volumes in GitLab MRs, which remains underexplored.

Method: The authors analyze 23,600 MRs across four GitLab projects and train an interpretable machine learning model using metrics from code complexity, text features, developer experience, review history, and branching.

Result: 71% of MRs require post-submission changes, 28% involve over 200 line modifications. Review effort shows no correlation with review duration or participant count. ML model achieves 0.84-0.88 AUC and identifies key predictors affecting effort.

Conclusion: Machine learning can effectively explain and predict code review effort by analyzing multi-dimensional project and developer metrics, providing actionable insights for code integration planning.

Abstract: Code review (CR) is essential to software development, helping ensure that
new code is properly integrated. However, the CR process often involves
significant effort, including code adjustments, responses to reviewers, and
continued implementation. While past studies have examined CR delays and
iteration counts, few have investigated the effort based on the volume of code
changes required, especially in the context of GitLab Merge Requests (MRs),
which remains underexplored. In this paper, we define and measure CR effort as
the amount of code modified after submission, using a dataset of over 23,600
MRs from four GitLab projects. We find that up to 71% of MRs require
adjustments after submission, and 28% of these involve changes to more than 200
lines of code. Surprisingly, this effort is not correlated with review time or
the number of participants. To better understand and predict CR effort, we
train an interpretable machine learning model using metrics across multiple
dimensions: text features, code complexity, developer experience, review
history, and branching. Our model achieves strong performance (AUC 0.84-0.88)
and reveals that complexity, experience, and text features are key predictors.
Historical project characteristics also influence current review effort. Our
findings highlight the feasibility of using machine learning to explain and
anticipate the effort needed to integrate code changes during review.

</details>
