<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 27]
- [cs.SE](#cs.SE) [Total: 12]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Blockchain-Based Decentralized Domain Name System](https://arxiv.org/abs/2508.05655)
*Guang Yang,Peter Trinh,Alma Nkemla,Amuru Serikyaku,Edward Tatchim,Osman Sharaf*

Main category: cs.CR

TL;DR: This paper proposes a blockchain-based Decentralized Domain Name System (DDNS) to address DNS vulnerabilities like poisoning attacks, censorship, and centralization by implementing a specialized Proof-of-Work blockchain integrated with IPFS and cryptographic verification.


<details>
  <summary>Details</summary>
Motivation: DNS infrastructure is prone to security threats (poisoning), censorship, and centralized failures, with recent ISP attacks underscoring the need for a decentralized, resilient alternative.

Method: The authors developed a custom Proof-of-Work blockchain optimized for DNS protocols, combined IPFS for distributed storage, cryptographic signatures for trust, and 'Never Trust, Always Verify' zero-trust mechanisms.

Result: The system achieves 15-second record propagation, supports 20 DNS record types, and handles up to 1,111.1 tx/s (minimal) and 266.7 tx/s (regular) domain operations, with sub-second query resolution through caching. Deployments in three U.S. locations demonstrate scalability and resistance to manipulation.

Conclusion: The DDNS offers a decentralized, secure, and scalable solution to traditional DNS weaknesses, validated by performance metrics and real-world deployment, effectively countering censorship and attacks while maintaining usability.

Abstract: The current Domain Name System (DNS) infrastructure faces critical
vulnerabilities including poisoning attacks, censorship mechanisms, and
centralized points of failure that compromise internet freedom and security.
Recent incidents such as DNS poisoning attacks on ISP customers highlight the
urgent need for resilient alternatives. This paper presents a novel
blockchain-based Decentralized Domain Name System (DDNS). We designed a
specialized Proof-of-Work blockchain to maximize support for DNS-related
protocols and achieve node decentralization. The system integrates our
blockchain with IPFS for distributed storage, implements cryptographic
primitives for end-to-end trust signatures, and achieves Never Trust, Always
Verify zero-trust verification. Our implementation achieves 15-second domain
record propagation times, supports 20 standard DNS record types, and provides
perpetual free .ddns domains. The system has been deployed across distributed
infrastructure in San Jose, Los Angeles, and Orange County, demonstrating
practical scalability and resistance to traditional DNS manipulation
techniques. Performance evaluation shows the system can handle up to Max Theor.
TPS 1,111.1 tx/s (minimal transactions) and Max Theor. TPS 266.7 tx/s (regular
transactions) for domain operations while maintaining sub-second query
resolution through intelligent caching mechanisms.

</details>


### [2] [Universally Unfiltered and Unseen:Input-Agnostic Multimodal Jailbreaks against Text-to-Image Model Safeguards](https://arxiv.org/abs/2508.05658)
*Song Yan,Hui Wei,Jinlong Fei,Guoliang Yang,Zhengyu Zhao,Zheng Wamg*

Main category: cs.CR

TL;DR: The paper introduces U3-Attack, a scalable and efficient multimodal method to bypass prompt filters and image safety checkers in Text-to-Image models by optimizing adversarial patches and safe paraphrase sets.


<details>
  <summary>Details</summary>
Motivation: Existing jailbreak attacks for T2I safeguards are limited to prompt-specific and image-specific perturbations, which are not scalable and require time-consuming optimization.

Method: U3-Attack simultaneously optimizes an adversarial patch on the image background (to bypass safety checkers) and a safe paraphrase set from sensitive words (to bypass prompt filters), eliminating redundant computations for universal bypass capabilities.

Result: Extensive experiments show U3-Attack outperforms state-of-the-art methods like MMA-Diffusion, achieving ~4× higher success rates on commercial models such as Runway-inpainting with dual safety filters.

Conclusion: U3-Attack demonstrates significant vulnerabilities in current T2I safety mechanisms while emphasizing the need for more robust safeguards, despite its effectiveness raising ethical concerns about content warning examples in the paper.

Abstract: Various (text) prompt filters and (image) safety checkers have been
implemented to mitigate the misuse of Text-to-Image (T2I) models in creating
Not-Safe-For-Work (NSFW) content.In order to expose potential security
vulnerabilities of such safeguards, multimodal jailbreaks have been
studied.However, existing jailbreaks are limited to prompt-specific and
image-specific perturbations, which suffer from poor scalability and
time-consuming optimization.To address these limitations, we propose
Universally Unfiltered and Unseen (U3)-Attack, a multimodal jailbreak attack
method against T2I safeguards.Specifically, U3-Attack optimizes an adversarial
patch on the image background to universally bypass safety checkers and
optimizes a safe paraphrase set from a sensitive word to universally bypass
prompt filters while eliminating redundant computations.Extensive experimental
results demonstrate the superiority of our U3-Attack on both open-source and
commercial T2I models.For example, on the commercial Runway-inpainting model
with both prompt filter and safety checker, our U3-Attack achieves $~4\times$
higher success rates than the state-of-the-art multimodal jailbreak attack,
MMA-Diffusion.Content Warning: This paper includes examples of NSFW content.

</details>


### [3] [Can LLMs effectively provide game-theoretic-based scenarios for cybersecurity?](https://arxiv.org/abs/2508.05670)
*Daniele Proverbio,Alessio Buscemi,Alessandro Di Stefano,The Anh Han,German Castignani,Pietro Liò*

Main category: cs.CR

TL;DR: The paper explores how Large Language Models (LLMs) behave in cybersecurity scenarios using classical game theory, finding that payoffs are influenced by agent traits and language sensitivity, highlighting risks for cross-cultural deployment.


<details>
  <summary>Details</summary>
Motivation: LLMs introduce new cybersecurity challenges and tools, but existing game-theoretic frameworks may not account for their unique biases or linguistic variations, necessitating a study of their behavior in security contexts.

Method: The authors developed a reproducible framework to test four LLMs across five languages in one-shot zero-sum games and dynamic Prisoner's Dilemma, analyzing payoff outcomes influenced by personality traits and language.

Result: LLM payoffs vary significantly with agent characteristics and language, with unexpected linguistic sensitivity observed, such as differences in decision-making and outcomes across English, French, Arabic, Vietnamese, and Mandarin Chinese.

Conclusion: LLMs' cybersecurity behavior is language-sensitive, recommending against their indiscriminate use in global applications and advocating for targeted research and model optimization to ensure security and stability.

Abstract: Game theory has long served as a foundational tool in cybersecurity to test,
predict, and design strategic interactions between attackers and defenders. The
recent advent of Large Language Models (LLMs) offers new tools and challenges
for the security of computer systems; In this work, we investigate whether
classical game-theoretic frameworks can effectively capture the behaviours of
LLM-driven actors and bots. Using a reproducible framework for game-theoretic
LLM agents, we investigate two canonical scenarios -- the one-shot zero-sum
game and the dynamic Prisoner's Dilemma -- and we test whether LLMs converge to
expected outcomes or exhibit deviations due to embedded biases. Our experiments
involve four state-of-the-art LLMs and span five natural languages, English,
French, Arabic, Vietnamese, and Mandarin Chinese, to assess linguistic
sensitivity. For both games, we observe that the final payoffs are influenced
by agents characteristics such as personality traits or knowledge of repeated
rounds. Moreover, we uncover an unexpected sensitivity of the final payoffs to
the choice of languages, which should warn against indiscriminate application
of LLMs in cybersecurity applications and call for in-depth studies, as LLMs
may behave differently when deployed in different countries. We also employ
quantitative metrics to evaluate the internal consistency and cross-language
stability of LLM agents, to help guide the selection of the most stable LLMs
and optimising models for secure applications.

</details>


### [4] [DINA: A Dual Defense Framework Against Internal Noise and External Attacks in Natural Language Processing](https://arxiv.org/abs/2508.05671)
*Ko-Wei Chuang,Hen-Hsen Huang,Tsai-Yen Li*

Main category: cs.CR

TL;DR: DINA addresses dual adversarial threats (external manipulation and internal label corruption) in NLP systems using a unified framework, improving robustness and accuracy on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Large language models in customer service and moderation face adversarial risks from data poisoning (internal) and direct attacks (external), necessitating robust solutions for reliable deployment.

Method: DINA integrates computer vision's noisy-label learning techniques with adversarial training to simultaneously defend against internal label corruption and external adversarial perturbations in NLP tasks.

Result: Experiments on an online gaming dataset show DINA outperforms baselines in both robustness to adversarial threats and task accuracy under realistic attack scenarios.

Conclusion: DINA demonstrates practical effectiveness for dual-threat mitigation in NLP, emphasizing the importance of proactive defenses for fair AI deployment in real-world applications.

Abstract: As large language models (LLMs) and generative AI become increasingly
integrated into customer service and moderation applications, adversarial
threats emerge from both external manipulations and internal label corruption.
In this work, we identify and systematically address these dual adversarial
threats by introducing DINA (Dual Defense Against Internal Noise and
Adversarial Attacks), a novel unified framework tailored specifically for NLP.
Our approach adapts advanced noisy-label learning methods from computer vision
and integrates them with adversarial training to simultaneously mitigate
internal label sabotage and external adversarial perturbations. Extensive
experiments conducted on a real-world dataset from an online gaming service
demonstrate that DINA significantly improves model robustness and accuracy
compared to baseline models. Our findings not only highlight the critical
necessity of dual-threat defenses but also offer practical strategies for
safeguarding NLP systems in realistic adversarial scenarios, underscoring
broader implications for fair and responsible AI deployment.

</details>


### [5] [Towards Effective Offensive Security LLM Agents: Hyperparameter Tuning, LLM as a Judge, and a Lightweight CTF Benchmark](https://arxiv.org/abs/2508.05674)
*Minghao Shao,Nanda Rani,Kimberly Milner,Haoran Xi,Meet Udeshi,Saksham Aggarwal,Venkata Sai Charan Putrevu,Sandeep Kumar Shukla,Prashanth Krishnamurthy,Farshad Khorrami,Ramesh Karri,Muhammad Shafique*

Main category: cs.CR

TL;DR: The paper presents CTFJudge, CCI metric, CTFTiny benchmark, and analyzes factors affecting LLM-based CTF agent performance in cybersecurity.


<details>
  <summary>Details</summary>
Motivation: LLM agentic systems are increasingly used for CTF challenges, but systematic analysis of their success factors and unified evaluation is lacking.

Method: Developed CTFJudge for trajectory analysis, CCI for partial correctness, and CTFTiny for benchmarking; conducted experimental analysis of LLM hyperparameters across 5 CTF categories.

Result: Identified optimal multi-agent settings, created open-source tools with 50 benchmark challenges, and revealed CCI effectiveness in measuring solution alignment.

Conclusion: Established foundational frameworks for evaluating LLM-based offensive security agents and demonstrated how design choices impact success, with CTFTiny and CTFJudge available for public use.

Abstract: Recent advances in LLM agentic systems have improved the automation of
offensive security tasks, particularly for Capture the Flag (CTF) challenges.
We systematically investigate the key factors that drive agent success and
provide a detailed recipe for building effective LLM-based offensive security
agents. First, we present CTFJudge, a framework leveraging LLM as a judge to
analyze agent trajectories and provide granular evaluation across CTF solving
steps. Second, we propose a novel metric, CTF Competency Index (CCI) for
partial correctness, revealing how closely agent solutions align with
human-crafted gold standards. Third, we examine how LLM hyperparameters, namely
temperature, top-p, and maximum token length, influence agent performance and
automated cybersecurity task planning. For rapid evaluation, we present
CTFTiny, a curated benchmark of 50 representative CTF challenges across binary
exploitation, web, reverse engineering, forensics, and cryptography. Our
findings identify optimal multi-agent coordination settings and lay the
groundwork for future LLM agent research in cybersecurity. We make CTFTiny open
source to public https://github.com/NYU-LLM-CTF/CTFTiny along with CTFJudge on
https://github.com/NYU-LLM-CTF/CTFJudge.

</details>


### [6] [Principle-Guided Verilog Optimization: IP-Safe Knowledge Transfer via Local-Cloud Collaboration](https://arxiv.org/abs/2508.05675)
*Jing Wang,Zheng Li,Lei Li,Fan He,Liyu Lin,Yao Lai,Yan Li,Xiaoyang Zeng,Yufeng Guo*

Main category: cs.CR

TL;DR: Proposes an IP-preserving edge-cloud collaborative framework for RTL code optimization using local LLMs for analysis and cloud LLMs for targeted improvements, achieving higher success rates.


<details>
  <summary>Details</summary>
Motivation: Cloud-based LLMs provide powerful RTL optimization but risk intellectual property (IP) leakage during hardware design processing. Existing approaches fail to balance optimization performance with IP security constraints.

Method: 1. Local small LLMs (e.g., Qwen-2.5-Coder-7B) analyze paired high-quality target designs and novice drafts to extract general design principles
2. Principles are abstracted and sent as non-sensitive guidance to cloud LLMs (e.g., Deepseek-V3)
3. Cloud LLMs deliver targeted code improvements using the principles without exposing raw design data

Result: Achieved 66.67% optimization success rate for power utilization with Qwen-2.5-Coder-7B + Deepseek-V3 combination, surpassing:
- 49.81% with Deepseek-V3 alone
- 55.81% with commercial GPT-4o
Demonstrated strategic effectiveness of model pairing and observed optimization trends with varying code pairs

Conclusion: Establishes a secure-by-design paradigm for hardware optimization that preserves IP confidentiality while maintaining strong performance through collaborative edge-cloud LLM orchestration, opening new possibilities for constrained engineering optimization.

Abstract: Recent years have witnessed growing interest in adopting large language
models (LLMs) for Register Transfer Level (RTL) code optimization. While
powerful cloud-based LLMs offer superior optimization capabilities, they pose
unacceptable intellectual property (IP) leakage risks when processing
proprietary hardware designs. In this paper, we propose a new scenario where
Verilog code must be optimized for specific attributes without leaking
sensitive IP information. We introduce the first IP-preserving edge-cloud
collaborative framework that leverages the benefits of both paradigms. Our
approach employs local small LLMs (e.g., Qwen-2.5-Coder-7B) to perform secure
comparative analysis between paired high-quality target designs and novice
draft codes, yielding general design principles that summarize key insights for
improvements. These principles are then used to query stronger cloud LLMs
(e.g., Deepseek-V3) for targeted code improvement, ensuring that only
abstracted and IP-safe guidance reaches external services. Our experimental
results demonstrate that the framework achieves significantly higher
optimization success rates compared to baseline methods. For example, combining
Qwen-2.5-Coder-7B and Deepseek-V3 achieves a 66.67\% optimization success rate
for power utilization, outperforming Deepseek-V3 alone (49.81\%) and even
commercial models like GPT-4o (55.81\%). Further investigation of local and
cloud LLM combinations reveals that different model pairings exhibit varying
strengths for specific optimization objectives, with interesting trends
emerging when varying the number of comparative code pairs. Our work
establishes a new paradigm for secure hardware design optimization that
balances performance gains with IP protection.

</details>


### [7] [Adversarial Attacks on Reinforcement Learning-based Medical Questionnaire Systems: Input-level Perturbation Strategies and Medical Constraint Validation](https://arxiv.org/abs/2508.05677)
*Peizhuo Liu*

Main category: cs.CR

TL;DR: This paper evaluates adversarial attack vulnerabilities in RL-based medical questionnaire systems using six attack methods under 247 medical constraints, finding significant flaws despite clinical plausibility. 


<details>
  <summary>Details</summary>
Motivation: RL-based medical systems show promise but lack resolved safety and robustness, necessitating evaluation of adversarial attack effectiveness to identify vulnerabilities. 

Method: The diagnosis process is modeled as an MDP; six attack methods (FGSM, PGD, C&W, BIM, DeepFool, AutoAttack) with seven ε values each are implemented, validated using 247 medical constraints (physiological bounds, symptom correlations) on the NHIS dataset (182,630 samples) to predict mortality risk. 

Result: 97.6% clinically plausible adversarial sample generation with attack success rates from 33.08% (FGSM) to 64.70% (AutoAttack) against the AdaptiveFS framework on the NHIS dataset. 

Conclusion: Adversarial attacks can significantly undermine RL-based medical questionnaire systems' diagnostic accuracy, indicating critical vulnerabilities persist even under strict medical input constraints. 

Abstract: RL-based medical questionnaire systems have shown great potential in medical
scenarios. However, their safety and robustness remain unresolved. This study
performs a comprehensive evaluation on adversarial attack methods to identify
and analyze their potential vulnerabilities. We formulate the diagnosis process
as a Markov Decision Process (MDP), where the state is the patient responses
and unasked questions, and the action is either to ask a question or to make a
diagnosis. We implemented six prevailing major attack methods, including the
Fast Gradient Signed Method (FGSM), Projected Gradient Descent (PGD), Carlini &
Wagner Attack (C&W) attack, Basic Iterative Method (BIM), DeepFool, and
AutoAttack, with seven epsilon values each. To ensure the generated adversarial
examples remain clinically plausible, we developed a comprehensive medical
validation framework consisting of 247 medical constraints, including
physiological bounds, symptom correlations, and conditional medical
constraints. We achieved a 97.6% success rate in generating clinically
plausible adversarial samples. We performed our experiment on the National
Health Interview Survey (NHIS) dataset (https://www.cdc.gov/nchs/nhis/), which
consists of 182,630 samples, to predict the participant's 4-year mortality
rate. We evaluated our attacks on the AdaptiveFS framework proposed in
arXiv:2004.00994. Our results show that adversarial attacks could significantly
impact the diagnostic accuracy, with attack success rates ranging from 33.08%
(FGSM) to 64.70% (AutoAttack). Our work has demonstrated that even under strict
medical constraints on the input, such RL-based medical questionnaire systems
still show significant vulnerabilities.

</details>


### [8] [Selection-Based Vulnerabilities: Clean-Label Backdoor Attacks in Active Learning](https://arxiv.org/abs/2508.05681)
*Yuhan Zhi,Longtian Wang,Xiaofei Xie,Chao Shen,Qiang Hu,Xiaohong Guan*

Main category: cs.CR

TL;DR: The paper introduces ALA, a poisoning attack framework that exploits acquisition functions in active learning, demonstrating high attack success rates (up to 94%) with low poisoning budgets while maintaining model utility and evasion of human detection.


<details>
  <summary>Details</summary>
Motivation: Raises critical safety concerns about active learning, showing that acquisition functions—designed to optimize label efficiency—can be weaponized as attack surfaces, a previously unexplored vulnerability.

Method: ALA optimizes imperceptible poisoned inputs with high uncertainty scores to increase selection probability by acquisition functions, leveraging weaknesses in their query strategy for adversarial exploitation.

Result: Experiments on three datasets, three acquisition functions, and two clean-label backdoor triggers showed ALA achieves 94% attack success with 0.5-1% poisoning budget, without detectable degradation of model performance.

Conclusion: Active learning systems are vulnerable to stealthy poisoning attacks through acquisition functions. The work urges caution in deployment for trusted data environments and highlights the need for robustness against adversarial query strategies.

Abstract: Active learning(AL), which serves as the representative label-efficient
learning paradigm, has been widely applied in resource-constrained scenarios.
The achievement of AL is attributed to acquisition functions, which are
designed for identifying the most important data to label. Despite this
success, one question remains unanswered: is AL safe? In this work, we
introduce ALA, a practical and the first framework to utilize the acquisition
function as the poisoning attack surface to reveal the weakness of active
learning. Specifically, ALA optimizes imperceptibly poisoned inputs to exhibit
high uncertainty scores, increasing their probability of being selected by
acquisition functions. To evaluate ALA, we conduct extensive experiments across
three datasets, three acquisition functions, and two types of clean-label
backdoor triggers. Results show that our attack can achieve high success rates
(up to 94%) even under low poisoning budgets (0.5%-1.0%) while preserving model
utility and remaining undetectable to human annotators. Our findings remind
active learning users: acquisition functions can be easily exploited, and
active learning should be deployed with caution in trusted data scenarios.

</details>


### [9] [MM-FusionNet: Context-Aware Dynamic Fusion for Multi-modal Fake News Detection with Large Vision-Language Models](https://arxiv.org/abs/2508.05684)
*Junhao He,Tianyu Liu,Jingyuan Zhao,Benjamin Turner*

Main category: cs.CR

TL;DR: The paper introduces MM-FusionNet, a framework using LVLMs with a Context-Aware Dynamic Fusion Module (CADFM) for robust multi-modal fake news detection, achieving an F1-score of 0.938 on the LMFND dataset.


<details>
  <summary>Details</summary>
Motivation: Traditional text-based detection methods fail to address deceptive combinations of text and images in multi-modal fake news, while existing multi-modal approaches struggle with unbalanced/contradictory information from different modalities.

Method: MM-FusionNet employs bi-directional cross-modal attention and a dynamic modal gating network in CADFM to adaptively assign relevance-based importance weights to text/visual features, enhancing LVLMs for fused multi-modal understanding.

Result: Achieves state-of-the-art 0.938 F1-score on LMFND (80,000 samples), outperforming multi-modal baselines by 0.5% and single-modal approaches significantly, with robust modality-perturbation handling and human-like performance.

Conclusion: MM-FusionNet demonstrates practical efficacy for real-world multi-modal fake news detection through its adaptive weighting mechanism, robustness, and nearly human-level performance, offering improved interpretability.

Abstract: The proliferation of multi-modal fake news on social media poses a
significant threat to public trust and social stability. Traditional detection
methods, primarily text-based, often fall short due to the deceptive interplay
between misleading text and images. While Large Vision-Language Models (LVLMs)
offer promising avenues for multi-modal understanding, effectively fusing
diverse modal information, especially when their importance is imbalanced or
contradictory, remains a critical challenge. This paper introduces
MM-FusionNet, an innovative framework leveraging LVLMs for robust multi-modal
fake news detection. Our core contribution is the Context-Aware Dynamic Fusion
Module (CADFM), which employs bi-directional cross-modal attention and a novel
dynamic modal gating network. This mechanism adaptively learns and assigns
importance weights to textual and visual features based on their contextual
relevance, enabling intelligent prioritization of information. Evaluated on the
large-scale Multi-modal Fake News Dataset (LMFND) comprising 80,000 samples,
MM-FusionNet achieves a state-of-the-art F1-score of 0.938, surpassing existing
multi-modal baselines by approximately 0.5% and significantly outperforming
single-modal approaches. Further analysis demonstrates the model's dynamic
weighting capabilities, its robustness to modality perturbations, and
performance remarkably close to human-level, underscoring its practical
efficacy and interpretability for real-world fake news detection.

</details>


### [10] [Leveraging large language models for SQL behavior-based database intrusion detection](https://arxiv.org/abs/2508.05690)
*Meital Shlezinger,Shay Akirav,Lei Zhou,Liang Guo,Avi Kessel,Guoliang Li*

Main category: cs.CR

TL;DR: This paper proposes a two-tiered anomaly detection approach for SQL database access using DistilBERT and machine learning to improve granularity and accuracy in identifying both out-of-scope and in-scope attacks.


<details>
  <summary>Details</summary>
Motivation: Existing database anomaly detection systems lack operational-level granularity, mislabeling normal activities as anomalies and failing to detect sophisticated attacks that mimic legitimate behavior. This is critical as threats like SQL intrusion continue to rise.

Method: The method combines unsupervised ensemble anomaly detection (via embedding vectors distance from normal patterns) for out-of-scope queries, with a supervised transformer-based model (fine-tuned DistilBERT with role-labeled classification) for in-scope attacks. This hybrid approach reduces labeling needs while handling limited SQL dataset challenges.

Result: The approach significantly enhances detection accuracy for both external attacks and internal masquerading threats, effectively addressing limitations in current methods that struggle with false positives and sophisticated in-scope anomalies.

Conclusion: The two-tiered framework bridges gaps in database security by combining efficient anomaly detection and role-based classification. It provides a scalable solution for identifying abnormal access patterns without relying heavily on labeled training data, particularly valuable against stealthy internal threats.

Abstract: Database systems are extensively used to store critical data across various
domains. However, the frequency of abnormal database access behaviors, such as
database intrusion by internal and external attacks, continues to rise.
Internal masqueraders often have greater organizational knowledge, making it
easier to mimic employee behavior effectively. In contrast, external
masqueraders may behave differently due to their lack of familiarity with the
organization. Current approaches lack the granularity needed to detect
anomalies at the operational level, frequently misclassifying entire sequences
of operations as anomalies, even though most operations are likely to represent
normal behavior. On the other hand, some anomalous behaviors often resemble
normal activities, making them difficult for existing detection methods to
identify. This paper introduces a two-tiered anomaly detection approach for
Structured Query Language (SQL) using the Bidirectional Encoder Representations
from Transformers (BERT) model, specifically DistilBERT, a more efficient,
pre-trained version. Our method combines both unsupervised and supervised
machine learning techniques to accurately identify anomalous activities while
minimizing the need for data labeling. First, the unsupervised method uses
ensemble anomaly detectors that flag embedding vectors distant from learned
normal patterns of typical user behavior across the database (out-of-scope
queries). Second, the supervised method uses fine-tuned transformer-based
models to detect internal attacks with high precision (in-scope queries), using
role-labeled classification, even on limited labeled SQL data. Our findings
make a significant contribution by providing an effective solution for
safeguarding critical database systems from sophisticated threats.

</details>


### [11] [AuthPrint: Fingerprinting Generative Models Against Malicious Model Providers](https://arxiv.org/abs/2508.05691)
*Kai Yao,Marc Juarez*

Main category: cs.CR

TL;DR: This paper introduces adversarially robust model fingerprinting techniques to verify the origin of generative model outputs in high-stakes domains, even when the model provider acts adversarially. The methods achieve near-zero false positive rates and resist adversarial modifications.


<details>
  <summary>Details</summary>
Motivation: Current generative model deployments lack mechanisms to verify output origins, especially when model providers may act maliciously. Traditional fingerprinting methods assume non-adversarial settings which is insufficient for real-world high-risk applications.

Method: The authors propose using a trusted third-party verifier to extract secret fingerprints from model outputs that remain hidden from the provider. These fingerprints are then trained to verify model provenance through detection in the output space.

Result: Empirical results show near-zero FPR@95%TPR for GAN and diffusion models even with architecture/data modifications and adversarial attacks attempting to bypass detection.

Conclusion: This work demonstrates the first provenance attribution framework for generative models under adversarial provider scenarios, establishing feasibility of robust fingerprinting techniques that withstand both architectural perturbations and targeted attacks.

Abstract: Generative models are increasingly adopted in high-stakes domains, yet
current deployments offer no mechanisms to verify the origin of model outputs.
We address this gap by extending model fingerprinting techniques beyond the
traditional collaborative setting to one where the model provider may act
adversarially. To our knowledge, this is the first work to evaluate
fingerprinting for provenance attribution under such a threat model. The
methods rely on a trusted verifier that extracts secret fingerprints from the
model's output space, unknown to the provider, and trains a model to predict
and verify them. Our empirical evaluation shows that our methods achieve
near-zero FPR@95%TPR for instances of GAN and diffusion models, even when
tested on small modifications to the original architecture and training data.
Moreover, the methods remain robust against adversarial attacks that actively
modify the outputs to bypass detection. Source codes are available at
https://github.com/PSMLab/authprint.

</details>


### [12] [DMFI: Dual-Modality Fine-Tuning and Inference Framework for LLM-Based Insider Threat Detection](https://arxiv.org/abs/2508.05694)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Guanggang Geng,Zhiying Li,Jian Weng*

Main category: cs.CR

TL;DR: DMFI is a dual-modality framework combining semantic and behavioral analysis for insider threat detection, outperforming existing methods on CERT datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional cybersecurity models fail to capture semantic intent of insider threats, and existing LLM-based approaches lack modality coverage and prompt adaptability. Context-dependent nature of insider threats requires more sophisticated solutions.

Method: DMFI integrates two structured views: (1) semantic inference through instruction-formatted prompts for content analysis, and (2) behavioral abstraction using 4W-guided transformation (When-Where-What-Which). Two LoRA-enhanced LLMs are fine-tuned separately and their outputs fused via MLP. DMFI-B further separates normal/abnormal behavior representations.

Result: Achieved superior detection accuracy compared to state-of-the-art methods on CERT r4.2 and r5.2 insider threat datasets, demonstrating effectiveness against severe class imbalance challenges.

Conclusion: The framework demonstrates that combining LLM semantic reasoning with structured behavioral modeling provides a scalable, effective solution for real-world insider threat detection, overcoming limitations of existing approaches.

Abstract: Insider threat detection (ITD) poses a persistent and high-impact challenge
in cybersecurity due to the subtle, long-term, and context-dependent nature of
malicious insider behaviors. Traditional models often struggle to capture
semantic intent and complex behavior dynamics, while existing LLM-based
solutions face limitations in prompt adaptability and modality coverage. To
bridge this gap, we propose DMFI, a dual-modality framework that integrates
semantic inference with behavior-aware fine-tuning. DMFI converts raw logs into
two structured views: (1) a semantic view that processes content-rich artifacts
(e.g., emails, https) using instruction-formatted prompts; and (2) a behavioral
abstraction, constructed via a 4W-guided (When-Where-What-Which) transformation
to encode contextual action sequences. Two LoRA-enhanced LLMs are fine-tuned
independently, and their outputs are fused via a lightweight MLP-based decision
module. We further introduce DMFI-B, a discriminative adaptation strategy that
separates normal and abnormal behavior representations, improving robustness
under severe class imbalance. Experiments on CERT r4.2 and r5.2 datasets
demonstrate that DMFI outperforms state-of-the-art methods in detection
accuracy. Our approach combines the semantic reasoning power of LLMs with
structured behavior modeling, offering a scalable and effective solution for
real-world insider threat detection. Our work demonstrates the effectiveness of
combining LLM reasoning with structured behavioral modeling, offering a
scalable and deployable solution for modern insider threat detection.

</details>


### [13] [MambaITD: An Efficient Cross-Modal Mamba Network for Insider Threat Detection](https://arxiv.org/abs/2508.05695)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Zhiying Li,Guanggang Geng,Jian Weng*

Main category: cs.CR

TL;DR: MambaITD is a novel insider threat detection framework leveraging Mamba state space models and cross-modal adaptive fusion to overcome limitations in temporal modeling, computational efficiency, and cross-modal integration compared to existing methods. The approach pre-processes multi-source logs, models long-range dependencies, and uses adaptive threshold optimization for improved anomaly identification.


<details>
  <summary>Details</summary>
Motivation: Current insider threat detection methods exhibit critical limitations: inadequate temporal dynamic feature modeling, poor computational efficiency for real-time operations, and the cross-modal information island problem. These deficiencies hinder effective identification of anomalous insider behavior in enterprise environments.

Method: The framework consists of three modules: 1) Multi-source log preprocessing with heterogeneous data alignment through behavioral sequence encoding, interval smoothing, and statistical feature extraction; 2) Mamba encoder modeling long-range dependencies in temporal sequences while dynamically combining sequential and statistical information via a gated fusion mechanism; 3) Adaptive threshold optimization using inter-class variance maximization to address class imbalance and concept drift through probability distribution analysis.

Result: MambaITD demonstrates 1) Superior computational efficiency compared to Transformer-based architectures; 2) Enhanced feature fusion capabilities across heterogeneous log data; 3) Improved anomaly detection performance validated against traditional approaches, effectively mitigating class imbalance and concept drift challenges.

Conclusion: This paper introduces MambaITD, a state space model-based framework that advances insider threat detection by effectively modeling temporal dynamics and cross-modal relationships. The results confirm its advantages in efficiency, fusion capability, and detection performance over conventional approaches, offering a promising solution for enterprise cybersecurity challenges.

Abstract: Enterprises are facing increasing risks of insider threats, while existing
detection methods are unable to effectively address these challenges due to
reasons such as insufficient temporal dynamic feature modeling, computational
efficiency and real-time bottlenecks and cross-modal information island
problem. This paper proposes a new insider threat detection framework MambaITD
based on the Mamba state space model and cross-modal adaptive fusion. First,
the multi-source log preprocessing module aligns heterogeneous data through
behavioral sequence encoding, interval smoothing, and statistical feature
extraction. Second, the Mamba encoder models long-range dependencies in
behavioral and interval sequences, and combines the sequence and statistical
information dynamically in combination with the gated feature fusion mechanism.
Finally, we propose an adaptive threshold optimization method based on
maximizing inter-class variance, which dynamically adjusts the decision
threshold by analyzing the probability distribution, effectively identifies
anomalies, and alleviates class imbalance and concept drift. Compared with
traditional methods, MambaITD shows significant advantages in modeling
efficiency and feature fusion capabilities, outperforming Transformer-based
methods, and provides a more effective solution for insider threat detection.

</details>


### [14] [Log2Sig: Frequency-Aware Insider Threat Detection via Multivariate Behavioral Signal Decomposition](https://arxiv.org/abs/2508.05696)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Zhiying Li,Guanggang Geng*

Main category: cs.CR

TL;DR: Log2Sig is a novel anomaly detection framework that transforms user logs into multivariate frequency signals and uses dual-view modeling to enhance insider threat detection beyond existing methods.


<details>
  <summary>Details</summary>
Motivation: Current approaches model system logs as flat event sequences, missing inherent frequency dynamics and multiscale disturbance patterns critical for distinguishing insider threats from normal behavior.

Method: Log2Sig employs Multivariate Variational Mode Decomposition (MVMD) to extract Intrinsic Mode Functions (IMFs), combines Mamba-based temporal encoding of daily behavior with linearly projected frequency components, and fuses dual-view representations via a multilayer perceptron.

Result: Achieves superior accuracy and F1 scores on CERT r4.2 and r5.2 datasets compared to state-of-the-art baselines.

Conclusion: By capturing behavioral fluctuations across temporal scales and integrating sequence-frequency modeling, Log2Sig offers a robust solution to the deceptive challenges of insider threat detection.

Abstract: Insider threat detection presents a significant challenge due to the
deceptive nature of malicious behaviors, which often resemble legitimate user
operations. However, existing approaches typically model system logs as flat
event sequences, thereby failing to capture the inherent frequency dynamics and
multiscale disturbance patterns embedded in user behavior. To address these
limitations, we propose Log2Sig, a robust anomaly detection framework that
transforms user logs into multivariate behavioral frequency signals,
introducing a novel representation of user behavior. Log2Sig employs
Multivariate Variational Mode Decomposition (MVMD) to extract Intrinsic Mode
Functions (IMFs), which reveal behavioral fluctuations across multiple temporal
scales. Based on this, the model further performs joint modeling of behavioral
sequences and frequency-decomposed signals: the daily behavior sequences are
encoded using a Mamba-based temporal encoder to capture long-term dependencies,
while the corresponding frequency components are linearly projected to match
the encoder's output dimension. These dual-view representations are then fused
to construct a comprehensive user behavior profile, which is fed into a
multilayer perceptron for precise anomaly detection. Experimental results on
the CERT r4.2 and r5.2 datasets demonstrate that Log2Sig significantly
outperforms state-of-the-art baselines in both accuracy and F1 score.

</details>


### [15] [System Security Framework for 5G Advanced /6G IoT Integrated Terrestrial Network-Non-Terrestrial Network (TN-NTN) with AI-Enabled Cloud Security](https://arxiv.org/abs/2508.05707)
*Sasa Maric,Rasil Baidar,Robert Abbas,Sam Reisenfeld*

Main category: cs.CR

TL;DR: This paper proposes an AI-enabled cloud-based security framework for integrated 5G Advanced/6G IoT TN-NTN systems, addressing challenges via zero-trust principles, federated learning, and satellite-enhanced security mechanisms.


<details>
  <summary>Details</summary>
Motivation: The heterogeneous, large-scale, and distributed nature of integrated TN-NTN (terrestrial and non-terrestrial) systems introduces new security vulnerabilities in 5G Advanced/6G IoT deployments, necessitating adaptive and real-time security solutions that leverage emerging technologies like AI-native cloud platforms.

Method: The authors design a system-level security framework that combines AI-native cloud infrastructure with NTN-specific security layers. Key components include NTN satellite access function for coverage resilience, AI models for threat detection and policy enforcement, federated learning for distributed threat intelligence sharing, and secure orchestration mechanisms.

Result: A comprehensive security framework is presented with detailed implementation proposals. The approach enables real-time threat mitigation, automated security orchestration for dynamic network slicing, and enhanced protection for cloudified RAN/core against adversarial attacks through layered defense and federated learning strategies.

Conclusion: The proposed AI-powered, satellite-based security framework establishes robust zero-trust architecture and federated threat intelligence for TN-NTN integration in 5G Advanced/6G IoT systems, emphasizing necessary technical requirements for future implementation. Resilience against adversarial threats and secure orchestration remain critical focus areas for next-generation connectivity.

Abstract: The integration of Terrestrial Networks (TN) and Non-Terrestrial Networks
(NTN), including 5G Advanced/6G and the Internet of Things (IoT) technologies,
using Low Earth Orbit (LEO) satellites, high-altitude platforms (HAPS), and
Unmanned Aerial Vehicles (UAVs), is redefining the landscape of global
connectivity. This paper introduces a new system-level security framework for
5G Advanced/6G IoT-integrated TN-NTN architectures with AI-native-enabled cloud
security. Due to the heterogeneity, scale, and distributed nature of these
networks, new security challenges have emerged. Leveraging AI-native cloud
platforms offers powerful capabilities for real-time threat detection, security
automation, and intelligent policy enforcement. The NTN satellite access
function enhances security for discontinuous coverage via satellite
connections. In addition, this paper explores the security risks associated
with integrated 5G Advanced/6G IoT TN-NTN systems, including full network
segmentation, network slicing, and the cloudification of the RAN and core. We
present a comprehensive AI-enabled cloud security framework and conclude with
proposals for implementing AI-powered, satellite-based NTN within future 5G
Advanced/6G IoT networks. Our approach emphasizes zero-trust principles,
federated learning, secure orchestration, a layered security framework, and
resilience against adversarial threats.

</details>


### [16] [On Digital Twins in Defence: Overview and Applications](https://arxiv.org/abs/2508.05717)
*Marco Giberna,Holger Voos,Paulo Tavares,João Nunes,Tobias Sorg,Andrea Masini,Jose Luis Sanchez-Lopez*

Main category: cs.CR

TL;DR: This paper examines digital twin integration in defence applications, presenting use cases, advantages, a novel characterization framework, and implementation challenges.


<details>
  <summary>Details</summary>
Motivation: Digital twin technology's potential in defence remains underexplored despite its widespread adoption in other sectors. Standardization and interoperability issues hinder its effective application. A unified framework is needed to address adoption barriers and unlock benefits in areas like system design, training, and mission execution.

Method: The research combines analysis of scientific literature, industry practices, governmental strategies, and survey data from defence stakeholders to evaluate use cases, advantages, challenges, and future research directions.

Result: 1. Key advantages in defence contexts: enhanced operational performance, predictive capabilities, and system uptime. 2. A novel characterization framework for standardizing cross-domain digital twin applications. 3. Identified implementation challenges including interoperability gaps, data integration complexities, and organizational adoption barriers.

Conclusion: The paper emphasizes the need for robust characterization frameworks and interdisciplinary collaboration to overcome challenges and fully enable digital twin adoption in defence. Future research should focus on refining these frameworks and addressing practical implementation barriers.

Abstract: Digital twin technology has gained increasing attention across various
sectors due to its ability to create virtual replicas of physical systems,
enabling real-time monitoring, optimization, and simulation. This paper
explores the integration of digital twins within defence applications, focusing
on key use cases ranging from system design and development, operational
planning and training, to mission execution and debriefing. By examining the
application of digital twin technologies across defense platforms, we highlight
their key advantages such as enhanced operational performance, predictive
capabilities, and increased system uptime. Additionally, we introduce a novel
characterization framework for digital twins that aims to standardize and unify
their application across different defence domains to facilitate
interoperability. Thereafter, we discuss the main challenges, gaps and
limitations in implementing and adopting digital twins within defence
organizations by analyzing a combination of scientific literature, current
industry practices, governmental strategies, and the findings from a
comprehensive survey of industrial stakeholders and ministries of defense.
Finally, we outline future research directions and development opportunities,
emphasizing the need for robust frameworks and interdisciplinary collaborations
to fully realize the potential of digital twins in the defence sector.

</details>


### [17] [Secure and Scalable Blockchain Voting: A Comparative Framework and the Role of Large Language Models](https://arxiv.org/abs/2508.05865)
*Kiana Kiashemshaki,Elvis Nnaemeka Chukwuani,Mohammad Jalili Torkamani,Negin Mahmoudi*

Main category: cs.CR

TL;DR: This paper introduces a comparative framework for blockchain-based E-Voting systems, analyzing architectures and consensus mechanisms while proposing optimizations using LLMs to address scalability and privacy challenges for national deployment.


<details>
  <summary>Details</summary>
Motivation: Blockchain's potential for secure, transparent E-Voting is hindered by scalability, computational intensity, and privacy complexities. The aim is to evaluate existing models and develop practical solutions for large-scale implementation.

Method: The authors analyze blockchain architectures, evaluate consensus mechanisms (PoW, PoS, DPoS), and propose optimization strategies including hybrid consensus, lightweight cryptography, decentralized identity, and integrate LLMs for smart contract generation and anomaly detection.

Result: Identified limitations of current consensus models, validated optimization strategies through systematic analysis, and demonstrated the viability of LLMs in enhancing smart contract processes and system security for scalable E-Voting.

Conclusion: The framework provides pathways to design secure, scalable, and intelligent blockchain E-Voting systems. It enables prototype development with LLM-driven smart contracts and simulation-based validation for real-world application.

Abstract: Blockchain technology offers a promising foundation for modernizing E-Voting
systems by enhancing transparency, decentralization, and security. Yet,
real-world adoption remains limited due to persistent challenges such as
scalability constraints, high computational demands, and complex privacy
requirements. This paper presents a comparative framework for analyzing
blockchain-based E-Voting architectures, consensus mechanisms, and
cryptographic protocols. We examine the limitations of prevalent models like
Proof of Work, Proof of Stake, and Delegated Proof of Stake, and propose
optimization strategies that include hybrid consensus, lightweight
cryptography, and decentralized identity management. Additionally, we explore
the novel role of Large Language Models (LLMs) in smart contract generation,
anomaly detection, and user interaction. Our findings offer a foundation for
designing secure, scalable, and intelligent blockchain-based E-Voting systems
suitable for national-scale deployment. This work lays the groundwork for
building an end-to-end blockchain E-Voting prototype enhanced by LLM-guided
smart contract generation and validation, supported by a systematic framework
and simulation-based analysis.

</details>


### [18] [Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System](https://arxiv.org/abs/2508.06059)
*Haorui He,Yupeng Li,Bin Benjamin Zhu,Dacheng Wen,Reynold Cheng,Francis C. M. Lau*

Main category: cs.CR

TL;DR: This paper introduces Fact2Fiction, a novel poisoning attack framework targeting LLM-based agentic fact-checking systems by exploiting decomposition strategies and system-generated justifications, achieving higher attack success rates than existing methods and revealing security vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Current autonomous LLM-based fact-checking systems, while effective at combating misinformation, remain underexplored in terms of security. Compromised systems could inadvertently amplify misinformation, necessitating analysis of their vulnerabilities.

Method: Fact2Fiction mimics the fact-checker's decomposition approach to generate tailored malicious evidence that specifically compromises sub-claim verification processes through poisoned justifications.

Result: Experiments demonstrate Fact2Fiction outperforms state-of-the-art attacks by 8.9-21.2% in success rates across different poisoning budgets, demonstrating its effectiveness against agentic fact-checkers.

Conclusion: The research exposes critical security weaknesses in LLM-based fact-checking systems' decomposition mechanisms and underscores the urgent need for defensive strategies against such poisoning attacks.

Abstract: State-of-the-art fact-checking systems combat misinformation at scale by
employing autonomous LLM-based agents to decompose complex claims into smaller
sub-claims, verify each sub-claim individually, and aggregate the partial
results to produce verdicts with justifications (explanatory rationales for the
verdicts). The security of these systems is crucial, as compromised
fact-checkers, which tend to be easily underexplored, can amplify
misinformation. This work introduces Fact2Fiction, the first poisoning attack
framework targeting such agentic fact-checking systems. Fact2Fiction mirrors
the decomposition strategy and exploits system-generated justifications to
craft tailored malicious evidences that compromise sub-claim verification.
Extensive experiments demonstrate that Fact2Fiction achieves 8.9\%--21.2\%
higher attack success rates than state-of-the-art attacks across various
poisoning budgets. Fact2Fiction exposes security weaknesses in current
fact-checking systems and highlights the need for defensive countermeasures.

</details>


### [19] [A Game-Theoretic Foundation for Bitcoin's Price: A Security-Utility Equilibrium](https://arxiv.org/abs/2508.06071)
*Liang Chen*

Main category: cs.CR

TL;DR: The paper proposes a game-theoretic model to value decentralized digital assets using RESUNE equilibrium, tying price to hash rate and network security through a mining coordination framework. It provides testable predictions about Bitcoin's price, hash rate, and security dynamics.


<details>
  <summary>Details</summary>
Motivation: Traditional valuation of digital assets relies on speculative beliefs; this paper aims to provide a structural model grounded in economic theory to better understand price fundamentals and security mechanisms.

Method: Introduces RESUNE as a fixed-point equilibrium where market price determines hash rate via a free-entry mining model. Security is modeled as a function of attack probability using a global games framework. Structural VAR analysis is applied for empirical testing.

Result: Proofs of RESUNE existence, uniqueness conditions, and predictions like protocol halving reducing hash rate and price. The model explains unidirectional price-hash rate causality and decomposes Bitcoin's value into transactional, security, and speculative components.

Conclusion: The RESUNE framework offers a testable mechanism for deconstructing Bitcoin's value into economic fundamentals, demonstrating strategic interactions between price and security that differ from speculative models.

Abstract: This paper introduces a structural game-theoretic model to value
decentralized digital assets like Bitcoin. Instead of relying on speculative
beliefs, it frames the asset's price within a Rational-Expectations
Security-Utility Nash Equilibrium (RESUNE). This equilibrium is a fixed point
where the market-clearing price dictates the hash rate through a free-entry
mining model, which in turn endogenously sets the network's security. The
security, defined as one minus the probability of a 51% attack, is determined
via a global games model of attacker coordination, providing a unique and
continuous security function. We prove the existence of a RESUNE and offer
conditions for its uniqueness and stability. The model predicts that the
stabilizing direct effect of price on demand must outweigh the potentially
destabilizing feedback from price to security. The framework generates testable
predictions, such as a protocol halving causing a contraction in both hash rate
and price. A structural Vector Autoregression (VAR) model is proposed to test
this mechanism. The model decomposes Bitcoin's value into transactional
utility, security, and speculative components and explains the observed
unidirectional causality from price to hash rate.

</details>


### [20] [ProvX: Generating Counterfactual-Driven Attack Explanations for Provenance-Based Detection](https://arxiv.org/abs/2508.06073)
*Weiheng Wu,Wei Qiao,Teng Li,Yebo Feng,Zhuo Ma,Jianfeng Ma,Yang Liu*

Main category: cs.CR

TL;DR: ProvX is a framework that enhances GNN-based intrusion detection models by providing verifiable explanations through counterfactual logic and a closed-loop feedback system for robustness.


<details>
  <summary>Details</summary>
Motivation: GNN-based security models struggle with adoption due to their black-box nature, offering no actionable explanations for analysts to verify predictions against real-world attacks.

Method: ProvX uses counterfactual explanation logic to identify minimal graph subsets critical for predictions, transforming discrete graph search into a dual-objective continuous optimization. A Staged Solidification strategy improves explanation precision and stability.

Result: ProvX achieved 51.59% average explanation necessity on authoritative datasets, outperforming SOTA methods, and demonstrated closed-loop feedback improves model robustness against adversarial attacks.

Conclusion: ProvX bridges the explanation gap in GNN-based intrusion detection, enabling trust through verifiable insights and enhancing model robustness via feedback-driven optimization.

Abstract: Provenance graph-based intrusion detection systems are deployed on hosts to
defend against increasingly severe Advanced Persistent Threat. Using Graph
Neural Networks to detect these threats has become a research focus and has
demonstrated exceptional performance. However, the widespread adoption of
GNN-based security models is limited by their inherent black-box nature, as
they fail to provide security analysts with any verifiable explanations for
model predictions or any evidence regarding the model's judgment in relation to
real-world attacks. To address this challenge, we propose ProvX, an effective
explanation framework for exlaining GNN-based security models on provenance
graphs. ProvX introduces counterfactual explanation logic, seeking the minimal
structural subset within a graph predicted as malicious that, when perturbed,
can subvert the model's original prediction. We innovatively transform the
discrete search problem of finding this critical subgraph into a continuous
optimization task guided by a dual objective of prediction flipping and
distance minimization. Furthermore, a Staged Solidification strategy is
incorporated to enhance the precision and stability of the explanations. We
conducted extensive evaluations of ProvX on authoritative datasets. The
experimental results demonstrate that ProvX can locate critical graph
structures that are highly relevant to real-world attacks and achieves an
average explanation necessity of 51.59\%, with these metrics outperforming
current SOTA explainers. Furthermore, we explore and provide a preliminary
validation of a closed-loop Detection-Explanation-Feedback enhancement
framework, demonstrating through experiments that the explanation results from
ProvX can guide model optimization, effectively enhancing its robustness
against adversarial attacks.

</details>


### [21] [Adaptive Backtracking for Privacy Protection in Large Language Models](https://arxiv.org/abs/2508.06087)
*Zhihao Yao,Yuxuan Gu,Xiachong Feng,Weitao Ma,Bo Li,Xiaocheng Feng*

Main category: cs.CR

TL;DR: This paper addresses enterprise privacy leakage risks in Retrieval-Augmented Generation (RAG) by introducing ABack (a training-free leakage prevention mechanism) and PriGenQA (a new benchmark for healthcare/finance scenarios), achieving a 15% improvement in privacy utility over baselines without performance trade-offs.


<details>
  <summary>Details</summary>
Motivation: Current privacy research in AI focuses on user-oriented concerns while neglecting enterprise data leakage risks in RAG systems, and lacks effective solutions to prevent performance degradation during privacy safeguarding or public datasets for evaluation.

Method: 1. ABack: Training-free mechanism using a Hidden State Model to detect and suppress privacy leakage. 2. PriGenQA: Enterprise-focused benchmark dataset in healthcare and finance domains. 3. Adaptive attacker with Group Relative Policy Optimization for rigorous evaluation.

Result: ABack achieves up to 15% higher overall privacy utility score compared to strong baselines while avoiding the performance degradation typically seen in prior data sanitization methods.

Conclusion: The paper demonstrates a novel enterprise privacy protection approach for RAG systems that outperforms existing methods on both efficiency (training-free) and effectiveness (15% utility gain), while establishing the first benchmark for evaluating these critical privacy concerns.

Abstract: The preservation of privacy has emerged as a critical topic in the era of
artificial intelligence. However, current work focuses on user-oriented
privacy, overlooking severe enterprise data leakage risks exacerbated by the
Retrieval-Augmented Generation paradigm. To address this gap, our paper
introduces a novel objective: enterprise-oriented privacy concerns. Achieving
this objective requires overcoming two fundamental challenges: existing methods
such as data sanitization severely degrade model performance, and the field
lacks public datasets for evaluation. We address these challenges with several
solutions. (1) To prevent performance degradation, we propose ABack, a
training-free mechanism that leverages a Hidden State Model to pinpoint the
origin of a leakage intention and rewrite the output safely. (2) To solve the
lack of datasets, we construct PriGenQA, a new benchmark for enterprise privacy
scenarios in healthcare and finance. To ensure a rigorous evaluation, we move
beyond simple static attacks by developing a powerful adaptive attacker with
Group Relative Policy Optimization. Experiments show that against this superior
adversary, ABack improves the overall privacy utility score by up to 15\% over
strong baselines, avoiding the performance trade-offs of prior methods.

</details>


### [22] [Simulation in Cybersecurity: Understanding Techniques, Applications, and Goals](https://arxiv.org/abs/2508.06106)
*Luca Serena,Gabriele D'Angelo,Stefano Ferretti,Moreno Marzolla*

Main category: cs.CR

TL;DR: This paper provides a comprehensive review of cybersecurity simulation methodologies, categorizing research across four dimensions to address methodological fragmentation in the field.


<details>
  <summary>Details</summary>
Motivation: Existing studies are limited by narrow focus on specific techniques or applications, making holistic analysis difficult. The authors seek to identify methodological trends that span diverse threat scenarios and objectives.

Method: Systematic classification of 200+ papers using a four-dimensional framework encompassing application domain, threat types, simulation techniques, and simulation objectives

Result: Identification of methodological strengths/weaknesses, optimal simulation approaches for different threat categories, and domain-specific modeling paradigm suitability. Revealed simulation techniques perform best in network security compared to application security.

Conclusion: Establishes a taxonomy for analyzing cybersecurity simulations, highlights gaps in multi-domain threat analysis, and provides recommendations for selecting appropriate methodologies based on specific research objectives.

Abstract: Modeling and simulation are widely used in cybersecurity research to assess
cyber threats, evaluate defense mechanisms, and analyze vulnerabilities.
However, the diversity of application areas, the variety of cyberattacks
scenarios, and the differing objectives of these simulations makes it difficult
to identify methodological trends. Existing reviews often focus on specific
modeling techniques or application domains, making it challenging to analyze
the field as a whole. To address these limitations, we present a comprehensive
review of the current state of the art, classifying the selected papers based
on four dimensions: the application domain, the types of cyber threats
represented, the simulation techniques employed, and the primary goals of the
simulation. The review discusses the strengths and limitations of different
approaches, identifies which cyber threats are the most suited for
simulation-based investigations, and analyzes which modeling paradigms are most
appropriate for specific cybersecurity challenges.

</details>


### [23] [SLIP: Soft Label Mechanism and Key-Extraction-Guided CoT-based Defense Against Instruction Backdoor in APIs](https://arxiv.org/abs/2508.06153)
*Zhengxian Wu,Juan Wen,Wanli Peng,Haowei Chang,Yinghan Zhou,Yiming Xue*

Main category: cs.CR

TL;DR: This paper proposes SLIP, a defense mechanism against black-box backdoor attacks in LLM agents by combining key-extraction-guided Chain-of-Thought and Soft Label Mechanism, achieving 25.13% average attack success rate with minimal clean data accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Existing LLM defenses relying on white-box access fail against black-box backdoor attacks, where malicious instructions are stealthily embedded in system prompts via APIs, creating critical security vulnerabilities.

Method: SLIP integrates two mechanisms: (1) Key-extraction-guided Chain-of-Thought (KCoT) focuses model attention on task-relevant phrases, reducing trigger sensitivity; (2) Soft Label Mechanism (SLM) filters anomalous scores through semantic correlation analysis and mean deviation filtering to generate reliable semantic representations.

Result: Experiments show SLIP reduces attack success rate from 90.2% to 25.13% while maintaining 98.5% clean data accuracy, surpassing state-of-the-art defenses in both classification and QA tasks with statistically significant results.

Conclusion: SLIP provides a novel defense framework against black-box backdoor attacks through dual-phase input sanitization, achieving substantial ASR reduction and clean data performance preservation, with open-source implementation available for further research.

Abstract: With the development of customized large language model (LLM) agents, a new
threat of black-box backdoor attacks has emerged, where malicious instructions
are injected into hidden system prompts. These attacks easily bypass existing
defenses that rely on white-box access, posing a serious security challenge. To
address this, we propose SLIP, a Soft Label mechanism and key-extraction-guided
CoT-based defense against Instruction backdoors in APIs. SLIP is designed based
on two key insights. First, to counteract the model's oversensitivity to
triggers, we propose a Key-extraction-guided Chain-of-Thought (KCoT). Instead
of only considering the single trigger or the input sentence, KCoT prompts the
agent to extract task-relevant key phrases. Second, to guide the LLM toward
correct answers, our proposed Soft Label Mechanism (SLM) prompts the agent to
quantify the semantic correlation between key phrases and candidate answers.
Crucially, to mitigate the influence of residual triggers or misleading content
in phrases extracted by KCoT, which typically causes anomalous scores, SLM
excludes anomalous scores deviating significantly from the mean and
subsequently averages the remaining scores to derive a more reliable semantic
representation. Extensive experiments on classification and question-answer
(QA) tasks demonstrate that SLIP is highly effective, reducing the average
attack success rate (ASR) from 90.2% to 25.13% while maintaining high accuracy
on clean data and outperforming state-of-the-art defenses. Our code are
available in
https://github.com/CAU-ISS-Lab/Backdoor-Attack-Defense-LLMs/tree/main/SLIP.

</details>


### [24] [Anti-Tamper Protection for Unauthorized Individual Image Generation](https://arxiv.org/abs/2508.06325)
*Zelin Li,Ruohan Zong,Yifan Liu,Ruichen Yao,Yaokun Liu,Yang Zhang,Dong Wang*

Main category: cs.CR

TL;DR: ATP introduces a dual-perturbation approach in the frequency domain to protect portraits from forgery attacks by combining protective and authorization mechanisms that counter both generation and purification-based tampering.


<details>
  <summary>Details</summary>
Motivation: Personalized image generation technologies pose increasing risks to portrait rights and privacy, with conventional protection methods failing against purification techniques that bypass perturbations.

Method: ATP employs frequency-domain perturbations with mask guidance to separate protection (forgery defense) and authorization (tamper detection) components, enabling comprehensive and non-interfering safeguards.

Result: Extensive experiments demonstrate ATP's robustness against varied forgery attack scenarios and its sensitivity to purification-based tampering detection.

Conclusion: ATP provides an effective, dual-layered solution for enforcing portrait privacy rights by addressing both generation and tamper-purification challenges in image forgery contexts.

Abstract: With the advancement of personalized image generation technologies, concerns
about forgery attacks that infringe on portrait rights and privacy are growing.
To address these concerns, protection perturbation algorithms have been
developed to disrupt forgery generation. However, the protection algorithms
would become ineffective when forgery attackers apply purification techniques
to bypass the protection. To address this issue, we present a novel approach,
Anti-Tamper Perturbation (ATP). ATP introduces a tamper-proof mechanism within
the perturbation. It consists of protection and authorization perturbations,
where the protection perturbation defends against forgery attacks, while the
authorization perturbation detects purification-based tampering. Both
protection and authorization perturbations are applied in the frequency domain
under the guidance of a mask, ensuring that the protection perturbation does
not disrupt the authorization perturbation. This design also enables the
authorization perturbation to be distributed across all image pixels,
preserving its sensitivity to purification-based tampering. ATP demonstrates
its effectiveness in defending forgery attacks across various attack settings
through extensive experiments, providing a robust solution for protecting
individuals' portrait rights and privacy. Our code is available at:
https://github.com/Seeyn/Anti-Tamper-Perturbation .

</details>


### [25] [When AIOps Become "AI Oops": Subverting LLM-driven IT Operations via Telemetry Manipulation](https://arxiv.org/abs/2508.06394)
*Dario Pasquini,Evgenios M. Kornaropoulos,Giuseppe Ateniese,Omer Akgul,Athanasios Theocharis,Petros Efstathopoulos*

Main category: cs.CR

TL;DR: This paper introduces the first security analysis of AIOps solutions, demonstrating that adversaries can exploit system telemetry to mislead autonomous AI agents into compromising infrastructure integrity, and proposes AIOpsShield as an effective defense mechanism to block such attacks while maintaining normal operations.


<details>
  <summary>Details</summary>
Motivation: Modern AIOps solutions automate IT operations using LLM-based agents, promising faster responses and cost savings. However, their security implications remain underexplored, creating a critical need to address vulnerabilities that could be exploited to compromise system infrastructure through automated AI attacks.

Method: The authors developed AIOpsDoom, a fully automated attack methodology combining reconnaissance, fuzzing, and LLM-driven adversarial input generation. It manipulates system telemetry via error-inducing requests and incorrect system error interpretations to hack agents' reward mechanisms, leading to unintended operational actions with no prior knowledge of the target system.

Result: Experiments validated that AIOpsDoom successfully compromises infrastructure integrity by manipulating telemetry data. AIOpsShield, the proposed defense, reliably sanitizes telemetry data by leveraging its structured nature and minimal user-generated content, blocking attacks while preserving agent performance.

Conclusion: AIOps, despite its automation benefits, introduces an emerging attack vector for system compromise. The study emphasizes the urgent need for security-aware design in AIOps frameworks, as current approaches lack sufficient protections against telemetry-based adversarial manipulation of autonomous agents.

Abstract: AI for IT Operations (AIOps) is transforming how organizations manage complex
software systems by automating anomaly detection, incident diagnosis, and
remediation. Modern AIOps solutions increasingly rely on autonomous LLM-based
agents to interpret telemetry data and take corrective actions with minimal
human intervention, promising faster response times and operational cost
savings.
  In this work, we perform the first security analysis of AIOps solutions,
showing that, once again, AI-driven automation comes with a profound security
cost. We demonstrate that adversaries can manipulate system telemetry to
mislead AIOps agents into taking actions that compromise the integrity of the
infrastructure they manage. We introduce techniques to reliably inject
telemetry data using error-inducing requests that influence agent behavior
through a form of adversarial reward-hacking; plausible but incorrect system
error interpretations that steer the agent's decision-making. Our attack
methodology, AIOpsDoom, is fully automated--combining reconnaissance, fuzzing,
and LLM-driven adversarial input generation--and operates without any prior
knowledge of the target system.
  To counter this threat, we propose AIOpsShield, a defense mechanism that
sanitizes telemetry data by exploiting its structured nature and the minimal
role of user-generated content. Our experiments show that AIOpsShield reliably
blocks telemetry-based attacks without affecting normal agent performance.
  Ultimately, this work exposes AIOps as an emerging attack vector for system
compromise and underscores the urgent need for security-aware AIOps design.

</details>


### [26] [ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls](https://arxiv.org/abs/2508.06457)
*Sanket Badhe*

Main category: cs.CR

TL;DR: ScamAgent is an autonomous, multi-turn LLM-based agent that generates realistic scam scripts and demonstrates how current safety guardrails can be bypassed in conversational settings. The study emphasizes the need for advanced safety mechanisms to combat AI-powered scamming.


<details>
  <summary>Details</summary>
Motivation: LLMs' growing misuse potential motivates research into adversarial capabilities of autonomous agents. Existing safety measures focusing on single-shot prompts are inadequate for complex, multi-turn conversational attacks.

Method: ScamAgent uses dialogue memory, dynamic adaptation to user responses, and incremental prompt delivery to simulate realistic scams. Researchers tested diverse safety guardrails (refusal mechanisms, content filters) and combined scam scripts with modern text-to-speech systems to create automated scam pipelines.

Result: Current LLM guardrails are shown to be ineffective against agent-based attacks. Scammers can bypass filters by decomposing/disguising prompts and using sequential interactions. The system successfully converts generated scripts into lifelike voice calls with high deception effectiveness.

Conclusion: LLMs require multi-turn safety auditing and agent-level control frameworks. Conventional prompt-based filters are insufficient. New solutions are needed to detect and prevent sequential conversational deception exploiting autonomous agents.

Abstract: Large Language Models (LLMs) have demonstrated impressive fluency and
reasoning capabilities, but their potential for misuse has raised growing
concern. In this paper, we present ScamAgent, an autonomous multi-turn agent
built on top of LLMs, capable of generating highly realistic scam call scripts
that simulate real-world fraud scenarios. Unlike prior work focused on
single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts
dynamically to simulated user responses, and employs deceptive persuasion
strategies across conversational turns. We show that current LLM safety
guardrails, including refusal mechanisms and content filters, are ineffective
against such agent-based threats. Even models with strong prompt-level
safeguards can be bypassed when prompts are decomposed, disguised, or delivered
incrementally within an agent framework. We further demonstrate the
transformation of scam scripts into lifelike voice calls using modern
text-to-speech systems, completing a fully automated scam pipeline. Our
findings highlight an urgent need for multi-turn safety auditing, agent-level
control frameworks, and new methods to detect and disrupt conversational
deception powered by generative AI.

</details>


### [27] [Voting-Based Semi-Parallel Proof-of-Work Protocol](https://arxiv.org/abs/2508.06489)
*Mustafa Doger,Sennur Ulukus*

Main category: cs.CR

TL;DR: This paper identifies vulnerabilities in existing parallel Proof-of-Work (PoW) protocols against incentive attacks and proposes a voting-based semi-parallel PoW protocol with improved security and performance, validated via MDP models and simulations.


<details>
  <summary>Details</summary>
Motivation: Existing parallel PoW protocols are less secure against incentive attacks compared to Nakamoto consensus due to lower profitability thresholds and higher relative rewards for attackers, necessitating a safer and more efficient design.

Method: The authors analyze attack structures in current protocols using theoretical models and simulations, then design a voting-based semi-parallel PoW framework leveraging state-of-the-art protocols and Markov Decision Process (MDP) analysis for resilience verification.

Result: Their protocol achieves superior communication efficiency, transaction throughput, conflict resolution, and incentive compatibility while demonstrating resistance to incentive attacks and equitable transaction fee distribution among voters and leaders.

Conclusion: The proposed voting-based semi-parallel PoW protocol offers a robust and practical alternative to both traditional Nakamoto consensus and existing parallel PoW structures, addressing security gaps and operational inefficiencies.

Abstract: Parallel Proof-of-Work (PoW) protocols are suggested to improve the safety
guarantees, transaction throughput and confirmation latencies of Nakamoto
consensus. In this work, we first consider the existing parallel PoW protocols
and develop hard-coded incentive attack structures. Our theoretical results and
simulations show that the existing parallel PoW protocols are more vulnerable
to incentive attacks than the Nakamoto consensus, e.g., attacks have smaller
profitability threshold and they result in higher relative rewards. Next, we
introduce a voting-based semi-parallel PoW protocol that outperforms both
Nakamoto consensus and the existing parallel PoW protocols from most practical
perspectives such as communication overheads, throughput, transaction
conflicts, incentive compatibility of the protocol as well as a fair
distribution of transaction fees among the voters and the leaders. We use
state-of-the-art analysis to evaluate the consistency of the protocol and
consider Markov decision process (MDP) models to substantiate our claims about
the resilience of our protocol against incentive attacks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [28] [Empirical Evaluation of AI-Assisted Software Package Selection: A Knowledge Graph Approach](https://arxiv.org/abs/2508.05693)
*Siamak Farshidi,Amir Saberhabibi,Behbod Eskafi,Niloofar Nikfarjam,Sadegh Eskandari,Slinger Jansen,Michel Chaudron,Bedir Tekinerdogan*

Main category: cs.SE

TL;DR: The paper proposes PySelect, a data-driven MCDM framework for selecting Python packages using GitHub, PyPI, and Stack Overflow data to improve recommendation accuracy and user transparency.


<details>
  <summary>Details</summary>
Motivation: Open-source software package selection is challenging due to limited transparent evidence and risks arising from generative AI's tendency to prioritize popularity over suitability.

Method: The framework uses automated data pipelines to collect software metadata, usage trends, vulnerabilities, and sentiment, structuring this into a decision model interpreted by LLMs to query user intent and match packages.

Result: Evaluation across 798k Python scripts and a user study showed high data precision, better recommendation quality than generative AI baselines, and strong user feedback on usefulness (positive acceptance metrics).

Conclusion: PySelect provides a scalable, reproducible MCDM approach combining empirical data and intent modeling, offering improved reliability over current AI-based tools for informed software selection.

Abstract: Selecting third-party software packages in open-source ecosystems like Python
is challenging due to the large number of alternatives and limited transparent
evidence for comparison. Generative AI tools are increasingly used in
development workflows, but their suggestions often overlook dependency
evaluation, emphasize popularity over suitability, and lack reproducibility.
This creates risks for projects that require transparency, long-term
reliability, maintainability, and informed architectural decisions. This study
formulates software package selection as a Multi-Criteria Decision-Making
(MCDM) problem and proposes a data-driven framework for technology evaluation.
Automated data pipelines continuously collect and integrate software metadata,
usage trends, vulnerability information, and developer sentiment from GitHub,
PyPI, and Stack Overflow. These data are structured into a decision model
representing relationships among packages, domain features, and quality
attributes. The framework is implemented in PySelect, a decision support system
that uses large language models to interpret user intent and query the model to
identify contextually appropriate packages. The approach is evaluated using
798,669 Python scripts from 16,887 GitHub repositories and a user study based
on the Technology Acceptance Model. Results show high data extraction
precision, improved recommendation quality over generative AI baselines, and
positive user evaluations of usefulness and ease of use. This work introduces a
scalable, interpretable, and reproducible framework that supports
evidence-based software selection using MCDM principles, empirical data, and
AI-assisted intent modeling.

</details>


### [29] [Klear-CodeTest: Scalable Test Case Generation for Code Reinforcement Learning](https://arxiv.org/abs/2508.05710)
*Jia Fu,Xinyu Yang,Hongzhi Zhang,Yahui Liu,Jingyuan Zhang,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.SE

TL;DR: This paper introduces Klear-CodeTest, a framework for generating high-quality, verified test cases for code reinforcement learning. It uses a Generator-Validation (G-V) framework with consistency checks and a secure sandbox system to enhance test coverage and model training performance.


<details>
  <summary>Details</summary>
Motivation: High-quality test cases are essential for training large language models (LLMs) in code reinforcement learning (RL), but synthesizing them remains a significant challenge due to the need for correctness, coverage, and safety during execution.

Method: The authors developed a Generator-Validation (G-V) framework that generates both regular and corner cases, validated via consistency checks against gold solutions. They also implemented a multi-layered security sandbox to ensure safe code execution during online verification.

Result: Experiments demonstrated significant improvements in model performance and training stability using the Klear-CodeTest dataset. The framework successfully enhanced test coverage and the discriminative power of solution correctness assessments.

Conclusion: Klear-CodeTest addresses critical challenges in test case synthesis for code RL by combining rigorous validation and secure execution. The open-source release of the framework, dataset, and sandbox system enables broader adoption and future research in this domain.

Abstract: Precise, correct feedback is crucial for effectively training large language
models (LLMs) in code reinforcement learning. However, synthesizing
high-quality test cases remains a profoundly challenging and unsolved problem.
In this work, we present Klear-CodeTest, a comprehensive test case synthesis
framework featuring rigorous verification to ensure quality and reliability of
test cases. Our approach achieves broad coverage of programming problems via a
novel Generator-Validation (G-V) framework, ensuring correctness through a
consistency validation mechanism that verifies outputs against gold solutions.
The proposed G-V framework generates comprehensive test cases including both
regular and corner cases, enhancing test coverage and discriminative power for
solution correctness assessment in code reinforcement learning. In addition, we
design a multi-layered security sandbox system optimized for online
verification platforms, guaranteeing safe and reliable code execution. Through
comprehensive experiments, we demonstrate the effectiveness of our curated
dataset, showing significant improvements in model performance and training
stability. The source codes, curated dataset and sandbox system are available
at: https://github.com/Kwai-Klear/CodeTest.

</details>


### [30] [Utilizing Composer Packages to Accelerate Laravel-Based Project Development Among Students: A Pedagogical and Practical Framework](https://arxiv.org/abs/2508.05747)
*Rohaizah Abdul Wahid,Muhamad Said Nizamuddin Nadim,Suliana Sulaiman,Syahmi Akmal Shaharudin,Muhammad Danial Jupikil,Iqqwan Jasman Su Azlan Su*

Main category: cs.SE

TL;DR: The paper advocates for integrating Composer packages into Laravel university curricula to accelerate project development and enhance professional software practices, balancing practical efficiency with pedagogical guidance.


<details>
  <summary>Details</summary>
Motivation: Students face limited academic timelines and struggle to complete Laravel projects while adopting professional practices. Traditional scaffolding alone may not address these challenges effectively.

Method: Categorizing curated Composer packages through a combination of practical (efficiency) and pedagogical (teaching value) evaluation, illustrating real-world application strategies, and addressing risks via best practices.

Result: Identification of packages that reduce development effort by 30-50%, frameworks for teaching Composer integration, and actionable recommendations for mitigating tool dependency risks in academic web development projects.

Conclusion: Composer packages can significantly enhance Laravel curriculum relevance and industry readiness when paired with structured instruction that emphasizes conceptual understanding, ensuring tools support rather than replace deep learning objectives.

Abstract: Laravel has emerged as a foundational framework in university web development
curricula. However, despite its scaffolding capabilities, students often
struggle to complete projects within limited academic timelines. This
conceptual paper introduces Composer, PHP's standard dependency manager, and
categorizes a curated selection of Composer packages that significantly reduce
development effort while fostering professional software practices. Grounded in
practical and pedagogical considerations, the paper illustrates how educators
and learners can strategically leverage these tools to build typical academic
or personal Laravel-based systems. Central to this approach is maintaining code
quality and reinforcing conceptual understanding. The paper also addresses
potential risks such as package conflicts and over-reliance on tools, providing
best-practice recommendations to mitigate them. While the goal is to accelerate
development, the deeper objective is to reinforce professional workflows and
industry readiness. Exposure to Composer packages enhances curriculum relevance
and smooths the transition from academia to the workplace. However, effective
integration requires deliberate instructional design aligned with learning
objectives. Without guidance, students may treat packages as black boxes. Thus,
educators must teach not only how to use these tools, but also when and why,
encouraging critical evaluation of their utility and limitations. This ensures
that practical convenience supports rather than supplants deep learning.

</details>


### [31] [AI-Guided Exploration of Large-Scale Codebases](https://arxiv.org/abs/2508.05799)
*Yoseph Berhanu Alebachew*

Main category: cs.SE

TL;DR: Hybrid approach integrates reverse engineering with LLM-driven interactive code exploration for enhanced system understanding.


<details>
  <summary>Details</summary>
Motivation: Traditional tools lack interactivity and contextual adaptability, while standalone LLMs face grounding and structural integration limitations in code comprehension tasks.

Method: The system combines UML-based visualization, dynamic interfaces, and historical context with an LLM's ability to interpret user queries and interaction patterns, creating intent-aware navigational support.

Result: A Java prototype validated the feasibility of hybrid integration, though empirical evaluations and scaling improvements remain future work.

Conclusion: This research establishes a foundation for cognitive-aligned, collaborative code understanding environments through structured-LLM hybrid systems.

Abstract: Understanding large-scale, complex software systems is a major challenge for
developers, who spend a significant portion of their time on program
comprehension. Traditional tools such as static visualizations and reverse
engineering techniques provide structural insights but often lack
interactivity, adaptability, and integration with contextual information.
Recent advancements in large language models (LLMs) offer new opportunities to
enhance code exploration workflows, yet their lack of grounding and integration
with structured views limits their effectiveness. This work introduces a hybrid
approach that integrates deterministic reverse engineering with LLM-guided,
intent-aware visual exploration. The proposed system combines UML-based
visualization, dynamic user interfaces, historical context, and collaborative
features into an adaptive tool for code comprehension. By interpreting user
queries and interaction patterns, the LLM helps developers navigate and
understand complex codebases more effectively. A prototype implementation for
Java demonstrates the feasibility of this approach. Future work includes
empirical evaluation, scaling to polyglot systems, and exploring GUI-driven LLM
interaction models. This research lays the groundwork for intelligent,
interactive environments that align with developer cognition and collaborative
workflows.

</details>


### [32] [Enhancing Software Vulnerability Detection Through Adaptive Test Input Generation Using Genetic Algorithm](https://arxiv.org/abs/2508.05923)
*Yanusha Mehendran,Maolin Tang,Yi Lu*

Main category: cs.SE

TL;DR: A genetic algorithm-based approach for test input generation combines crossover operators and adaptive learning to enhance software vulnerability detection, achieving significant coverage improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional vulnerability detection methods struggle with increasing software complexity, necessitating more effective and adaptive techniques for thorough security testing.

Method: The method employs a genetic algorithm with crossover operators to broaden test input exploration and an adaptive feedback mechanism to guide input generation based on system execution behavior, evolving valid test cases through feedback-driven selection.

Result: The approach outperformed benchmark evolutionary fuzzing in nine JSON-processing libraries, achieving 39.8% higher class coverage, 62.4% method coverage, 105.0% line coverage, 114.0% instruction coverage, and 166.0% branch coverage.

Conclusion: The integration of genetic operators and adaptive learning enables efficient exploration and exploitation of test inputs, demonstrating a scalable solution for detecting complex software vulnerabilities.

Abstract: Software vulnerabilities continue to undermine the reliability and security
of modern systems, particularly as software complexity outpaces the
capabilities of traditional detection methods. This study introduces a genetic
algorithm-based method for test input generation that innovatively integrates
genetic operators and adaptive learning to enhance software vulnerability
detection. A key contribution is the application of the crossover operator,
which facilitates exploration by searching across a broader space of potential
test inputs. Complementing this, an adaptive feedback mechanism continuously
learns from the system's execution behavior and dynamically guides input
generation toward promising areas of the input space. Rather than relying on
fixed or randomly selected inputs, the approach evolves a population of
structurally valid test cases using feedback-driven selection, enabling deeper
and more effective code traversal. This strategic integration of exploration
and exploitation ensures that both diverse and targeted test inputs are
developed over time. Evaluation was conducted across nine open-source
JSON-processing libraries. The proposed method achieved substantial
improvements in coverage compared to a benchmark evolutionary fuzzing method,
with average gains of 39.8% in class coverage, 62.4% in method coverage, 105.0%
in line coverage, 114.0% in instruction coverage, and 166.0% in branch
coverage. These results highlight the method's capacity to detect deeper and
more complex vulnerabilities, offering a scalable and adaptive solution to
software security testing.

</details>


### [33] [A Survey on Task Scheduling in Carbon-Aware Container Orchestration](https://arxiv.org/abs/2508.05949)
*Jialin Yang,Zainab Saad,Jiajun Wu,Xiaoguang Niu,Henry Leung,Steve Drew*

Main category: cs.SE

TL;DR: This paper systematically reviews Kubernetes scheduling strategies for sustainability in cloud computing, providing a taxonomy and insights for future solutions.


<details>
  <summary>Details</summary>
Motivation: Addressing the environmental impact of energy-intensive cloud computing driven by large language model training and deployment to reduce carbon emissions.

Method: Categorizes scheduling strategies into hardware and software centric, annotates with sustainability goals, and groups them by algorithms using a systematic review.

Result: Elaborates a comprehensive taxonomy for cloud task scheduling focused on environmental sustainability, identifies research trends, and outlines open challenges.

Conclusion: Highlights the importance of integrating environmental objectives into scheduling strategies and offers critical design insights for next-generation sustainable cloud systems.

Abstract: The soaring energy demands of large-scale software ecosystems and cloud data
centers, accelerated by the intensive training and deployment of large language
models, have driven energy consumption and carbon footprint to unprecedented
levels. In response, both industry and academia are increasing efforts to
reduce the carbon emissions associated with cloud computing through more
efficient task scheduling and infrastructure orchestration. In this work, we
present a systematic review of various Kubernetes scheduling strategies,
categorizing them into hardware-centric and software-centric, annotating each
with its sustainability objectives, and grouping them according to the
algorithms they use. We propose a comprehensive taxonomy for cloud task
scheduling studies, with a particular focus on the environmental sustainability
aspect. We analyze emerging research trends and open challenges, and our
findings provide critical insight into the design of sustainable scheduling
solutions for next-generation cloud computing systems.

</details>


### [34] [Impact-driven Context Filtering For Cross-file Code Completion](https://arxiv.org/abs/2508.05970)
*Yanzhou Li,Shangqing Liu,Kangjie Chen,Tianwei Zhang,Yang Liu*

Main category: cs.SE

TL;DR: This paper investigates retrieval-augmented generation (RAG) in repository-level code completion and proposes CODEFILTER, a framework that uses a likelihood-based metric to filter detrimental cross-file code chunks, improving accuracy and efficiency while reducing input prompt length.


<details>
  <summary>Details</summary>
Motivation: Existing repository-level code completion models struggle with noise from irrelevant or harmful retrieved cross-file contexts, limiting their performance. Only a small fraction of retrieved code chunks effectively contribute to completion tasks.

Method: 1) Developed a likelihood-based metric to quantify retrieved code chunks' impact on completion. 2) Created a labeled dataset where chunks are categorized as positive/neutral/negative based on this metric. 3) Designed CODEFILTER, an adaptive filtering framework trained on the dataset to remove harmful contexts.

Result: CODEFILTER improved completion accuracy by ~14-18% on benchmarks (RepoEval and CrossCodeLongEval). It reduced input sequences by ~40% while maintaining performance, demonstrated strong model generalizability, and improved computational efficiency (lower perplexity by 22-28%).

Conclusion: Adaptive filtering of retrieved contexts using task-dependent criteria is crucial for high-quality code completion. CODEFILTER achieves better accuracy and efficiency by eliminating harmful cross-file context noise while preserving useful information.

Abstract: Retrieval-augmented generation (RAG) has recently demonstrated considerable
potential for repository-level code completion, as it integrates cross-file
knowledge with in-file preceding code to provide comprehensive contexts for
generation. To better understand the contribution of the retrieved cross-file
contexts, we introduce a likelihood-based metric to evaluate the impact of each
retrieved code chunk on the completion. Our analysis reveals that, despite
retrieving numerous chunks, only a small subset positively contributes to the
completion, while some chunks even degrade performance. To address this issue,
we leverage this metric to construct a repository-level dataset where each
retrieved chunk is labeled as positive, neutral, or negative based on its
relevance to the target completion. We then propose an adaptive retrieval
context filtering framework, CODEFILTER, trained on this dataset to mitigate
the harmful effects of negative retrieved contexts in code completion.
Extensive evaluation on the RepoEval and CrossCodeLongEval benchmarks
demonstrates that CODEFILTER consistently improves completion accuracy compared
to approaches without filtering operations across various tasks. Additionally,
CODEFILTER significantly reduces the length of the input prompt, enhancing
computational efficiency while exhibiting strong generalizability across
different models. These results underscore the potential of CODEFILTER to
enhance the accuracy, efficiency, and attributability of repository-level code
completion.

</details>


### [35] [Position: Intelligent Coding Systems Should Write Programs with Justifications](https://arxiv.org/abs/2508.06017)
*Xiangzhe Xu,Shiwei Feng,Zian Su,Chengpeng Wang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: The paper proposes neuro-symbolic approaches for generating justifications in AI-driven code generation to enhance transparency and user trust, especially for non-experts.


<details>
  <summary>Details</summary>
Motivation: AI coding systems' opaque decision-making creates trust and usability issues for non-expert users; existing methods (formal verification, static analysis, post-hoc explainability) lack mechanisms to bridge model reasoning with user understanding.

Method: Identifies cognitive alignment and semantic faithfulness as key justification properties. Proposes neuro-symbolic frameworks where symbolic constraints guide training, and neural representations enrich program semantics for automated consistency checks during inference.

Result: Highlights limitations of traditional methods and establishes a conceptual framework for justification generation through neuro-symbolic integration, suggesting it improves transparency compared to existing solutions.

Conclusion: Neuro-symbolic approaches can address the transparency gap in intelligent coding systems by combining symbolic reasoning with neural learning for justifications that ensure semantic consistency and cognitive alignment.

Abstract: Intelligent coding systems are transforming software development by enabling
users to specify code behavior in natural language. However, the opaque
decision-making of AI-driven coders raises trust and usability concerns,
particularly for non-expert users who cannot inspect low-level implementations.
We argue that these systems should not only generate code but also produce
clear, consistent justifications that bridge model reasoning and user
understanding. To this end, we identify two critical justification
properties-cognitive alignment and semantic faithfulness-and highlight the
limitations of existing methods, including formal verification, static
analysis, and post-hoc explainability. We advocate exploring neuro-symbolic
approaches for justification generation, where symbolic constraints guide model
behavior during training and program semantics are enriched through neural
representations, enabling automated consistency checks at inference time.

</details>


### [36] [Understanding Inconsistent State Update Vulnerabilities in Smart Contracts](https://arxiv.org/abs/2508.06192)
*Lantian Li,Yuyu Chen,Jingwen Wu,Yue Pan,Zhongxing Yu*

Main category: cs.SE

TL;DR: This paper presents the first large-scale empirical study on inconsistent state update vulnerabilities in smart contracts, analyzing 116 vulnerabilities across 352 projects to derive 11 key findings and a proof-of-concept checker that found confirmed issues in 19/64 GitHub projects, offering actionable insights for developers and tool designers.


<details>
  <summary>Details</summary>
Motivation: Inconsistent state updates in smart contracts, caused by unsynchronized modifications, are critical vulnerabilities frequently exploited by attackers. Current detection tools lack effectiveness, necessitating a deeper understanding of root causes, exploitation patterns, and mitigation strategies to improve smart contract security.

Method: The authors conducted a systematic analysis of 116 inconsistent state update vulnerabilities across 352 real-world projects, categorizing root causes, fix approaches, and exploitation methods. They developed a proof-of-concept checker based on one finding to validate the practical relevance of their results in 64 popular GitHub repositories.

Result: The study uncovers 11 original findings about patterns and fixes for inconsistent state updates. The checker detected 64 issues, with 19 confirmed by project maintainers, demonstrating the vulnerability's prevalence and the value of the proposed analysis.

Conclusion: This work establishes a comprehensive framework for understanding inconsistent state update vulnerabilities in smart contracts, provides actionable guidance for developers, and validates the practical utility through a real-world checker that identifies confirmed issues, highlighting the importance of addressing this critical security flaw.

Abstract: Smart contracts enable contract terms to be automatically executed and
verified on the blockchain, and recent years have witnessed numerous
applications of them in areas such as financial institutions and supply chains.
The execution logic of a smart contract is closely related to the contract
state, and thus the correct and safe execution of the contract depends heavily
on the precise control and update of the contract state. However, the contract
state update process can have issues. In particular, inconsistent state update
issues can arise for reasons such as unsynchronized modifications. Inconsistent
state update bugs have been exploited by attackers many times, but existing
detection tools still have difficulty in effectively identifying them. This
paper conducts the first large-scale empirical study about inconsistent state
update vulnerabilities (that is, inconsistent state update bugs that are
exploitable) in smart contracts, aiming to shed light for developers,
researchers, tool builders, and language or library designers in order to avoid
inconsistent state update vulnerabilities. We systematically investigate 116
inconsistent state update vulnerabilities in 352 real-world smart contract
projects, summarizing their root causes, fix strategies, and exploitation
methods. Our study provides 11 original and important findings, and we also
give the implications of our findings. To illustrate the potential benefits of
our research, we also develop a proof-of-concept checker based on one of our
findings. The checker effectively detects issues in 64 popular GitHub projects,
and 19 project owners have confirmed the detected issues at the time of
writing. The result demonstrates the usefulness and importance of our findings
for avoiding inconsistent state update vulnerabilities in smart contracts.

</details>


### [37] [Improving the Developer Experience with a Low-Code Process Modelling Language](https://arxiv.org/abs/2508.06299)
*Henrique Henriques,Hugo Lourenço,Vasco Amaral,Miguel Goulão*

Main category: cs.SE

TL;DR: The paper describes the improvement of the Business Process Technology (BPT) DSL in the OutSystems Platform by addressing usability issues through user studies and empirical evaluations, resulting in significant gains in semantic transparency, correctness, and usability metrics.


<details>
  <summary>Details</summary>
Motivation: The BPT DSL had low adoption and usability problems despite high maintenance costs, creating a need for iterative improvements based on user feedback to enhance developer experience and reduce overhead.

Method: Combined interviews, Physics of Notation analysis, and empirical evaluations (SUS and NASA TLX) with 25 professional software engineers to iteratively design and assess the new BPT version.

Result: New BPT version achieved 31% → 69% increase in semantic transparency, 51% → 89% in correctness, SUS score 42.25 → 64.78, and TLX score 36.50 → 20.78 (significant differences confirmed statistically).

Conclusion: The redesigned BPT significantly improved usability and developer experience, confirming the importance of aligning concrete syntax with users’ background and prioritizing evidence-based design decisions.

Abstract: Context: The OutSystems Platform is a development environment composed of
several DSLs, used to specify, quickly build, and validate web and mobile
applications. The DSLs allow users to model different perspectives such as
interfaces and data models, define custom business logic and construct process
models. Problem: The DSL for process modelling (Business Process Technology
(BPT)), has a low adoption rate and is perceived as having usability problems
hampering its adoption. This is problematic given the language maintenance
costs. Method: We used a combination of interviews, a critical review of BPT
using the "Physics of Notation" and empirical evaluations of BPT using the
System Usability Scale (SUS) and the NASA Task Load indeX (TLX), to develop a
new version of BPT, taking these inputs and Outsystems' engineers' culture into
account. Results: Evaluations conducted with 25 professional software engineers
showed an increase of the semantic transparency on the new version, from 31% to
69%, an increase in the correctness of responses, from 51% to 89%, an increase
in the SUS score, from 42.25 to 64.78, and a decrease of the TLX score, from
36.50 to 20.78. These differences were statistically significant. Conclusions:
These results suggest that the new version of BPT significantly improved the
developer experience of the previous version. The end users' background with
OutSystems had a relevant impact on the final concrete syntax choices and
achieved usability indicators.

</details>


### [38] [Execution-Feedback Driven Test Generation from SWE Issues](https://arxiv.org/abs/2508.06365)
*Toufique Ahmed,Jatin Ganhotra,Avraham Shinnar,Martin Hirzel*

Main category: cs.SE

TL;DR: This paper introduces innovative techniques to automatically generate reproduction tests for software engineering issues where code is missing or incorrect, achieving a 63% fail-to-pass rate using the e-Otter++ tool.


<details>
  <summary>Details</summary>
Motivation: Most software engineering issues lack functional reproduction tests, hindering debugging and resolution. Automated test generation is critical but challenging when execution feedback is unavailable due to broken/missing code.

Method: The authors developed e-Otter++, a tool that leverages novel execution feedback approaches to generate tests for issues with missing/wrong code by analyzing the problem description and reconstructing invalid program states.

Result: Evaluation on the TDD-Bench Verified benchmark showed a 63% average fail-to-pass rate, representing significant advancement over prior state-of-the-art methods for this specific problem.

Conclusion: The paper demonstrates that execution feedback can be effectively utilized for reproduction test generation even when the code under test is initially invalid, with e-Otter++ establishing a new technical benchmark in this domain.

Abstract: A software engineering issue (SWE issue) is easier to resolve when
accompanied by a reproduction test. Unfortunately, most issues do not come with
functioning reproduction tests, so this paper explores how to generate them
automatically. The primary challenge in this setting is that the code to be
tested is either missing or wrong, as evidenced by the existence of the issue
in the first place. This has held back test generation for this setting:
without the correct code to execute, it is difficult to leverage execution
feedback to generate good tests. This paper introduces novel techniques for
leveraging execution feedback to get around this problem, implemented in a new
reproduction test generator called e-Otter++. Experiments show that e-Otter++
represents a leap ahead in the state-of-the-art for this problem, generating
tests with an average fail-to-pass rate of 63% on the TDD-Bench Verified
benchmark.

</details>


### [39] [What Builds Effective In-Context Examples for Code Generation?](https://arxiv.org/abs/2508.06414)
*Dongze Li,Songqiang Chen,Jialun Cao,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: This paper investigates how specific code features (e.g., identifier naming, formatting) impact the effectiveness of In-Context Learning (ICL) in code generation by conducting controlled ablation studies. Key findings include the critical role of semantic naming and challenges in extracting generalizable problem-solving insights from ICL examples.


<details>
  <summary>Details</summary>
Motivation: Despite the success of code example-based ICL in enhancing LLM code generation, the specific features driving this effectiveness and limitations in reflection-based learning remain unclear.

Method: Controlled ablation studies that systematically modify code features like identifier naming styles, formatting conventions, and solution insights in ICL prompts to evaluate their impact on model performance.

Result: 1) Removing semantic identifier names reduces performance by up to 30pp. 2) LLMs prioritize meaningful names over formatting conventions. 3) Language-specific preferences for identifier verbosity exist. 4) Current LLMs cannot extract generalizable insights from similar code solutions, despite using direct information effectively.

Conclusion: The findings provide actionable guidance for optimizing ICL systems in code generation while revealing fundamental challenges in reflection-based learning for these tasks.

Abstract: In-Context Learning (ICL) has emerged as a promising solution to enhance the
code generation capabilities of Large Language Models (LLMs), which
incorporates code examples inside the prompt to let LLMs learn from
demonstrations. However, despite the substantial effectiveness of the code
example-based ICL approach, the specific features (e.g., identifier naming
styles, code formatting, solution insight) within the ICL-provided code
examples that significantly contribute to the ICL's effectiveness remain
unclear. This paper systematically investigates the impact of various code
features on ICL with code examples through controlled ablation studies. Our
findings reveal that the appropriate naming of variables and functions is
crucial for effective code generation, with their elimination leading to
performance decreases of up to 30 percentage points. We further demonstrate
that LLMs prioritize semantically meaningful identifier names over formatting
conventions, with language-specific preferences regarding identifier verbosity.
Additionally, our investigation into ICL's potential for enhancing reflection
and inference capabilities reveals that current LLMs struggle to extract
generalizable problem-solving insights from similar code solutions, despite
being capable of utilizing direct information effectively. These findings are
expected to provide valuable insights for optimizing ICL systems in code
generation applications and highlight fundamental challenges in
reflection-based learning for code generation tasks.

</details>
