{"id": "2510.07452", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07452", "abs": "https://arxiv.org/abs/2510.07452", "authors": ["Anthony Hughes", "Vasisht Duddu", "N. Asokan", "Nikolaos Aletras", "Ning Ma"], "title": "PATCH: Mitigating PII Leakage in Language Models with Privacy-Aware Targeted Circuit PatcHing", "comment": null, "summary": "Language models (LMs) may memorize personally identifiable information (PII)\nfrom training data, enabling adversaries to extract it during inference.\nExisting defense mechanisms such as differential privacy (DP) reduce this\nleakage, but incur large drops in utility. Based on a comprehensive study using\ncircuit discovery to identify the computational circuits responsible PII\nleakage in LMs, we hypothesize that specific PII leakage circuits in LMs should\nbe responsible for this behavior. Therefore, we propose PATCH (Privacy-Aware\nTargeted Circuit PatcHing), a novel approach that first identifies and\nsubsequently directly edits PII circuits to reduce leakage. PATCH achieves\nbetter privacy-utility trade-off than existing defenses, e.g., reducing recall\nof PII leakage from LMs by up to 65%. Finally, PATCH can be combined with DP to\nreduce recall of residual leakage of an LM to as low as 0.01%. Our analysis\nshows that PII leakage circuits persist even after the application of existing\ndefense mechanisms. In contrast, PATCH can effectively mitigate their impact.", "AI": {"tldr": "The paper introduces PATCH, a method that identifies and edits PII leakage circuits in LMs to effectively reduce PII leakage with minimal utility loss, achieving significant privacy improvements over existing defenses like DP.", "motivation": "LMs may memorize and leak PII from their training data, even when using DP. This poses a privacy risk. A more targeted solution is needed to address this issue without large utility drops.", "method": "The authors first use circuit discovery to identify the PII leakage circuits in LMs. Once these circuits are identified, they are  directly edited using PATCH to reduce PII leakage, and PATCH can also be combined with DP for further privacy enhancement.", "result": "PATCH achieved a 65% reduction in PII leakage recall, and when combined with DP, reduced residual leakage to as low as 0.01%. This indicates a better performance in privacy-utility trade-off compared to existing defenses.", "conclusion": "PATCH presents a more effective and efficient solution to mitigate PII leakage from LMs by directly targeting the responsible computational circuits, leading to significant privacy improvements with less utility loss."}}
{"id": "2510.07457", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07457", "abs": "https://arxiv.org/abs/2510.07457", "authors": ["Kalyan Cheerla", "Lotfi Ben Othmane", "Kirill Morozov"], "title": "Comparison of Fully Homomorphic Encryption and Garbled Circuit Techniques in Privacy-Preserving Machine Learning Inference", "comment": "8 pages, 9 figures, 2 tables, 32 references", "summary": "Machine Learning (ML) is making its way into fields such as healthcare,\nfinance, and Natural Language Processing (NLP), and concerns over data privacy\nand model confidentiality continue to grow. Privacy-preserving Machine Learning\n(PPML) addresses this challenge by enabling inference on private data without\nrevealing sensitive inputs or proprietary models. Leveraging Secure Computation\ntechniques from Cryptography, two widely studied approaches in this domain are\nFully Homomorphic Encryption (FHE) and Garbled Circuits (GC). This work\npresents a comparative evaluation of FHE and GC for secure neural network\ninference. A two-layer neural network (NN) was implemented using the CKKS\nscheme from the Microsoft SEAL library (FHE) and the TinyGarble2.0 framework\n(GC) by IntelLabs. Both implementations are evaluated under the semi-honest\nthreat model, measuring inference output error, round-trip time, peak memory\nusage, communication overhead, and communication rounds. Results reveal a\ntrade-off: modular GC offers faster execution and lower memory consumption,\nwhile FHE supports non-interactive inference.", "AI": {"tldr": "This paper compares FHE and GC for secure neural network inference. GC offers faster, lower-memory execution, while FHE enables non-interactive but slower inference, highlighting a key privacy-performance trade-off in PPML.", "motivation": "Growing privacy concerns in ML applications (e.g., healthcare, finance, NLP) necessitate solutions to protect sensitive data during inference. Privacy-preserving ML, particularly via FHE and GC, is critical but poorly understood in terms of performance trade-offs.", "method": "The study implemented a two-layer neural network using CKKS (FHE) via Microsoft SEAL and TinyGarble2.0 (GC) under the semi-honest threat model. Evaluations focused on error, latency, memory, communication overhead, and rounds.", "result": "FHE exhibits non-interactive inference but higher memory and slower execution. GC achieves faster runtime and lower memory consumption at the cost of interactivity. Trade-offs exist between privacy guarantees and computational efficiency.", "conclusion": "The paper concludes that FHE and GC offer distinct advantages for secure neural network inference. GC provides faster execution and lower memory usage, making it suitable for interactive applications, while FHE enables non-interactive inference, prioritizing privacy over performance."}}
{"id": "2510.07462", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.07462", "abs": "https://arxiv.org/abs/2510.07462", "authors": ["Maryam Ataei Nezhad", "Hamid Barati", "Ali Barati"], "title": "A Secure Authentication-Driven Protected Data Collection Protocol in Internet of Things", "comment": null, "summary": "Internet of Things means connecting different devices through the Internet.\nThe Internet of things enables humans to remotely manage and control the\nobjects they use with the Internet infrastructure. After the advent of the\nInternet of Things in homes, organizations, and private companies, privacy and\ninformation security are the biggest concern. This issue has challenged the\nspread of the Internet of things as news of the users theft of information by\nhackers intensified. The proposed method in this paper consists of three\nphases. In the first phase, a star structure is constructed within each\ncluster, and a unique key is shared between each child and parent to encrypt\nand secure subsequent communications. The second phase is for intracluster\ncommunications, in which members of the cluster send their data to the cluster\nhead in a multi hop manner. Also, in this phase, the data is encrypted with\ndifferent keys in each hop, and at the end of each connection, the keys are\nupdated to ensure data security. The third phase is to improve the security of\ninter cluster communications using an authentication protocol. In this way, the\ncluster heads are authenticated before sending information to prevent malicious\nnodes in the network. The proposed method is also simulated using NS2 software.\nThe results showed that the proposed method has improved in terms of energy\nconsumption, end-to-end delay, flexibility, packet delivery rate, and the\nnumber of alive nodes compared to other methods.", "AI": {"tldr": "This paper proposes a three-phase IoT security framework with layered encryption and authentication protocols. NS2 simulations demonstrate enhanced performance metrics and robustness against security threats in IoT networks.", "motivation": "Privacy and information security concerns\u963b\u788d IoT adoption due to hacking risks. Existing solutions lack effective multi-layer security mechanisms for IoT networks.", "method": "Three-phase approach: 1) Star structure with unique shared keys between parent-child nodes for encryption. 2) Intracluster multi-hop communication using hop-specific encryption keys. 3) Intercluster authentication protocol for cluster heads. Evaluated via NS2 simulations.", "result": "Simulation results show improvements in energy consumption, end-to-end delay, flexibility, packet delivery rate, and node survival rate compared to existing methods. NS2 validation confirms security effectiveness.", "conclusion": "The proposed method enhances IoT security through three phases: star structure with unique keys, intracluster multi-hop encryption with key updates, and intercluster authentication. It outperforms others in energy, delay, and packet delivery."}}
{"id": "2510.07479", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.07479", "abs": "https://arxiv.org/abs/2510.07479", "authors": ["Alain Couvreur", "Thomas Debris-Alazard", "Philippe Gaborit", "Adrien Vin\u00e7otte"], "title": "MIRANDA: short signatures from a leakage-free full-domain-hash scheme", "comment": null, "summary": "We present $\\mathsf{Miranda}$, the first family of full-domain-hash\nsignatures based on matrix codes. This signature scheme fulfils the paradigm of\nGentry, Peikert and Vaikuntanathan ($\\mathsf{GPV}$), which gives strong\nsecurity guarantees. Our trapdoor is very simple and generic: if we propose it\nwith matrix codes, it can actually be instantiated in many other ways since it\nonly involves a subcode of a decodable code (or lattice) in a unique decoding\nregime of parameters. Though $\\mathsf{Miranda}$ signing algorithm relies on a\ndecoding task where there is exactly one solution, there are many possible\nsignatures given a message to sign and we ensure that signatures are not\nleaking information on their underlying trapdoor by means of a very simple\nprocedure involving the drawing of a small number of uniform bits. In\nparticular $\\mathsf{Miranda}$ does not use a rejection sampling procedure which\nmakes its implementation a very simple task contrary to other\n$\\mathsf{GPV}$-like signatures schemes such as $\\mathsf{Falcon}$ or even\n$\\mathsf{Wave}$.\n  We instantiate $\\mathsf{Miranda}$ with the famous family of Gabidulin codes\nrepresented as spaces of matrices and we study thoroughly its security (in the\nEUF-CMA security model). For~$128$ bits of classical security, the signature\nsizes are as low as~$90$ bytes and the public key sizes are in the order\nof~$2.6$ megabytes.", "AI": {"tldr": "Miranda is the first full-domain-hash signature scheme based on matrix codes, achieving efficient and secure signatures with minimal size (90 bytes) and avoiding rejection sampling. It leverages a simple trapdoor mechanism using subcode decoding and offers strong EUF-CMA security.", "motivation": "The need for post-quantum secure, efficient signature schemes with practical implementations motivates Miranda. Existing GPV-based schemes like Falcon/Wave require complex rejection sampling, increasing implementation complexity and computational overhead.", "method": "Miranda uses the GPV paradigm with a trapdoor based on subcodes of decodable matrix codes (e.g., Gabidulin codes). Signatures are generated via unique decoding solutions, and a uniform bit-sampling technique prevents trapdoor leakage without rejection sampling. The instance uses Gabidulin code lattices (matrix spaces).", "result": "Achieved 128-bit security with 90-byte signatures and 2.6 MB public keys. Security analysis under EUF-CMA confirms robustness. The design eliminates rejection sampling, simplifying implementation compared to Falcon/Wave.", "conclusion": "Miranda provides a practical, secure post-quantum signature alternative with minimal overhead and simplified implementation, enabling real-world adoption for lightweight and high-security applications."}}
{"id": "2510.07435", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.07435", "abs": "https://arxiv.org/abs/2510.07435", "authors": ["Zixuan Feng", "Sadia Afroz", "Anita Sarma"], "title": "Modeling Developer Burnout with GenAI Adoption", "comment": "10 pages, LLM", "summary": "Generative AI (GenAI) is rapidly reshaping software development workflows.\nWhile prior studies emphasize productivity gains, the adoption of GenAI also\nintroduces new pressures that may harm developers' well-being. In this paper,\nwe investigate the relationship between the adoption of GenAI and developers'\nburnout. We utilized the Job Demands--Resources (JD--R) model as the analytic\nlens in our empirical study. We employed a concurrent embedded mixed-methods\nresearch design, integrating quantitative and qualitative evidence. We first\nsurveyed 442 developers across diverse organizations, roles, and levels of\nexperience. We then employed Partial Least Squares--Structural Equation\nModeling (PLS-SEM) and regression to model the relationships among job demands,\njob resources, and burnout, complemented by a qualitative analysis of\nopen-ended responses to contextualize the quantitative findings. Our results\nshow that GenAI adoption heightens burnout by increasing job demands, while job\nresources and positive perceptions of GenAI mitigate these effects, reframing\nadoption as an opportunity.", "AI": {"tldr": "GenAI boosts developer productivity but risks burnout by raising demands. This study finds that investing in job resources and fostering positive GenAI attitudes can transform this risk into an opportunity for healthier, more effective workflows.", "motivation": "Prior research focused on productivity gains from GenAI, but this paper addresses a critical gap by investigating how GenAI adoption may harm developers' well-being through burnout, exploring contextual factors like job demands and resources.", "method": "The paper employs a concurrent embedded mixed-methods design, combining a survey of 442 developers with statistical analyses (PLS-SEM and regression) and qualitative analysis of open-ended responses, grounded in the Job Demands--Resources (JD-R) model.", "result": "GenAI adoption is shown to increase job demands and burnout risks; however, sufficient job resources and positive developer perceptions of GenAI reduce these effects, revealing a path to mitigate harm.", "conclusion": "The study concludes that while GenAI adoption can heighten developer burnout by increasing job demands, the negative impacts can be mitigated through adequate job resources and positive perceptions of GenAI, reframing adoption as an opportunity for organizations to support balanced workflows."}}
{"id": "2510.07533", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.07533", "abs": "https://arxiv.org/abs/2510.07533", "authors": ["Haowen Xu", "Tianya Zhao", "Xuyu Wang", "Lei Ma", "Jun Dai", "Alexander Wyglinski", "Xiaoyan Sun"], "title": "EMPalm: Exfiltrating Palm Biometric Data via Electromagnetic Side-Channels", "comment": null, "summary": "Palm recognition has emerged as a dominant biometric authentication\ntechnology in critical infrastructure. These systems operate in either\nsingle-modal form, using palmprint or palmvein individually, or dual-modal\nform, fusing the two modalities. Despite this diversity, they share similar\nhardware architectures that inadvertently emit electromagnetic (EM) signals\nduring operation. Our research reveals that these EM emissions leak palm\nbiometric information, motivating us to develop EMPalm--an attack framework\nthat covertly recovers both palmprint and palmvein images from eavesdropped EM\nsignals. Specifically, we first separate the interleaved transmissions of the\ntwo modalities, identify and combine their informative frequency bands, and\nreconstruct the images. To further enhance fidelity, we employ a diffusion\nmodel to restore fine-grained biometric features unique to each domain.\nEvaluations on seven prototype and two commercial palm acquisition devices show\nthat EMPalm can recover palm biometric information with high visual fidelity,\nachieving SSIM scores up to 0.79, PSNR up to 29.88 dB, and FID scores as low as\n6.82 across all tested devices, metrics that collectively demonstrate strong\nstructural similarity, high signal quality, and low perceptual discrepancy. To\nassess the practical implications of the attack, we further evaluate it against\nfour state-of-the-art palm recognition models, achieving a model-wise average\nspoofing success rate of 65.30% over 6,000 samples from 100 distinct users.", "AI": {"tldr": "Researchers developed EMPalm, an EM-leakage attack that covertly reconstructs high-quality palmprint/palmvein images from hardware emissions, achieving 65% spoofing accuracy on commercial systems.", "motivation": "Current palm recognition systems emit EM signals during operation, inadvertently leaking biometric data. The authors aim to expose this under-explored attack vector and quantify risks to justify countermeasure development.", "method": "EMPalm separates EM signals of palmprint and palmvein, identifies their frequency bands, reconstructs images, and employs a diffusion model to enhance fidelity. This involves signal processing, domain-specific feature restoration, and adversarial training for improved accuracy.", "result": "EMPalm achieves 65.30% spoofing success rate across four models and demonstrates high-reconstruction quality (SSIM=0.79, PSNR=29.88 dB, FID=6.82) on 9 tested devices, proving the practicality of the attack.", "conclusion": "The paper demonstrates significant vulnerabilities in palm biometric systems due to electromagnetic emissions, proving that existing hardware is susceptible to EM-based attacks. It underscores the need for hardware-level countermeasures and raises awareness of physical-layer security risks in biometric authentication."}}
{"id": "2510.07529", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.07529", "abs": "https://arxiv.org/abs/2510.07529", "authors": ["Carol Hanna", "Federica Sarro", "Mark Harman", "Justyna Petke"], "title": "HotBugs.jar: A Benchmark of Hot Fixes for Time-Critical Bugs", "comment": null, "summary": "Hot fixes are urgent, unplanned changes deployed to production systems to\naddress time-critical issues. Despite their importance, no existing evaluation\nbenchmark focuses specifically on hot fixes. We present HotBugs$.$jar, the\nfirst dataset dedicated to real-world hot fixes. From an initial mining of 10\nactive Apache projects totaling over 190K commits and 150K issue reports, we\nidentified 746 software patches that met our hot-fix criteria. After manual\nevaluation, 679 were confirmed as genuine hot fixes, of which 110 are\nreproducible using a test suite. Building upon the Bugs$.$jar framework,\nHotBugs$.$jar integrates these 110 reproducible cases and makes available all\n679 manually validated hot fixes, each enriched with comprehensive metadata to\nsupport future research. Each hot fix was systematically identified using Jira\nissue data, validated by independent reviewers, and packaged in a reproducible\nformat with buggy and fixed versions, test suites, and metadata. HotBugs$.$jar\nhas already been adopted as the official challenge dataset for the Search-Based\nSoftware Engineering (SBSE) Conference Challenge Track, demonstrating its\nimmediate impact. This benchmark enables the study and evaluation of tools for\nrapid debugging, automated repair, and production-grade resilience in modern\nsoftware systems to drive research in this essential area forward.", "AI": {"tldr": "Presents HotBugs.jar, the first benchamark of real-world hot fixes from 10 Apache projects containing 679 validated patches (110 reproducible), enabling evaluation of tools for rapid debugging and automated repair.", "motivation": "Hot fixes are critical yet lack dedicated research benchmarks, preventing evaluation of approaches for rapid debugging and automated repair in production systems.", "method": "Collected 10 Apache projects with 190k commits/150k issues, identified 746 potential hot fixes via mining, manually validated 679 cases, and used Jira data/internal control-flow analysis to package 110 reproducible fixes with test suites and metadata extending the Bugs.jar framework.", "result": "Released HotBugs.jar with 679 validated hot fixes (110 reproducible), adopted as SBSE conference challenge dataset, providing reproduceable evaluation criteria for production-critical software tools.", "conclusion": "First benchmark enables systematic study of tools for hot-fix scenarios, filling critical gap in evaluating automated repair and resilience techniques for production systems, with metadata to enable future research directions."}}
{"id": "2510.07584", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.07584", "abs": "https://arxiv.org/abs/2510.07584", "authors": ["Thomas Debris-Alazard", "Philippe Gaborit", "Romaric Neveu", "Olivier Ruatta"], "title": "A Minrank-based Encryption Scheme \u00e0 la Alekhnovich-Regev", "comment": null, "summary": "Introduced in 2003 and 2005, Alekhnovich and Regev' schemes were the first\npublic-key encryptions whose security is only based on the average hardness of\ndecoding random linear codes and LWE, without other security assumptions. Such\nsecurity guarantees made them very popular, being at the origin of the now\nstandardized HQC or Kyber.\n  We present an adaptation of Alekhnovich and Regev' encryption scheme whose\nsecurity is only based on the hardness of a slight variation of MinRank, the\nso-called stationary-MinRank problem. We succeeded to reach this strong\nsecurity guarantee by showing that stationary-MinRank benefits from a\nsearch-to-decision reduction. Our scheme therefore brings a partial answer to\nthe long-standing open question of building an encryption scheme whose security\nrelies solely on the hardness of MinRank.\n  Finally, we show after a thoroughly security analysis that our scheme is\npractical and competitive with other encryption schemes admitting such strong\nsecurity guarantees. Our scheme is slightly less efficient than FrodoKEM, but\nmuch more efficient than Alekhnovich and Regev' original schemes, with\npossibilities of improvements by considering more structure, in the same way as\nHQC and Kyber.", "AI": {"tldr": "This paper presents a new encryption scheme based on the stationary-MinRank problem, offering strong security guarantees similar to Alekhnovich and Regev's schemes but with improved practical efficiency.", "motivation": "The motivation is to address the long-standing open question of constructing an encryption scheme whose security relies solely on the hardness of the MinRank problem, building on the foundations of earlier schemes based on LWE and decoding random linear codes that led to standardized protocols like HQC and Kyber.", "method": "The method involves adapting existing Alekhnovich and Regev encryption schemes by basing their security on a variation of the MinRank problem, termed the stationary-MinRank problem. A key achievement is demonstrating a search-to-decision reduction for this problem, enabling strong security proofs.", "result": "The result is a practical and competitive encryption scheme that provides security based solely on the stationary-MinRank problem. The scheme is more efficient than the original Alekhnovich and Regev's schemes and slightly less efficient than FrodoKEM, suggesting potential for further optimization through structural enhancements similar to those used in HQC and Kyber.", "conclusion": "The paper concludes that the proposed encryption scheme partially solves the open problem of constructing MinRank-based encryption and meets practical efficiency criteria by drawing on techniques like those in HQC and Kyber for potential improvements."}}
{"id": "2510.07604", "categories": ["cs.SE", "D.2.4"], "pdf": "https://arxiv.org/pdf/2510.07604", "abs": "https://arxiv.org/abs/2510.07604", "authors": ["Yubo Bai", "Tapti Palit"], "title": "RustAssure: Differential Symbolic Testing for LLM-Transpiled C-to-Rust Code", "comment": "13 pages to appear in Proceedings of ASE 2025", "summary": "Rust is a memory-safe programming language that significantly improves\nsoftware security. Existing codebases written in unsafe memory languages, such\nas C, must first be transpiled to Rust to take advantage of Rust's improved\nsafety guarantees. RustAssure presents a system that uses Large Language Models\n(LLMs) to automatically transpile existing C codebases to Rust. RustAssure uses\nprompt engineering techniques to maximize the chances of the LLM generating\nidiomatic and safe Rust code. Moreover, because LLMs often generate code with\nsubtle bugs that can be missed under traditional unit or fuzz testing,\nRustAssure performs differential symbolic testing to establish the semantic\nsimilarity between the original C and LLM-transpiled Rust code. We evaluated\nRustAssure with five real-world applications and libraries, and showed that our\nsystem is able to generate compilable Rust functions for 89.8% of all C\nfunctions, of which 69.9% produced equivalent symbolic return values for both\nthe C and Rust functions.", "AI": {"tldr": "Motivated by memory safety concerns in legacy C code, RustAssure uses LLMs and differential symbolic testing to transpile C to Rust, achieving 89.8% compilability and 69.9% semantic equivalence in real-world codebases.", "motivation": "Legacy C codebases are vulnerable to memory safety issues; this work addresses the challenge of automatically converting such code to memory-safe Rust while preserving correctness.", "method": "RustAssure leverages Large Language Models (LLMs) and prompt engineering for transpilation, followed by differential symbolic testing to verify semantic equivalence between C and Rust code.", "result": "The system achieved 89.8% compilable Rust function conversion from C, with 69.9% of those passing symbolic equivalence tests across five real-world applications.", "conclusion": "RustAssure demonstrates that combining LLMs with prompt engineering and differential symbolic testing can effectively transpile C code to Rust while maintaining semantic similarity, offering a promising approach for enhancing software security."}}
{"id": "2510.07697", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07697", "abs": "https://arxiv.org/abs/2510.07697", "authors": ["Man Hu", "Xinyi Wu", "Zuofeng Suo", "Jinbo Feng", "Linghui Meng", "Yanhao Jia", "Anh Tuan Luu", "Shuai Zhao"], "title": "Rethinking Reasoning: A Survey on Reasoning-based Backdoors in LLMs", "comment": null, "summary": "With the rise of advanced reasoning capabilities, large language models\n(LLMs) are receiving increasing attention. However, although reasoning improves\nLLMs' performance on downstream tasks, it also introduces new security risks,\nas adversaries can exploit these capabilities to conduct backdoor attacks.\nExisting surveys on backdoor attacks and reasoning security offer comprehensive\noverviews but lack in-depth analysis of backdoor attacks and defenses targeting\nLLMs' reasoning abilities. In this paper, we take the first step toward\nproviding a comprehensive review of reasoning-based backdoor attacks in LLMs by\nanalyzing their underlying mechanisms, methodological frameworks, and\nunresolved challenges. Specifically, we introduce a new taxonomy that offers a\nunified perspective for summarizing existing approaches, categorizing\nreasoning-based backdoor attacks into associative, passive, and active. We also\npresent defense strategies against such attacks and discuss current challenges\nalongside potential directions for future research. This work offers a novel\nperspective, paving the way for further exploration of secure and trustworthy\nLLM communities.", "AI": {"tldr": "This paper reviews reasoning-based backdoor attacks in LLMs, proposes a taxonomy, analyzes defenses, and outlines challenges to advance secure LLM research.", "motivation": "Existing surveys on backdoor attacks lack focus on LLM reasoning capabilities, which are exploited for new security risks despite improving task performance.", "method": "The paper introduces a taxonomy categorizing reasoning-based backdoor attacks into associative, passive, and active types, analyzes attack mechanisms, and presents defense strategies.", "result": "The study systematically reviews reasoning-based backdoor attacks, establishes a unified framework, discusses defenses, and identifies unresolved challenges for future research.", "conclusion": "This work provides a novel perspective on reasoning-based backdoor attacks in LLMs, offering a taxonomy, defenses, and future research directions to enhance LLM security."}}
{"id": "2510.07740", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07740", "abs": "https://arxiv.org/abs/2510.07740", "authors": ["Dezhi Ran", "Yuan Cao", "Mengzhou Wu", "Simin Chen", "Yuzhe Guo", "Jun Ren", "Zihe Song", "Hao Yu", "Jialei Wei", "Linyi Li", "Wei Yang", "Baishakhi Ray", "Tao Xie"], "title": "AppForge: From Assistant to Independent Developer -- Are GPTs Ready for Software Development?", "comment": "Under Review. Benchmark and leadboards at\n  https://appforge-bench.github.io/", "summary": "Large language models (LLMs) have demonstrated remarkable capability in\nfunction-level code generation tasks. Unlike isolated functions, real-world\napplications demand reasoning over the entire software system: developers must\norchestrate how different components interact, maintain consistency across\nstates over time, and ensure the application behaves correctly within the\nlifecycle and framework constraints. Yet, no existing benchmark adequately\nevaluates whether LLMs can bridge this gap and construct entire software\nsystems from scratch. To address this gap, we propose APPFORGE, a benchmark\nconsisting of 101 software development problems drawn from real-world Android\napps. Given a natural language specification detailing the app functionality, a\nlanguage model is tasked with implementing the functionality into an Android\napp from scratch. Developing an Android app from scratch requires understanding\nand coordinating app states, lifecycle management, and asynchronous operations,\ncalling for LLMs to generate context-aware, robust, and maintainable code. To\nconstruct APPFORGE, we design a multi-agent system to automatically summarize\nthe main functionalities from app documents and navigate the app to synthesize\ntest cases validating the functional correctness of app implementation.\nFollowing rigorous manual verification by Android development experts, APPFORGE\nincorporates the test cases within an automated evaluation framework that\nenables reproducible assessment without human intervention, making it easily\nadoptable for future research. Our evaluation on 12 flagship LLMs show that all\nevaluated models achieve low effectiveness, with the best-performing model\n(GPT-5) developing only 18.8% functionally correct applications, highlighting\nfundamental limitations in current models' ability to handle complex,\nmulti-component software engineering challenges.", "AI": {"tldr": "The paper introduces APPFORGE, a benchmark for LLMs to build full Android apps from specs, revealing their current struggles in complex software development.", "motivation": "Existing benchmarks focus on code generation at the function level, while real-world app development involves system-wide reasoning about components, states, and lifecycles. The authors find a gap in evaluating LLMs' ability to manage complex software systems.\\n", "method": "APPFORGE consists of 101 real-world Android app development tasks derived from app documents, and a multi-agent system is used to automatically generate test cases for validation. A manual verification process is followed to ensure accuracy, and an automated evaluation framework is set up for reproducibility.\\n", "result": "Across 12 LLMs, the effectiveness is low. GPT-5, the best model, produces correct app implementations in only 18.8% of the cases.\\n", "conclusion": "The findings emphasize that current LLMs still lack the competence to handle software engineering challenges involving complex, multi-component development tasks for Android. APPFORGE is made available for advancing future research in this area.\\n"}}
{"id": "2510.07806", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.07806", "abs": "https://arxiv.org/abs/2510.07806", "authors": ["Yihao Peng", "Biao Ma", "Hai Wan", "Xibin Zhao"], "title": "ANCORA: Accurate Intrusion Recovery for Web Applications", "comment": null, "summary": "Modern web application recovery presents a critical dilemma. Coarse-grained\nsnapshot rollbacks cause unacceptable data loss for legitimate users.\nSurgically removing an attack's impact is hindered by a fundamental challenge\nin high-concurrency environments: it is difficult to attribute resulting file\nand database modifications to a specific attack-related request. We present\nANCORA, a system for precise intrusion recovery in web applications without\ninvasive instrumentation. ANCORA first isolates the full sequence of syscalls\ntriggered by a single malicious request. Based on this sequence, ANCORA\naddresses file and database modifications separately. To trace file changes, it\nbuilds a provenance graph that reveals all modifications, including those by\nexploit-spawned processes. To attribute database operations, a more difficult\nchallenge due to connection pooling, ANCORA introduces a novel spatiotemporal\nanchor. This anchor uses the request's network connection tuple and active time\nwindow to pinpoint exact database operations. With all malicious file and\ndatabase operations precisely identified, ANCORA performs a unified rewind and\nselective replay recovery. It reverts the system to a clean snapshot taken\nbefore the attack, then selectively re-applies only legitimate operations to\nboth the file system and database. This completely removes the attack's effects\nwhile preserving concurrent legitimate data. We evaluated ANCORA on 10 web\napplications and 20 CVE-based attack scenarios with concurrency up to 150\nconnections. Experiments demonstrate ANCORA achieves 99.9% recovery accuracy\nwith manageable overhead: up to 19.8% response latency increase and 17.8% QPS\ndecrease in worst cases, and recovery throughput of 110.7 database operations\nper second and 27.2 affected files per second, effectively preserving\nlegitimate data.", "AI": {"tldr": "ANCORA is a non-invasive system for precise intrusion recovery in web apps, enabling selective removal of attack effects while preserving legitimate data through syscall isolation, provenance graphs, and spatiotemporal anchors.", "motivation": "Coarse-grained rollbacks cause data loss for legitimate users, and existing methods fail to attribute file/database modifications to specific malicious requests in high-concurrency environments.", "method": "1. Isolates attack-triggered syscalls via request-level tracing. 2. Creates provenance graph to track file changes, including exploit-spawned processes. 3. Uses spatiotemporal anchors (network tuple + time window search) to attribute database operations. 4. Performs unified rewind+selective replay to clean state.", "result": "99.9\\% recovery accuracy across 10 apps/20 attack scenarios. 19.8\\%/17.8\\% max latency/QPS overhead. Recovery throughput: 110.7 DB ops/s, 27.2 files/s.", "conclusion": "ANCORA achieves high-precision, low-overhead attack recovery without application changes, effectively preserving concurrent legitimate operations and removing attack fingerprints."}}
{"id": "2510.07815", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.07815", "abs": "https://arxiv.org/abs/2510.07815", "authors": ["Zeyu Sun", "Jingjing Liang", "Weiyi Wang", "Chenyao Suo", "Junjie Chen", "Fanjiang Xu"], "title": "Interleaved Learning and Exploration: A Self-Adaptive Fuzz Testing Framework for MLIR", "comment": null, "summary": "MLIR (Multi-Level Intermediate Representation) has rapidly become a\nfoundational technology for modern compiler frameworks, enabling extensibility\nacross diverse domains. However, ensuring the correctness and robustness of\nMLIR itself remains challenging. Existing fuzzing approaches-based on manually\ncrafted templates or rule-based mutations-struggle to generate sufficiently\ndiverse and semantically valid test cases, making it difficult to expose subtle\nor deep-seated bugs within MLIR's complex and evolving code space. In this\npaper, we present FLEX, a novel self-adaptive fuzzing framework for MLIR. FLEX\nleverages neural networks for program generation, a perturbed sampling strategy\nto encourage diversity, and a feedback-driven augmentation loop that\niteratively improves its model using both crashing and non-crashing test cases.\nStarting from a limited seed corpus, FLEX progressively learns valid syntax and\nsemantics and autonomously produces high-quality test inputs. We evaluate FLEX\non the upstream MLIR compiler against four state-of-the-art fuzzers. In a\n30-day campaign, FLEX discovers 80 previously unknown bugs-including multiple\nnew root causes and parser bugs-while in 24-hour fixed-revision comparisons, it\ndetects 53 bugs (over 3.5x as many as the best baseline) and achieves 28.2%\ncode coverage, outperforming the next-best tool by 42%. Ablation studies\nfurther confirm the critical role of both perturbed generation and diversity\naugmentation in FLEX's effectiveness.", "AI": {"tldr": "FLEX, a self-adaptive MLIR fuzzer, discovers 80 bugs in 30 days using neural program generation and diversity-driven strategies.", "motivation": "Existing MLIR fuzzing methods fail to generate sufficiently diverse and semantically valid test cases, limiting their ability to expose deep-seated bugs in MLIR's complex codebase.", "method": "FLEX employs neural networks for program generation, perturbed sampling for diversity, and a feedback-driven augmentation loop using both crashing and non-crashing test cases to iteratively enhance its model.", "result": "FLEX discovers 80 new bugs in 30 days (including 53 in 24 hours with 3.5x more bugs than the best baseline) and achieves 28.2% code coverage (42% better than the next-best tool).", "conclusion": "FLEX significantly outperforms existing fuzzing tools in finding bugs and improving code coverage for MLIR, demonstrating the effectiveness of its self-adaptive framework."}}
{"id": "2510.07809", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07809", "abs": "https://arxiv.org/abs/2510.07809", "authors": ["Renhua Ding", "Xiao Yang", "Zhengwei Fang", "Jun Luo", "Kun He", "Jun Zhu"], "title": "Effective and Stealthy One-Shot Jailbreaks on Deployed Mobile Vision-Language Agents", "comment": null, "summary": "Large vision-language models (LVLMs) enable autonomous mobile agents to\noperate smartphone user interfaces, yet vulnerabilities to UI-level attacks\nremain critically understudied. Existing research often depends on conspicuous\nUI overlays, elevated permissions, or impractical threat models, limiting\nstealth and real-world applicability. In this paper, we present a practical and\nstealthy one-shot jailbreak attack that leverages in-app prompt injections:\nmalicious applications embed short prompts in UI text that remain inert during\nhuman interaction but are revealed when an agent drives the UI via ADB (Android\nDebug Bridge). Our framework comprises three crucial components: (1)\nlow-privilege perception-chain targeting, which injects payloads into malicious\napps as the agent's visual inputs; (2) stealthy user-invisible activation, a\ntouch-based trigger that discriminates agent from human touches using physical\ntouch attributes and exposes the payload only during agent operation; and (3)\none-shot prompt efficacy, a heuristic-guided, character-level\niterative-deepening search algorithm (HG-IDA*) that performs one-shot,\nkeyword-level detoxification to evade on-device safety filters. We evaluate\nacross multiple LVLM backends, including closed-source services and\nrepresentative open-source models within three Android applications, and we\nobserve high planning and execution hijack rates in single-shot scenarios\n(e.g., GPT-4o: 82.5% planning / 75.0% execution). These findings expose a\nfundamental security vulnerability in current mobile agents with immediate\nimplications for autonomous smartphone operation.", "AI": {"tldr": "Researchers develop a stealthy one-shot jailbreak attack against vision-language mobile agents by injecting invisible prompts into legitimate apps, achieving high success rates in hijacking autonomous smartphone operations", "motivation": "Existing UI-based attacks against mobile agents require conspicuous overlays, elevated permissions, or impractical assumptions, limiting their covert operation and real-world applicability. Current autonomous agents using large vision-language models remain under-protected against subtle prompt injection attacks through standard UI interactions.", "method": "The attack framework combines three components: (1) low-privilege in-app prompt injection via UI text inputs, (2) touch-based activation that distinguishes agent vs. human interactions based on physical touch attributes, and (3) keyword-level detoxification using a heuristic-guided iterative-deepening search (HG-IDA*) to bypass filters. The method operates via the Android Debug Bridge (ADB) interface.", "result": "Evaluation across closed-source and open-source LVLM backends shows high success rates in single-shot attack scenarios: 82.5% planning hijack and 75.0% execution hijack with GPT-4o. The attack demonstrates practicality in standard Android apps without requiring malicious UI overlays or privileged access.", "conclusion": "The paper reveals a critical security flaw in mobile agents driven by LVLMs, demonstrating that practical, stealthy one-shot jailbreak attacks can be mounted with minimal permissions and without detectable UI manipulations. This exposes the urgent need for security countermeasures to protect autonomous smartphone operations."}}
{"id": "2510.07834", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.07834", "abs": "https://arxiv.org/abs/2510.07834", "authors": ["Lingjun Liu", "Feiran Qin", "Owolabi Legunsen", "Marcelo d'Amorim"], "title": "Bug Histories as Sources of Compiler Fuzzing Mutators", "comment": null, "summary": "Bugs in compilers, which are critical infrastructure today, can have outsized\nnegative impacts. Mutational fuzzers aid compiler bug detection by\nsystematically mutating compiler inputs, i.e., programs. Their effectiveness\ndepends on the quality of the mutators used. Yet, no prior work used compiler\nbug histories as a source of mutators. We propose IssueMut, the first approach\nfor extracting compiler fuzzing mutators from bug histories. Our insight is\nthat bug reports contain hints about program elements that induced compiler\nbugs; they can guide fuzzers towards similar bugs. IssueMut uses an automated\nmethod to mine mutators from bug reports and retrofit such mutators into\nexisting mutational compiler fuzzers. Using IssueMut, we mine 587 mutators from\n1760 GCC and LLVM bug reports. Then, we run IssueMut on these compilers, with\nall their test inputs as seed corpora. We find that \"bug history\" mutators are\neffective: they find new bugs that a state-of-the-art mutational compiler\nfuzzer misses-28 in GCC and 37 in LLVM. Of these, 60 were confirmed or fixed,\nvalidating our idea that bug histories have rich information that compiler\nfuzzers should leverage.", "AI": {"tldr": "IssueMut extracts mutators from compiler bug histories to enhance fuzzing, discovering 28-37 new bugs in GCC/LLVM that existing tools missed, proving historical data can improve compiler testing.", "motivation": "Prior compiler fuzzers lack methods to leverage historical bug data, despite the potential of such information to guide fuzzers toward high-impact mutations. This gap limits the effectiveness of existing mutators.", "method": "IssueMut proposes an automated approach to mine mutators from compiler bug reports by analyzing program elements linked to past bugs. It retrofits these data-driven mutators into existing fuzzers, extracting 587 mutators from 1,760 GCC and LLVM bug reports.", "result": "IssueMut discovered 28 new bugs in GCC and 37 in LLVM, with 60 confirmed or fixed, significantly outperforming state-of-the-art mutational fuzzers which missed these vulnerabilities.", "conclusion": "Bug histories are a rich source of information for compiler fuzzing, and IssueMut demonstrates that leveraging these histories can uncover new bugs missed by existing tools, suggesting developers should incorporate historical data into fuzzing strategies."}}
{"id": "2510.07901", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.07901", "abs": "https://arxiv.org/abs/2510.07901", "authors": ["Georgios Diamantopoulos", "Nikos Tziritas", "Rami Bahsoon", "Georgios Theodoropoulos"], "title": "Decentralised Blockchain Management Through Digital Twins", "comment": "Accepted for publication in the proceedings of the 24th Asia\n  Simulation Conference 2025", "summary": "The necessity of blockchain systems to remain decentralised limits current\nsolutions to blockchain governance and dynamic management, forcing a trade-off\nbetween control and decentralisation. In light of the above, this work proposes\na dynamic and decentralised blockchain management mechanism based on digital\ntwins. To ensure decentralisation, the proposed mechanism utilises multiple\ndigital twins that the system's stakeholders control. To facilitate\ndecentralised decision-making, the twins are organised in a secondary\nblockchain system that orchestrates agreement on, and propagation of decisions\nto the managed blockchain. This enables the management of blockchain systems\nwithout centralised control. A preliminary evaluation of the performance and\nimpact of the overheads introduced by the proposed mechanism is conducted\nthrough simulation. The results demonstrate the proposed mechanism's ability to\nreach consensus on decisions quickly and reconfigure the primary blockchain\nwith minimal overhead.", "AI": {"tldr": "This paper proposes a decentralized blockchain management framework using multiple stakeholder-controlled digital twins organized in a secondary blockchain to enable dynamic governance without centralised control.", "motivation": "Existing blockchain systems face a control vs decentralization trade-off in governance, limiting dynamic system management capabilities.", "method": "Develops a dual-layer architecture where stakeholder-managed digital twins in a secondary blockchain coordinate consensus and decision propagation to the primary blockchain through decentralised orchestration.", "result": "Simulation evaluations show the mechanism achieves rapid consensus formation and efficient blockchain reconfiguration with minimal operational overhead.", "conclusion": "The digital twin-based approach successfully demonstrates decentralized blockchain management capabilities that resolve the traditional control-decentralization trade-off, enabling dynamic governance while maintaining decentralisation."}}
{"id": "2510.07941", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.07941", "abs": "https://arxiv.org/abs/2510.07941", "authors": ["Srijita Basu", "Haraldsson Bengt", "Miroslaw Staron", "Christian Berger", "Jennifer Horkoff", "Magnus Almgren"], "title": "An AUTOSAR-Aligned Architectural Study of Vulnerabilities in Automotive SoC Software", "comment": "16 pages, 7 figures, 18th International Conference on the Quality of\n  Information and Communications Technology", "summary": "Cooperative, Connected and Automated Mobility (CCAM) are complex\ncyber-physical systems (CPS) that integrate computation, communication, and\ncontrol in safety-critical environments. At their core, System-on-Chip (SoC)\nplatforms consolidate processing units, communication interfaces, AI\naccelerators, and security modules into a single chip. AUTOSAR (AUTomotive Open\nSystem ARchitecture) standard was developed in the automotive domain to better\nmanage this complexity, defining layered software structures and interfaces to\nfacilitate reuse of HW/SW components. However, in practice, this integrated SoC\nsoftware architecture still poses security challenges, particularly in\nreal-time, safety-critical environments. Recent reports highlight a surge in\nSoC-related vulnerabilities, yet systematic analysis of their root causes and\nimpact within AUTOSAR-aligned architectures is lacking. This study fills that\ngap by analyzing 180 publicly reported automotive SoC vulnerabilities, mapped\nto a representative SoC software architecture model that is aligned with\nAUTOSAR principles for layered abstraction and service orientation. We identify\n16 root causes and 56 affected software modules, and examine mitigation delays\nacross Common Weakness Enumeration (CWE) categories and architectural layers.\nWe uncover dominant vulnerability patterns and critical modules with prolonged\npatch delays, and provide actionable insights for securing automotive CPS\nplatforms, including guides for improved detection, prioritization, and\nlocalization strategies for SoC software architectures in SoC-based vehicle\nplatforms.", "AI": {"tldr": "This paper analyzes 180 automotive SoC vulnerabilities in AUTOSAR-aligned architectures, identifies 16 root causes and 56 affected modules, and provides security strategies for automotive CPS platforms.", "motivation": "The complexity of CCAM systems and the lack of systematic analysis of SoC vulnerabilities in AUTOSAR-aligned architectures motivate this study to understand root causes and mitigation delays in safety-critical environments.", "method": "The authors mapped 180 publicly reported vulnerabilities to a representative SoC architecture model aligned with AUTOSAR principles, identifying root causes, affected modules, and mitigation delays by CWE categories and architectural layers.", "result": "16 root causes and 56 affected modules were identified, revealing dominant vulnerability patterns and critical modules with prolonged patch delays, alongside actionable security strategies for automotive CPS platforms.", "conclusion": "The study highlights priority areas for securing automotive SoCs under AUTOSAR, emphasizing detection, prioritization, and localization strategies to address systemic vulnerability challenges in layered CPS architectures."}}
{"id": "2510.07968", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.07968", "abs": "https://arxiv.org/abs/2510.07968", "authors": ["Xiangtao Meng", "Tianshuo Cong", "Li Wang", "Wenyu Chen", "Zheng Li", "Shanqing Guo", "Xiaoyun Wang"], "title": "From Defender to Devil? Unintended Risk Interactions Induced by LLM Defenses", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable performance across various\napplications, but their deployment in sensitive domains raises significant\nconcerns. To mitigate these risks, numerous defense strategies have been\nproposed. However, most existing studies assess these defenses in isolation,\noverlooking their broader impacts across other risk dimensions. In this work,\nwe take the first step in investigating unintended interactions caused by\ndefenses in LLMs, focusing on the complex interplay between safety, fairness,\nand privacy. Specifically, we propose CrossRiskEval, a comprehensive evaluation\nframework to assess whether deploying a defense targeting one risk\ninadvertently affects others. Through extensive empirical studies on 14\ndefense-deployed LLMs, covering 12 distinct defense strategies, we reveal\nseveral alarming side effects: 1) safety defenses may suppress direct responses\nto sensitive queries related to bias or privacy, yet still amplify indirect\nprivacy leakage or biased outputs; 2) fairness defenses increase the risk of\nmisuse and privacy leakage; 3) privacy defenses often impair safety and\nexacerbate bias. We further conduct a fine-grained neuron-level analysis to\nuncover the underlying mechanisms of these phenomena. Our analysis reveals the\nexistence of conflict-entangled neurons in LLMs that exhibit opposing\nsensitivities across multiple risk dimensions. Further trend consistency\nanalysis at both task and neuron levels confirms that these neurons play a key\nrole in mediating the emergence of unintended behaviors following defense\ndeployment. We call for a paradigm shift in LLM risk evaluation, toward\nholistic, interaction-aware assessment of defense strategies.", "AI": {"tldr": "This work reveals problematic cross-dimensional interactions among LLM defense strategies, showing how safety, fairness, and privacy defenses can harm each other's effectiveness through neuron-level conflicts. The introduced CrossRiskEval framework highlights the need for holistic defense evaluation.", "motivation": "Existing defense strategies for LLMs are evaluated in isolation, overlooking their cross-dimensional impacts on safety, fairness, and privacy. This paper aims to investigate unintended interactions between defenses and risk dimensions to advance more effective risk mitigation in sensitive domains.", "method": "The authors propose CrossRiskEval, a comprehensive evaluation framework to assess how defenses deployed in LLMs inadvertently affect other risk dimensions (safety, fairness, privacy). They conduct extensive empirical studies on 14 defense-deployed LLMs with 12 strategies and perform fine-grained neuron-level analysis to uncover mechanisms of unintended interactions.", "result": "Three key findings are identified: 1) Safety defenses may amplify indirect privacy leakage and bias despite suppressing direct sensitive responses; 2) Fairness defenses increase misuse and privacy leakage risks; 3) Privacy defenses impair safety and exacerbate bias. Neuron-level analysis reveals conflict-entangled neurons that cause these unintended interactions.", "conclusion": "The paper calls for a paradigm shift in LLM risk evaluation toward holistic, interaction-aware assessment of defense strategies, emphasizing the need to recognize and address conflict-entangled neurons that mediate unintended behaviors."}}
{"id": "2510.08005", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08005", "abs": "https://arxiv.org/abs/2510.08005", "authors": ["Utku Boran Torun", "Mehmet Taha Demircan", "Mahmut Furkan G\u00f6n", "Eray T\u00fcz\u00fcn"], "title": "Past, Present, and Future of Bug Tracking in the Generative AI Era", "comment": "Submitted to ACM TOSEM Special Issue: 2030 Software Engineering\n  Roadmap", "summary": "Traditional bug tracking systems rely heavily on manual reporting,\nreproduction, triaging, and resolution, each carried out by different\nstakeholders such as end users, customer support, developers, and testers. This\ndivision of responsibilities requires significant coordination and widens the\ncommunication gap between non-technical users and technical teams, slowing the\nprocess from bug discovery to resolution. Moreover, current systems are highly\nasynchronous; users often wait hours or days for a first response, delaying\nfixes and contributing to frustration. This paper examines the evolution of bug\ntracking, from early paper-based reporting to today's web-based and SaaS\nplatforms. Building on this trajectory, we propose an AI-powered bug tracking\nframework that augments existing tools with intelligent, large language model\n(LLM)-driven automation. Our framework addresses two main challenges: reducing\ntime-to-fix and minimizing human overhead. Users report issues in natural\nlanguage, while AI agents refine reports, attempt reproduction, and request\nmissing details. Reports are then classified, invalid ones resolved through\nno-code fixes, and valid ones localized and assigned to developers. LLMs also\ngenerate candidate patches, with human oversight ensuring correctness. By\nintegrating automation into each phase, our framework accelerates response\ntimes, improves collaboration, and strengthens software maintenance practices\nfor a more efficient, user-centric future.", "AI": {"tldr": "This paper proposes an AI-powered bug tracking framework using LLMs to reduce manual processes, accelerate response times, and improve collaboration between non-technical users and technical teams.", "motivation": "Traditional systems are slow, fragmented, and inefficient due to manual coordination, communication gaps, and asynchronous workflows, delaying resolutions and frustrating users.", "method": "The framework integrates LLMs for natural language bug reporting, report refinement, classification, automated invalid bug fixes, issue localization, patch generation, and human-in-the-loop validation across all bug-tracking phases.", "result": "The design aims to minimize human overhead, reduce time-to-fix, and enhance software maintenance through intelligent automation, though evaluation details or metrics are not specified in the abstract.", "conclusion": "By evolving from paper-based to AI-augmented systems, the framework presents a transformative approach to modern bug tracking, fostering efficiency, user-centricity, and seamless collaboration."}}
{"id": "2510.08013", "categories": ["cs.CR", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2510.08013", "abs": "https://arxiv.org/abs/2510.08013", "authors": ["Yurang R. Kuang"], "title": "Composition Law of Conjugate Observables in Random Permutation Sorting Systems", "comment": null, "summary": "We present the discovery of a fundamental composition law governing conjugate\nobservables in the Random Permutation Sorting System (RPSS). The law links the\ndiscrete permutation count Np and the continuous elapsed time T through a\nfunctional relation connecting the characteristic function of timing\ndistributions to the probability generating function of permutation counts.\nThis framework enables entropy purification, transforming microarchitectural\ntiming fluctuations into uniform randomness via geometric convergence. We\nestablish convergence theorems with explicit bounds and validate the results\nexperimentally, achieving Shannon entropy above 7.9998 bits per byte and\nchi-square uniformity across diverse platforms. The composition law provides a\nuniversal foundation for generating provably uniform randomness from\ngeneral-purpose computation, securing cryptographic purity from emergent\ncomputational dynamics.", "AI": {"tldr": "This paper introduces a theoretically grounded, universally applicable method for generating high-purity randomness from computation using novel mathematical properties of permutation sorting dynamics.", "motivation": "Current methods for extracting randomness from microarchitectural timing fluctuations lack guarantees of uniformity. The paper aims to address this by establishing a principled, platform-independent mechanism for achieving cryptographic-grade randomness.", "method": "The authors derive a mathematical relationship between discrete permutation counts and continuous time variables via characteristic and probability generating functions, enabling entropy purification through geometric convergence. They prove convergence theorems with explicit bounds and validate the framework experimentally.", "result": "Experimental results demonstrate Shannon entropy exceeding 7.9998 bits per byte and chi-square uniformity across diverse platforms, proving the method's effectiveness.", "conclusion": "The paper presents a universal framework for generating provably uniform cryptographic randomness from general-purpose computational processes by leveraging a fundamental composition law in the Random Permutation Sorting System (RPSS)."}}
{"id": "2510.08200", "categories": ["cs.SE", "68N15", "D.2.13"], "pdf": "https://arxiv.org/pdf/2510.08200", "abs": "https://arxiv.org/abs/2510.08200", "authors": ["Alexander Hellwig", "Nico Jansen", "Bernhard Rumpe"], "title": "Building Whitespace-Sensitive Languages Using Whitespace-Insensitive Components", "comment": "11 pages, 4 figures, 6 listings", "summary": "In Software Language Engineering, there is a trend towards reusability by\ncomposing modular language components. However, this reusability is severely\ninhibited by a gap in integrating whitespace-sensitive and\nwhitespace-insensitive languages. There is currently no consistent procedure\nfor seamlessly reusing such language components in both cases, such that\nlibraries often cannot be reused, and whitespacesensitive languages are\ndeveloped from scratch. This paper presents a technique for using modular,\nwhitespaceinsensitive language modules to construct whitespace sensitive\nlanguages by pre-processing language artifacts before parsing. The approach is\nevaluated by reconstructing a simplified version of the programming language\nPython. Our solution aims to increase the reusability of existing language\ncomponents to reduce development time and increase the overall quality of\nsoftware languages.", "AI": {"tldr": "This paper proposes a pre-processing method to reuse whitespace-insensitive language modules for whitespace-sensitive languages, reducing development time and improving language quality.", "motivation": "The motivation addresses the lack of consistent integration methods between whitespace-sensitive and whitespace-insensitive languages, which hinders reusability and forces redundant development of whitespace-sensitive languages.", "method": "The method involves pre-processing language artifacts to adapt whitespace-insensitive modules for constructing whitespace-sensitive languages, as demonstrated by reconstructing a simplified Python version.", "result": "The approach successfully reconstructs a simplified Python implementation, validating its effectiveness in reusing whitespace-insensitive components to build whitespace-sensitive languages.", "conclusion": "The paper concludes that the proposed pre-processing technique bridges the gap between whitespace-sensitive and whitespace-insensitive languages, enabling modular reuse of language components and enhancing software language development efficiency and quality."}}
{"id": "2510.08084", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08084", "abs": "https://arxiv.org/abs/2510.08084", "authors": ["Hikmat A. M. Abdeljaber", "Md. Alamgir Hossain", "Sultan Ahmad", "Ahmed Alsanad", "Md Alimul Haque", "Sudan Jha", "Jabeen Nazeer"], "title": "A Novel Ensemble Learning Approach for Enhanced IoT Attack Detection: Redefining Security Paradigms in Connected Systems", "comment": "14 pages, 5 fiugres, 7 tables", "summary": "The rapid expansion of Internet of Things (IoT) devices has transformed\nindustries and daily life by enabling widespread connectivity and data\nexchange. However, this increased interconnection has introduced serious\nsecurity vulnerabilities, making IoT systems more exposed to sophisticated\ncyber attacks. This study presents a novel ensemble learning architecture\ndesigned to improve IoT attack detection. The proposed approach applies\nadvanced machine learning techniques, specifically the Extra Trees Classifier,\nalong with thorough preprocessing and hyperparameter optimization. It is\nevaluated on several benchmark datasets including CICIoT2023, IoTID20,\nBotNeTIoT L01, ToN IoT, N BaIoT, and BoT IoT. The results show excellent\nperformance, achieving high recall, accuracy, and precision with very low error\nrates. These outcomes demonstrate the model efficiency and superiority compared\nto existing approaches, providing an effective and scalable method for securing\nIoT environments. This research establishes a solid foundation for future\nprogress in protecting connected devices from evolving cyber threats.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.08101", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.08101", "abs": "https://arxiv.org/abs/2510.08101", "authors": ["Simone Bozzolan", "Stefano Calzavara", "Lorenzo Cazzaro"], "title": "LLM-Assisted Web Measurements", "comment": "12 pages, 4 figures, 4 tables", "summary": "Web measurements are a well-established methodology for assessing the\nsecurity and privacy landscape of the Internet. However, existing top lists of\npopular websites commonly used as measurement targets are unlabeled and lack\nsemantic information about the nature of the sites they include. This\nlimitation makes targeted measurements challenging, as researchers often need\nto rely on ad-hoc techniques to bias their datasets toward specific categories\nof interest. In this paper, we investigate the use of Large Language Models\n(LLMs) as a means to enable targeted web measurement studies through their\nsemantic understanding capabilities. Building on prior literature, we identify\nkey website classification tasks relevant to web measurements and construct\ndatasets to systematically evaluate the performance of different LLMs on these\ntasks. Our results demonstrate that LLMs may achieve strong performance across\nmultiple classification scenarios. We then conduct LLM-assisted web measurement\nstudies inspired by prior work and rigorously assess the validity of the\nresulting research inferences. Our results demonstrate that LLMs can serve as a\npractical tool for analyzing security and privacy trends on the Web.", "AI": {"tldr": "This paper explores leveraging Large Language Models (LLMs for targeted web measurement studies by utilizing their semantic understanding to classify websites, demonstrating that LLMs can effectively support security and privacy research.", "motivation": "The challenge lies in the lack of labeled semantic information in traditional top website lists, hindering targeted measurements. Researchers must resort to ad-hoc methods for dataset biasing, creating a need for systematic LLM-based classification and validation approaches.", "method": "The authors (1 identify key website classification tasks relevant to web measurements, (2 construct datasets to evaluate LLM performance on these tasks, and (3 conduct LLM-assisted web measurement studies to validate research inferences using rigorous assessment.", "result": "LLMs achieved strong performance across classification scenarios, and LLM-assisted measurement studies successfully validated research inferences, demonstrating their practicality for analyzing Web security/privacy trends.", "conclusion": "LLMs can serve as a reliable tool for targeted web measurements by providing semantic classification capabilities, enabling researchers to draw valid inferences about security and privacy trends with minimal manual effort."}}
{"id": "2510.08225", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.08225", "abs": "https://arxiv.org/abs/2510.08225", "authors": ["Daniel Pressens\u00e9", "Elisavet Kozyri"], "title": "TracE2E: Easily Deployable Middleware for Decentralized Data Traceability", "comment": null, "summary": "This paper presents TracE2E, a middleware written in Rust, that can provide\nboth data explainability and compliance across multiple nodes. By mediating\ninputs and outputs of processes, TracE2E records provenance information and\nenforces data-protection policies (e.g., confidentiality, integrity) that\ndepend on the recorded provenance. Unlike existing approaches that necessitate\nsubstantial application modifications, TracE2E is designed for easy integration\ninto existing and future applications through a wrapper of the Rust standard\nlibrary's IO module. We describe how TracE2E consistently records provenance\ninformation across nodes, and we demonstrate how the compliance layer of\nTracE2E can accommodate the enforcement of multiple policies.", "AI": {"tldr": "TracE2E is a Rust-based middleware enabling data explainability and compliance across distributed systems through provenance tracking and policy enforcement without requiring significant application modifications.", "motivation": "Existing solutions require substantial changes to applications to implement data-protection and compliance policies. There is a need for a lightweight, integration-friendly approach that maintains data provenance and policy enforcement across distributed nodes.", "method": "TracE2E mediates input/output processes to record provenance information and enforce data protection policies (confidentiality/integrity). It achieves integration through a wrapper of the Rust standard library\u2019s IO module, avoiding deep application modifications.", "result": "The paper demonstrates consistent provenance recording across nodes and validates the compliance layer\u2019s ability to enforce multiple data-protection policies effectively.", "conclusion": "TracE2E provides a practical and minimally invasive solution for ensuring data explainability and regulatory compliance in distributed systems, addressing limitations in prior approaches."}}
{"id": "2510.08272", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.08272", "abs": "https://arxiv.org/abs/2510.08272", "authors": ["C\u00e9drick Austa", "Jan Tobias M\u00fchlberg", "Jean-Michel Dricot"], "title": "Systematic Assessment of Cache Timing Vulnerabilities on RISC-V Processors", "comment": null, "summary": "While interest in the open RISC-V instruction set architecture is growing,\ntools to assess the security of concrete processor implementations are lacking.\nThere are dedicated tools and benchmarks for common microarchitectural\nside-channel vulnerabilities for popular processor families such as Intel\nx86-64 or ARM, but not for RISC-V. In this paper we describe our efforts in\nporting an Intel x86-64 benchmark suite for cache-based timing vulnerabilities\nto RISC-V. We then use this benchmark to evaluate the security of three\ncommercially available RISC-V processors, the T-Head C910 and the SiFive U54\nand U74 cores. We observe that the C910 processor exhibits more distinct timing\ntypes than the other processors, leading to the assumption that code running on\nthe C910 would be exposed to more microarchitectural vulnerability sources. In\naddition, our evaluation reveals that $37.5\\%$ of the vulnerabilities covered\nby the benchmark exist in all processors, while only $6.8\\%$ are absent from\nall cores. Our work, in particular the ported benchmark, aims to support RISC-V\nprocessor designers to identify leakage sources early in their designs and to\nsupport the development of countermeasures.", "AI": {"tldr": "The authors ported an Intel x86-64 benchmark suite for cache-based timing vulnerabilities to RISC-V and evaluated three RISC-V processors, finding varying levels of vulnerability to microarchitectural attacks.", "motivation": "As RISC-V gains popularity, there is a need for tools to evaluate the security of its processor implementations, particularly for microarchitectural side-channel vulnerabilities. Existing tools focus on established architectures like x86-64 and ARM, but not RISC-V.", "method": "The authors ported a benchmark suite used for Intel x86-64 to RISC-V and used it to evaluate the cache-based timing vulnerabilities of three commercial RISC-V processors: the T-Head C910, and the SiFive U54 and U74 cores.", "result": "The C910 processor showed more distinct timing types, suggesting a higher risk of vulnerability compared to the SiFive cores. Additionally, 37.5% of the vulnerabilities were present in all three processors, while only 6.8% were completely absent.", "conclusion": "The ported benchmark and vulnerability assessment can help RISC-V designers identify and address microarchitectural leakage sources earlier in the design process, improving the architecture's security posture."}}
{"id": "2510.08333", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08333", "abs": "https://arxiv.org/abs/2510.08333", "authors": ["Mika\u00ebla Ngambo\u00e9", "Jean-Simon Marrocco", "Jean-Yves Ouattara", "Jos\u00e9 M. Fernandez", "Gabriela Nicolescu"], "title": "New Machine Learning Approaches for Intrusion Detection in ADS-B", "comment": "This is the author's version of the work accepted for publication\n  Digital Avionics Systems Conference (DASC) 2025. The final version will be\n  available via IEEE Xplore", "summary": "With the growing reliance on the vulnerable Automatic Dependent\nSurveillance-Broadcast (ADS-B) protocol in air traffic management (ATM),\nensuring security is critical. This study investigates emerging machine\nlearning models and training strategies to improve AI-based intrusion detection\nsystems (IDS) for ADS-B. Focusing on ground-based ATM systems, we evaluate two\ndeep learning IDS implementations: one using a transformer encoder and the\nother an extended Long Short-Term Memory (xLSTM) network, marking the first\nxLSTM-based IDS for ADS-B. A transfer learning strategy was employed, involving\npre-training on benign ADS-B messages and fine-tuning with labeled data\ncontaining instances of tampered messages. Results show this approach\noutperforms existing methods, particularly in identifying subtle attacks that\nprogressively undermine situational awareness. The xLSTM-based IDS achieves an\nF1-score of 98.9%, surpassing the transformer-based model at 94.3%. Tests on\nunseen attacks validated the generalization ability of the xLSTM model.\nInference latency analysis shows that the 7.26-second delay introduced by the\nxLSTM-based IDS fits within the Secondary Surveillance Radar (SSR) refresh\ninterval (5-12 s), although it may be restrictive for time-critical operations.\nWhile the transformer-based IDS achieves a 2.1-second latency, it does so at\nthe cost of lower detection performance.", "AI": {"tldr": "This paper proposes novel AI-based intrusion detection systems (IDSs) for securing the vulnerable ADS-B protocol in air traffic management, comparing transformer and xLSTM architectures. While the xLSTM model demonstrates superior attack detection accuracy (98.9 F1-score), it introduces latency that must be balanced against operational requirements.", "motivation": "The widespread adoption of the security-critical yet inherently vulnerable ADS-B protocol in air traffic management necessitates advanced intrusion detection solutions to counter threats requiring real-time detection of subtle, progressive attacks.", "method": "The study evaluates two deep learning IDS implementations (transformer-based and xLSTM-based) using a transfer learning strategy: pre-training on benign ADS-B messages followed by fine-tuning on labeled hybrid datasets containing attack instances. This represents the first xLSTM application to ADS-B security.", "result": "The xLSTM-based IDS achieves 98.9% F1-score vs. 94.3% for the transformer model while maintaining generalizability to unseen attacks. However, it introduces 7.26s latency (vs. 2.1s for transformers), constrained by Secondary Surveillance Radar refresh intervals.", "conclusion": "While xLSTM outperforms in detection accuracy and attack generalization, its latency suggests suitability for SSR-interval compatible applications rather than time-critical operations, highlighting a critical performance-latency tradeoff in ADS-B IDS deployment."}}
{"id": "2510.08343", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.08343", "abs": "https://arxiv.org/abs/2510.08343", "authors": ["Anne M\u00fcller", "Mohd Kashif", "Nico D\u00f6ttling"], "title": "A Haskell to FHE Transpiler", "comment": null, "summary": "Fully Homomorphic Encryption (FHE) enables the evaluation of programs\ndirectly on encrypted data. However, because only basic operations can be\nperformed on ciphertexts, programs must be expressed as boolean or arithmetic\ncircuits. This low-level representation makes implementing applications for FHE\nsignificantly more cumbersome than writing code in a high-level language. To\nreduce this burden, several transpilers have been developed that translate\nhigh-level code into circuit representations. In this work, we extend the range\nof high-level languages that can target FHE by introducing a transpiler for\nHaskell, which converts Haskell programs into Boolean circuits suitable for\nhomomorphic evaluation. Our second contribution is the automatic\nparallelization of these generated circuits. We implement an evaluator that\nexecutes gates in parallel by parallelizing each layer of the circuit. We\ndemonstrate the effectiveness of our approach on two key applications: Private\nInformation Retrieval (PIR) and the AES encryption standard. Prior work has\nparallelized AES encryption manually. We demonstrate that the automated method\noutperforms some but not all manual parallelizations of AES evaluations under\nFHE. We achieve an evaluation time of 28 seconds for a parallel execution with\n16 threads and an evaluation time of 8 seconds for a parallel execution with\n100 threads", "AI": {"tldr": "This paper introduces a Haskell transpiler for circuit generation compatible with Fully Homomorphic Encryption (FHE), along with an automatic parallelization method for these circuits, tested on PIR and AES with varying performance outcomes compared to manual methods.", "motivation": "Fully Homomorphic Encryption (FHE) allows computations on encrypted data but requires programs to be expressed as circuits, making application development cumbersome. Current transpilers are limited to a few languages, and manual parallelization is necessary, which is labor-intensive.", "method": "The authors developed a Haskell transpiler that converts high-level Haskell programs into Boolean circuits suitable for FHE. They implemented an evaluator that parallelizes the circuits by executing layer-wise gates, making the process automatic rather than manual.", "result": "The paper demonstrates successful transpilation and parallelization of PIR and AES applications. For AES, the automated parallelization achieved evaluation times of 28 seconds with 16 threads and 8 seconds with 100 threads, outperforming some manual parallelizations.", "conclusion": "While the automated parallelization is effective, manual parallelization still holds advantages in certain scenarios. The Haskell transpiler and automatic circuit parallelization reduce the burden of FHE application development, opening new opportunities but also highlighting the current limitations in performance."}}
{"id": "2510.08355", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.08355", "abs": "https://arxiv.org/abs/2510.08355", "authors": ["Kaustabh Barman", "Fabian Piper", "Sanjeet Raj Pandey", "Axel Kuepper"], "title": "ExPrESSO: Zero-Knowledge backed Extensive Privacy Preserving Single Sign-on", "comment": null, "summary": "User authentication is one of the most important aspects for secure\ncommunication between services and end-users over the Internet. Service\nproviders leverage Single-Sign On (SSO) to make it easier for their users to\nauthenticate themselves. However, standardized systems for SSO, such as OIDC,\ndo not guarantee user privacy as identity providers can track user activities.\nWe propose a zero-knowledge-based mechanism that integrates with OIDC to let\nusers authenticate through SSO without revealing information about the service\nprovider. Our system leverages Groth's zk-SNARK to prove membership of\nsubscribed service providers without revealing their identity. We adopt a\ndecentralized and verifiable approach to set up the prerequisites of our\nconstruction that further secures and establishes trust in the system. We set\nup high security targets and achieve them with minimal storage and latency\ncost, proving that our research can be adopted for production.", "AI": {"tldr": "This paper presents a privacy-preserving SSO solution using zk-SNARKs to hide service provider identities during authentication, achieving strong security and efficiency for real-world deployment.", "motivation": "Standardized SSO systems like OIDC fail to protect user privacy as identity providers can track activities. This motivates the need for privacy-preserving authentication mechanisms.", "method": "The system employs Groth's zk-SNARK to prove membership of subscribed service providers without revealing identities, coupled with a decentralized and verifiable setup to ensure trust and security.", "result": "The approach satisfies stringent security requirements while maintaining low storage and latency overheads, validating its feasibility for production use.", "conclusion": "The proposed zero-knowledge-based mechanism enhances user privacy in Single-Sign On (SSO) systems by integrating with OIDC, enabling secure authentication without exposing service provider information. It achieves high security with minimal performance costs, demonstrating practical deployment potential."}}
{"id": "2510.08479", "categories": ["cs.CR", "cs.OS"], "pdf": "https://arxiv.org/pdf/2510.08479", "abs": "https://arxiv.org/abs/2510.08479", "authors": ["Jinsong Mao", "Benjamin E. Ujcich", "Shiqing Ma"], "title": "Rethinking Provenance Completeness with a Learning-Based Linux Scheduler", "comment": null, "summary": "Provenance plays a critical role in maintaining traceability of a system's\nactions for root cause analysis of security threats and impacts. Provenance\ncollection is often incorporated into the reference monitor of systems to\nensure that an audit trail exists of all events, that events are completely\ncaptured, and that logging of such events cannot be bypassed. However, recent\nresearch has questioned whether existing state-of-the-art provenance collection\nsystems fail to ensure the security guarantees of a true reference monitor due\nto the 'super producer threat' in which provenance generation can overload a\nsystem to force the system to drop security-relevant events and allow an\nattacker to hide their actions. One approach towards solving this threat is to\nenforce resource isolation, but that does not fully solve the problems\nresulting from hardware dependencies and performance limitations.\n  In this paper, we show how an operating system's kernel scheduler can\nmitigate this threat, and we introduce Venus, a learned scheduler for Linux\nspecifically designed for provenance. Unlike conventional schedulers that\nignore provenance completeness requirements, Venus leverages reinforcement\nlearning to learn provenance task behavior and to dynamically optimize resource\nallocation. We evaluate Venus's efficacy and show that Venus significantly\nimproves both the completeness and efficiency of provenance collection systems\ncompared to traditional scheduling, while maintaining reasonable overheads and\neven improving overall runtime in certain cases compared to the default Linux\nscheduler.", "AI": {"tldr": "This paper introduces Venus, a reinforcement learning-based kernel scheduler for Linux that mitigates the 'super producer threat' by improving provenance collection completeness and efficiency. It outperforms traditional schedulers while maintaining low overheads.", "motivation": "Existing provenance systems risk missing security-relevant events under high load (via the 'super producer threat'), and current solutions like resource isolation fail to address hardware and performance limitations effectively.", "method": "Venus is a learned scheduler using reinforcement learning to dynamically optimize resource allocation for provenance tasks. It learns task behaviors and integrates into Linux's kernel scheduler to ensure completeness requirements.", "result": "Venus improves both provenance completeness and efficiency compared to traditional scheduling. It maintains reasonable overheads and, in some cases, enhances overall runtime versus the default Linux scheduler.", "conclusion": "Venus demonstrates that kernel-level machine learning can mitigate the super producer threat, offering a practical solution to maintain secure, complete provenance while adapting to system constraints."}}
{"id": "2510.08496", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.08496", "abs": "https://arxiv.org/abs/2510.08496", "authors": ["Akid Abrar", "Sagar Dasgupta", "Mizanur Rahman", "Ahmad Alsharif"], "title": "AI-Driven Post-Quantum Cryptography for Cyber-Resilient V2X Communication in Transportation Cyber-Physical Systems", "comment": null, "summary": "Transportation Cyber-Physical Systems (TCPS) integrate physical elements,\nsuch as transportation infrastructure and vehicles, with cyber elements via\nadvanced communication technologies, allowing them to interact seamlessly. This\nintegration enhances the efficiency, safety, and sustainability of\ntransportation systems. TCPS rely heavily on cryptographic security to protect\nsensitive information transmitted between vehicles, transportation\ninfrastructure, and other entities within the transportation ecosystem,\nensuring data integrity, confidentiality, and authenticity. Traditional\ncryptographic methods have been employed to secure TCPS communications, but the\nadvent of quantum computing presents a significant threat to these existing\nsecurity measures. Therefore, integrating Post-Quantum Cryptography (PQC) into\nTCPS is essential to maintain secure and resilient communications. While PQC\noffers a promising approach to developing cryptographic algorithms resistant to\nquantum attacks, artificial intelligence (AI) can enhance PQC by optimizing\nalgorithm selection, resource allocation, and adapting to evolving threats in\nreal-time. AI-driven PQC approaches can improve the efficiency and\neffectiveness of PQC implementations, ensuring robust security without\ncompromising system performance. This chapter introduces TCPS communication\nprotocols, discusses the vulnerabilities of corresponding communications to\ncyber-attacks, and explores the limitations of existing cryptographic methods\nin the quantum era. By examining how AI can strengthen PQC solutions, the\nchapter presents cyber-resilient communication strategies for TCPS.", "AI": {"tldr": "The chapter explores the integration of AI-enhanced Post-Quantum Cryptography (PQC) into Transportation Cyber-Physical Systems (TCPS) to address security vulnerabilities in the era of quantum computing while maintaining system performance.", "motivation": "Quantum computing poses a significant threat to traditional cryptographic methods used in TCPS, necessitating the shift to PQC and the use of AI to enhance these algorithms for secure and resilient communications.", "method": "The paper analyzes current TCPS communication protocols, identifies their vulnerabilities to cyber-attacks, illustrates the limitations of traditional cryptographic methods against quantum threats, and then discusses how AI can be leveraged to optimize PQC implementations.", "result": "The analysis shows that the combination of AI and PQC can optimize algorithm selection and resource allocation, adapt to evolving threats, and improve the efficiency and effectiveness of secure communications.", "conclusion": "Integrating AI-driven Post-Quantum Cryptography into TCPS is critical for ensuring robust and future-proof security against quantum threats while preserving system performance."}}
