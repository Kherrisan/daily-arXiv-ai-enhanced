<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 1]
- [cs.SE](#cs.SE) [Total: 5]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Reactive Bottom-Up Testing](https://arxiv.org/abs/2509.03711)
*Siddharth Muralee,Sourag Cherupattamoolayil,James C. Davis,Antonio Bianchi,Aravind Machiry*

Main category: cs.CR

TL;DR: This paper introduces Reactive Bottom-Up Testing, a three-stage method combining fuzzing and symbolic execution to validate software vulnerabilities in context. The Griller implementation successfully detects both known and new vulnerabilities, outperforming existing bottom-up approaches.


<details>
  <summary>Details</summary>
Motivation: Top-down dynamic testing struggles with deep call graph functions, while existing bottom-up approaches face false positives and context-adherent input generation. A systematic method is needed to validate vulnerabilities in isolated and broader program contexts.

Method: The authors propose a three-stage Reactive Bottom-Up Testing scheme: (1) Identify likely-vulnerable functions and generate type/context-aware harnesses; (2) Fuzz testing combined with symbolic execution to extract input constraints; (3) Constraint combination to verify crashes and eliminate false positives. Implemented in Griller.

Result: Griller detected 28/48 known vulnerabilities in 5 open-source projects and discovered 6 new vulnerabilities in real-world applications like Pacman. Results demonstrate improved reachability and triggerability analysis compared to prior approaches.

Conclusion: Reactive Bottom-Up Testing enhances vulnerability detection by integrating function-level testing with context-aware validation, reducing false positives and enabling discovery of deeper vulnerabilities. This approach advances security practices for complex systems.

Abstract: Modern computing systems remain rife with software vulnerabilities. Engineers
apply many means to detect them, of which dynamic testing is one of the most
common and effective. However, most dynamic testing techniques follow a
top-down paradigm, and struggle to reach and exercise functions deep within the
call graph. While recent works have proposed Bottom-Up approaches to address
these limitations, they face challenges with false positives and generating
valid inputs that adhere to the context of the entire program.
  In this work, we introduce a new paradigm that we call Reactive Bottom-Up
Testing. Our insight is that function-level testing is necessary but not
sufficient for the validation of vulnerabilities in functions. What we need is
a systematic approach that not only tests functions in isolation but also
validates their behavior within the broader program context, ensuring that
detected vulnerabilities are both reachable and triggerable. We develop a
three-stage bottom-up testing scheme: (1) identify likely-vulnerable functions
and generate type- and context-aware harnesses; (2) fuzz to find crashes and
extract input constraints via symbolic execution; (3) verify crashes by
combining constraints to remove false positives. We implemented an automated
prototype, which we call Griller. We evaluated Griller in a controlled setting
using a benchmark of 48 known vulnerabilities across 5 open-source projects,
where we successfully detected 28 known vulnerabilities. Additionally, we
evaluated Griller on several real-world applications such as Pacman, and it
discovered 6 previously unknown vulnerabilities. Our findings suggest that
Reactive Bottom-Up Testing can significantly enhance the detection of
vulnerabilities in complex systems, paving the way for more robust security
practices.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Towards the Datasets Used in Requirements Engineering of Mobile Apps: Preliminary Findings from a Systematic Mapping Study](https://arxiv.org/abs/2509.03541)
*Chong Wang,Haoning Wu,Peng Liang,Maya Daneva,Marten van Sinderen*

Main category: cs.SE

TL;DR: This paper analyzes mobile app RE research datasets, finding heavy reliance on Google Play/Apple (90%+), a focus on elicitation/analysis, and calls for diversifying data sources to improve generalizability and expanding research to other RE activities.


<details>
  <summary>Details</summary>
Motivation: The study was motivated by the lack of understanding about the origins and usage of mobile app datasets in requirements engineering research, particularly regarding platform sources and RE activity coverage.

Method: The authors conducted a systematic mapping study following Kitchenham et al.'s guidelines, analyzing 43 selected papers to evaluate datasets and RE activities in mobile app research.

Result: Results show Google Play and Apple App Store provide data for over 90% of mobile app RE research, with requirements elicitation and analysis being the most studied activities. Alternative data sources are beginning to be explored but remain underutilized.

Conclusion: The paper concludes that there is a growing use of datasets in mobile app RE research since 2012, but overreliance on Google Play and Apple App Store may skew findings. It highlights a need for alternative data sources and expanded research into RE activities beyond elicitation and analysis.

Abstract: [Background] Research on requirements engineering (RE) for mobile apps
employs datasets formed by app users, developers or vendors. However, little is
known about the sources of these datasets in terms of platforms and the RE
activities that were researched with the help of the respective datasets.
[Aims] The goal of this paper is to investigate the state-of-the-art of the
datasets of mobile apps used in existing RE research. [Method] We carried out a
systematic mapping study by following the guidelines of Kitchenham et al.
[Results] Based on 43 selected papers, we found that Google Play and Apple App
Store provide the datasets for more than 90% of published research in RE for
mobile apps. We also found that the most investigated RE activities - based on
datasets, are requirements elicitation and requirements analysis. [Conclusions]
Our most important conclusions are: (1) there is a growth in the use of
datasets for RE research of mobile apps since 2012, (2) the RE knowledge for
mobile apps might be skewed due to the overuse of Google Play and Apple App
Store, (3) there are attempts to supplement reviews of apps from repositories
with other data sources, (4) there is a need to expand the alternative sources
and experiments with complimentary use of multiple sources, if the community
wants more generalizable results. Plus, it is expected to expand the research
on other RE activities, beyond elicitation and analysis.

</details>


### [3] [A Multi-stage Error Diagnosis for APB Transaction](https://arxiv.org/abs/2509.03554)
*Cheng-Yang Tsai,Tzu-Wei Huang,Jen-Wei Shih,I-Hsiang Wang,Yu-Cheng Lin,Rung-Bin Lin*

Main category: cs.SE

TL;DR: This paper proposes a hierarchical Random Forest framework for automated APB error diagnosis, achieving 91.36% accuracy and first-place performance in ICCAD 2025 contest beta testing.


<details>
  <summary>Details</summary>
Motivation: Manual analysis of APB transactions in large VCD files for SoC verification is inefficient and error-prone, necessitating automated diagnostic tools for functional debugging tasks.

Method: The study implements a hierarchical Random Forest architecture with four pre-trained binary classifiers. It employs a sequential multi-stage approach to prioritize address-related fault detection (Out-of-Range Access, Address Corruption) before addressing data corruption errors.

Result: The framework achieved 91.36% overall accuracy with near-perfect performance on address errors and robust results on data errors. It secured first place in the ICCAD 2025 contest beta stage despite final results pending.

Conclusion: This research demonstrates that hierarchical machine learning can serve as an effective automated tool for hardware debugging in EDA, particularly for APB transaction error detection.

Abstract: Functional verification and debugging are critical bottlenecks in modern
System-on-Chip (SoC) design, with manual detection of Advanced Peripheral Bus
(APB) transaction errors in large Value Change Dump (VCD) files being
inefficient and error-prone. Addressing the 2025 ICCAD Contest Problem D, this
study proposes an automated error diagnosis framework using a hierarchical
Random Forest-based architecture. The multi-stage error diagnosis employs four
pre-trained binary classifiers to sequentially detect Out-of-Range Access,
Address Corruption, and Data Corruption errors, prioritizing high-certainty
address-related faults before tackling complex data errors to enhance
efficiency. Experimental results show an overall accuracy of 91.36%, with
near-perfect precision and recall for address errors and robust performance for
data errors. Although the final results of the ICCAD 2025 CAD Contest are yet
to be announced as of the submission date, our team achieved first place in the
beta stage, highlighting the method's competitive strength. This research
validates the potential of hierarchical machine learning as a powerful
automated tool for hardware debugging in Electronic Design Automation (EDA).

</details>


### [4] [Parse Tree Tracking Through Time for Programming Process Analysis at Scale](https://arxiv.org/abs/2509.03668)
*Matt Rau,Chris Brown,John Edwards*

Main category: cs.SE

TL;DR: This paper introduces the first algorithms to track parse trees over time from programming logs, enabling scalable analysis of student coding patterns using a large 2021 CS1 dataset. Results reveal novel insights like balanced deletion rates in code structures and reinterpretation of code-jumping behaviors.


<details>
  <summary>Details</summary>
Motivation: Previous analyses of student programming behavior were limited to low-level event logs (e.g., keystrokes) due to the inability to automatically track high-level code structures across time and unparseable states. This hinders understanding nuanced coding behaviors like attention to specific constructs (e.g., loops).

Method: The study introduces two algorithms to track parse tree nodes through time and handle unparseable code states by analyzing public keystroke data from a CS1 course. These algorithms were used to construct and analyze parse trees for student programming behavior.

Result: The study found (1) equal deletion rates in/around control structures (loops/conditionals) versus surrounding code, (2) 33% of commented-out code is eventually restored, and (3) code navigation patterns do not reliably indicate struggle.

Conclusion: The ability to track parse trees through time opens the door to understanding new dimensions of student programming, such as structural development best practices, syntactic struggle measurement, refactoring behavior, and attention patterns within code.

Abstract: Background and Context: Programming process data can be utilized to
understand the processes students use to write computer programming
assignments. Keystroke- and line-level event logs have been used in the past in
various ways, primarily in high-level descriptive statistics (e.g., timings,
character deletion rate, etc). Analysis of behavior in context (e.g., how much
time students spend working on loops) has been cumbersome because of our
inability to automatically track high-level code representations, such as
abstract syntax trees, through time and unparseable states.
  Objective: Our study has two goals. The first is to design the first
algorithm that tracks parse tree nodes through time. Second, we utilize this
algorithm to perform a partial replication study of prior work that used manual
tracking of code representations, as well as other novel analyses of student
programming behavior that can now be done at scale.
  Method: We use two algorithms presented in this paper to track parse tree
nodes through time and construct tree representations for unparseable code
states. We apply these algorithms to a public keystroke data from student
coursework in a 2021 CS1 course and conduct analysis on the resulting parse
trees.
  Findings: We discover newly observable statistics at scale, including that
code is deleted at similar rates inside and outside of conditionals and loops,
a third of commented out code is eventually restored, and that frequency with
which students jump around in their code may not be indicative of struggle.
  Implications: The ability to track parse trees through time opens the door to
understanding new dimensions of student programming, such as best practices of
structural development of code over time, quantitative measurement of what
syntactic constructs students struggle most with, refactoring behavior, and
attention shifting within the code.

</details>


### [5] [Towards an Understanding of Developer Experience-Driven Transparency in Software Ecosystems](https://arxiv.org/abs/2509.03848)
*Rodrigo Oliveira Zacarias,Rodrigo Pereira dos Santos,Patricia Lago*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Software ecosystems (SECO) have become a dominant paradigm in the software
industry, enabling third-party developers to co-create value through
complementary components and services. While Developer Experience (DX) is
increasingly recognized as critical for sustainable SECO, transparency remains
an underexplored factor shaping how developers perceive and interact with
ecosystems. Existing studies acknowledge transparency as essential for trust,
fairness, and engagement, yet its relationship with DX has not been
systematically conceptualized. Hence, this work aims to advance the
understanding of transparency in SECO from a developer-centered perspective. To
this end, we propose SECO-TransDX (Transparency in Software Ecosystems from a
Developer Experience Perspective), a conceptual model that introduces the
notion of DX-driven transparency. The model identifies 63 interrelated
concepts, including conditioning factors, ecosystem procedures, artifacts, and
relational dynamics that influence how transparency is perceived and
constructed during developer interactions. SECO-TransDX was built upon prior
research and refined through a Delphi study with experts from academia and
industry. It offers a structured lens to examine how transparency mediates DX
across technical, social, and organizational layers. For researchers, it lays
the groundwork for future studies and tool development; for practitioners, it
supports the design of trustworthy, developer-centered platforms that improve
transparency and foster long-term engagement in SECO.

</details>


### [6] [VulRTex: A Reasoning-Guided Approach to Identify Vulnerabilities from Rich-Text Issue Report](https://arxiv.org/abs/2509.03875)
*Ziyou Jiang,Mingyang Li,Guowei Yang,Lin Shi,Qing Wang*

Main category: cs.SE

TL;DR: VulRTex: An LLM-powered approach using reasoning and rich-text analysis to accurately identify software vulnerabilities in open-source projects, outperforming prior methods and enabling faster threat detection.


<details>
  <summary>Details</summary>
Motivation: Manual identification of vulnerability-related issue reports is time-consuming and risky, while prior automated methods overlook rich-text contextual information critical for accurate detection.

Method: VulRTex leverages large language models (LLMs) with a Vulnerability Reasoning Database to analyze rich-text information, combining historical case retrieval and reasoning analysis for vulnerability identification.

Result: VulRTex outperforms baselines by +11.0% F1, +20.2% AUPRC, and +10.5% Macro-F1 with 2× lower time cost on 973,572 IRs, and detected 11 emerging vulnerabilities with assigned CVE-IDs in 2024 GitHub projects.

Conclusion: VulRTex demonstrates effectiveness in identifying vulnerability-related IRs through reasoning-guided methods, showing practical applicability and superior performance over existing approaches.

Abstract: Software vulnerabilities exist in open-source software (OSS), and the
developers who discover these vulnerabilities may submit issue reports (IRs) to
describe their details. Security practitioners need to spend a lot of time
manually identifying vulnerability-related IRs from the community, and the time
gap may be exploited by attackers to harm the system. Previously, researchers
have proposed automatic approaches to facilitate identifying these
vulnerability-related IRs, but these works focus on textual descriptions but
lack the comprehensive analysis of IR's rich-text information. In this paper,
we propose VulRTex, a reasoning-guided approach to identify
vulnerability-related IRs with their rich-text information. In particular,
VulRTex first utilizes the reasoning ability of the Large Language Model (LLM)
to prepare the Vulnerability Reasoning Database with historical IRs. Then, it
retrieves the relevant cases from the prepared reasoning database to generate
reasoning guidance, which guides LLM to identify vulnerabilities by reasoning
analysis on target IRs' rich-text information. To evaluate the performance of
VulRTex, we conduct experiments on 973,572 IRs, and the results show that
VulRTex achieves the highest performance in identifying the
vulnerability-related IRs and predicting CWE-IDs when the dataset is
imbalanced, outperforming the best baseline with +11.0% F1, +20.2% AUPRC, and
+10.5% Macro-F1, and 2x lower time cost than baseline reasoning approaches.
Furthermore, VulRTex has been applied to identify 30 emerging vulnerabilities
across 10 representative OSS projects in 2024's GitHub IRs, and 11 of them are
successfully assigned CVE-IDs, which illustrates VulRTex's practicality.

</details>
