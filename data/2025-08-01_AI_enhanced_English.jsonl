{"id": "2507.23229", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.23229", "abs": "https://arxiv.org/abs/2507.23229", "authors": ["Yufei Chen", "Yao Wang", "Haibin Zhang", "Tao Gu"], "title": "Fine-Grained Privacy Extraction from Retrieval-Augmented Generation Systems via Knowledge Asymmetry Exploitation", "comment": null, "summary": "Retrieval-augmented generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge bases, but this advancement introduces\nsignificant privacy risks. Existing privacy attacks on RAG systems can trigger\ndata leakage but often fail to accurately isolate knowledge-base-derived\nsentences within mixed responses. They also lack robustness when applied across\nmultiple domains. This paper addresses these challenges by presenting a novel\nblack-box attack framework that exploits knowledge asymmetry between RAG and\nstandard LLMs to achieve fine-grained privacy extraction across heterogeneous\nknowledge landscapes. We propose a chain-of-thought reasoning strategy that\ncreates adaptive prompts to steer RAG systems away from sensitive content.\nSpecifically, we first decompose adversarial queries to maximize information\ndisparity and then apply a semantic relationship scoring to resolve lexical and\nsyntactic ambiguities. We finally train a neural network on these feature\nscores to precisely identify sentences containing private information. Unlike\nprior work, our framework generalizes to unseen domains through iterative\nrefinement without pre-defined knowledge. Experimental results show that we\nachieve over 91% privacy extraction rate in single-domain and 83% in\nmulti-domain scenarios, reducing sensitive sentence exposure by over 65% in\ncase studies. This work bridges the gap between attack and defense in RAG\nsystems, enabling precise extraction of private information while providing a\nfoundation for adaptive mitigation.", "AI": {"tldr": "This paper introduces a novel black-box attack framework for RAG systems to extract private information with high accuracy (91% single-domain, 83% multi-domain) while generalizing across domains without prior knowledge.", "motivation": "Current RAG privacy attacks struggle with isolating knowledge-base content in mixed responses and lack cross-domain robustness, creating gaps in understanding and mitigating these privacy risks.", "method": "The framework combines adversarial query decomposition to maximize information disparity, semantic relationship scoring to resolve ambiguities, and a neural network trained on these features to identify sensitive sentences. It leverages knowledge asymmetry between RAG and standard LLMs through iterative domain-agnostic refinement.", "result": "Achieved 91% privacy extraction rate in single-domain scenarios, 83% in multi-domain, and reduced sensitive information exposure by >65% in case studies while generalizing to unseen domains.", "conclusion": "The work establishes a foundational approach for both attacking and defending RAG systems by enabling precise privacy extraction and adaptive mitigation, demonstrating robustness against knowledge heterogeneity."}}
{"id": "2507.23453", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23453", "abs": "https://arxiv.org/abs/2507.23453", "authors": ["Lijia Liu", "Takumi Kondo", "Kyohei Atarashi", "Koh Takeuchi", "Jiyi Li", "Shigeru Saito", "Hisashi Kashima"], "title": "Counterfactual Evaluation for Blind Attack Detection in LLM-based Evaluation Systems", "comment": null, "summary": "This paper investigates defenses for LLM-based evaluation systems against\nprompt injection. We formalize a class of threats called blind attacks, where a\ncandidate answer is crafted independently of the true answer to deceive the\nevaluator. To counter such attacks, we propose a framework that augments\nStandard Evaluation (SE) with Counterfactual Evaluation (CFE), which\nre-evaluates the submission against a deliberately false ground-truth answer.\nAn attack is detected if the system validates an answer under both standard and\ncounterfactual conditions. Experiments show that while standard evaluation is\nhighly vulnerable, our SE+CFE framework significantly improves security by\nboosting attack detection with minimal performance trade-offs.", "AI": {"tldr": "This paper introduces a framework to defend LLM evaluation systems from 'blind attacks' by combining standard evaluation with counterfactual re-evaluation using false ground truths, improving security with minimal performance impact.", "motivation": "LLM evaluation systems are vulnerable to prompt injection attacks, specifically 'blind attacks' where candidate answers are crafted to deceive evaluators without reference to the true answer, threatening evaluation integrity.", "method": "The authors formalize blind attacks and propose a framework that augments Standard Evaluation (SE) with Counterfactual Evaluation (CFE), which re-evaluates submissions against deliberately false ground truths, detecting attacks when answers pass both evaluations.", "result": "Experiments demonstrated that standard evaluation is highly vulnerable to blind attacks, while the SE+CFE framework significantly enhances security against these attacks with minimal performance trade-offs.", "conclusion": "The SE+CFE framework effectively mitigates blind attacks on LLM evaluation systems by introducing counterfactual re-evaluations, offering a robust and practical defense with negligible efficiency costs."}}
{"id": "2507.23611", "categories": ["cs.CR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23611", "abs": "https://arxiv.org/abs/2507.23611", "authors": ["Estelle Ruellan", "Eric Clay", "Nicholas Ascoli"], "title": "LLM-Based Identification of Infostealer Infection Vectors from Screenshots: The Case of Aurora", "comment": null, "summary": "Infostealers exfiltrate credentials, session cookies, and sensitive data from\ninfected systems. With over 29 million stealer logs reported in 2024, manual\nanalysis and mitigation at scale are virtually unfeasible/unpractical. While\nmost research focuses on proactive malware detection, a significant gap remains\nin leveraging reactive analysis of stealer logs and their associated artifacts.\nSpecifically, infection artifacts such as screenshots, image captured at the\npoint of compromise, are largely overlooked by the current literature. This\npaper introduces a novel approach leveraging Large Language Models (LLMs), more\nspecifically gpt-4o-mini, to analyze infection screenshots to extract potential\nIndicators of Compromise (IoCs), map infection vectors, and track campaigns.\nFocusing on the Aurora infostealer, we demonstrate how LLMs can process\nscreenshots to identify infection vectors, such as malicious URLs, installer\nfiles, and exploited software themes. Our method extracted 337 actionable URLs\nand 246 relevant files from 1000 screenshots, revealing key malware\ndistribution methods and social engineering tactics. By correlating extracted\nfilenames, URLs, and infection themes, we identified three distinct malware\ncampaigns, demonstrating the potential of LLM-driven analysis for uncovering\ninfection workflows and enhancing threat intelligence. By shifting malware\nanalysis from traditional log-based detection methods to a reactive,\nartifact-driven approach that leverages infection screenshots, this research\npresents a scalable method for identifying infection vectors and enabling early\nintervention.", "AI": {"tldr": "This paper proposes using LLMs (specifically gpt-4o-mini) to analyze infection screenshots for IoCs, infection vectors, and campaign tracking, addressing scalability challenges in malware log analysis.", "motivation": "Manual analysis of over 29 million stealer logs is unfeasible, and reactive methods leveraging infection artifacts like screenshots remain underexplored in existing research.", "method": "A novel LLM-driven approach uses gpt-4o-mini to process infection screenshots, extracting IoCs, mapping vectors, and correlating filenames/URLs/themes to identify campaigns.", "result": "The method extracted 337 actionable URLs and 246 files from 1,000 screenshots, revealing malware distribution methods, social engineering tactics, and three distinct campaigns by correlating artifacts.", "conclusion": "LLM-based artifact analysis offers a scalable solution for identifying infection vectors through reactive methods, enabling earlier threat detection and improved threat intelligence."}}
{"id": "2507.23641", "categories": ["cs.CR", "11T71, 94A60"], "pdf": "https://arxiv.org/pdf/2507.23641", "abs": "https://arxiv.org/abs/2507.23641", "authors": ["Michael Schaller"], "title": "Polynomial Lattices for the BIKE Cryptosystem", "comment": null, "summary": "In this paper we introduce a rank $2$ lattice over a polynomial ring arising\nfrom the public key of the BIKE cryptosystem \\cite{aragon2022bike}. The secret\nkey is a sparse vector in this lattice. We study properties of this lattice and\ngeneralize the recovery of weak keys from \\cite{BardetDLO16}. In particular, we\nshow that they implicitly solved a shortest vector problem in the lattice we\nconstructed. Rather than finding only a shortest vector, we obtain a reduced\nbasis of the lattice which makes it possible to check for more weak keys.", "AI": {"tldr": "This paper constructs a rank-2 lattice over a polynomial ring from the BIKE cryptosystem's public key, generalizes weak key recovery techniques, and shows that previous approaches implicitly solved shortest vector (SVP) problems in this lattice to enable more extensive weak key detection via reduced basis computation.", "motivation": "Analyzing the security of the BIKE cryptosystem by modeling its secret key as a sparse vector in a lattice structure, building upon prior work to improve weak key detection capabilities.", "method": "1. Construct a rank-2 polynomial ring lattice from BIKE public keys. 2. Generalize the weak key recovery approach from Bardet et al. (2016) by solving the shortest vector problem (SVP). 3. Compute reduced lattice bases instead of just finding shortest vectors.", "result": "Demonstration that previous weak key recovery implicitly involved SVP resolution in the constructed lattice, with improved results through reduced basis computation that reveals more potential weak keys.", "conclusion": "The ability to compute reduced bases in these lattices enhances weak key detection for BIKE cryptosystems, suggesting the need for new countermeasures against SVP-based lattice attacks beyond existing sparse key protections."}}
{"id": "2507.23087", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23087", "abs": "https://arxiv.org/abs/2507.23087", "authors": ["Fabian Stiehle", "Hans Weytjens", "Ingo Weber"], "title": "On LLM-Assisted Generation of Smart Contracts from Business Processes", "comment": "Accepted at the Workshop on Distributed Ledger Technologies in\n  Business Process Management, At the International Conference for Business\n  Process Management (BPM), 2025", "summary": "Large language models (LLMs) have changed the reality of how software is\nproduced. Within the wider software engineering community, among many other\npurposes, they are explored for code generation use cases from different types\nof input. In this work, we present an exploratory study to investigate the use\nof LLMs for generating smart contract code from business process descriptions,\nan idea that has emerged in recent literature to overcome the limitations of\ntraditional rule-based code generation approaches. However, current LLM-based\nwork evaluates generated code on small samples, relying on manual inspection,\nor testing whether code compiles but ignoring correct execution. With this\nwork, we introduce an automated evaluation framework and provide empirical data\nfrom larger data sets of process models. We test LLMs of different types and\nsizes in their capabilities of achieving important properties of process\nexecution, including enforcing process flow, resource allocation, and\ndata-based conditions. Our results show that LLM performance falls short of the\nperfect reliability required for smart contract development. We suggest future\nwork to explore responsible LLM integrations in existing tools for code\ngeneration to ensure more reliable output. Our benchmarking framework can serve\nas a foundation for developing and evaluating such integrations.", "AI": {"tldr": "This paper explores using LLMs for smart contract code generation from business process descriptions, introduces an automated evaluation framework, and shows LLMs currently lack the reliability required. It suggests integrating LLMs responsibly into existing tools for more dependable outputs.", "motivation": "Existing code generation approaches for smart contracts (rule-based and manually Inspected LLM-based methods) are limited in reliability and scalability. There's a need for robust evaluation tools to ensure process execution correctness in smart contracts.", "method": "The authors developed an automated evaluation framework that tests LLM-generated code against process execution properties (flow, resource allocation, data conditions) using large datasets of process models to assess LLMs of varying types and sizes.", "result": "Empirical data reveals LLM performance fails to meet the reliability standards necessary for smart contract development, despite their wide adoption in code generation. The framework provides scalable, systematic evaluation capabilities that were previously lacking.", "conclusion": "LLMs require integration with domain-specific constraints and validation tools to achieve reliability in smart contract code generation. The proposed benchmarking framework enables future research in this area and establishes evaluation criteria for responsible LLM implementation."}}
{"id": "2507.23118", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23118", "abs": "https://arxiv.org/abs/2507.23118", "authors": ["Mattia Di Profio", "Mingjun Zhong", "Yaji Sripada", "Marcel Jaspars"], "title": "FlowETL: An Autonomous Example-Driven Pipeline for Data Engineering", "comment": null, "summary": "The Extract, Transform, Load (ETL) workflow is fundamental for populating and\nmaintaining data warehouses and other data stores accessed by analysts for\ndownstream tasks. A major shortcoming of modern ETL solutions is the extensive\nneed for a human-in-the-loop, required to design and implement\ncontext-specific, and often non-generalisable transformations. While related\nwork in the field of ETL automation shows promising progress, there is a lack\nof solutions capable of automatically designing and applying these\ntransformations. We present FlowETL, a novel example-based autonomous ETL\npipeline architecture designed to automatically standardise and prepare input\ndatasets according to a concise, user-defined target dataset. FlowETL is an\necosystem of components which interact together to achieve the desired outcome.\nA Planning Engine uses a paired input-output datasets sample to construct a\ntransformation plan, which is then applied by an ETL worker to the source\ndataset. Monitoring and logging provide observability throughout the entire\npipeline. The results show promising generalisation capabilities across 14\ndatasets of various domains, file structures, and file sizes.", "AI": {"tldr": "FlowETL is an autonomous ETL pipeline architecture that automatically standardizes datasets using example-based plans, reducing human-in-the-loop requirements.", "motivation": "Modern ETL workflows require significant manual effort for context-specific transformations, and existing automation lacks the ability to design and apply these transformations effectively.", "method": "FlowETL employs a Planning Engine to generate transformation plans from paired input-output dataset samples, an ETL worker to execute the plan, and monitoring components for observability.", "result": "Demonstrates promising generalization across 14 datasets with diverse domains (1.5MB-50GB), file formats, and transformation complexity.", "conclusion": "FlowETL advances ETL automation by providing a scalable, example-based framework capable of handling diverse data standardization tasks with minimal human intervention."}}
{"id": "2507.23120", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23120", "abs": "https://arxiv.org/abs/2507.23120", "authors": ["Jordi Cabot"], "title": "Vibe Modeling: Challenges and Opportunities", "comment": null, "summary": "There is a pressing need for better development methods and tools to keep up\nwith the growing demand and increasing complexity of new software systems. New\ntypes of user interfaces, the need for intelligent components, sustainability\nconcerns, ... bring new challenges that we need to handle. In the last years,\nmodel-driven engineering (MDE) has been key to improving the quality and\nproductivity of software development, but models themselves are becoming\nincreasingly complex to specify and manage. At the same time, we are witnessing\nthe growing popularity of vibe coding approaches that rely on Large Language\nModels (LLMs) to transform natural language descriptions into running code at\nthe expenses of code vulnerabilities, scalability issues and maintainability\nconcerns. In this paper, we introduce the concept of \\textit{vibe modeling} as\na novel approach to integrate the best of both worlds (AI and MDE) to speed up\nthe development of reliable complex systems. We outline the key concepts of\nvibe modeling and highlight the opportunities and open challenges it presents\nfor the future of modeling.", "AI": {"tldr": "This paper proposes 'vibe modeling' as a hybrid approach combining AI-driven methods and model-driven engineering (MDE) to address complexity in modern software systems, balancing speed with reliability.", "motivation": "Software systems face challenges from complex requirements (new UIs, intelligent components, sustainability) while traditional MDE struggles with model complexity and LLM-based 'vibe coding' has shortcomings in code quality and maintainability.", "method": "The authors formalize 'vibe modeling' as a framework integrating large language models into the model-driven engineering workflow, leveraging natural language descriptions for model generation while retaining MDE's rigor.", "result": "Preliminary identification of core concepts, opportunities for AI-enhanced modeling, and critical challenges in ensuring system reliability, scalability, and maintainability.", "conclusion": "Vibe modeling represents a dual-opportunity space: AI can streamline model creation while MDE ensures structure, but overcoming technical and methodological barriers will determine its success in complex system development."}}
{"id": "2507.23168", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23168", "abs": "https://arxiv.org/abs/2507.23168", "authors": ["Elmira Onagh", "Maleknaz Nayebi"], "title": "Extension Decisions in Open Source Software Ecosystem", "comment": "Paper published in JSS journal", "summary": "GitHub Marketplace is expanding by approximately 41% annually, with new\ntools; however, many additions replicate existing functionality. We study this\nphenomenon in the platform's largest segment, Continuous Integration (CI), by\nlinking 6,983 CI Actions to 3,869 providers and mining their version histories.\nOur graph model timestamps every functionality's debut, tracks its adoption,\nand clusters redundant tools. We find that approximately 65% of new CI Actions\nreplicate existing capabilities, typically within six months, and that a small\nset of first-mover Actions accounts for most subsequent forks and extensions.\nThese insights enable developers to choose the optimal moment to launch, target\nunmet functionality, and help maintainers eliminate redundant tools. We publish\nthe complete graph and dataset to encourage longitudinal research on innovation\nand competition in software ecosystems, and to provide practitioners with a\ndata-driven roadmap for identifying emerging trends and guiding product\nstrategy.", "AI": {"tldr": "The paper analyzes redundancy in GitHub Marketplace's CI tools, finding 65% of new Actions replicate existing capabilities within six months. It proposes methods to track functionality evolution and eliminate redundancies.", "motivation": "GitHub Marketplace's rapid growth (41% annual expansion) creates redundancy with many tools replicating existing CI functionalities, necessitating better understanding and management of this phenomenon.", "method": "The authors link 6,983 CI Actions to 3,869 providers, mine version histories, and create a graph model to timestamp functionality debut, track adoption trends, and cluster redundant tools.", "result": "65% of new CI Actions replicate existing capabilities within six months, first-mover tools dominate forks/extensions, and the analysis reveals patterns of replication and competition in the CI ecosystem.", "conclusion": "The findings suggest strategies for developers to time launches and target unmet needs, while enabling maintainers to deprioritize redundant tools. The dataset is published for further research on software ecosystem dynamics."}}
{"id": "2507.23178", "categories": ["cs.SE", "cs.AI", "I.2.5"], "pdf": "https://arxiv.org/pdf/2507.23178", "abs": "https://arxiv.org/abs/2507.23178", "authors": ["Siyuan Liu", "Zhice Yang", "Huangxun Chen"], "title": "AutoBridge: Automating Smart Device Integration with Centralized Platform", "comment": "14 pages, 12 figures, under review", "summary": "Multimodal IoT systems coordinate diverse IoT devices to deliver\nhuman-centered services. The ability to incorporate new IoT devices under the\nmanagement of a centralized platform is an essential requirement. However, it\nrequires significant human expertise and effort to program the complex IoT\nintegration code that enables the platform to understand and control the device\nfunctions. Therefore, we propose AutoBridge to automate IoT integration code\ngeneration. Specifically, AutoBridge adopts a divide-and-conquer strategy: it\nfirst generates device control logic by progressively retrieving\ndevice-specific knowledge, then synthesizes platformcompliant integration code\nusing platform-specific knowledge. To ensure correctness, AutoBridge features a\nmulti-stage debugging pipeline, including an automated debugger for virtual IoT\ndevice testing and an interactive hardware-in-the-loop debugger that requires\nonly binary user feedback (yes and no) for real-device verification. We\nevaluate AutoBridge on a benchmark of 34 IoT devices across two open-source IoT\nplatforms. The results demonstrate that AutoBridge can achieves an average\nsuccess rate of 93.87% and an average function coverage of 94.87%, without any\nhuman involvement. With minimal binary yes and no feedback from users, the code\nis then revised to reach 100% function coverage. A user study with 15\nparticipants further shows that AutoBridge outperforms expert programmers by\n50% to 80% in code accuracy, even when the programmers are allowed to use\ncommercial code LLMs.", "AI": {"tldr": "AutoBridge automates IoT integration code generation using a divide-and-conquer strategy and multi-stage debugging pipeline, achieving 93.87% success rate and 94.87% function coverage with minimal human feedback.", "motivation": "Manual programming of complex IoT integration code for centralized platforms requires significant human expertise and effort, motivating the need for automation.", "method": "AutoBridge employs a divide-and-conquer approach by 1) generating device control logic through progressive retrieval of device-specific knowledge and 2) synthesizing platform-compliant code using platform-specific knowledge. It includes two-stage debugging: automated virtual IoT device testing and hardware-in-the-loop debugging with binary user feedback.", "result": "Evaluates on 34 IoT devices across two platforms: 93.87% average success rate / 94.87% function coverage with no human input, reaching 100% coverage with minimal binary feedback. User study shows outperforms expert programmers by 50-80% accuracy.", "conclusion": "AutoBridge enables significantly more accurate automated IoT integration code generation compared to human experts using commercial code LLMs, demonstrating potential as a new standard in multimodal IoT development."}}
{"id": "2507.23269", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.23269", "abs": "https://arxiv.org/abs/2507.23269", "authors": ["Peter Fettke", "Fabiana Fournier", "Lior Limonad", "Andreas Metzger", "Stefanie Rinderle-Ma", "Barbara Weber"], "title": "XABPs: Towards eXplainable Autonomous Business Processes", "comment": null, "summary": "Autonomous business processes (ABPs), i.e., self-executing workflows\nleveraging AI/ML, have the potential to improve operational efficiency, reduce\nerrors, lower costs, improve response times, and free human workers for more\nstrategic and creative work. However, ABPs may raise specific concerns\nincluding decreased stakeholder trust, difficulties in debugging, hindered\naccountability, risk of bias, and issues with regulatory compliance. We argue\nfor eXplainable ABPs (XABPs) to address these concerns by enabling systems to\narticulate their rationale. The paper outlines a systematic approach to XABPs,\ncharacterizing their forms, structuring explainability, and identifying key BPM\nresearch challenges towards XABPs.", "AI": {"tldr": "The paper introduces eXplainable Autonomous Business Processes (XABPs) to address concerns like trust, accountability, and regulatory compliance in AI/ML-driven workflows by enabling systematic articulation of rationale.", "motivation": "Autonomous Business Processes (ABPs) offer operational efficiency and error reduction but face challenges in stakeholder trust, debugging, accountability, bias, and compliance. The motivation is to address these limitations through explainability while maintaining autonomy.", "method": "The authors present a systematic approach to XABPs, including (1) characterizing forms of XABPs, (2) structuring explainability components, and (3) identifying key BPM research challenges for implementing XABPs.", "result": "The paper proposes a structured framework for XABPs, categorizes different explainability forms, defines structural requirements for rationale articulation, and highlights foundational BPM research questions in this domain.", "conclusion": "XABPs provide a critical pathway to balance autonomy with transparency in business workflows. The conclusion emphasizes the need for targeted research to formalize BPM methodologies addressing explainability, accountability, and regulatory demands in AI-driven processes."}}
{"id": "2507.23348", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23348", "abs": "https://arxiv.org/abs/2507.23348", "authors": ["Han Li", "Yuling Shi", "Shaoxin Lin", "Xiaodong Gu", "Heng Lian", "Xin Wang", "Yantao Jia", "Tao Huang", "Qianxiang Wang"], "title": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution", "comment": "Our code and data are available at\n  https://github.com/YerbaPage/SWE-Debate", "summary": "Issue resolution has made remarkable progress thanks to the advanced\nreasoning capabilities of large language models (LLMs). Recently, agent-based\nframeworks such as SWE-agent have further advanced this progress by enabling\nautonomous, tool-using agents to tackle complex software engineering tasks.\nWhile existing agent-based issue resolution approaches are primarily based on\nagents' independent explorations, they often get stuck in local solutions and\nfail to identify issue patterns that span across different parts of the\ncodebase. To address this limitation, we propose SWE-Debate, a competitive\nmulti-agent debate framework that encourages diverse reasoning paths and\nachieves more consolidated issue localization. SWE-Debate first creates\nmultiple fault propagation traces as localization proposals by traversing a\ncode dependency graph. Then, it organizes a three-round debate among\nspecialized agents, each embodying distinct reasoning perspectives along the\nfault propagation trace. This structured competition enables agents to\ncollaboratively converge on a consolidated fix plan. Finally, this consolidated\nfix plan is integrated into an MCTS-based code modification agent for patch\ngeneration. Experiments on the SWE-bench benchmark show that SWE-Debate\nachieves new state-of-the-art results in open-source agent frameworks and\noutperforms baselines by a large margin.", "AI": {"tldr": "SWE-Debate is a competitive multi-agent debate framework for issue resolution in software engineering, improving over SWE-agent by using code dependency graphs and structured debates among specialized agents to avoid local solutions.", "motivation": "Current agent-based issue resolution approaches rely on independent exploration, leading to local solutions and failure to identify issue patterns across different codebase parts.", "method": "SWE-Debate creates multiple fault propagation traces via code dependency graph traversal, organizes three-round debates among agents with distinct reasoning perspectives along these traces, and integrates the consensus into an MCTS-based code modification agent for patch generation.", "result": "Experiments on the SWE-bench benchmark show SWE-Debate achieves state-of-the-art results in open-source agent frameworks and outperforms baselines by a large margin.", "conclusion": "The structured competition and diverse reasoning paths in SWE-Debate enable more consolidated issue localization and effective resolution for complex software engineering tasks."}}
{"id": "2507.23356", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23356", "abs": "https://arxiv.org/abs/2507.23356", "authors": ["Shmulik Froimovich", "Raviv Gal", "Wesam Ibraheem", "Avi Ziv"], "title": "Quality Evaluation of COBOL to Java Code Transformation", "comment": "Submitted to ASE 2025", "summary": "We present an automated evaluation system for assessing COBOL-to-Java code\ntranslation within IBM's watsonx Code Assistant for Z (WCA4Z). The system\naddresses key challenges in evaluating LLM-based translators, including model\nopacity and the complexity of translation quality assessment. Our approach\ncombines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver\nscalable, multi-faceted evaluations. The system supports continuous integration\nworkflows, enables large-scale benchmarking, and reduces reliance on manual\nreview. We describe the system architecture, evaluation strategies, and\nreporting mechanisms that provide actionable insights for developers and\nproject managers, facilitating the evolution of high-quality, modernized\ncodebases.", "AI": {"tldr": "An automated evaluation system for COBOL-to-Java translation in IBM's watsonx Code Assistant for Z combined analytic checkers and LLM-as-a-judge techniques to address translation quality assessment challenges.", "motivation": "The paper addresses challenges in evaluating LLM-based translators, particularly COBOL-to-Java translation, which faces model opacity and complex quality assessment requirements due to legacy system modernization needs.", "method": "The approach integrates analytic checkers (for syntactic/structural validation) with LLM-as-a-judge (LaaJ) techniques (for semantic evaluation) within continuous integration workflows. This combines automated benchmarking with scalable evaluation metrics.", "result": "The resulting system provides multi-faceted evaluations, actionable insights for developers and project managers, and significantly reduces manual review requirements while enabling large-scale translation validation.", "conclusion": "This automated evaluation framework offers a scalable solution for evaluating code translators, demonstrating the effectiveness of combining domain-specific analytic tools with LLM-driven evaluation to modernize legacy mainframe systems."}}
{"id": "2507.23361", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23361", "abs": "https://arxiv.org/abs/2507.23361", "authors": ["Silin Chen", "Shaoxin Lin", "Xiaodong Gu", "Yuling Shi", "Heng Lian", "Longfei Yun", "Dong Chen", "Weiguo Sun", "Lin Cao", "Qianxiang Wang"], "title": "SWE-Exp: Experience-Driven Software Issue Resolution", "comment": "Our code and data are available at\n  https://github.com/YerbaPage/SWE-Exp", "summary": "Recent advances in large language model (LLM) agents have shown remarkable\nprogress in software issue resolution, leveraging advanced techniques such as\nmulti-agent collaboration and Monte Carlo Tree Search (MCTS). However, current\nagents act as memoryless explorers - treating each problem separately without\nretaining or reusing knowledge from previous repair experiences. This leads to\nredundant exploration of failed trajectories and missed chances to adapt\nsuccessful issue resolution methods to similar problems. To address this\nproblem, we introduce SWE-Exp, an experience - enhanced approach that distills\nconcise and actionable experience from prior agent trajectories, enabling\ncontinuous learning across issues. Our method introduces a multi-faceted\nexperience bank that captures both successful and failed repair attempts.\nSpecifically, it extracts reusable issue resolution knowledge at different\nlevels - from high-level problem comprehension to specific code changes.\nExperiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%\nPass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach\nestablishes a new paradigm in which automated software engineering agents\nsystematically accumulate and leverage repair expertise, fundamentally shifting\nfrom trial-and-error exploration to strategic, experience-driven issue\nresolution.", "AI": {"tldr": "Introduces SWE-Exp, an experience-enhanced LLM agent framework for software issue resolution that systematically captures and reuses prior repair knowledge through distilled experience banks, achieving state-of-the-art 41.6% Pass@1 rate on SWE-bench-Verified.", "motivation": "Current LLM agents for software engineering operate as memoryless systems, repeatedly performing redundant exploration of failed solutions and failing to repurpose successful repair patterns for similar problems, leading to inefficiencies in issue resolution.", "method": "SWE-Exp implements a multi-faceted experience bank distilling reusable knowledge from both successful and failed agent trajectories across hierarchical abstraction levels. This includes high-level problem understanding patterns and concrete code modification strategies through a continuous learning mechanism.", "result": "Achieves 41.6% Pass@1 resolution rate on SWE-bench-Verified benchmark, outperforming prior memoryless approaches by systematically leveraging distilled experience rather than relying solely on brute-force exploration.", "conclusion": "Establishes a fundamental shift in automated software engineering by enabling agents to strategically accumulate and apply repair expertise through experience banks, transitioning from trial-and-error exploration to knowledge-guided issue resolution."}}
{"id": "2507.23370", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23370", "abs": "https://arxiv.org/abs/2507.23370", "authors": ["Trae Research Team", "Pengfei Gao", "Zhao Tian", "Xiangxin Meng", "Xinchen Wang", "Ruida Hu", "Yuanan Xiao", "Yizhou Liu", "Zhao Zhang", "Junjie Chen", "Cuiyun Gao", "Yun Lin", "Yingfei Xiong", "Chao Peng", "Xia Liu"], "title": "Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling", "comment": "Pengfei Gao and Zhao Tian contributed equally to this technical\n  report", "summary": "Software issue resolution is a critical challenge in software engineering and\nhas garnered increasing attention in recent years. With the rapid advancement\nof large language models (LLMs), substantial progress has been made in\naddressing real-world software engineering tasks. Recent studies have\nintroduced ensemble reasoning techniques to enhance the performance of\nLLM-based issue resolution. However, existing prompting-based methods still\nface limitations in effectively exploring large ensemble spaces and lack the\ncapacity for repository-level understanding, both of which constrain their\noverall effectiveness. In this paper, we propose Trae Agent, the first\nagent-based ensemble reasoning approach for repository-level issue resolution.\nTrae Agent formulates our goal as an optimal solution search problem and\naddresses two key challenges, i.e., large ensemble spaces and repository-level\nunderstanding, through modular agents for generation, pruning, and selection.\nWe conduct extensive experiments using three leading LLMs on the widely-adopted\nSWE-bench benchmark, comparing Trae Agent against four state-of-the-art\nensemble reasoning techniques. Experimental results demonstrate that Trae Agent\nconsistently achieves superior performance, with an average improvement of\n10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first\nplace on the SWE-bench Verified leaderboard, with a notable Pass@1 score of\n75.20%. We are pleased to release Trae Agent as an open-source project to\nsupport the research community, with all resources available at\nhttps://github.com/bytedance/trae-agent.", "AI": {"tldr": "Trae Agent introduces an agent-based ensemble reasoning approach for repository-level software issue resolution, achieving first-place results on SWE-bench with 75.20% Pass@1 and outperforming existing methods by 10.22%.", "motivation": "Current LLM-based issue resolution methods struggle with exploring large ensemble spaces and lack repository-level understanding, limiting their effectiveness.", "method": "Trae Agent formulates issue resolution as an optimal solution search problem using modular agents for generation, pruning, and selection, enabling efficient exploration of ensemble spaces and repository-level analysis.", "result": "Outperforms four baselines with 10.22% average Pass@1 improvement on SWE-bench, achieving a 75.20% Pass@1 score with three leading LLMs.", "conclusion": "Trae Agent demonstrates significant advances in repository-level issue resolution through its agent-based ensemble framework and is released as open-source to support further research."}}
{"id": "2507.23425", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23425", "abs": "https://arxiv.org/abs/2507.23425", "authors": ["Daphn\u00e9 Larrivain", "Shinhyung Yang", "Wilhelm Hasselbring"], "title": "Dynamic and Static Analysis of Python Software with Kieker Including Reconstructed Architectures", "comment": "9 pages, 9 figures", "summary": "The Kieker observability framework is a tool that provides users with the\nmeans to design a custom observability pipeline for their application.\nOriginally tailored for Java, supporting Python with Kieker is worthwhile.\nPython's popularity has exploded over the years, thus making structural\ninsights of Python applications highly valuable. Our Python analysis pipeline\ncombines static and dynamic analysis in order to build a complete picture of a\ngiven system.", "AI": {"tldr": "This paper introduces Python support in the Kieker observability framework, combining static and dynamic analysis to provide structural insights into Python applications.", "motivation": "Python's popularity has grown significantly, making structural analysis of Python applications valuable. Kieker, originally designed for Java, needed to support Python to meet this demand.", "method": "The proposed Python analysis pipeline integrates static and dynamic analysis techniques to create a comprehensive system overview.", "result": "The pipeline enables users to design custom observability pipelines for Python applications, offering detailed structural insights similar to Kieker's Java capabilities.", "conclusion": "Supporting Python with Kieker's combined static-dynamic analysis approach is beneficial for observability, leveraging Python's widespread use while maintaining the framework's effectiveness."}}
{"id": "2507.23640", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23640", "abs": "https://arxiv.org/abs/2507.23640", "authors": ["Samah Kansab", "Mohammed Sayagh", "Francis Bordeleau", "Ali Tizghadam"], "title": "An Empirical Study on the Amount of Changes Required for Merge Request Acceptance", "comment": null, "summary": "Code review (CR) is essential to software development, helping ensure that\nnew code is properly integrated. However, the CR process often involves\nsignificant effort, including code adjustments, responses to reviewers, and\ncontinued implementation. While past studies have examined CR delays and\niteration counts, few have investigated the effort based on the volume of code\nchanges required, especially in the context of GitLab Merge Requests (MRs),\nwhich remains underexplored. In this paper, we define and measure CR effort as\nthe amount of code modified after submission, using a dataset of over 23,600\nMRs from four GitLab projects. We find that up to 71% of MRs require\nadjustments after submission, and 28% of these involve changes to more than 200\nlines of code. Surprisingly, this effort is not correlated with review time or\nthe number of participants. To better understand and predict CR effort, we\ntrain an interpretable machine learning model using metrics across multiple\ndimensions: text features, code complexity, developer experience, review\nhistory, and branching. Our model achieves strong performance (AUC 0.84-0.88)\nand reveals that complexity, experience, and text features are key predictors.\nHistorical project characteristics also influence current review effort. Our\nfindings highlight the feasibility of using machine learning to explain and\nanticipate the effort needed to integrate code changes during review.", "AI": {"tldr": "This study defines code review effort in GitLab Merge Requests as post-submission code changes and uses ML to identify key predictors like complexity and developer experience, achieving high model accuracy (AUC 0.84-0.88).", "motivation": "Existing research on code review emphasizes delays and iterations but lacks empirical analysis of actual effort from code modification volume, particularly in GitLab Merge Requests. The paper aims to address this gap for better understanding and prediction.", "method": "1. Defined CR effort as post-submission code changes using 23,600+ GitLab MRs from four projects\n2. Trained an interpretable ML model with features from text, code complexity, developer experience, review history, and branching\n3. Analyzed metrics across five dimensions to predict effort requirements", "result": "1. 71%% of MRs require post-submission adjustments (28%% with >200 LOC changes)\n2. Machine learning model achieved 0.84-0.88 AUC for effort prediction\n3. Key predictors were code complexity, developer experience, text features, and historical project patterns\n4. Review effort not correlated with review duration or participant count", "conclusion": "1. CR effort can be effectively explained and predicted using interpretable ML models\n2. Complexity metrics and historical patterns provide critical insights\n3. Text features and developer experience show significant impact on review effort\n4. Demonstrates new approaches for improving software integration processes"}}
