{"id": "2510.06343", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06343", "abs": "https://arxiv.org/abs/2510.06343", "authors": ["Fikret Mert G\u00fcltekin", "Oscar Lilja", "Ranim Khojah", "Rebekka Wohlrab", "Marvin Damschen", "Mazen Mohamad"], "title": "Leveraging Large Language Models for Cybersecurity Risk Assessment -- A Case from Forestry Cyber-Physical Systems", "comment": "Accepted at Autonomous Agents in Software Engineering (AgenticSE)\n  Workshop, co-located with ASE 2025", "summary": "In safety-critical software systems, cybersecurity activities become\nessential, with risk assessment being one of the most critical. In many\nsoftware teams, cybersecurity experts are either entirely absent or represented\nby only a small number of specialists. As a result, the workload for these\nexperts becomes high, and software engineers would need to conduct\ncybersecurity activities themselves. This creates a need for a tool to support\ncybersecurity experts and engineers in evaluating vulnerabilities and threats\nduring the risk assessment process. This paper explores the potential of\nleveraging locally hosted large language models (LLMs) with retrieval-augmented\ngeneration to support cybersecurity risk assessment in the forestry domain\nwhile complying with data protection and privacy requirements that limit\nexternal data sharing. We performed a design science study involving 12 experts\nin interviews, interactive sessions, and a survey within a large-scale project.\nThe results demonstrate that LLMs can assist cybersecurity experts by\ngenerating initial risk assessments, identifying threats, and providing\nredundancy checks. The results also highlight the necessity for human oversight\nto ensure accuracy and compliance. Despite trust concerns, experts were willing\nto utilize LLMs in specific evaluation and assistance roles, rather than solely\nrelying on their generative capabilities. This study provides insights that\nencourage the use of LLM-based agents to support the risk assessment process of\ncyber-physical systems in safety-critical domains.", "AI": {"tldr": "The paper investigates using LLMs to help with cybersecurity risk assessments in the forestry domain, emphasizing both security and privacy. It found that LLMs can generate initial assessments and identify threats but need expert oversight for accuracy and compliance.", "motivation": "There is a shortage of cybersecurity experts in many software teams, leading to high workloads for these specialists and requiring software engineers to take on cybersecurity activities themselves. This creates a need for tools to support these efforts while maintaining data privacy, particularly in safety-critical domains like forestry.", "method": "A design science study was conducted, involving 12 cybersecurity experts through interviews, interactive sessions, and a survey within a large-scale project. The focus was on the use of locally hosted LLMs with retrieval-augmented generation to conduct risk assessments, while also addressing trust concerns and requirements for data protection.", "result": "LLMs demonstrated the ability to assist in generating initial cybersecurity risk assessments, identifying potential threats, and providing redundancy checks. However, there was a significant finding of the necessity of human oversight to ensure accuracy and compliance, especially given the critical nature of the domain and trust concerns.", "conclusion": "The study highlights that LLMs can be valuable in assisting with cybersecurity risk assessments in safety-critical fields, though they must be used with caution and under the supervision of cybersecurity experts. This usage encourages LLM-based agents for streamlined yet safe risk evaluation in such domains while considering privacy and compliance."}}
{"id": "2510.06363", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06363", "abs": "https://arxiv.org/abs/2510.06363", "authors": ["Ololade Babatunde", "Tomisin Ayodabo", "Raqibul Raqibul"], "title": "Improving Assignment Submission in Higher Education through a Git-Enabled System: An Iterative Case Study", "comment": null, "summary": "This study addresses challenges in traditional assignment submission methods\nused in higher education by introducing and evaluating a customized Git-based\nsubmission system. Employing iterative software development and user-centered\ndesign methodologies, the system was integrated within a real-world university\nenvironment. Empirical evaluation, including usability testing and student\nfeedback, indicated significant improvements in assignment tracking,\ncollaboration, and submission efficiency. Students reported positive\nexperiences using distributed version control workflows, highlighting improved\nlearning outcomes and reduced administrative burden. Challenges related to\ninitial adoption and student learning curves were identified and mitigated\nthrough iterative improvements. The proposed system contributes practical\ninsights for integrating distributed version control into educational settings,\nenhancing both instructor oversight and student engagement in software\nengineering and related disciplines. Based on our results, the research showed\nthat 85% of instructors found the git based system easier to use, with 84% of\nstudents preferring it over traditional methods, as it provides a 38% reduction\nin time taken for submission and review, while also leading to a 48% reduction\nin storage requirements.", "AI": {"tldr": "A Git-based assignment submission system was developed and tested in higher education, showing improved efficiency, collaboration, and user satisfaction compared to traditional methods, with 85/84/38/48 metrics for instructor/student preference, time reduction, and storage savings.", "motivation": "Traditional assignment submission methods in higher education face challenges in tracking, collaboration, and administrative efficiency, necessitating a modern solution for improved workflows and learning outcomes.", "method": "Iterative software development and user-centered design were used to create a Git-based submission system, evaluated through empirical testing, usability studies, and feedback from real-world university implementations.", "result": "85% of instructors and 84%% of students preferred the Git system, achieving 38%% faster submissions/reviews and 48%% lower storage needs. Usability studies showed improved collaboration and learning outcomes, with challenges in initial adoption mitigated via iterative improvements.", "conclusion": "The Git-based system demonstrates effective integration of distributed version control in education, enhancing instructor oversight and student engagement while reducing administrative burdens, with practical implications for software engineering pedagogy."}}
{"id": "2510.06483", "categories": ["cs.SE", "D.2.1; D.2.2; K.4.2; D.2.13"], "pdf": "https://arxiv.org/pdf/2510.06483", "abs": "https://arxiv.org/abs/2510.06483", "authors": ["Judith Michael", "Lukas Netz", "Bernhard Rumpe", "Ingo M\u00fcller", "John Grundy", "Shavindra Wickramathilaka", "Hourieh Khalajzadeh"], "title": "Addressing Visual Impairments with Model-Driven Engineering: A Systematic Literature Review", "comment": "41 pages", "summary": "Software applications often pose barriers for users with accessibility needs,\ne.g., visual impairments. Model-driven engineering (MDE), with its systematic\nnature of code derivation, offers systematic methods to integrate accessibility\nconcerns into software development while reducing manual effort. This paper\npresents a systematic literature review on how MDE addresses accessibility for\nvision impairments. From 447 initially identified papers, 30 primary studies\nmet the inclusion criteria. About two-thirds reference the Web Content\nAccessibility Guidelines (WCAG), yet their project-specific adaptions and\nend-user validations hinder wider adoption in MDE. The analyzed studies model\nuser interface structures, interaction and navigation, user capabilities,\nrequirements, and context information. However, only few specify concrete\nmodeling techniques on how to incorporate accessibility needs or demonstrate\nfully functional systems. Insufficient details on MDE methods, i.e.,\ntransformation rules or code templates, hinder the reuse, generalizability, and\nreproducibility. Furthermore, limited involvement of affected users and limited\ndeveloper expertise in accessibility contribute to weak empirical validation.\nOverall, the findings indicate that current MDE research insufficiently\nsupports vision-related accessibility. Our paper concludes with a research\nagenda outlining how support for vision impairments can be more effectively\nembedded in MDE processes.", "AI": {"tldr": "This paper reviews MDE's approach to accessibility for visual impairments, finding that while 30 studies were analyzed, most lack concrete modeling techniques and user validation, hindering MDE's adoption for accessibility. It proposes a research agenda for improvement.", "motivation": "Integrating accessibility into software development for visually impaired users is challenging, and MDE offers systematic methods but requires better techniques and validation to reduce manual barriers.", "method": "A systematic literature review of 447 papers, selecting 30 primary studies, analyzing WCAG adherence, modeling methods, and empirical validation aspects.", "result": "66% of studies reference WCAG but lack generalizable MDE techniques. Insufficient details on transformations/code templates and limited user/developer involvement lead to weak validation.", "conclusion": "Current MDE research inadequately supports vision-related accessibility; a research agenda is proposed to embed accessibility more effectively in MDE processes."}}
{"id": "2510.06606", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06606", "abs": "https://arxiv.org/abs/2510.06606", "authors": ["Uswat Yusuf", "Genevieve Caumartin", "Diego Elias Costa"], "title": "Beyond More Context: How Granularity and Order Drive Code Completion Quality", "comment": null, "summary": "Context plays an important role in the quality of code completion, as Large\nLanguage Models (LLMs) require sufficient and relevant information to assist\ndevelopers in code generation tasks. However, composing a relevant context for\ncode completion poses challenges in large repositories: First, the limited\ncontext length of LLMs makes it impractical to include all repository files.\nSecond, the quality of generated code is highly sensitive to noisy or\nirrelevant context. In this paper, we present our approach for the ASE 2025\nContext Collection Challenge. The challenge entails outperforming JetBrains\nbaselines by designing effective retrieval and context collection strategies.\nWe develop and evaluate a series of experiments that involve retrieval\nstrategies at both the file and chunk levels. We focus our initial experiments\non examining the impact of context size and file ordering on LLM performance.\nOur results show that the amount and order of context can significantly\ninfluence the performance of the models. We introduce chunk-based retrieval\nusing static analysis, achieving a 6% improvement over our best file-retrieval\nstrategy and a 16% improvement over the no-context baseline for Python in the\ninitial phase of the competition. Our results highlight the importance of\nretrieval granularity, ordering and hybrid strategies in developing effective\ncontext collection pipelines for real-world development scenarios.", "AI": {"tldr": "The paper introduces context collection techniques for code completion, using a combination of retrieval methods and optimization strategies for LLM context to improve code generation performance in large repositories.", "motivation": "The LLM code completion performance is highly dependent on quality and relevance of context provided to it, which is not straightforward to collect in large code repositories due to LLMs' limited context length and the presence of noisy or irrelevant data.", "method": "Authors experiment with file- and chunk-based retrieval techniques, using static analysis to improve the ability of LLMs to understand and generate accurate code. They also investigate the effects of varying context sizes and file ordering on LLM performance.", "result": "The authors achieve a 6% and 16% improvement in code generation performance in the ASE 2025 Context Collection Challenge for Python, using chunk-based retrieval and optimized context selection over their file-retrieval and no-context baselines, respectively.", "conclusion": "Fine-grained retrieval and context ordering are important for LLM code completion, especially in large and complex codebases. Applying a hybrid strategy combining file and chunk level retrieval can significantly enhance code generation performance."}}
