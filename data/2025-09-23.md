<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 45]
- [cs.SE](#cs.SE) [Total: 21]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Reconnecting Citizens to Politics via Blockchain - Starting the Debate](https://arxiv.org/abs/2509.16274)
*Uwe Serdült*

Main category: cs.CR

TL;DR: This paper explores using blockchain technology and a dedicated cryptocurrency to improve the transparency and fairness of election campaign financing, despite existing challenges.


<details>
  <summary>Details</summary>
Motivation: The study addresses the pervasive issue of money in politics, highlighting the need for more transparent and secure methods to prevent scandals in election campaign financing.

Method: The paper proposes introducing a blockchain-based cryptocurrency specifically designed to pay for political campaigning and advertising costs, aiming to enhance transparency and security.

Result: While specific results are not detailed, the paper suggests that blockchain could potentially offer a more secure and traceable system, though many open questions remain about its implementation.

Conclusion: Blockchain technology offers a promising avenue for improving election campaign financing, warranting further exploration despite existing challenges and uncertainties.

Abstract: Elections are not the only but arguably one of the most important pillars for
the proper functioning of liberal democracies. Recent evidence across the globe
shows that it is not straightforward to conduct them in a free and fair manner.
One constant concern is the role of money in politics, more specifically,
election campaign financing. Frequent scandals are proof of the difficulties
encountered with current approaches to tackle the issue. Suggestions on how to
overcome the problem exist but seem difficult to implement. With the help of
blockchain technology we might be able to make a step forward. A separate
crypto currency specifically designed to pay for costs of political campaigning
and advertising could be introduced. Admittedly, at this stage, there are many
open questions. However, under the assumption that blockchain technology is
here to stay, it is an idea that deserves further exploration.

</details>


### [2] [SecureFixAgent: A Hybrid LLM Agent for Automated Python Static Vulnerability Repair](https://arxiv.org/abs/2509.16275)
*Jugal Gajjar,Kamalasankari Subramaniakuppusamy,Relsy Puthal,Kaustik Ranaware*

Main category: cs.CR

TL;DR: SecureFixAgent is a hybrid vulnerability repair framework combining static analysis (Bandit) with lightweight local LLMs via iterative detection-repair-validation cycles, reducing false positives by 10.8%, improving fix accuracy by 13.51%, and achieving high developer trust through explainable local workflows.


<details>
  <summary>Details</summary>
Motivation: Modern software pipelines require secure, efficient vulnerability remediation as static analysis tools exhibit high false positives while LLMs produce hallucinatory fixes without validation.

Method: SecureFixAgent integrates Bandit (detection), LoRA-fine-tuned local LLMs (<8B parameters) (repair with explanations), and Bandit re-validation (verification) in a three-iteration loop, using dataset diversity to mitigate bias and reduce unnecessary edits.

Result: Reduces false positives by 10.8%, improves fix accuracy by 13.51%, lowers LLM false positives by 5.46%, achieves 4.5/5 developer satisfaction for explanation quality, and converges in 3 iterations with resource-efficient local execution.

Conclusion: SecureFixAgent advances trustworthy automated remediation by combining verifiable static analysis with explainable LLM-based repairs in a privacy-preserving, low-resource framework, enabling human-AI collaboration in modern DevSecOps.

Abstract: Modern software development pipelines face growing challenges in securing
large codebases with extensive dependencies. Static analysis tools like Bandit
are effective at vulnerability detection but suffer from high false positives
and lack repair capabilities. Large Language Models (LLMs), in contrast, can
suggest fixes but often hallucinate changes and lack self-validation. We
present SecureFixAgent, a hybrid repair framework integrating Bandit with
lightweight local LLMs (<8B parameters) in an iterative detect-repair-validate
loop. To improve precision, we apply parameter-efficient LoRA-based fine-tuning
on a diverse, curated dataset spanning multiple Python project domains,
mitigating dataset bias and reducing unnecessary edits. SecureFixAgent uses
Bandit for detection, the LLM for candidate fixes with explanations, and Bandit
re-validation for verification, all executed locally to preserve privacy and
reduce cloud reliance. Experiments show SecureFixAgent reduces false positives
by 10.8% over static analysis, improves fix accuracy by 13.51%, and lowers
false positives by 5.46% compared to pre-trained LLMs, typically converging
within three iterations. Beyond metrics, developer studies rate explanation
quality 4.5/5, highlighting its value for human trust and adoption. By
combining verifiable security improvements with transparent rationale in a
resource-efficient local framework, SecureFixAgent advances trustworthy,
automated vulnerability remediation for modern pipelines.

</details>


### [3] [Decoding TRON: A Comprehensive Framework for Large-Scale Blockchain Data Extraction and Exploration](https://arxiv.org/abs/2509.16292)
*Qian'ang Mao,Jiaxin Wang,Zhiqi Feng,Yi Zhang,Jiaqi Yan*

Main category: cs.CR

TL;DR: This paper introduces a high-performance ETL system to extract and analyze TRON blockchain data, revealing insights into its ecosystem, including USDT dominance, gambling applications, and illicit activities, while proposing future research directions.


<details>
  <summary>Details</summary>
Motivation: TRON's unique blockchain architecture and prominence in stablecoin applications contrast with the lack of on-chain data analysis, creating a research gap that hinders understanding of its ecosystem and use cases.

Method: The authors developed a specialized ETL (Extract, Transform, Load) system to collect TRON's raw on-chain data (blocks, transactions, contracts) and conducted exploratory analysis across transaction trends, resource delegation, smart contract patterns, and USDT dynamics.

Result: Key findings include TRON's block generation efficiency, exchange centralization, resource delegation market behavior, smart contract adoption, USDT's critical role, and empirical evidence of gambling platform proliferation and potential illicit transaction patterns.

Conclusion: The framework establishes a foundational dataset for TRON research, enabling future studies in delegate services, stablecoin mechanics, and anti-money laundering applications, while highlighting the ecosystem's unique characteristics and risks.

Abstract: Cryptocurrencies and Web3 applications based on blockchain technology have
flourished in the blockchain research field. Unlike Bitcoin and Ethereum, due
to its unique architectural designs in consensus mechanisms, resource
management, and throughput, TRON has developed a more distinctive ecosystem and
application scenarios centered around stablecoins. Although it is popular in
areas like stablecoin payments and settlement, research on analyzing on-chain
data from the TRON blockchain is remarkably scarce. To fill this gap, this
paper proposes a comprehensive data extraction and exploration framework for
the TRON blockchain. An innovative high-performance ETL system aims to
efficiently extract raw on-chain data from TRON, including blocks,
transactions, smart contracts, and receipts, establishing a research dataset.
An in-depth analysis of the extracted dataset reveals insights into TRON's
block generation, transaction trends, the dominance of exchanges, the resource
delegation market, smart contract usage patterns, and the central role of the
USDT stablecoin. The prominence of gambling applications and potential illicit
activities related to USDT is emphasized. The paper discusses opportunities for
future research leveraging this dataset, including analysis of delegate
services, gambling scenarios, stablecoin activities, and illicit transaction
detection. These contributions enhance blockchain data management capabilities
and understanding of the rapidly evolving TRON ecosystem.

</details>


### [4] [To Unpack or Not to Unpack: Living with Packers to Enable Dynamic Analysis of Android Apps](https://arxiv.org/abs/2509.16340)
*Mohammad Hossein Asghari,Lianying Zhao*

Main category: cs.CR

TL;DR: The paper addresses the challenge of analyzing packed Android apps by proposing Purifire, an eBPF-based evasion engine to bypass packers’ anti-analysis techniques. It reveals the prevalence of packers in real-world apps and demonstrates significant improvements in dynamic analysis compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Packers hinder dynamic analysis of Android apps by embedding anti-analysis techniques, rendering existing unpackers ineffective. This obstructs security research and detection of malicious behaviors, necessitating a new solution for runtime analysis of packed apps.

Method: 1) A large-scale study of 12,341 apps to analyze packer prevalence and impact on dynamic analysis. 2Development of Purifire, leveraging eBPF to enforce evasion rules at the kernel level without unpacking apps, circumventing anti-analysis checks.

Result: Purifire successfully bypasses commercial packers’ anti-analysis mechanisms. Experimental results show dramatic improvements in security tools’ performance, such as a higher detection rate of device fingerprints, demonstrating its effectiveness in restoring dynamic analysis capabilities.

Conclusion: Purifire provides a scalable, low-profile solution to bypass packers’ defenses, enabling reliable dynamic analysis of packed Android apps. It addresses critical limitations of existing unpackers and enhances the practicality of runtime security analysis.

Abstract: Android apps have become a valuable target for app modifiers and imitators
due to its popularity and being trusted with highly sensitive data. Packers, on
the other hand, protect apps from tampering with various anti-analysis
techniques embedded in the app. Meanwhile, packers also conceal certain
behavior potentially against the interest of the users, aside from being abused
by malware for stealth. Security practitioners typically try to capture
undesired behavior at runtime with hooking (e.g., Frida) or debugging
techniques, which are heavily affected by packers. Unpackers have been the
community's continuous effort to address this, but due to the emerging
commercial packers, our study shows that none of the unpackers remain
effective, and they are unfit for this purpose as unpacked apps can no longer
run. We first perform a large-scale prevalence analysis of Android packers with
a real-world dataset of 12,341 apps, the first of its kind, to find out what
percentage of Android apps are actually packed and to what extent dynamic
analysis is hindered. We then propose Purifire, an evasion engine to bypass
packers' anti-analysis techniques and enable dynamic analysis on packed apps
without unpacking them. Purifire is based on eBPF, a low-level kernel feature,
which provides observability and invisibility to userspace apps to enforce
defined evasion rules while staying low-profile. Our evaluation shows that
Purifire is able to bypass packers' anti-analysis checks and more importantly,
for previous research works suffering from packers, we observe a significant
improvement (e.g., a much higher number of detected items such as device
fingerprints).

</details>


### [5] [Secure Confidential Business Information When Sharing Machine Learning Models](https://arxiv.org/abs/2509.16352)
*Yunfan Yang,Jiarong Xu,Hongzhe Zhang,Xiao Fang*

Main category: cs.CR

TL;DR: The paper introduces a new defense method, based on a novel responsive CPI attack and an attack-defense arms race, for secure model sharing which considers responsive real-world adversaries. The method is evaluated in different realistic scenarios and shown to outperform existing methods in CPI defense, model utility, and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Model-sharing is economically beneficial, but CPI attacks pose a data confidentiality risk. Existing defenses are not effective against responsive adversaries, who adapt their attacks to target and defense-specific information.

Method: The method involves two key components: a novel Responsive CPI attack which simulates real-world adversaries' responsiveness, and an attack-defense arms race framework where target and attack models are iteratively improved. It also includes an approximation strategy to enhance computational efficiency.

Result: In various realistic model-sharing scenarios, the new method demonstrates superior performance against CPI attacks while maintaining model utility and reducing computational overhead, compared to existing defense methods.

Conclusion: The proposed defense method, which accounts for the responsive nature of real-world adversaries in model-sharing, is effective and efficient against CPI attacks, offering a better solution for secure model sharing.

Abstract: Model-sharing offers significant business value by enabling firms with
well-established Machine Learning (ML) models to monetize and share their
models with others who lack the resources to develop ML models from scratch.
However, concerns over data confidentiality remain a significant barrier to
model-sharing adoption, as Confidential Property Inference (CPI) attacks can
exploit shared ML models to uncover confidential properties of the model
provider's private model training data. Existing defenses often assume that CPI
attacks are non-adaptive to the specific ML model they are targeting. This
assumption overlooks a key characteristic of real-world adversaries: their
responsiveness, i.e., adversaries' ability to dynamically adjust their attack
models based on the information of the target and its defenses. To overcome
this limitation, we propose a novel defense method that explicitly accounts for
the responsive nature of real-world adversaries via two methodological
innovations: a novel Responsive CPI attack and an attack-defense arms race
framework. The former emulates the responsive behaviors of adversaries in the
real world, and the latter iteratively enhances both the target and attack
models, ultimately producing a secure ML model that is robust against
responsive CPI attacks. Furthermore, we propose and integrate a novel
approximate strategy into our defense, which addresses a critical computational
bottleneck of defense methods and improves defense efficiency. Through
extensive empirical evaluations across various realistic model-sharing
scenarios, we demonstrate that our method outperforms existing defenses by more
effectively defending against CPI attacks, preserving ML model utility, and
reducing computational overhead.

</details>


### [6] [LiteRSan: Lightweight Memory Safety Via Rust-specific Program Analysis and Selective Instrumentation](https://arxiv.org/abs/2509.16389)
*Tianrou Xia,Kaiming Huang,Dongyeon Yu,Yuseok Jeon,Jie Zhou,Dinghao Wu,Taegyu Kim*

Main category: cs.CR

TL;DR: LiteRSan improves Rust memory safety with lower overhead by targeting risky pointers via ownership-aware analysis, outperforming existing sanitizers.


<details>
  <summary>Details</summary>
Motivation: Rust’s unsafe code introduces memory vulnerabilities, and existing ASan-based sanitizers suffer from high overhead and detection limitations.

Method: LiteRSan uses Rust-specific static analysis of pointer lifetimes to identify risky pointers and applies selective instrumentation for minimal checks.

Result: LiteRSan achieves 18.84% runtime overhead and 0.81% memory overhead, outperforming ERASan and RustSan by 76% and 87% in runtime, with broader bug detection coverage.

Conclusion: LiteRSan effectively reduces runtime and memory overhead while detecting previously missed memory safety bugs in Rust by leveraging its ownership model.

Abstract: Rust is a memory-safe language, and its strong safety guarantees combined
with high performance have been attracting widespread adoption in systems
programming and security-critical applications. However, Rust permits the use
of unsafe code, which bypasses compiler-enforced safety checks and can
introduce memory vulnerabilities. A widely adopted approach for detecting
memory safety bugs in Rust is Address Sanitizer (ASan). Optimized versions,
such as ERASan and RustSan, have been proposed to selectively apply security
checks in order to reduce performance overhead. However, these tools still
incur significant performance and memory overhead and fail to detect many
classes of memory safety vulnerabilities due to the inherent limitations of
ASan. In this paper, we present LiteRSan, a novel memory safety sanitizer that
addresses the limitations of prior approaches. By leveraging Rust's unique
ownership model, LiteRSan performs Rust-specific static analysis that is aware
of pointer lifetimes to identify risky pointers. It then selectively
instruments risky pointers to enforce only the necessary spatial or temporal
memory safety checks. Consequently, LiteRSan introduces significantly lower
runtime overhead (18.84% versus 152.05% and 183.50%) and negligible memory
overhead (0.81% versus 739.27% and 861.98%) compared with existing ASan-based
sanitizers while being capable of detecting memory safety bugs that prior
techniques miss.

</details>


### [7] [B5GRoam: A Zero Trust Framework for Secure and Efficient On-Chain B5G Roaming](https://arxiv.org/abs/2509.16390)
*Mohamed Abdessamed Rezazi,Mouhamed Amine Bouchiha,Ahmed Mounsf Rafik Bendada,Yacine Ghamri-Doudane*

Main category: cs.CR

TL;DR: B5GRoam is a blockchain-based framework for secure, privacy-preserving, and scalable 5G roaming settlements using zero-trust architecture and zk-SNARKs, achieving high throughput and cost efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing blockchain solutions for roaming settlement face data privacy risks, mutual trust assumptions, and scalability issues. 5G networks require high-throughput mechanisms to prevent billing reconciliation disputes while maintaining security and trust.

Method: B5GRoam combines cryptographically verifiable call detail records (CDRs) with non-interactive zero-knowledge proofs (zkSNARKs) and Layer 2 zk-Rollups. It enables smart contracts to authenticate usage claims without exposing data, maintaining privacy while achieving scalability through off-chain transaction aggregation.

Result: Experiments show B5GRoam processes over 7,200 transactions/second with strong privacy guarantees and significant cost reductions. It eliminates intermediaries and reduces gas costs via Layer 2 optimizations.

Conclusion: B5GRoam provides a decentralized, verifiable, and practical solution for 5G roaming settlements, addressing privacy, scalability, and trust limitations of current blockchain approaches.

Abstract: Roaming settlement in 5G and beyond networks demands secure, efficient, and
trustworthy mechanisms for billing reconciliation between mobile operators.
While blockchain promises decentralization and auditability, existing solutions
suffer from critical limitations-namely, data privacy risks, assumptions of
mutual trust, and scalability bottlenecks. To address these challenges, we
present B5GRoam, a novel on-chain and zero-trust framework for secure,
privacy-preserving, and scalable roaming settlements. B5GRoam introduces a
cryptographically verifiable call detail record (CDR) submission protocol,
enabling smart contracts to authenticate usage claims without exposing
sensitive data. To preserve privacy, we integrate non-interactive
zero-knowledge proofs (zkSNARKs) that allow on-chain verification of roaming
activity without revealing user or network details. To meet the high-throughput
demands of 5G environments, B5GRoam leverages Layer 2 zk-Rollups, significantly
reducing gas costs while maintaining the security guarantees of Layer 1.
Experimental results demonstrate a throughput of over 7,200 tx/s with strong
privacy and substantial cost savings. By eliminating intermediaries and
enhancing verifiability, B5GRoam offers a practical and secure foundation for
decentralized roaming in future mobile networks.

</details>


### [8] [LenslessMic: Audio Encryption and Authentication via Lensless Computational Imaging](https://arxiv.org/abs/2509.16418)
*Petr Grinberg,Eric Bezzam,Paolo Prandoni,Martin Vetterli*

Main category: cs.CR

TL;DR: LenslessMic is a new hardware-based audio encryption method that uses a lensless camera for security. It enables authentication and strong encryption similar to 256-bit standards with high signal quality and low cost.


<details>
  <summary>Details</summary>
Motivation: Society needs better protection for sensitive shared audio data. Existing encryption methods in the audio domain depend too much on software or embedded hardware which might be vulnerable to cyber threats.

Method: LenslessMic works by adding a physical layer of security using a lensless camera to obscure the audio signals, making it tougher for unauthorized individuals to both record and recover the content. we use a raspberry pi to test this method in real-world conditions.

Result: The experiment shows that LenslessMic provides robust authentication for audio, has good encryption strength (seen to rival 256-bit digital standards), and keeps the signal quality high with minimal information loss.

Conclusion: Integration of physical and digital security through LenslessMic offers an efficient method for protecting digital audio data. it also has low-cost implementations, where it can be a viable alternative to traditional audio encryption methods.

Abstract: With society's increasing reliance on digital data sharing, the protection of
sensitive information has become critical. Encryption serves as one of the
privacy-preserving methods; however, its realization in the audio domain
predominantly relies on signal processing or software methods embedded into
hardware. In this paper, we introduce LenslessMic, a hybrid optical
hardware-based encryption method that utilizes a lensless camera as a physical
layer of security applicable to multiple types of audio. We show that
LenslessMic enables (1) robust authentication of audio recordings and (2)
encryption strength that can rival the search space of 256-bit digital
standards, while maintaining high-quality signals and minimal loss of content
information. The approach is validated with a low-cost Raspberry Pi prototype
and is open-sourced together with datasets to facilitate research in the area.

</details>


### [9] [End-to-End Co-Simulation Testbed for Cybersecurity Research and Development in Intelligent Transportation Systems](https://arxiv.org/abs/2509.16489)
*Minhaj Uddin Ahmad,Akid Abrar,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.CR

TL;DR: This paper proposes a CARLA-SUMO-OMNeT++ co-simulation testbed for ITS cybersecurity evaluation, demonstrating its utility through a post-quantum cryptography case study.


<details>
  <summary>Details</summary>
Motivation: ITS cyber threats endanger safety and infrastructure, but vulnerability assessments for heterogeneous systems are prohibitively costly/time-consuming, requiring simulation-based solutions for secure development.

Method: An integrated co-simulation platform combining CARLA (3D environment/sensor modeling), SUMO (microscopic traffic simulation), and OMNeT++ (V2X communication) for cybersecurity evaluation in large-scale ITS.

Result: Enables end-to-end experimentation and vulnerability analysis through a C-V2X proactive safety alert case study enhanced with post-quantum cryptography, providing practical cybersecurity insights.

Conclusion: The co-simulation testbed demonstrates practical effectiveness in advancing secure, efficient, and resilient ITS infrastructures through end-to-end experimentation and mitigation benchmarking.

Abstract: Intelligent Transportation Systems (ITS) have been widely deployed across
major metropolitan regions worldwide to improve roadway safety, optimize
traffic flow, and reduce environmental impacts. These systems integrate
advanced sensors, communication networks, and data analytics to enable
real-time traffic monitoring, adaptive signal control, and predictive
maintenance. However, such integration significantly broadens the ITS attack
surface, exposing critical infrastructures to cyber threats that jeopardize
safety, data integrity, and operational resilience. Ensuring robust
cybersecurity is therefore essential, yet comprehensive vulnerability
assessments, threat modeling, and mitigation validations are often
cost-prohibitive and time-intensive when applied to large-scale, heterogeneous
transportation systems. Simulation platforms offer a cost-effective and
repeatable means for cybersecurity evaluation, and the simulation platform
should encompass the full range of ITS dimensions - mobility, sensing,
networking, and applications. This chapter discusses an integrated
co-simulation testbed that links CARLA for 3D environment and sensor modeling,
SUMO for microscopic traffic simulation and control, and OMNeT++ for V2X
communication simulation. The co-simulation testbed enables end-to-end
experimentation, vulnerability identification, and mitigation benchmarking,
providing practical insights for developing secure, efficient, and resilient
ITS infrastructures. To illustrate its capabilities, the chapter incorporates a
case study on a C-V2X proactive safety alert system enhanced with post-quantum
cryptography, highlighting the role of the testbed in advancing secure and
resilient ITS infrastructures.

</details>


### [10] [Train to Defend: First Defense Against Cryptanalytic Neural Network Parameter Extraction Attacks](https://arxiv.org/abs/2509.16546)
*Ashley Kurian,Aydin Aysu*

Main category: cs.CR

TL;DR: This paper presents the first training-time defense against neural network parameter extraction attacks by reducing neuron uniqueness through weight-regularization, maintaining accuracy (≤1% drop) and inference efficiency while delaying attacks from minutes/hours with theoretical analysis.


<details>
  <summary>Details</summary>
Motivation: Neural networks represent significant intellectual property requiring protection from increasingly capable cryptanalytic attacks, which threaten both competitive advantage and data privacy.

Method: An extraction-aware training method that adds a regularization term to minimize distances between neuron weights within a layer, eliminating neuron uniqueness needed for attacks without requiring runtime modifications.

Result: The defense demonstrates empirical robustness against extraction attacks (surviving prolonged attack periods vs. 14m-4h for unprotected networks) while maintaining model accuracy and offering a theoretical quantification of attack success probabilities.

Conclusion: The proposed defense mechanism effectively mitigates parameter extraction attacks with minimal impact on model accuracy (less than 1% decrease) and zero inference overhead, while providing a theoretical framework to quantify attack probabilities.

Abstract: Neural networks are valuable intellectual property due to the significant
computational cost, expert labor, and proprietary data involved in their
development. Consequently, protecting their parameters is critical not only for
maintaining a competitive advantage but also for enhancing the model's security
and privacy. Prior works have demonstrated the growing capability of
cryptanalytic attacks to scale to deeper models. In this paper, we present the
first defense mechanism against cryptanalytic parameter extraction attacks. Our
key insight is to eliminate the neuron uniqueness necessary for these attacks
to succeed. We achieve this by a novel, extraction-aware training method.
Specifically, we augment the standard loss function with an additional
regularization term that minimizes the distance between neuron weights within a
layer. Therefore, the proposed defense has zero area-delay overhead during
inference. We evaluate the effectiveness of our approach in mitigating
extraction attacks while analyzing the model accuracy across different
architectures and datasets. When re-trained with the same model architecture,
the results show that our defense incurs a marginal accuracy change of less
than 1% with the modified loss function. Moreover, we present a theoretical
framework to quantify the success probability of the attack. When tested
comprehensively with prior attack settings, our defense demonstrated empirical
success for sustained periods of extraction, whereas unprotected networks are
extracted between 14 minutes to 4 hours.

</details>


### [11] [MoPE: A Mixture of Password Experts for Improving Password Guessing](https://arxiv.org/abs/2509.16558)
*Mingjian Duan,Ming Xu,Shenghao Zhang,Jiaheng Zhang,Weili Han*

Main category: cs.CR

TL;DR: This paper proposes MoPE, a structure-aware password guessing framework that leverages mixed experts to address training bias from uniform password modeling, achieving significant performance improvements over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing password models treat all passwords uniformly, creating training bias towards frequent structural patterns. The authors argue passwords require structure-aware processing as complex short textual data to reduce this bias.

Method: The MoPE framework introduces (1) a structure-based method to generate specialized expert models for password patterns, and (2) a lightweight gate mechanism to select experts for reliable guessing aligned with password guessing task demands.

Result: MoPE achieves 38.80% improvement in offline cracking rates and 9.27% in online scenarios over state-of-the-art baselines. A real-time Password Strength Meter implementation provides millisecond-level response latency.

Conclusion: Structure-aware modeling via MoPE's expert mixture framework effectively mitigates training bias in password analysis, demonstrating superior password guessing performance and practical applications for strength estimation.

Abstract: Textual passwords remain a predominant authentication mechanism in web
security. To evaluate their strength, existing research has proposed several
data-driven models across various scenarios. However, these models generally
treat passwords uniformly, neglecting the structural differences among
passwords. This typically results in biased training that favors frequent
password structural patterns. To mitigate the biased training, we argue that
passwords, as a type of complex short textual data, should be processed in a
structure-aware manner by identifying their structural patterns and routing
them to specialized models accordingly. In this paper, we propose MoPE, a
Mixture of Password Experts framework, specifically designed to leverage the
structural patterns in passwords to improveguessing performance. Motivated by
the observation that passwords with similar structural patterns (e.g.,
fixed-length numeric strings) tend to cluster in high-density regions within
the latent space, our MoPE introduces: (1) a novel structure-based method for
generating specialized expert models; (2) a lightweight gate method to select
appropriate expert models to output reliable guesses, better aligned with the
high computational frequency of password guessing tasks. Our evaluation shows
that MoPE significantly outperforms existing state-of-the-art baselines in both
offline and online guessing scenarios, achieving up to 38.80% and 9.27%
improvement in cracking rate, respectively, showcasing that MoPE can
effectively exploit the capabilities of data-driven models for password
guessing. Additionally, we implement a real-time Password Strength Meter (PSM)
based on offline MoPE, assisting users in choosing stronger passwords more
precisely with millisecond-level response latency.

</details>


### [12] [Towards Cost-Effective ZK-Rollups: Modeling and Optimization of Proving Infrastructure](https://arxiv.org/abs/2509.16581)
*Mohsen Ahmadvand,Pedro Souto*

Main category: cs.CR

TL;DR: Optimizing ZK-rollup costs via parametric modeling and constraint solving reduces costs by 70% in scalable environments.


<details>
  <summary>Details</summary>
Motivation: Scaling zero-knowledge rollups faces economic viability challenges due to rising hardware costs, throughput demands, and volatile gas prices, necessitating cost optimization strategies.

Method: Developed a cost model capturing rollup constraints, formulated it as a constraint system solvable by the Z3 SMT solver, and validated it with a simulator measuring lag and costs.

Result: A 70% cost reduction potential demonstrated via simulation, with the model successfully identifying optimal configurations while meeting transaction load and finality constraints.

Conclusion: The proposed parametric cost model and constraint-solving approach effectively reduce operational costs for zero-knowledge rollups by up to 70% while maintaining scalability and finality requirements.

Abstract: Zero-knowledge rollups rely on provers to generate multi-step state
transition proofs under strict finality and availability constraints. These
steps require expensive hardware (e.g., GPUs), and finality is reached only
once all stages complete and results are posted on-chain. As rollups scale,
staying economically viable becomes increasingly difficult due to rising
throughput, fast finality demands, volatile gas prices, and dynamic resource
needs. We base our study on Halo2-based proving systems and identify
transactions per second (TPS), average gas usage, and finality time as key cost
drivers. To address this, we propose a parametric cost model that captures
rollup-specific constraints and ensures provers can keep up with incoming
transaction load. We formulate this model as a constraint system and solve it
using the Z3 SMT solver to find cost-optimal configurations. To validate our
approach, we implement a simulator that detects lag and estimates operational
costs. Our method shows a potential cost reduction of up to 70\%.

</details>


### [13] [Reproducing a Security Risk Assessment Using Computer Aided Design](https://arxiv.org/abs/2509.16593)
*Avi Shaked*

Main category: cs.CR

TL;DR: This paper evaluates computer-aided security tools by reproducing a prior assessment, showing they enhance reliability and sustainability compared to traditional methods, with benefits for both research and practice.


<details>
  <summary>Details</summary>
Motivation: The study addresses limitations in traditional 'pen and paper' security assessments (even when digitized), which are prone to authoring errors and inconsistencies, and aims to demonstrate the advantages of computer-aided tools for both practitioners and researchers.

Method: The authors applied a model-based security design tool to reproduce a previously published security risk assessment, comparing the computer-aided approach with the original non-computer method using a real-world case study.

Result: Three contributions: (1) successful reproduction of a refereed security assessment article, (2) comparative analysis of computer-aided vs non-computer methods using a published real-world case, and (3) demonstration of computer-aided tools' potential to improve assessment rigor and usability.

Conclusion: The paper concludes that computer-aided design approaches enhance the rigor and sustainability of security risk assessments, benefiting both industry practitioners and researchers through improved consistency and reliability.

Abstract: Security risk assessment is essential in establishing the trustworthiness and
reliability of modern systems. While various security risk assessment
approaches exist, prevalent applications are "pen and paper" implementations
that -- even if performed digitally using computers -- remain prone to
authoring mistakes and inconsistencies. Computer-aided design approaches can
transform security risk assessments into more rigorous and sustainable efforts.
This is of value to both industrial practitioners and researchers, who practice
security risk assessments to reflect on systems' designs and to contribute to
the discipline's state-of-the-art. In this article, we report the application
of a model-based security design tool to reproduce a previously reported
security assessment. The main contributions are: 1) an independent attempt to
reproduce a refereed article describing a real security risk assessment of a
system; 2) comparison of a new computer-aided application with a previous
non-computer-aided application, based on a published, real-world case study; 3)
a showcase for the potential advantages -- for both practitioners and
researchers -- of using computer-aided design approaches to analyze reports and
to assess systems.

</details>


### [14] [Delving into Cryptanalytic Extraction of PReLU Neural Networks](https://arxiv.org/abs/2509.16620)
*Yi Chen,Xiaoyang Dong,Ruijie Ma,Yantian Shen,Anyu Wang,Hongbo Yu,Xiaoyun Wang*

Main category: cs.CR

TL;DR: This paper introduces the first practical model extraction attacks on PReLU neural networks, extending cryptoanalytic methods beyond ReLU networks.


<details>
  <summary>Details</summary>
Motivation: While model extraction has focused on ReLU-based networks for decades, PReLU networks with complex activation functions represent a critical yet understudied security threat. The authors aim to bridge this research gap as PReLU models become more prevalent in real-world applications.

Method: The paper proposes a raw output-based parameter recovery attack for PReLU networks and adapts it for scenarios with limited access to top-m probability scores. The attacks are validated through systematice experiments on multiple PReLU architectures (including MNIST-trained models).

Result: The proposed attacks achieve successful model extraction empirically, demonstrating feasibility across three distinct attack scenarios for PReLU networks. This includes the first end-to-end extraction proof on PReLU architectures with constrained output access.

Conclusion: The work establishes PReLU network extraction as a valid security concern, highlighting vulnerabilities in models with sophisticated activation functions. It opens new directions for cryptoanalytic defenses and attacks on modern neural network architectures.

Abstract: The machine learning problem of model extraction was first introduced in 1991
and gained prominence as a cryptanalytic challenge starting with Crypto 2020.
For over three decades, research in this field has primarily focused on
ReLU-based neural networks. In this work, we take the first step towards the
cryptanalytic extraction of PReLU neural networks, which employ more complex
nonlinear activation functions than their ReLU counterparts. We propose a raw
output-based parameter recovery attack for PReLU networks and extend it to more
restrictive scenarios where only the top-m probability scores are accessible.
Our attacks are rigorously evaluated through end-to-end experiments on diverse
PReLU neural networks, including models trained on the MNIST dataset. To the
best of our knowledge, this is the first practical demonstration of PReLU
neural network extraction across three distinct attack scenarios.

</details>


### [15] ["Digital Camouflage": The LLVM Challenge in LLM-Based Malware Detection](https://arxiv.org/abs/2509.16671)
*Ekin Böke,Simon Torka*

Main category: cs.CR

TL;DR: The paper evaluates the robustness of three state-of-the-art Large Language Models (LLMs) against compiler-level obfuscation techniques in malware detection, finding significant drops in performance metrics like precision, recall, and F1-score after obfuscation, which indicates critical vulnerabilities in these models.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to understand the reliability of Large Language Models (LLMs) in malware detection when faced with adversarial compiler-level obfuscations, which are widely used to evade detection while maintaining malicious behavior. The study seeks to uncover potential vulnerabilities in LLMs that adversaries might exploit, thus contributing to adversarial threat modeling.

Method: The method involves an empirical evaluation of three state-of-the-art LLMs (ChatGPT-4o, Gemini Flash 2.5, and Claude Sonnet 4) against four compiler-level obfuscation techniques (control flow flattening, bogus control flow injection, instruction substitution, and split basic blocks) implemented using the LLVM infrastructure. The evaluation is performed on a set of 40 C functions (20 vulnerable, 20 secure) sourced from the Devign dataset, which have been obfuscated using LLVM passes.

Result: The key results of the study show that all three Large Language Models (LLMs) tested performed very poorly on obfuscated code. Specifically, the models failed to correctly classify obfuscated code, with precision, recall, and F1-score dropping significantly after the application of obfuscation techniques. This indicates that compiler-based obfuscation can effectively mislead the models, undermining their utility in malware detection.

Conclusion: The conclusion is that LLMs, despite their advanced language understanding capabilities, are critically vulnerable to compiler-level obfuscations in the context of malware detection. The models can be easily misled by transformations like control flow flattening, highlighting the need for improving LLM efficiency through compiler-aware defenses, obfuscation-resilient design, or software watermarking. The study also encourages future work in these areas and promotes reproducibility through the release of datasets and evaluation tools.

Abstract: Large Language Models (LLMs) have emerged as promising tools for malware
detection by analyzing code semantics, identifying vulnerabilities, and
adapting to evolving threats. However, their reliability under adversarial
compiler-level obfuscation is yet to be discovered. In this study, we
empirically evaluate the robustness of three state-of-the-art LLMs: ChatGPT-4o,
Gemini Flash 2.5, and Claude Sonnet 4 against compiler-level obfuscation
techniques implemented via the LLVM infrastructure. These include control flow
flattening, bogus control flow injection, instruction substitution, and split
basic blocks, which are widely used to evade detection while preserving
malicious behavior. We perform a structured evaluation on 40~C functions (20
vulnerable, 20 secure) sourced from the Devign dataset and obfuscated using
LLVM passes. Our results show that these models often fail to correctly
classify obfuscated code, with precision, recall, and F1-score dropping
significantly after transformation. This reveals a critical limitation: LLMs,
despite their language understanding capabilities, can be easily misled by
compiler-based obfuscation strategies. To promote reproducibility, we release
all evaluation scripts, prompts, and obfuscated code samples in a public
repository. We also discuss the implications of these findings for adversarial
threat modeling, and outline future directions such as software watermarking,
compiler-aware defenses, and obfuscation-resilient model design.

</details>


### [16] [Design and Development of an Intelligent LLM-based LDAP Honeypot](https://arxiv.org/abs/2509.16682)
*Javier Jiménez-Román,Florina Almenares-Mendoza,Alfonso Sánchez-Macián*

Main category: cs.CR

TL;DR: This paper introduces an LLM-powered honeypot for LDAP servers to improve cybersecurity through adaptive deception techniques, addressing the limitations of traditional tools and enhancing threat detection against evolving attacks on identity management systems.


<details>
  <summary>Details</summary>
Motivation: Traditional honeypots suffer from rigidity and configuration complexity, limiting their effectiveness in dynamic attack scenarios. The rise of AI and LLMs enables new deception solutions that can better simulate realistic interactions, address evolving threats, and improve attacker profiling for proactive defense strategies.

Method: The paper proposes designing and implementing an LLM-based honeypot to simulate an LDAP server, leveraging artificial intelligence to enhance adaptability, realism, and ease of use compared to traditional rigid honeypot configurations.

Result: The proposed LLM-based honeypot solution provides a flexible and realistic simulation of LDAP services, enabling credible attacker interaction for early threat detection, method analysis, and enhanced infrastructure defense against LDAP-targeted intrusions.

Conclusion: The integration of LLMs into honeypot design offers a more adaptive and effective approach to combating evolving cybersecurity threats, particularly for critical services like LDAP servers, by improving early detection and threat analysis capabilities.

Abstract: Cybersecurity threats continue to increase, with a growing number of
previously unknown attacks each year targeting both large corporations and
smaller entities. This scenario demands the implementation of advanced security
measures, not only to mitigate damage but also to anticipate emerging attack
trends. In this context, deception tools have become a key strategy, enabling
the detection, deterrence, and deception of potential attackers while
facilitating the collection of information about their tactics and methods.
Among these tools, honeypots have proven their value, although they have
traditionally been limited by rigidity and configuration complexity, hindering
their adaptability to dynamic scenarios. The rise of artificial intelligence,
and particularly general-purpose Large Language Models (LLMs), is driving the
development of new deception solutions capable of offering greater adaptability
and ease of use. This work proposes the design and implementation of an
LLM-based honeypot to simulate an LDAP server, a critical protocol present in
most organizations due to its central role in identity and access management.
The proposed solution aims to provide a flexible and realistic tool capable of
convincingly interacting with attackers, thereby contributing to early
detection and threat analysis while enhancing the defensive capabilities of
infrastructures against intrusions targeting this service.

</details>


### [17] [Evaluating LLM Generated Detection Rules in Cybersecurity](https://arxiv.org/abs/2509.16749)
*Anna Bertiger,Bobby Filar,Aryan Luthra,Stefano Meschiari,Aiden Mitchell,Sam Scholten,Vivek Sharath*

Main category: cs.CR

TL;DR: The paper introduces an open-source evaluation framework with benchmark metrics for assessing the effectiveness of LLM-generated cybersecurity rules, using a holdout set-based methodology and three key metrics inspired by expert evaluations.


<details>
  <summary>Details</summary>
Motivation: LLMs are becoming more common in cybersecurity, but their effectiveness in this field is not well understood, which restricts trust and practical application among security professionals.

Method: The research presents a benchmark using a holdout set-based methodology to evaluate LLM-generated security rules. It introduces three metrics based on expert evaluation practices for a comprehensive assessment.

Result: The results section includes a detailed analysis of the performance of ADE, Sublime Security's Automated Detection Engineer, showcasing its capabilities and limitations through the proposed metrics.

Conclusion: The open-source evaluation framework provides realistic, multifaceted metrics, helping advance trust in LLM-based cybersecurity solutions by comparing them with human-generated rules through expert-inspired criteria.

Abstract: LLMs are increasingly pervasive in the security environment, with limited
measures of their effectiveness, which limits trust and usefulness to security
practitioners. Here, we present an open-source evaluation framework and
benchmark metrics for evaluating LLM-generated cybersecurity rules. The
benchmark employs a holdout set-based methodology to measure the effectiveness
of LLM-generated security rules in comparison to a human-generated corpus of
rules. It provides three key metrics inspired by the way experts evaluate
security rules, offering a realistic, multifaceted evaluation of the
effectiveness of an LLM-based security rule generator. This methodology is
illustrated using rules from Sublime Security's detection team and those
written by Sublime Security's Automated Detection Engineer (ADE), with a
thorough analysis of ADE's skills presented in the results section.

</details>


### [18] [AdaptiveGuard: Towards Adaptive Runtime Safety for LLM-Powered Software](https://arxiv.org/abs/2509.16861)
*Rui Yang,Michael Fu,Chakkrit Tantithamthavorn,Chetan Arora,Gunel Gulmammadova,Joey Chua*

Main category: cs.CR

TL;DR: This paper introduces AdaptiveGuard, an adaptive guardrail system that achieves 96% novel attack detection and rapid adaptation (2 steps) via continual learning, outperforming prior methods which fail 70%+ of the time.


<details>
  <summary>Details</summary>
Motivation: Existing guardrails (e.g., LlamaGuard) show over 70% failure rates against jailbreak attacks, and their performance drops sharply (<12% accuracy) when facing unseen threats, highlighting the need for post-deployment adaptation.

Method: Proposes AdaptiveGuard, which uses out-of-distribution (OOD) detection and continual learning to identify novel attacks and incrementally adapt defenses through two update steps.

Result: 96% OOD detection accuracy, adaptation within two update steps, and 85% F1-score retention on in-distribution data post-adaptation, outperforming baselines.

Conclusion: AdaptiveGuard is a guardrail that dynamically adapts to emerging jailbreak strategies post-deployment, maintaining high accuracy (96% OOD detection) while retaining performance on existing threats.

Abstract: Guardrails are critical for the safe deployment of Large Language Models
(LLMs)-powered software. Unlike traditional rule-based systems with limited,
predefined input-output spaces that inherently constrain unsafe behavior, LLMs
enable open-ended, intelligent interactions--opening the door to jailbreak
attacks through user inputs. Guardrails serve as a protective layer, filtering
unsafe prompts before they reach the LLM. However, prior research shows that
jailbreak attacks can still succeed over 70% of the time, even against advanced
models like GPT-4o. While guardrails such as LlamaGuard report up to 95%
accuracy, our preliminary analysis shows their performance can drop sharply--to
as low as 12%--when confronted with unseen attacks. This highlights a growing
software engineering challenge: how to build a post-deployment guardrail that
adapts dynamically to emerging threats? To address this, we propose
AdaptiveGuard, an adaptive guardrail that detects novel jailbreak attacks as
out-of-distribution (OOD) inputs and learns to defend against them through a
continual learning framework. Through empirical evaluation, AdaptiveGuard
achieves 96% OOD detection accuracy, adapts to new attacks in just two update
steps, and retains over 85% F1-score on in-distribution data post-adaptation,
outperforming other baselines. These results demonstrate that AdaptiveGuard is
a guardrail capable of evolving in response to emerging jailbreak strategies
post deployment. We release our AdaptiveGuard and studied datasets at
https://github.com/awsm-research/AdaptiveGuard to support further research.

</details>


### [19] [Security Vulnerabilities in Software Supply Chain for Autonomous Vehicles](https://arxiv.org/abs/2509.16899)
*Md Wasiul Haque,Md Erfan,Sagar Dasgupta,Md Rayhanur Rahman,Mizanur Rahman*

Main category: cs.CR

TL;DR: This paper investigates cybersecurity risks in AV open-source software, using static analysis to expose vulnerabilities and advocate for earlier security practices in development.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles (AVs) increasingly rely on open-source software supply chains, which face significant cybersecurity risks due to lax security practices, with 49.5% of automotive cyberattacks exploiting software vulnerabilities.

Method: The authors analyzed security vulnerabilities in open-source AV software (Autoware, Apollo, openpilot) using static analyzers, comparing prevalence of vulnerabilities and static analyzer outputs across repositories.

Result: The analysis identified prevalent security vulnerabilities in AV open-source software and provided a comparative evaluation of static analyzer outputs, highlighting critical flaws across popular repositories.

Conclusion: The study concludes that integrating security best practices early in the software development lifecycle is essential for reducing cybersecurity risks in AV open-source software, ensuring system reliability, data protection, and public trust.

Abstract: The interest in autonomous vehicles (AVs) for critical missions, including
transportation, rescue, surveillance, reconnaissance, and mapping, is growing
rapidly due to their significant safety and mobility benefits. AVs consist of
complex software systems that leverage artificial intelligence (AI), sensor
fusion algorithms, and real-time data processing. Additionally, AVs are
becoming increasingly reliant on open-source software supply chains, such as
open-source packages, third-party software components, AI models, and
third-party datasets. Software security best practices in the automotive sector
are often an afterthought for developers. Thus, significant cybersecurity risks
exist in the software supply chain of AVs, particularly when secure software
development practices are not rigorously implemented. For example, Upstream's
2024 Automotive Cybersecurity Report states that 49.5% of cyberattacks in the
automotive sector are related to exploiting security vulnerabilities in
software systems. In this chapter, we analyze security vulnerabilities in
open-source software components in AVs. We utilize static analyzers on popular
open-source AV software, such as Autoware, Apollo, and openpilot. Specifically,
this chapter covers: (1) prevalent software security vulnerabilities of AVs;
and (2) a comparison of static analyzer outputs for different open-source AV
repositories. The goal is to inform researchers, practitioners, and
policymakers about the existing security flaws in the commonplace open-source
software ecosystem in the AV domain. The findings would emphasize the necessity
of security best practices earlier in the software development lifecycle to
reduce cybersecurity risks, thereby ensuring system reliability, safeguarding
user data, and maintaining public trust in an increasingly automated world.

</details>


### [20] [Temporal Logic-Based Multi-Vehicle Backdoor Attacks against Offline RL Agents in End-to-end Autonomous Driving](https://arxiv.org/abs/2509.16950)
*Xuan Chen,Shiwei Feng,Zikang Xiong,Shengwei An,Yunshu Mao,Lu Yan,Guanhong Tao,Wenbo Guo,Xiangyu Zhang*

Main category: cs.CR

TL;DR: This paper introduces a trajectory-based backdoor attack method for end-to-end autonomous driving systems, demonstrating vulnerabilities not addressed by existing pixel-level attack frameworks.\n\n


<details>
  <summary>Details</summary>
Motivation: Current backdoor attack studies on autonomous driving systems focus on impractical pixel-level triggers, creating a gap in understanding real-world security threats based on vehicle trajectory patterns.\n\n

Method: 1. Utilize temporal logic specifications to define attacker vehicle behavior patterns\n2. Generate trigger trajectories with configurable behavior models that meet specification constraints\n3. Implement negative training with patched trajectories to enhance attack stealthiness\n4.Quantitative evaluation with iterative refinement ensures trajectory accuracy.\n\n

Result: Demonstrated 100\u202f%-effective trajectory-based backdoor attacks on 5 offline RL driving agents with 6 different attack patterns, showing existing AD systems are under-exposed to temporal strategy attacks.\n\n

Conclusion: Existing evaluations of end-to-end autonomous driving systems\u2019 security are insufficient against trajectory-pattern based backdoor attacks, suggesting a need for developing new defense strategies considering temporal behavior patterns.\n\n

Abstract: Assessing the safety of autonomous driving (AD) systems against security
threats, particularly backdoor attacks, is a stepping stone for real-world
deployment. However, existing works mainly focus on pixel-level triggers that
are impractical to deploy in the real world. We address this gap by introducing
a novel backdoor attack against the end-to-end AD systems that leverage one or
more other vehicles' trajectories as triggers. To generate precise trigger
trajectories, we first use temporal logic (TL) specifications to define the
behaviors of attacker vehicles. Configurable behavior models are then used to
generate these trajectories, which are quantitatively evaluated and iteratively
refined based on the TL specifications. We further develop a negative training
strategy by incorporating patch trajectories that are similar to triggers but
are designated not to activate the backdoor. It enhances the stealthiness of
the attack and refines the system's responses to trigger scenarios. Through
extensive experiments on 5 offline reinforcement learning (RL) driving agents
with 6 trigger patterns and target action combinations, we demonstrate the
flexibility and effectiveness of our proposed attack, showing the
under-exploration of existing end-to-end AD systems' vulnerabilities to such
trajectory-based backdoor attacks.

</details>


### [21] [In Numeris Veritas: An Empirical Measurement of Wi-Fi Integration in Industry](https://arxiv.org/abs/2509.16987)
*Vyron Kampourakis,Christos Smiliotopoulos,Vasileios Gkioulos,Sokratis Katsikas*

Main category: cs.CR

TL;DR: This paper analyzes global industrial Wi-Fi security by creating a dataset of 1,087 networks from WiGLE, revealing widespread adoption of IEEE 802.11ax/be but alarming use of weak security configurations in critical infrastructure.


<details>
  <summary>Details</summary>
Motivation: The integration of IT into OT systems is accelerating industrial Wi-Fi adoption (especially next-gen standards), but raises critical security concerns. Current understanding lacks empirical data on real-world deployment patterns and vulnerabilities.

Method: First publicly available industrial Wi-Fi dataset was created by mining the WiGLE crowdsourced database. Analyzed 1,087 high-confidence networks across SSID patterns, encryption methods, vendor types, and global distribution.

Result: Confirmed growing industrial Wi-Fi adoption while exposing persistent security issues: weak/outdated encryption remains prevalent, directly exposing critical infrastructure to threats. Dataset and findings offer new baseline for further research.

Conclusion: Establishes foundational understanding of industrial Wi-Fi security landscape. By combining dataset and empirical analysis, the work directly informs future wireless security research and practice in industrial environments.

Abstract: Traditional air gaps in industrial systems are disappearing as IT
technologies permeate the OT domain, accelerating the integration of wireless
solutions like Wi-Fi. Next-generation Wi-Fi standards (IEEE 802.11ax/be) meet
performance demands for industrial use cases, yet their introduction raises
significant security concerns. A critical knowledge gap exists regarding the
empirical prevalence and security configuration of Wi-Fi in real-world
industrial settings. This work addresses this by mining the global crowdsourced
WiGLE database to provide a data-driven understanding. We create the first
publicly available dataset of 1,087 high-confidence industrial Wi-Fi networks,
examining key attributes such as SSID patterns, encryption methods, vendor
types, and global distribution. Our findings reveal a growing adoption of Wi-Fi
across industrial sectors but underscore alarming security deficiencies,
including the continued use of weak or outdated security configurations that
directly expose critical infrastructure. This research serves as a pivotal
reference point, offering both a unique dataset and practical insights to guide
future investigations into wireless security within industrial environments.

</details>


### [22] [Electronic Reporting Using SM2-Based Ring Signcryption](https://arxiv.org/abs/2509.17048)
*Huifang Yu,Jiaxing Jie,Lei Li*

Main category: cs.CR

TL;DR: A secure and efficient electronic whistleblowing system based on SM2 traceable ring signcryption is proposed to protect whistleblower identity privacy and prevent malicious activities while ensuring information confidentiality.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need to protect whistleblower identity privacy, prevent malicious whistleblowing, and ensure the confidentiality of whistleblowing information in electronic systems.

Method: The authors combined the SM2 elliptic curve public key cryptography algorithm with the ring signature algorithm to propose a SM2 traceable ring signcryption scheme for electronic voting.

Result: Security analysis shows the scheme provides confidentiality, unforgeability, traceability, linkability, and deniability. Efficiency analysis indicates significant advantages in the signature phase compared to existing ring signature schemes.

Conclusion: The designed system using the SM2 traceable ring signcryption scheme can track malicious whistleblowers while protecting user identity privacy and keeping whistleblowing content confidential from third parties.

Abstract: Electronic whistleblowing systems are widely used due to their efficiency and
convenience. The key to designing such systems lies in protecting the identity
privacy of whistleblowers, preventing malicious whistleblowing, and ensuring
the confidentiality of whistleblowing information. To address these issues, a
SM2 traceable ring signcryption scheme for electronic voting is proposed. This
scheme combines the SM2 elliptic curve public key cryptography algorithm with
the ring signature algorithm, enhancing the overall efficiency of the scheme
while ensuring the autonomy and controllability of the core cryptographic
algorithms. Security analysis demonstrates that the proposed scheme satisfies
confidentiality, unforgeability, traceability, linkability, and deniability.
Efficiency analysis shows that, compared to existing ring signature schemes,
the proposed scheme exhibits significant efficiency advantages during the
signature phase. The electronic whistleblowing system designed using the
proposed scheme can track malicious whistleblowers while protecting user
identity privacy, and ensures that the content of whistleblowing remains
unknown to third parties.

</details>


### [23] [Localizing Malicious Outputs from CodeLLM](https://arxiv.org/abs/2509.17070)
*Mayukh Borana,Junyi Liang,Sai Sathiesh Rajan,Sudipta Chattopadhyay*

Main category: cs.CR

TL;DR: This paper presents FreqRank, a method to detect and localize malicious components and backdoor triggers in LLM outputs. The approach is based on the assumption that malicious substrings appear consistently when triggered. It outperforms existing defense methods by 35-50%.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the problem of malicious components and backdoor triggers in Large Language Models (LLMs), which can be exploited to produce harmful or backdoored outputs.

Method: FreqRank uses a frequency-based ranking system that assumes malicious substrings consistently appear when the model is triggered by input samples. Then ranking system leverages to locate the backdoor trigger in the inputs.

Result: FreqRank achieves a high localization accuracy, highlighting malicious outputs as one of the top five suggestions in 98% of cases. The method's effectiveness increases with more mutants and works even with few triggered samples. It is also shown to be 35-50% more effective than other defense methods.

Conclusion: The conclusion is that FreqRank provides a novel and effective defense against backdoor triggers in LLM outputs by effectively localizing malicious components, even in cases with limited data, and outperforms existing methods by a significant margin.

Abstract: We introduce FreqRank, a mutation-based defense to localize malicious
components in LLM outputs and their corresponding backdoor triggers. FreqRank
assumes that the malicious sub-string(s) consistently appear in outputs for
triggered inputs and uses a frequency-based ranking system to identify them.
Our ranking system then leverages this knowledge to localize the backdoor
triggers present in the inputs. We create nine malicious models through
fine-tuning or custom instructions for three downstream tasks, namely, code
completion (CC), code generation (CG), and code summarization (CS), and show
that they have an average attack success rate (ASR) of 86.6%. Furthermore,
FreqRank's ranking system highlights the malicious outputs as one of the top
five suggestions in 98% of cases. We also demonstrate that FreqRank's
effectiveness scales as the number of mutants increases and show that FreqRank
is capable of localizing the backdoor trigger effectively even with a limited
number of triggered samples. Finally, we show that our approach is 35-50% more
effective than other defense methods.

</details>


### [24] [Unaligned Incentives: Pricing Attacks Against Blockchain Rollups](https://arxiv.org/abs/2509.17126)
*Stefanos Chaliasos,Conner Swann,Sina Pilehchiha,Nicolas Mohnblatt,Benjamin Livshits,Assimakis Kattis*

Main category: cs.CR

TL;DR: This paper identifies critical vulnerabilities in Ethereum rollup transaction fee mechanisms (TFMs) enabling adversarial attacks, analyzes their impact across major rollups, and proposes mitigations. Attacks include data-heavy DoS and prover-killing transactions, with quantified costs (as low as 0.8 ETH/hour) and up to 94x finality delays.


<details>
  <summary>Details</summary>
Motivation: Ethereum rollups, securing $55B in assets, must address TFM mispricings to prevent security risks and maintain scalability. Existing TFMs fail to adequately balance Layer 2 execution, Layer 1 data availability (DA), and verification costs.

Method: The authors (1) analyze TFM mispricings, (2) design adversarial attack vectors (data-heavy transactions and prover killers), (3) evaluate attack costs and protocol losses across leading rollups, and (4 ) assess finality delays via comparison to L1 blob-stuffing methods.

Result: Key findings: (1.) Data-heavy attacks cause up to 15-minute DoS for <2 ETH, indefinite DoS for 0.8-2.7 ETH/hour on three rollups, and 1.45x-2.73x finality delays. (2.) Prover killer attacks induce 94x finality latency. (3.) Analysis spans major Ethereum rollup implementations.

Conclusion: The paper proposes TFM redesigns to address mispricings, including multi-dimensional pricing mechanisms. It advocates for fee structures that properly value data, computation, and proving costs to prevent adversarial exploitation while maintaining rollup scalability.

Abstract: Rollups have become the de facto scalability solution for Ethereum, securing
more than $55B in assets. They achieve scale by executing transactions on a
Layer 2 ledger, while periodically posting data and finalizing state on the
Layer 1, either optimistically or via validity proofs. Their fees must
simultaneously reflect the pricing of three resources: L2 costs (e.g.,
execution), L1 DA, and underlying L1 gas costs for batch settlement and proof
verification. In this work, we identify critical mis-pricings in existing
rollup transaction fee mechanisms (TFMs) that allow for two powerful attacks.
Firstly, an adversary can saturate the L2's DA batch capacity with
compute-light data-heavy transactions, forcing low-gas transaction batches that
enable both L2 DoS attacks, and finality-delay attacks. Secondly, by crafting
prover killer transactions that maximize proving cycles relative to the gas
charges, an adversary can effectively stall proof generation, delaying finality
by hours and inflicting prover-side economic losses to the rollup at a minimal
cost.
  We analyze the above attack vectors across the major Ethereum rollups,
quantifying adversarial costs and protocol losses. We find that the first
attack enables periodic DoS on rollups, lasting up to 30 minutes, at a cost
below 2 ETH for most rollups. Moreover, we identify three rollups that are
exposed to indefinite DoS at a cost of approximately 0.8 to 2.7 ETH per hour.
The attack can be further modified to increase finalization delays by a factor
of about 1.45x to 2.73x, compared to direct L1 blob-stuffing, depending on the
rollup's parameters. Furthermore, we find that the prover killer attack induces
a finalization latency increase of about 94x. Finally, we propose comprehensive
mitigations to prevent these attacks and suggest how some practical uses of
multi-dimensional rollup TFMs can rectify the identified mis-pricing attacks.

</details>


### [25] [Bribers, Bribers on The Chain, Is Resisting All in Vain? Trustless Consensus Manipulation Through Bribing Contracts](https://arxiv.org/abs/2509.17185)
*Bence Soóki-Tóth,István András Seres,Kamilla Kara,Ábel Nagy,Balázs Pejó,Gergely Biczók*

Main category: cs.CR

TL;DR: This paper presents three trustless bribery contracts targeting Ethereum validators, enabling blockchain forks, validator exits, and RANDAO manipulation, along with a game-theoretical analysis of one market.


<details>
  <summary>Details</summary>
Motivation: Cryptocurrencies rely on validator incentives, but trustless smart contract-facilitated bribery attacks threaten this foundation.

Method: Three novel bribery contracts were developed: (1) fork-enabling vote-buying, (2) validator exit incentives, and (3) RANDAO manipulation through auctioned bribe markets. One market underwent game-theoretical analysis.

Result: Implementation and evaluation of the contracts demonstrate their efficiency and potential threat. Game-theoretical analysis provides initial insights into market dynamics.

Conclusion: The presented contracts highlight critical vulnerabilities in Ethereum's staking model and randomness mechanisms. Their analysis underscores the need for countermeasures against trustless collusion attacks.

Abstract: The long-term success of cryptocurrencies largely depends on the incentive
compatibility provided to the validators. Bribery attacks, facilitated
trustlessly via smart contracts, threaten this foundation. This work
introduces, implements, and evaluates three novel and efficient bribery
contracts targeting Ethereum validators. The first bribery contract enables a
briber to fork the blockchain by buying votes on their proposed blocks. The
second contract incentivizes validators to voluntarily exit the consensus
protocol, thus increasing the adversary's relative staking power. The third
contract builds a trustless bribery market that enables the briber to auction
off their manipulative power over the RANDAO, Ethereum's distributed randomness
beacon. Finally, we provide an initial game-theoretical analysis of one of the
described bribery markets.

</details>


### [26] [Seeing is Deceiving: Mirror-Based LiDAR Spoofing for Autonomous Vehicle Deception](https://arxiv.org/abs/2509.17253)
*Selma Yahia,Ildi Alla,Girija Bangalore Mohan,Daniel Rau,Mridula Singh,Valeria Loscri*

Main category: cs.CR

TL;DR: Researchers demonstrate cost-effective LiDAR spoofing attacks using mirrors to deceive self-driving cars into seeing/non-seeing objects, with experiments showing these threats can bypass existing perception systems and evade proposed defenses effectively.


<details>
  <summary>Details</summary>
Motivation: This work addresses critical security vulnerabilities in AV LiDAR systems by demonstrating low-cost, hardware-free spoofing attacks using everyday reflective materials, which prior research has overlooked in real-world deployment scenarios.

Method: The researchers employed geometric optics modeling, conducted real-world outdoor experiments with commercial LiDAR and an Autoware car, and implemented CARLA simulations to validate passive mirror-based attacks for object insertion/removal.

Result: Experiments confirmed mirror attacks can distort occupancy grids, generate false obstacles, cause AVs to fail object detection, and induce safety-critical control failures, while proposed defenses face technical limitations.

Conclusion: The study concludes that mirror-based LiDAR spoofing attacks pose significant security risks to autonomous vehicles by manipulating perception, highlighting urgent needs for robust defense mechanisms against passive optical threats.

Abstract: Autonomous vehicles (AVs) rely heavily on LiDAR sensors for accurate 3D
perception. We show a novel class of low-cost, passive LiDAR spoofing attacks
that exploit mirror-like surfaces to inject or remove objects from an AV's
perception. Using planar mirrors to redirect LiDAR beams, these attacks require
no electronics or custom fabrication and can be deployed in real settings. We
define two adversarial goals: Object Addition Attacks (OAA), which create
phantom obstacles, and Object Removal Attacks (ORA), which conceal real
hazards. We develop geometric optics models, validate them with controlled
outdoor experiments using a commercial LiDAR and an Autoware-equipped vehicle,
and implement a CARLA-based simulation for scalable testing. Experiments show
mirror attacks corrupt occupancy grids, induce false detections, and trigger
unsafe planning and control behaviors. We discuss potential defenses (thermal
sensing, multi-sensor fusion, light-fingerprinting) and their limitations.

</details>


### [27] [Bridging Cybersecurity Practice and Law: a Hands-on, Scenario-Based Curriculum Using the NICE Framework to Foster Skill Development](https://arxiv.org/abs/2509.17263)
*Colman McGuan,Aadithyan V. Raghavan,Komala M. Mandapati,Chansu Yu,Brian E. Ray,Debbie K. Jackson,Sathish Kumar*

Main category: cs.CR

TL;DR: This paper designs a scenario-driven cybersecurity model for SMBs using the NIST NICE Framework, combining technical and non-technical training to improve real-world threat response and workforce preparation.


<details>
  <summary>Details</summary>
Motivation: Small and medium businesses (SMBs) face significant challenges in implementing cybersecurity frameworks like NIST NICE, necessitating simplified, practical solutions to identify, prepare, and respond to cyber threats effectively.

Method: The paper employs a four-pronged approach: identifying frequent attack vectors, proposing a TKSA-based NICE Framework model, developing a scenario-based curriculum for immersive learning, and integrating real-world skill development.

Result: The research delivers a scenario-based training model for SMBs to evaluate and enhance workforce capabilities, alongside a curriculum for educational institutions to produce adequately trained cybersecurity professionals.

Conclusion: The paper concludes that integrating scenario-based learning and practical experience, aligned with the NIST NICE Framework's TKSA, provides a structured approach for SMBs to enhance their cybersecurity workforce readiness through realistic training and workforce evaluation.

Abstract: In an increasingly interconnected world, cybersecurity professionals play a
pivotal role in safeguarding organizations from cyber threats. To secure their
cyberspace, organizations are forced to adopt a cybersecurity framework such as
the NIST National Initiative for Cybersecurity Education Workforce Framework
for Cybersecurity (NICE Framework). Although these frameworks are a good
starting point for businesses and offer critical information to identify,
prevent, and respond to cyber incidents, they can be difficult to navigate and
implement, particularly for small-medium businesses (SMB). To help overcome
this issue, this paper identifies the most frequent attack vectors to SMBs
(Objective 1) and proposes a practical model of both technical and
non-technical tasks, knowledge, skills, abilities (TKSA) from the NICE
Framework for those attacks (Objective 2). The research develops a
scenario-based curriculum. By immersing learners in realistic cyber threat
scenarios, their practical understanding and preparedness in responding to
cybersecurity incidents is enhanced (Objective 3). Finally, this work
integrates practical experience and real-life skill development into the
curriculum (Objective 4). SMBs can use the model as a guide to evaluate, equip
their existing workforce, or assist in hiring new employees. In addition,
educational institutions can use the model to develop scenario-based learning
modules to adequately equip the emerging cybersecurity workforce for SMBs.
Trainees will have the opportunity to practice both technical and legal issues
in a simulated environment, thereby strengthening their ability to identify,
mitigate, and respond to cyber threats effectively.

</details>


### [28] [Privacy-Preserving State Estimation with Crowd Sensors: An Information-Theoretic Respective](https://arxiv.org/abs/2509.17266)
*Farhad Farokhi*

Main category: cs.CR

TL;DR: This paper proposed privacy-preserving state estimation method for LTI system using randomly selected crowd sensors, and it is possible to achieve any desired level of privacy by adjusting noise variance added during estimation process.


<details>
  <summary>Details</summary>
Motivation: Existing methods of state estimation in LTI systems using crowd sensors do not account for privacy concerns. The paper addresses this by suggesting a method that limits the information released through state estimation.

Method: Paper introduces a Luenberger-like observer, fusing measured data from a randomly sampled sensor with the actual system's model. Then, a specific property of mutual information has been leveraged to internally quantify the information leak and ensure the model preserves the privacy through the mathematical proof of this property, not just by selecting low leakage values.

Result: It is validated that through adjusting the variance of added privacy-preserving noise, any predetermined information leakage level can be achieved. Thus, the paper presents a technically robust solution that allows for a precise adjustment of the privacy-utility trade-off in the state estimation process.

Conclusion: The proposed method not only effectively handles the privacy-utility trade-off in state estimation involving crowd sensors but also provides an analytical assurance that it can achieve any desired privacy level, making it a reliable framework for applications where privacy is a concern.

Abstract: Privacy-preserving state estimation for linear time-invariant dynamical
systems with crowd sensors is considered. At any time step, the estimator has
access to measurements from a randomly selected sensor from a pool of sensors
with pre-specified models and noise profiles. A Luenberger-like observer is
used to fuse the measurements with the underlying model of the system to
recursively generate the state estimates. An additive privacy-preserving noise
is used to constrain information leakage. Information leakage is measured via
mutual information between the identity of the sensors and the state estimate
conditioned on the actual state of the system. This captures an omnipotent
adversary that not only can access state estimates but can also gather direct
high-quality state measurements. Any prescribed level of information leakage is
shown to be achievable by appropriately selecting the variance of the
privacy-preserving noise. Therefore, privacy-utility trade-off can be
fine-tuned.

</details>


### [29] [TextCrafter: Optimization-Calibrated Noise for Defending Against Text Embedding Inversion](https://arxiv.org/abs/2509.17302)
*Duoxun Tang,Xinhang Jiang,Jiajun Niu*

Main category: cs.CR

TL;DR: TextCrafter defends against text inversion attacks using RL-guided directional perturbations, maintaining 70% accuracy under strict privacy while surpassing baseline methods in privacy-utility tradeoffs.


<details>
  <summary>Details</summary>
Motivation: Text embedding inversion attacks threaten privacy in collaborative inference systems by reconstructing sensitive input text from latent representations. Existing defenses either lack learnability or ignore directional perturbation strategies, necessitating a more effective privacy-utility balanced solution.

Method: TextCrafter combines reinforcement learning (RL), geometry-aware orthogonal noise injection, cluster priors, and PII (Personally Identifiable Information) guidance to create adversarial perturbations that suppress text inversion attacks without compromising task utility.

Result: Achieved 70% classification accuracy under strong privacy settings across four datasets, while consistently outperforming Gaussian noise and LDP (Local Differential Privacy) baselines at lower privacy budgets.

Conclusion: TextCrafter effectively balances privacy and utility by introducing a directional perturbation mechanism that outperforms existing baselines while maintaining significant task performance.

Abstract: Text embedding inversion attacks reconstruct original sentences from latent
representations, posing severe privacy threats in collaborative inference and
edge computing. We propose TextCrafter, an optimization-based adversarial
perturbation mechanism that combines RL learned, geometry aware noise injection
orthogonal to user embeddings with cluster priors and PII signal guidance to
suppress inversion while preserving task utility. Unlike prior defenses either
non learnable or agnostic to perturbation direction, TextCrafter provides a
directional protective policy that balances privacy and utility. Under strong
privacy setting, TextCrafter maintains 70 percentage classification accuracy on
four datasets and consistently outperforms Gaussian/LDP baselines across lower
privacy budgets, demonstrating a superior privacy utility trade off.

</details>


### [30] [SilentStriker:Toward Stealthy Bit-Flip Attacks on Large Language Models](https://arxiv.org/abs/2509.17371)
*Haotian Xu,Qingsong Peng,Jie Shi,Huadi Zheng,Yu Li,Cheng Zhuo*

Main category: cs.CR

TL;DR: SilentStriker introduces a stealthy bit-flip attack for LLMs that maintains output naturalness while causing performance degradation, overcoming limitations of prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing bit-flip attacks fail to balance performance degradation and output naturalness, limiting their practicality and stealthiness.

Method: Reformulates attack loss using key output token suppression and employs an iterative, progressive search strategy to optimize effectiveness and stealthiness.

Result: Experiments demonstrate SilentStriker significantly outperforms prior baselines in both attack efficacy and maintaining text naturalness compared to perplexity-based methods.

Conclusion: SilentStriker addresses key challenges in BFA design by introducing a token-based loss formulation and progressive search strategy, setting new benchmarks for stealthy LLM attacks.

Abstract: The rapid adoption of large language models (LLMs) in critical domains has
spurred extensive research into their security issues. While input manipulation
attacks (e.g., prompt injection) have been well studied, Bit-Flip Attacks
(BFAs) -- which exploit hardware vulnerabilities to corrupt model parameters
and cause severe performance degradation -- have received far less attention.
Existing BFA methods suffer from key limitations: they fail to balance
performance degradation and output naturalness, making them prone to discovery.
In this paper, we introduce SilentStriker, the first stealthy bit-flip attack
against LLMs that effectively degrades task performance while maintaining
output naturalness. Our core contribution lies in addressing the challenge of
designing effective loss functions for LLMs with variable output length and the
vast output space. Unlike prior approaches that rely on output perplexity for
attack loss formulation, which inevitably degrade output naturalness, we
reformulate the attack objective by leveraging key output tokens as targets for
suppression, enabling effective joint optimization of attack effectiveness and
stealthiness. Additionally, we employ an iterative, progressive search strategy
to maximize attack efficacy. Experiments show that SilentStriker significantly
outperforms existing baselines, achieving successful attacks without
compromising the naturalness of generated text.

</details>


### [31] [A Lightweight Authentication and Key Agreement Protocol Design for FANET](https://arxiv.org/abs/2509.17409)
*Yao Wu,Ziye Jia,Qihui Wu,Yian Zhu*

Main category: cs.CR

TL;DR: This paper proposes a lightweight authentication and key agreement protocol for FANETs integrating PUFs and dynamic credentials to enhance security and efficiency.


<details>
  <summary>Details</summary>
Motivation: FANETs face security and communication challenges due to resource constraints, dynamic topologies, and open environments. Existing protocols are vulnerable as they rely on stored sensitive information.

Method: The protocol integrates physical unclonable functions (PUFs), dynamic credential management, and lightweight cryptographic primitives to reduce overhead and enhance security.

Result: Security analysis confirms resilience against attacks, and evaluations show superiority in security, communication efficiency, and computational cost compared to existing protocols.

Conclusion: The proposed protocol effectively addresses FANETs' security limitations with a lightweight, efficient solution.

Abstract: The advancement of low-altitude intelligent networks enables unmanned aerial
vehicle (UAV) interconnection via flying ad-hoc networks (FANETs), offering
flexibility and decentralized coordination. However, resource constraints,
dynamic topologies, and UAV operations in open environments present significant
security and communication challenges. Existing multi-factor and public-key
cryptography protocols are vulnerable due to their reliance on stored sensitive
information, increasing the risk of exposure and compromise. This paper
proposes a lightweight authentication and key agreement protocol for FANETs,
integrating physical unclonable functions with dynamic credential management
and lightweight cryptographic primitives. The protocol reduces computational
and communication overhead while enhancing security. Security analysis confirms
its resilience against various attacks, and comparative evaluations demonstrate
its superiority in security, communication efficiency, and computational cost.

</details>


### [32] [DINVMark: A Deep Invertible Network for Video Watermarking](https://arxiv.org/abs/2509.17416)
*Jianbin Ji,Dawen Xu,Li Dong,Lin Yang,Songhan He*

Main category: cs.CR

TL;DR: This paper proposes DINVMark, a Deep Invertible Network for video watermarking that improves capacity and robustness through an INN-based architecture and a HEVC compression-simulated noise layer.


<details>
  <summary>Details</summary>
Motivation: Existing video watermarking methods suffer from limited capacity, poor robustness, and lack of HEVC-specific noise layers, necessitating a more effective solution for copyright protection and authentication.

Method: DINVMark employs an Invertible Neural Network (INN), sharing encoder-decoder structures for watermarking and extraction, and introduces a noise layer to simulate HEVC compression, ensuring tight encoder-decoder coupling.

Result: Experiments show significant robustness improvements, higher embedding capacity, and preserved video quality under HEVC compression compared to prior methods.

Conclusion: DINVMark addresses key challenges in video watermarking by leveraging invertible networks and HEVC-simulated noise, achieving superior performance in capacity, robustness, and quality.

Abstract: With the wide spread of video, video watermarking has become increasingly
crucial for copyright protection and content authentication. However, video
watermarking still faces numerous challenges. For example, existing methods
typically have shortcomings in terms of watermarking capacity and robustness,
and there is a lack of specialized noise layer for High Efficiency Video
Coding(HEVC) compression. To address these issues, this paper introduces a Deep
Invertible Network for Video watermarking (DINVMark) and designs a noise layer
to simulate HEVC compression. This approach not only in creases watermarking
capacity but also enhances robustness. DINVMark employs an Invertible Neural
Network (INN), where the encoder and decoder share the same network structure
for both watermark embedding and extraction. This shared architecture ensures
close coupling between the encoder and decoder, thereby improving the accuracy
of the watermark extraction process. Experimental results demonstrate that the
proposed scheme significantly enhances watermark robustness, preserves video
quality, and substantially increases watermark embedding capacity.

</details>


### [33] [Privacy in Action: Towards Realistic Privacy Mitigation and Evaluation for LLM-Powered Agents](https://arxiv.org/abs/2509.17488)
*Shouju Wang,Fenglin Yu,Xirui Liu,Xiaoting Qin,Jue Zhang,Qingwei Lin,Dongmei Zhang,Saravan Rajmohan*

Main category: cs.CR

TL;DR: Addresses agentic LLM privacy risks with novel benchmarking and mitigation frameworks achieving 75%+ privacy leakage reduction across major models


<details>
  <summary>Details</summary>
Motivation: Autonomous LLM agents using MCP/A2A frameworks reveal significant privacy leakage risks that static benchmarks fail to capture, with performance gaps between privacy Q&A capabilities and actual agent behavior.

Method: Proposes PrivacyChecker (contextual integrity-based mitigation) and PrivacyLens-Live (dynamic benchmarking framework) with three deployment strategies for agent protocols.

Result: Reduces privacy leakage to 7.30% (DeepSeek-R1) and 8.32% (GPT-4o) while maintaining task helpfulness; dynamic benchmarking identifies substantially higher practical risks.

Conclusion: The presented approach offers practical privacy protection for emerging agentic ecosystems through modular integration into agent protocols, reducing privacy risks in real-world applications.

Abstract: The increasing autonomy of LLM agents in handling sensitive communications,
accelerated by Model Context Protocol (MCP) and Agent-to-Agent (A2A)
frameworks, creates urgent privacy challenges. While recent work reveals
significant gaps between LLMs' privacy Q&A performance and their agent
behavior, existing benchmarks remain limited to static, simplified scenarios.
We present PrivacyChecker, a model-agnostic, contextual integrity based
mitigation approach that effectively reduces privacy leakage from 36.08% to
7.30% on DeepSeek-R1 and from 33.06% to 8.32% on GPT-4o, all while preserving
task helpfulness. We also introduce PrivacyLens-Live, transforming static
benchmarks into dynamic MCP and A2A environments that reveal substantially
higher privacy risks in practical. Our modular mitigation approach integrates
seamlessly into agent protocols through three deployment strategies, providing
practical privacy protection for the emerging agentic ecosystem. Our data and
code will be made available at https://aka.ms/privacy_in_action.

</details>


### [34] [Community Covert Communication - Dynamic Mass Covert Communication Through Social Media](https://arxiv.org/abs/2509.17508)
*Eric Filiol*

Main category: cs.CR

TL;DR: The paper examines the evolution of social network-based influence technologies and explores how techniques used for managing avatar communities can facilitate the encrypted dissemination of large data volumes to specific individuals, fundamentally redefining traditional communication and making eavesdropping and interception ineffective.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the desire to investigate the potential of using community management techniques on social networks for encrypted communication, building upon the exponential growth of automated influence operations such as those carried out by sock puppet masters.

Method: The researchers considered the use cases of sock puppet master activities, where specialized software automates the creation and management of avatars within communities to conduct influence operations. They analyzed how these same techniques can be leveraged to transmit multi-level encrypted information across controlled data spaces.

Result: The research demonstrated that the techniques used for orchestrating large-scale, automated influence operations can be adapted to securely and efficiently disseminate large volumes of encrypted data to targeted actors, disrupting conventional eavesdropping, interception, and jamming methods.

Conclusion: The study concluded that the principles of managing avatar communities on social networks can redefine communication, leveraging encrypted data transmission to a select group of actors in a way that is resilient to traditional surveillance and interference tactics.

Abstract: Since the early 2010s, social network-based influence technologies have grown
almost exponentially. Initiated by the U.S. Army's early OEV system in 2011, a
number of companies specializing in this field have emerged. The most
(in)famous cases are Bell Pottinger, Cambridge Analytica, Aggregate-IQ and,
more recently, Team Jorge.
  In this paper, we consider the use-case of sock puppet master activities,
which consist in creating hundreds or even thousands of avatars, in organizing
them into communities and implement influence operations. On-purpose software
is used to automate these operations (e.g. Ripon software, AIMS) and organize
these avatar populations into communities. The aim is to organize targeted and
directed influence communication to rather large communities (influence
targets).
  The goal of the present research work is to show how these community
management techniques (social networks) can also be used to
communicate/disseminate relatively large volumes (up to a few tens of Mb) of
multi-level encrypted information to a limited number of actors. To a certain
extent, this can be compared to a Dark Post-type function, with a number of
much more powerful potentialities. As a consequence, the concept of
communication has been totally redefined and disrupted, so that eavesdropping,
interception and jamming operations no longer make sense.

</details>


### [35] [Impossibility Results of Card-Based Protocols via Mathematical Optimization](https://arxiv.org/abs/2509.17595)
*Shunnosuke Ikeda,Kazumasa Shinagawa*

Main category: cs.CR

TL;DR: This paper applies mathematical optimization to prove impossibility of new single-cut full-open (SCFO) card-based protocols for three-variable Boolean functions when all additional cards share the same color, creating a scalable proof framework.


<details>
  <summary>Details</summary>
Motivation: Previous impossibility proofs in card-based cryptography were limited to small card numbers; this work addresses the need for generalizable proofs applicable to large card sets.

Method: The authors use mathematical optimization techniques to analyze SCFO protocols, which involve one random cut followed by full card revelation, focusing on constraints of three-variable Boolean functions and same-colored additional cards.

Result: They proved no new SCFO protocols exist for three-variable Boolean functions under the same-colored cards condition, with the framework valid for any number of cards in this scenario.

Conclusion: The research provides a novel optimization-based framework for impossibility proofs in card-based cryptography, delivering a general result for card counts with same-colored additional cards.

Abstract: This paper introduces mathematical optimization as a new method for proving
impossibility proofs in the field of card-based cryptography. While previous
impossibility proofs were often limited to cases involving a small number of
cards, this new approach establishes results that hold for a large number of
cards. The research focuses on single-cut full-open (SCFO) protocols, which
consist of performing one random cut and then revealing all cards. The main
contribution is that for any three-variable Boolean function, no new SCFO
protocols exist beyond those already known, under the condition that all
additional cards have the same color. The significance of this work is that it
provides a new framework for impossibility proofs and delivers a proof that is
valid for any number of cards, as long as all additional cards have the same
color.

</details>


### [36] [Ordered Multi-Signatures with Public-Key Aggregation from SXDH Assumption](https://arxiv.org/abs/2509.17709)
*Masayuki Tezuka,Keisuke Tanaka*

Main category: cs.CR

TL;DR: This work introduces an efficient ordered multi-signature scheme with public-key aggregation, proven secure under SXDH without random oracles.


<details>
  <summary>Details</summary>
Motivation: To address security and efficiency needs for verifying signer order in multi-signature systems, particularly improving parameter size and eliminating reliance on random oracles.

Method: The authors modified the sequential aggregate signature scheme by Chatterjee and Kabaleeshwaran (ACISP 2020) to include ordered signing and public-key aggregation properties, enabling short aggregated keys.

Result: The scheme achieves compact public parameters, public-key aggregation, and formal security guarantees under the SXDH assumption, offering practical advantages over existing methods.

Conclusion: The proposed ordered multi-signature scheme enhances efficiency through compact parameters and public-key aggregation while maintaining security under the SXDH assumption without relying on the random oracle model.

Abstract: An ordered multi-signature scheme allows multiple signers to sign a common
message in a sequential manner and allows anyone to verify the signing order of
signers with a public-key list. In this work, we propose an ordered
multi-signature scheme by modifying the sequential aggregate signature scheme
by Chatterjee and Kabaleeshwaran (ACISP 2020). Our scheme offers compact public
parameter size and the public-key aggregation property. This property allows us
to compress a public-key list into a short aggregated key. We prove the
security of our scheme under the symmetric external Diffie-Hellman (SXDH)
assumption without the random oracle model.

</details>


### [37] [Public Key Encryption with Equality Test from Tag-Based Encryption](https://arxiv.org/abs/2509.17722)
*Masayuki Tezuka,Keisuke Tanaka*

Main category: cs.CR

TL;DR: This paper proposes a novel generic PKEET construction based on tag-based encryption without requiring the random oracle model or identity-based encryption, enabling practical and secure equality testing across ciphertexts under different keys.


<details>
  <summary>Details</summary>
Motivation: Existing PKEET schemes rely on impractical assumptions (random oracle model or hierarchical identity-based encryption). The authors aim to develop a robust PKEET construction with weaker cryptographic primitives to enhance real-world applicability and security.

Method: The construction leverages tag-based encryption (a weaker primitive than identity-based encryption) as the foundation. It presents a generic framework and instantiates it with specific tag-based encryption schemes, including a pairing-free model (Kiltz 2006) and a scheme based on the Learning Parity with Noise (LPN)-assumption.

Result: Achieves a PKEET scheme without the random oracle model; instantiations yield a pairing-free PKEET and one based on the LPN assumption, demonstrating flexibility and security from diverse assumptions.

Conclusion: The proposed method overcomes limitations of prior work by utilizing tag-based encryption, enabling efficient and secure PKEET schemes with broader practical relevance and theoretical robustness.

Abstract: Public key encryption with equality test (PKEET), proposed by Yang et al.
(CT-RSA 2010), is a variant of public key encryption that enables an equality
test to determine whether two ciphertexts correspond to the same plaintext.
This test applies not only for ciphertexts generated under the same encryption
key but also for those generated under different encryption keys. To date,
several generic constructions of PKEET have been proposed. However, these
generic constructions have the drawback of reliance on the random oracle model
or a (hierarchical) identity-based encryption scheme. In this paper, we propose
a generic construction of a PKEET scheme based on tag-based encryption without
the random oracle model. Tag-based encryption is a weaker primitive than
identity-based encryption. Our scheme allows to derive new PKEET schemes
without the random oracle model. By instantiating our construction with the
pairing-free tag-based encryption scheme by Kiltz (TCC 2006), we obtain a
pairing-free PKEET scheme without the random oracle model. Moreover, by
instantiating our construction with a tag-based encryption scheme based on the
learning parity with noise (LPN) assumption, we obtain a PKEET scheme based on
the LPN assumption without the random oracle model.

</details>


### [38] [AEAS: Actionable Exploit Assessment System](https://arxiv.org/abs/2509.17832)
*Xiangmin Shen,Wenyuan Cheng,Yan Chen,Zhenyuan Li,Yuqiao Gu,Lingzhi Wang,Wencheng Zhao,Dawei Sun,Jiashui Wang*

Main category: cs.CR

TL;DR: AEAS is an automated system that prioritizes actionable exploits using static analysis of code and documentation, achieving high accuracy in recommending functional exploits.


<details>
  <summary>Details</summary>
Motivation: Current exploit scoring systems (CVSS, EPSS)

Method: AEAS performs static analysis on exploit code and documentation to extract features like availability, functionality, and setup complexity, generating an actionability score and ranked recommendations.

Result: AEAS achieved 100%

Conclusion: AEAS demonstrates significant effectiveness in exploit-driven vulnerability prioritization, outperforming existing systems through transparent, code-based analysis.

Abstract: Security practitioners face growing challenges in exploit assessment, as
public vulnerability repositories are increasingly populated with inconsistent
and low-quality exploit artifacts. Existing scoring systems, such as CVSS and
EPSS, offer limited support for this task. They either rely on theoretical
metrics or produce opaque probability estimates without assessing whether
usable exploit code exists. In practice, security teams often resort to manual
triage of exploit repositories, which is time-consuming, error-prone, and
difficult to scale. We present AEAS, an automated system designed to assess and
prioritize actionable exploits through static analysis. AEAS analyzes both
exploit code and associated documentation to extract a structured set of
features reflecting exploit availability, functionality, and setup complexity.
It then computes an actionability score for each exploit and produces ranked
exploit recommendations. We evaluate AEAS on a dataset of over 5,000
vulnerabilities derived from 600+ real-world applications frequently
encountered by red teams. Manual validation and expert review on representative
subsets show that AEAS achieves a 100% top-3 success rate in recommending
functional exploits and shows strong alignment with expert-validated rankings.
These results demonstrate the effectiveness of AEAS in supporting
exploit-driven vulnerability prioritization.

</details>


### [39] [Federated Learning in the Wild: A Comparative Study for Cybersecurity under Non-IID and Unbalanced Settings](https://arxiv.org/abs/2509.17836)
*Roberto Doriguzzi-Corin,Petr Sabel,Silvio Cretti,Silvio Ranise*

Main category: cs.CR

TL;DR: This study evaluates Federated Learning methods for intrusion detection, focusing on DDoS attacks in a Kubernetes testbed under non-i.i.d. and unbalanced data conditions, aiming to find algorithms that converge well while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: Machine Learning needs large, representative datasets for network traffic analysis, but data-sharing in cybersecurity is hindered by privacy constraints. FL allows collaborative model training without sharing sensitive data, but existing algorithms like FedAvg struggle with convergence in non-i.i.d. and imbalanced environments typical in cybersecurity.

Method: The paper conducts a systematic review and empirical evaluation of various FL algorithms using a Kubernetes-based testbed and network attack datasets. Key metrics include convergence efficiency, model accuracy, computational overhead, and bandwidth usage.

Result: The study identifies FL algorithms that outperform FedAvg in terms of convergence and accuracy under non-i.i.d. and unbalanced data conditions. Specific methods are compared to evaluate their effectiveness in detecting DDoS attacks.

Conclusion: This work provides the first comprehensive comparison of FL algorithms for intrusion detection under realistic non-i.i.d. and unbalanced data scenarios, offering guidance for developing more effective and privacy-aware network security systems.

Abstract: Machine Learning (ML) techniques have shown strong potential for network
traffic analysis; however, their effectiveness depends on access to
representative, up-to-date datasets, which is limited in cybersecurity due to
privacy and data-sharing restrictions. To address this challenge, Federated
Learning (FL) has recently emerged as a novel paradigm that enables
collaborative training of ML models across multiple clients while ensuring that
sensitive data remains local. Nevertheless, Federated Averaging (FedAvg), the
canonical FL algorithm, has proven poor convergence in heterogeneous
environments where data distributions are non-independent and identically
distributed (i.i.d.) and client datasets are unbalanced, conditions frequently
observed in cybersecurity contexts. To overcome these challenges, several
alternative FL strategies have been developed, yet their applicability to
network intrusion detection remains insufficiently explored. This study
systematically reviews and evaluates a range of FL methods in the context of
intrusion detection for DDoS attacks. Using a dataset of network attacks within
a Kubernetes-based testbed, we assess convergence efficiency, computational
overhead, bandwidth consumption, and model accuracy. To the best of our
knowledge, this is the first comparative analysis of FL algorithms for
intrusion detection under realistic non-i.i.d. and unbalanced settings,
providing new insights for the design of robust, privacypreserving network
security solutions.

</details>


### [40] [B-Privacy: Defining and Enforcing Privacy in Weighted Voting](https://arxiv.org/abs/2509.17871)
*Samuel Breckenridge,Dani Vilardell,Andrés Fábrega,Amy Zhao,Patrick McCorry,Rafael Solari,Ari Juels*

Main category: cs.CR

TL;DR: This paper addresses privacy challenges in weighted-voting systems (common in crypto/web3), proposing B-privacy (bribery-based privacy metric) and a tally-noising mechanism. While effective in balanced settings, it struggles against concentrated voting power ('whales'), offering the first principled analysis of transparency-privacy tradeoffs in these systems.


<details>
  <summary>Details</summary>
Motivation: Traditional ballot secrecy frameworks fail in weighted-voting systems where vote weights correlate with token holdings. Published tallies can leak individual choices due to vote-weight mediation, necessitating new privacy notions beyond ballot secrecy.

Method: Introduce B-privacy (quantifies adversary costs for hypothetical bribes based on revealed tallies). Propose a privacy mechanism that adds noise to voting tallies, mathematically analyzing its privacy/transparency tradeoffs. Empirically test on 3,582 DAO proposals to assess effectiveness.

Result: Mechanism achieves 4.1× mean B-privacy improvement in proposals requiring ≥5-voter coalitions. Shows large-voter 'whales' significantly limit privacy gains. Discrepancy between privacy theory and practice: real-world vote weight concentration breaks many privacy assumptions.

Conclusion: Weighted voting systems require rebalancing privacy and transparency through bribery economics. While existing confidentiality-centric approaches are insufficient, our work establishes theoretical foundations showing voting weight concentration fundamentally constrains privacy mechanism effectiveness.

Abstract: In traditional, one-vote-per-person voting systems, privacy equates with
ballot secrecy: voting tallies are published, but individual voters' choices
are concealed.
  Voting systems that weight votes in proportion to token holdings, though, are
now prevalent in cryptocurrency and web3 systems. We show that these
weighted-voting systems overturn existing notions of voter privacy. Our
experiments demonstrate that even with secret ballots, publishing raw tallies
often reveals voters' choices.
  Weighted voting thus requires a new framework for privacy. We introduce a
notion called B-privacy whose basis is bribery, a key problem in voting systems
today. B-privacy captures the economic cost to an adversary of bribing voters
based on revealed voting tallies.
  We propose a mechanism to boost B-privacy by noising voting tallies. We prove
bounds on its tradeoff between B-privacy and transparency, meaning
reported-tally accuracy. Analyzing 3,582 proposals across 30 Decentralized
Autonomous Organizations (DAOs), we find that the prevalence of large voters
("whales") limits the effectiveness of any B-Privacy-enhancing technique.
However, our mechanism proves to be effective in cases without extreme voting
weight concentration: among proposals requiring coalitions of $\geq5$ voters to
flip outcomes, our mechanism raises B-privacy by a geometric mean factor of
$4.1\times$.
  Our work offers the first principled guidance on transparency-privacy
tradeoffs in weighted-voting systems, complementing existing approaches that
focus on ballot secrecy and revealing fundamental constraints that voting
weight concentration imposes on privacy mechanisms.

</details>


### [41] [What if we could hot swap our Biometrics?](https://arxiv.org/abs/2509.17962)
*Jon Crowcroft,Anil Madhavapeddy,Chris Hicks,Richard Mortier,Vasilios Mavroudis*

Main category: cs.CR

TL;DR: This paper explores speculative biotech methods for swapping biometric identities, raising concerns about their security risks while acknowledging potential benefits.


<details>
  <summary>Details</summary>
Motivation: Current biometric systems are vulnerable if they become increasingly counterfeitable, potentially making biometric authentication less secure and more costly for individuals to manage.

Method: The authors propose novel biotechnology-based mechanisms for hot swapping biometric identities and analyze potential use cases and consequences.

Result: The paper outlines potential positive and negative outcomes of identity-swapping technology, highlighting a critical trade-off in biometric security when counterfeiting becomes easier than identity alteration.

Conclusion: The paper emphasizes the need to consider potential scenarios where biometric identity swapping could lead to significant security and ethical challenges, urging further discussion and precautionary measures.

Abstract: What if you could really revoke your actual biometric identity, and install a
new one, by live rewriting your biological self? We propose some novel
mechanisms for hot swapping identity based in novel biotechnology. We discuss
the potential positive use cases, and negative consequences if such technology
was to become available and affordable. Biometrics are selected on the basis
that they are supposed to be unfakeable, or at least not at reasonable cost. If
they become easier to fake, it may be much cheaper to fake someone else's
biometrics than it is for you to change your own biometrics if someone does
copy yours. This potentially makes biometrics a bad trade-off for the user. At
the time of writing, this threat is highly speculative, but we believe it is
worth raising and considering the potential consequences.

</details>


### [42] [The Reverse File System: Towards open cost-effective secure WORM storage devices for logging](https://arxiv.org/abs/2509.17969)
*Gorka Guardiola Múzquiz,Juan González-Gómez,Enrique Soriano-Salvador*

Main category: cs.CR

TL;DR: Socarrat is a cost-effective, local WORM storage solution using a USB-connected single-board Linux device with a Reverse File System to enforce data immutability, offering a tamper-evident alternative to costly or complex existing solutions.


<details>
  <summary>Details</summary>
Motivation: Current WORM storage systems rely on expensive/closed hardware or introduce security risks via distributed architectures, necessitating an accessible, secure, and low-cost alternative.

Method: Leverages a USB-connected Linux single-board computer (with USB On-The-Go support) to implement a tamper-evident WORM device using a 'Reverse File System' that isolates enforcement logic hardware and avoids privileged software layers.

Result: A prototype implemented in Go, evaluated across SBCs, demonstrating practical WORM enforcement with ext4/exFAT compatibility, tamper resistance, and low operational overhead.

Conclusion: Socarrat addresses WORM storage limitations with a secure, software-free hardware approach, offering free/libre software and performance validation on affordable hardware.

Abstract: Write Once Read Many (WORM) properties for storage devices are desirable to
ensure data immutability for applications such as secure logging, regulatory
compliance, archival storage, and other types of backup systems. WORM devices
guarantee that data, once written, cannot be altered or deleted. However,
implementing secure and compatible WORM storage remains a challenge.
Traditional solutions often rely on specialized hardware, which is either
costly, closed, or inaccessible to the general public. Distributed approaches,
while promising, introduce additional risks such as denial-of-service
vulnerabilities and operational complexity. We introduce Socarrat, a novel,
cost-effective, and local WORM storage solution that leverages a simple
external USB device (specifically, a single-board computer running Linux with
USB On-The-Go support). The resulting device can be connected via USB,
appearing as an ordinary external disk formatted with an ext4 or exFAT file
system, without requiring any specialized software or drivers. By isolating the
WORM enforcement mechanism in a dedicated USB hardware module, Socarrat
significantly reduces the attack surface and ensures that even privileged
attackers cannot modify or erase stored data. In addition to the WORM capacity,
the system is designed to be tamper-evident, becoming resilient against
advanced attacks. This work describes a novel approach, the Reverse File
System, based on inferring the file system operations occurring at higher
layers in the host computer where Socarrat is mounted. The paper also describes
the current Socarrat prototype, implemented in Go and available as free/libre
software. Finally, it provides a complete evaluation of the logging performance
on different single-board computers.

</details>


### [43] [Synth-MIA: A Testbed for Auditing Privacy Leakage in Tabular Data Synthesis](https://arxiv.org/abs/2509.18014)
*Joshua Ward,Xiaofeng Lin,Chi-Hua Wang,Guang Cheng*

Main category: cs.CR

TL;DR: This paper addresses the challenge of auditing privacy risks in synthetic data generated by tabular generative models by proposing a unified framework and open-source library called Synth-MIA, which integrates multiple membership inference attacks (MIAs) for systematic privacy evaluation.


<details>
  <summary>Details</summary>
Motivation: Current privacy audits for synthetic data rely on inconsistent and unreliable similarity metrics or MIAs that fail to generalize, leading to inaccurate risk assessments and underestimation of vulnerabilities.

Method: The authors developed a model-agnostic framework that combines 13 distinct MIA methods through a Scikit-Learn-like API (Synth-MIA), enabling systematic privacy leakage estimation. The library integrates into existing evaluation pipelines to test privacy risks comprehensively.

Result: Experiments on the largest synthetic data privacy benchmark revealed: (1)...

Conclusion: The work highlights the necessity of MIA-based auditing for synthetic data privacy, showing limitations of quality metrics and differential privacy techniques, and provides a reproducible toolset for robust privacy evaluation.

Abstract: Tabular Generative Models are often argued to preserve privacy by creating
synthetic datasets that resemble training data. However, auditing their
empirical privacy remains challenging, as commonly used similarity metrics fail
to effectively characterize privacy risk. Membership Inference Attacks (MIAs)
have recently emerged as a method for evaluating privacy leakage in synthetic
data, but their practical effectiveness is limited. Numerous attacks exist
across different threat models, each with distinct implementations targeting
various sources of privacy leakage, making them difficult to apply
consistently. Moreover, no single attack consistently outperforms the others,
leading to a routine underestimation of privacy risk.
  To address these issues, we propose a unified, model-agnostic threat
framework that deploys a collection of attacks to estimate the maximum
empirical privacy leakage in synthetic datasets. We introduce Synth-MIA, an
open-source Python library that streamlines this auditing process through a
novel testbed that integrates seamlessly into existing synthetic data
evaluation pipelines through a Scikit-Learn-like API. Our software implements
13 attack methods through a Scikit-Learn-like API, designed to enable fast
systematic estimation of privacy leakage for practitioners as well as
facilitate the development of new attacks and experiments for researchers.
  We demonstrate our framework's utility in the largest tabular synthesis
privacy benchmark to date, revealing that higher synthetic data quality
corresponds to greater privacy leakage, that similarity-based privacy metrics
show weak correlation with MIA results, and that the differentially private
generator PATEGAN can fail to preserve privacy under such attacks. This
underscores the necessity of MIA-based auditing when designing and deploying
Tabular Generative Models.

</details>


### [44] [STAFF: Stateful Taint-Assisted Full-system Firmware Fuzzing](https://arxiv.org/abs/2509.18039)
*Alessio Izzillo,Riccardo Lazzeretti,Emilio Coppa*

Main category: cs.CR

TL;DR: The paper introduces STAFF, a firmware fuzzing framework for Linux-based embedded devices that addresses inter-daemon dependencies through three innovations: multi-request protocol recording, system-wide taint analysis, and protocol-aware taint-guided fuzzing. STAFF discovers 42 new bugs across 15 firmware targets.


<details>
  <summary>Details</summary>
Motivation: Embedded Linux devices have complex, poorly-documented software stacks where daemons interact in undocumented ways. Existing fuzzing tools ignore daemon dependencies and persistent state, leading to incomplete testing of these critical security targets.

Method: 1) Users record request sequences (e.g., HTTP) through firmware emulation; 2)
Whole-system taint analysis tracks input byte propagation across daemons/files/sockets; 3)
Mutations use identified dependencies with multi-staged forkservers to maintain protocol states efficiently.

Result: STAFF discovers 42 bugs spanning multiple daemons and requests on 15 firmware targets. It detects 3.4x more unique bugs than DeepState and identifies 50% more bugs with multi-request sequences compared to AFL++.

Conclusion: STAFF demonstrates that capturing daemon interdependencies through protocol-aware stateful fuzzing significantly improves firmware testing effectiveness, uncovering both more and more reproducible security issues in embedded Linux devices.

Abstract: Modern embedded Linux devices, such as routers, IP cameras, and IoT gateways,
rely on complex software stacks where numerous daemons interact to provide
services. Testing these devices is crucial from a security perspective since
vendors often use custom closed- or open-source software without documenting
releases and patches. Recent coverage-guided fuzzing solutions primarily test
individual processes, ignoring deep dependencies between daemons and their
persistent internal state. This article presents STAFF, a firmware fuzzing
framework for discovering bugs in Linux-based firmware built around three key
ideas: (a) user-driven multi-request recording, which monitors user
interactions with emulated firmware to capture request sequences involving
application-layer protocols (e.g., HTTP); (b) intra- and inter-process
dependency detection, which uses whole-system taint analysis to track how input
bytes influence user-space states, including files, sockets, and memory areas;
(c) protocol-aware taint-guided fuzzing, which applies mutations to request
sequences based on identified dependencies, exploiting multi-staged forkservers
to efficiently checkpoint protocol states. When evaluating STAFF on 15
Linux-based firmware targets, it identifies 42 bugs involving multiple network
requests and different firmware daemons, significantly outperforming existing
state-of-the-art fuzzing solutions in both the number and reproducibility of
discovered bugs.

</details>


### [45] [Hybrid Reputation Aggregation: A Robust Defense Mechanism for Adversarial Federated Learning in 5G and Edge Network Environments](https://arxiv.org/abs/2509.18044)
*Saeid Sheikhi,Panos Kostakos,Lauri Loven*

Main category: cs.CR

TL;DR: HRA is a novel FL aggregation method for 5G/edge networks that combines anomaly detection and reputation tracking to defend against unknown attacks, achieving state-of-the-art 98.66% model accuracy.


<details>
  <summary>Details</summary>
Motivation: FL in 5G/edge networks faces severe security risks from adversarial clients (label flipping, backdoors, Sybil attacks). Existing aggregation methods lack robustness against diverse unknown attacks, necessitating a defense mechanism without prior attack knowledge.

Method: Hybrid Reputation Aggregation (HRA) integrates geometric anomaly detection (distance-based outlier identification) with momentum-based reputation tracking (historical trust scoring). This combines real-time anomaly filtering with long-term client reliability assessment to counter attacks adaptively.

Result: HRA achieved 98.66% accuracy on a 3M+ 5G dataset (outperforming Krum by +10.3%) and 96.60% on NF-CSE-CIC-IDS2018. Ablation studies showed 84.77% accuracy for anomaly-only and 78.52% for reputation-only, confirming the synergy of the hybrid design.

Conclusion: HRA demonstrates enhanced resilience in 5G/edge FL against adversarial attacks, achieving robust model accuracy even under significant threats. The hybrid approach outperforms existing aggregation methods, validating its dual-mechanism design.

Abstract: Federated Learning (FL) in 5G and edge network environments face severe
security threats from adversarial clients. Malicious participants can perform
label flipping, inject backdoor triggers, or launch Sybil attacks to corrupt
the global model. This paper introduces Hybrid Reputation Aggregation (HRA), a
novel robust aggregation mechanism designed to defend against diverse
adversarial behaviors in FL without prior knowledge of the attack type. HRA
combines geometric anomaly detection with momentum-based reputation tracking of
clients. In each round, it detects outlier model updates via distance-based
geometric analysis while continuously updating a trust score for each client
based on historical behavior. This hybrid approach enables adaptive filtering
of suspicious updates and long-term penalization of unreliable clients,
countering attacks ranging from backdoor insertions to random noise Byzantine
failures. We evaluate HRA on a large-scale proprietary 5G network dataset (3M+
records) and the widely used NF-CSE-CIC-IDS2018 benchmark under diverse
adversarial attack scenarios. Experimental results reveal that HRA achieves
robust global model accuracy of up to 98.66% on the 5G dataset and 96.60% on
NF-CSE-CIC-IDS2018, outperforming state-of-the-art aggregators such as Krum,
Trimmed Mean, and Bulyan by significant margins. Our ablation studies further
demonstrate that the full hybrid system achieves 98.66% accuracy, while the
anomaly-only and reputation-only variants drop to 84.77% and 78.52%,
respectively, validating the synergistic value of our dual-mechanism approach.
This demonstrates HRA's enhanced resilience and robustness in 5G/edge federated
learning deployments, even under significant adversarial conditions.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [46] [Digging Into the Internal: Causality-Based Analysis of LLM Function Calling](https://arxiv.org/abs/2509.16268)
*Zhenlan Ji,Daoyuan Wu,Wenxuan Wang,Pingchuan Ma,Shuai Wang,Lei Ma*

Main category: cs.SE

TL;DR: FC for LLM compliance inspection


<details>
  <summary>Details</summary>
Motivation: Understanding FC mechanism to enhance LLM compliance

Method: Causal interventions at layer and token levels and FC-based instruction experiments

Result: FC performance improvements by 135% over conventional methods in safety robustness

Conclusion: FC has potential to improve LLM reliability and capability

Abstract: Function calling (FC) has emerged as a powerful technique for facilitating
large language models (LLMs) to interact with external systems and perform
structured tasks. However, the mechanisms through which it influences model
behavior remain largely under-explored. Besides, we discover that in addition
to the regular usage of FC, this technique can substantially enhance the
compliance of LLMs with user instructions. These observations motivate us to
leverage causality, a canonical analysis method, to investigate how FC works
within LLMs. In particular, we conduct layer-level and token-level causal
interventions to dissect FC's impact on the model's internal computational
logic when responding to user queries. Our analysis confirms the substantial
influence of FC and reveals several in-depth insights into its mechanisms. To
further validate our findings, we conduct extensive experiments comparing the
effectiveness of FC-based instructions against conventional prompting methods.
We focus on enhancing LLM safety robustness, a critical LLM application
scenario, and evaluate four mainstream LLMs across two benchmark datasets. The
results are striking: FC shows an average performance improvement of around
135% over conventional prompting methods in detecting malicious inputs,
demonstrating its promising potential to enhance LLM reliability and capability
in practical applications.

</details>


### [47] [Constrained Co-evolutionary Metamorphic Differential Testing for Autonomous Systems with an Interpretability Approach](https://arxiv.org/abs/2509.16478)
*Hossein Yousefizadeh,Shenghui Gu,Lionel C. Briand,Ali Nasr*

Main category: cs.SE

TL;DR: CoCoMagic addresses testing challenges in evolving autonomous systems by combining metamorphic/differential testing with co-evolutionary search, achieving 287% more defect detection than baselines and enabling interpretable fault diagnosis.


<details>
  <summary>Details</summary>
Motivation: Rapid updates to autonomous systems (e.g., ADS) risk unintended behavioral degradations. System-level testing faces challenges due to vast scenario spaces, lack of reliable test oracles, and the need for both practically applicable and interpretable test cases.

Method: CoCoMagic combines metamorphic testing, differential testing, and search-based techniques through a constrained cooperative co-evolutionary approach. It evolves source scenarios and metamorphic perturbations to maximize violation differences across versions, using constraints and initialization strategies for realism, and incorporates an interpretability framework for root-cause analysis.

Result: Evaluation on the InterFuser ADS in the Carla simulator showed CoCoMagic identified 287% more high-severity behavioral differences compared to baseline methods while preserving scenario realism. The interpretability approach provided actionable insights for debugging and safety assessment.

Conclusion: CoCoMagic offers an efficient, effective, and interpretable solution for differential testing of evolving autonomous systems across versions, significantly improving detection of behavioral degradations while maintaining scenario realism.

Abstract: Autonomous systems, such as autonomous driving systems, evolve rapidly
through frequent updates, risking unintended behavioral degradations. Effective
system-level testing is challenging due to the vast scenario space, the absence
of reliable test oracles, and the need for practically applicable and
interpretable test cases. We present CoCoMagic, a novel automated test case
generation method that combines metamorphic testing, differential testing, and
advanced search-based techniques to identify behavioral divergences between
versions of autonomous systems. CoCoMagic formulates test generation as a
constrained cooperative co-evolutionary search, evolving both source scenarios
and metamorphic perturbations to maximize differences in violations of
predefined metamorphic relations across versions. Constraints and population
initialization strategies guide the search toward realistic, relevant
scenarios. An integrated interpretability approach aids in diagnosing the root
causes of divergences. We evaluate CoCoMagic on an end-to-end ADS, InterFuser,
within the Carla virtual simulator. Results show significant improvements over
baseline search methods, identifying up to 287\% more distinct high-severity
behavioral differences while maintaining scenario realism. The interpretability
approach provides actionable insights for developers, supporting targeted
debugging and safety assessment. CoCoMagic offers an efficient, effective, and
interpretable way for the differential testing of evolving autonomous systems
across versions.

</details>


### [48] [Causal Fuzzing for Verifying Machine Unlearning](https://arxiv.org/abs/2509.16525)
*Anna Mazhar,Sainyam Galhotra*

Main category: cs.SE

TL;DR: The paper introduces CAFÉ, a causality-based framework for verifying machine unlearning in black-box models by analyzing direct and indirect effects of unlearning targets through causal dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning verification methods fail to capture indirect influences, limiting their effectiveness in ensuring model adaptability, fairness, and privacy.

Method: CAFÉ unifies datapoint- and feature-level unlearning verification by evaluating causal dependencies to identify both direct and indirect residual influences in black-box ML models.

Result: Experiments on five datasets and three architectures show CAFÉ detects residual influences missed by baselines while maintaining computational efficiency.

Conclusion: CAFÉ provides a unified, efficient solution for detecting unlearning residual effects, offering actionable insights for improving model reliability in sensitive applications.

Abstract: As machine learning models become increasingly embedded in decision-making
systems, the ability to "unlearn" targeted data or features is crucial for
enhancing model adaptability, fairness, and privacy in models which involves
expensive training. To effectively guide machine unlearning, a thorough testing
is essential. Existing methods for verification of machine unlearning provide
limited insights, often failing in scenarios where the influence is indirect.
In this work, we propose CAF\'E, a new causality based framework that unifies
datapoint- and feature-level unlearning for verification of black-box ML
models. CAF\'E evaluates both direct and indirect effects of unlearning targets
through causal dependencies, providing actionable insights with fine-grained
analysis. Our evaluation across five datasets and three model architectures
demonstrates that CAF\'E successfully detects residual influence missed by
baselines while maintaining computational efficiency.

</details>


### [49] [Is Measurement Enough? Rethinking Output Validation in Quantum Program Testing](https://arxiv.org/abs/2509.16595)
*Jiaming Ye,Xiongfei Wu,Shangzhou Xia,Fuyuan Zhang,Jianjun Zhao*

Main category: cs.SE

TL;DR: This paper analyzes measurement-based validation methods in quantum program testing, categorizing them into distribution and output-value levels, and compares their effectiveness with statevector-based methods, finding that statevector validation excels in complex assessments.


<details>
  <summary>Details</summary>
Motivation: Quantum program quality assurance is critical as quantum computing advances, yet existing measurement-based validation methods have inherent limitations due to quantum's probabilistic nature.

Method: The authors conducted an empirical study to categorize and compare measurement-based validation methods (distribution/output-value) against statevector-based approaches, evaluating their suitability for different testing tasks.

Result: Measurement-based validation is effective for simple output verification, but statevector validation outperforms it in assessing complex program behaviors like execution dynamics and non-trivial properties.

Conclusion: The study suggests that statevector-based validation is superior for comprehensive quantum program analysis, while measurement-based methods remain viable for basic correctness checks, guiding future method development and application strategies.

Abstract: As quantum computing continues to emerge, ensuring the quality of quantum
programs has become increasingly critical. Quantum program testing has emerged
as a prominent research area within the scope of quantum software engineering.
While numerous approaches have been proposed to address quantum program quality
assurance, our analysis reveals that most existing methods rely on
measurement-based validation in practice. However, due to the inherently
probabilistic nature of quantum programs, measurement-based validation methods
face significant limitations.
  To investigate these limitations, we conducted an empirical study of recent
research on quantum program testing, analyzing measurement-based validation
methods in the literature. Our analysis categorizes existing measurement-based
validation methods into two groups: distribution-level validation and
output-value-level validation. We then compare measurement-based validation
with statevector-based validation methods to evaluate their pros and cons. Our
findings demonstrate that measurement-based validation is suitable for
straightforward assessments, such as verifying the existence of specific output
values, while statevector-based validation proves more effective for
complicated tasks such as assessing the program behaviors.

</details>


### [50] [Incentives and Outcomes in Bug Bounties](https://arxiv.org/abs/2509.16655)
*Serena Wang,Martino Banchio,Krzysztof Kotowicz,Katrina Ligett,R. Preston McAfee,Eduardo' Vela'' Nava*

Main category: cs.SE

TL;DR: Google's bug bounty program raised rewards by 200-300%, leading to increased high-value bug reports from both veteran and new researchers.


<details>
  <summary>Details</summary>
Motivation: To understand how reward incentives impact the quantity/quality of bug reports in security programs

Method: Empirical analysis of Google VRP pre/post July 2024 reward increase, with elasticity calculations and contributor segmentation (veteran vs new researchers)

Result: 213% increase in highest-impact bug submissions after pay raise, with significant growth from both veteran researchers (34%) and new top researchers (13x increase)

Conclusion: Strategic reward increases effectively attract and redirect top security talent while improving program outcomes.

Abstract: Bug bounty programs have contributed significantly to security in technology
firms in the last decade, but little is known about the role of reward
incentives in producing useful outcomes. We analyze incentives and outcomes in
Google's Vulnerability Rewards Program (VRP), one of the world's largest bug
bounty programs. We analyze the responsiveness of the quality and quantity of
bugs received to changes in payments, focusing on a change in Google's reward
amounts posted in July, 2024, in which reward amounts increased by up to 200%
for the highest impact tier. Our empirical results show an increase in the
volume of high-value bugs received after the reward increase, for which we also
compute elasticities. We further break down the sources of this increase
between veteran researchers and new researchers, showing that the reward
increase both redirected the attention of veteran researchers and attracted new
top security researchers into the program.

</details>


### [51] [Verifying User Interfaces using SPARK Ada: A Case Study of the T34 Syringe Driver](https://arxiv.org/abs/2509.16681)
*Peterson Jean*

Main category: cs.SE

TL;DR: This paper investigates using SPARK Ada's formal verification to improve safety in medical devices like the T34 syringe pump, aiming to identify human factor risks early. It finds that SPARK can effectively verify models but faces challenges with UI design abstractions, suggesting formal methods could enhance safety if standardized frameworks are adopted.


<details>
  <summary>Details</summary>
Motivation: Despite stringent industry standards for safety-critical healthcare systems, human factor risks in user interaction remain largely undetected until real-world testing—posing catastrophic risks (e.g., for devices like the T34 syringe pump). Formal methods are proposed to anticipate and mitigate these risks during early development stages.

Method: The study employs SPARK Ada's formal verification tool to develop and refine a generic behavioral model of the T34 syringe pump. It evaluates the tool’s verification effectiveness while addressing challenges in abstracting user interface components within SPARK Ada.

Result: The implementation reveals SPARK Ada’s potential to verify safety-critical models rigorously but highlights limitations in handling UI abstraction complexity. The study identifies practical constraints of using SPARK as a formal verification tool subset of Ada for such systems.

Conclusion: The research demonstrates that formal methods, specifically SPARK Ada, can enhance safety in medical device development by enabling early error detection through formal verification. However, limitations in SPARK’s abstraction capabilities, especially in modeling UI components, highlight the need for further research to expand standard frameworks for safer design.

Abstract: The increase in safety and critical systems improved Healthcare. Due to their
risk of harm, such systems are subject to stringent guidelines and compliances.
These safety measures ensure a seamless experience and mitigate the risk to
end-users. Institutions like the Food and Drug Administration and the NHS,
respectively, established international standards and competency frameworks to
ensure industry compliance with these safety concerns. Medical device
manufacturing is mainly concerned with standards. Consequently, these standards
now advocate for better human factors considered in user interaction for
medical devices. This forces manufacturers to rely on heavy testing and review
to cover many of these factors during development. Sadly, many human factor
risks will not be caught until proper testing in real life, which might be
catastrophic in the case of an ambulatory device like the T34 syringe pump.
Therefore, effort in formal methods research may propose new solutions in
anticipating these errors in the early stages of development or even reducing
their occurrence based on the use of standard generic model. These generically
developed models will provide a common framework for safety integration in
industry and may potentially be proven using formal verification mathematical
proofs. This research uses SPARK Ada's formal verification tool against a
behavioural model of the T34 syringe driver. A Generic Infusion Pump model
refinement is explored and implemented in SPARK Ada. As a subset of the Ada
language, the verification level of the end prototype is evaluated using SPARK.
Exploring potential limitations defines the proposed model's implementation
liability when considering abstraction and components of User Interface design
in SPARK Ada.

</details>


### [52] [RelRepair: Enhancing Automated Program Repair by Retrieving Relevant Code](https://arxiv.org/abs/2509.16701)
*Shunyu Liu,Guangdong Bai,Mark Utting,Guowei Yang*

Main category: cs.SE

TL;DR: RelRepair addresses LLM shortcomings in project-specific bug fixing by providing relevant code context, achieving superior APR performance on major datasets.


<details>
  <summary>Details</summary>
Motivation: LLMs lack project-specific knowledge required for accurate automated program repair, leading to incorrect patches when contextual code understanding is needed.

Method: RelRepair retrieves project-specific code by analyzing function names/comments and implementing deeper contextual analysis, then injects this information into LLM prompts to guide patch generation.

Result: Repairs 101/Defects4J V1.2 bugs (48.3%) with 17.1% improvement over existing methods on ManySStuBs4J dataset.

Conclusion: Project-specific code context is essential for effective LLM-based APR, with RelRepair's approach providing a generalizable strategy for incorporating domain knowledge into code generation.

Abstract: Automated Program Repair (APR) has emerged as a promising paradigm for
reducing debugging time and improving the overall efficiency of software
development. Recent advances in Large Language Models (LLMs) have demonstrated
their potential for automated bug fixing and other software engineering tasks.
Nevertheless, the general-purpose nature of LLM pre-training means these models
often lack the capacity to perform project-specific repairs, which require
understanding of domain-specific identifiers, code structures, and contextual
relationships within a particular codebase. As a result, LLMs may struggle to
generate correct patches when the repair depends on project-specific
information.
  To address this limitation, we introduce RelRepair, a novel approach that
retrieves relevant project-specific code to enhance automated program repair.
RelRepair first identifies relevant function signatures by analyzing function
names and code comments within the project. It then conducts deeper code
analysis to retrieve code snippets relevant to the repair context. The
retrieved relevant information is then incorporated into the LLM's input
prompt, guiding the model to generate more accurate and informed patches. We
evaluate RelRepair on two widely studied datasets, Defects4J V1.2 and
ManySStuBs4J, and compare its performance against several state-of-the-art
LLM-based APR approaches. RelRepair successfully repairs 101 bugs in Defects4J
V1.2. Furthermore, RelRepair achieves a 17.1\% improvement in the ManySStuBs4J
dataset, increasing the overall fix rate to 48.3\%. These results highlight the
importance of providing relevant project-specific information to LLMs, shedding
light on effective strategies for leveraging LLMs in APR tasks.

</details>


### [53] [Can We Trust the AI Pair Programmer? Copilot for API Misuse Detection and Correction](https://arxiv.org/abs/2509.16795)
*Saikat Mondal,Chanchal K. Roy,Hong Wang,Juan Arguello,Samantha Mathan*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: API misuse introduces security vulnerabilities, system failures, and
increases maintenance costs, all of which remain critical challenges in
software development. Existing detection approaches rely on static analysis or
machine learning-based tools that operate post-development, which delays defect
resolution. Delayed defect resolution can significantly increase the cost and
complexity of maintenance and negatively impact software reliability and user
trust. AI-powered code assistants, such as GitHub Copilot, offer the potential
for real-time API misuse detection within development environments. This study
evaluates GitHub Copilot's effectiveness in identifying and correcting API
misuse using MUBench, which provides a curated benchmark of misuse cases. We
construct 740 misuse examples, manually and via AI-assisted variants, using
correct usage patterns and misuse specifications. These examples and 147
correct usage cases are analyzed using Copilot integrated in Visual Studio
Code. Copilot achieved a detection accuracy of 86.2%, precision of 91.2%, and
recall of 92.4%. It performed strongly on common misuse types (e.g.,
missing-call, null-check) but struggled with compound or context-sensitive
cases. Notably, Copilot successfully fixed over 95% of the misuses it
identified. These findings highlight both the strengths and limitations of
AI-driven coding assistants, positioning Copilot as a promising tool for
real-time pair programming and detecting and fixing API misuses during software
development.

</details>


### [54] [Implementation of the Collision Avoidance System for DO-178C Compliance](https://arxiv.org/abs/2509.16844)
*Rim Zrelli,Henrique Amaral Misson,Sorelle Kamkuimo,Maroua Ben Attia,Abdo Shabah,Felipe Gohring de Magalhaes,Gabriela Nicolescu*

Main category: cs.SE

TL;DR: This report details a DO-178C-compliant Collision Avoidance System (CAS) for UAVs using formal methods and automated tools like Alloy, SPIN, and Simulink. The approach ensures early defect detection, traceability, and verification, effectively addressing certification challenges for safety-critical UAV software.


<details>
  <summary>Details</summary>
Motivation: The motivation centers on enabling safe UAV integration into civil airspace via DO-178C-compliant systems, addressing certification challenges in safety-critical software through rigorous methodologies.

Method: The method integrates formal modeling (Alloy, SPIN), model-based development (Simulink Embedded Coder), and automated verification (LDRA) within the full software lifecycle, emphasizing traceability and compliance with DO-178C Design Assurance Level B.

Result: Results show early defect detection, robust traceability, and verification across all phases. Static/dynamic analyses confirmed code quality, while formal methods mathematically assured critical component correctness, validating the approach's effectiveness despite incomplete integration testing.

Conclusion: The paper concludes that the applied methodology effectively tackles certification challenges for UAV safety-critical systems, demonstrating a practical approach to achieving DO-178C compliance through formal methods and automated toolchains.

Abstract: This technical report presents the detailed implementation of a Collision
Avoidance System (CAS) for Unmanned Aerial Vehicles (UAVs), developed as a case
study to demonstrate a rigorous methodology for achieving DO-178C compliance in
safety-critical software. The CAS is based on functional requirements inspired
by NASA's Access 5 project and is designed to autonomously detect, evaluate,
and avoid potential collision threats in real-time, supporting the safe
integration of UAVs into civil airspace.
  The implementation environment combines formal methods, model-based
development, and automated verification tools, including Alloy, SPIN, Simulink
Embedded Coder, and the LDRA tool suite. The report documents each phase of the
software lifecycle: requirements specification and validation, architectural
and detailed design, coding, verification, and traceability, with a strong
focus on compliance with DO-178C Design Assurance Level B objectives.
  Results demonstrate that formal modelling and automated toolchains enabled
early detection and correction of specification defects, robust traceability,
and strong evidence of verification and validation across all development
stages. Static and dynamic analyses confirmed code quality and coverage, while
formal verification methods provided mathematical assurance of correctness for
critical components. Although the integration phase was not fully implemented,
the approach proved effective in addressing certification challenges for UAV
safety-critical systems.
  \keywords Collision Avoidance System (CAS), Unmanned Aerial Vehicles (UAVs),
DO-178C compliance, Safety-critical software, Formal methods, Model-based
development, Alloy, SPIN model checker, Simulink Embedded Coder, LDRA tool
suite, Software verification and validation, Traceability, Certification.

</details>


### [55] [MobileUPReg: Identifying User-Perceived Performance Regressions in Mobile OS Versions](https://arxiv.org/abs/2509.16864)
*Wei Liu,Yi Wen Heng,Feng Lin,Tse-Hsun,Chen,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: MobileUPReg is a black-box framework for detecting user-perceived performance regressions in mobile OS updates using perceptual metrics like response time and dropped frames, achieving high accuracy through large-scale analysis.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on system-level metrics or specific components, failing to capture user-perceived regressions such as UI delays or stutters that impact real-world experience.

Method: MobileUPReg compares user-perceived performance metrics (response time, launch time, dropped frames, etc.) across OS versions using a controlled app execution framework, validated against statistical baselines.

Result: Achieved 0.96 precision, 0.91 recall, and 0.93 F1-score in regression detection, outperforming Wilcoxon and Cliff's Delta. Integrated into an industrial CI pipeline processing ~1k daily app tests, uncovering regressions missed by traditional tools.

Conclusion: MobileUPReg provides a scalable, perceptually-aligned solution for mobile OS validation, bridging the gap between system-level measurements and user experience.

Abstract: Mobile operating systems (OS) are frequently updated, but such updates can
unintentionally degrade user experience by introducing performance regressions.
Existing detection techniques often rely on system-level metrics (e.g., CPU or
memory usage) or focus on specific OS components, which may miss regressions
actually perceived by users -- such as slower responses or UI stutters. To
address this gap, we present MobileUPReg, a black-box framework for detecting
user-perceived performance regressions across OS versions. MobileUPReg runs the
same apps under different OS versions and compares user-perceived performance
metrics -- response time, finish time, launch time, and dropped frames -- to
identify regressions that are truly perceptible to users. In a large-scale
study, MobileUPReg achieves high accuracy in extracting user-perceived metrics
and detects user-perceived regressions with 0.96 precision, 0.91 recall, and
0.93 F1-score -- significantly outperforming a statistical baseline using the
Wilcoxon rank-sum test and Cliff's Delta. MobileUPReg has been deployed in an
industrial CI pipeline, where it analyzes thousands of screencasts across
hundreds of apps daily and has uncovered regressions missed by traditional
tools. These results demonstrate that MobileUPReg enables accurate, scalable,
and perceptually aligned regression detection for mobile OS validation.

</details>


### [56] [DecipherGuard: Understanding and Deciphering Jailbreak Prompts for a Safer Deployment of Intelligent Software Systems](https://arxiv.org/abs/2509.16870)
*Rui Yang,Michael Fu,Chakkrit Tantithamthavorn,Chetan Arora,Gunel Gulmammadova,Joey Chua*

Main category: cs.SE

TL;DR: This paper addresses runtime safety of LLM-powered systems by proposing DecipherGuard, a defense framework that outperforms existing guardrails by 36–65% in blocking jailbreak attacks using deciphering layers and low-rank adaptation.


<details>
  <summary>Details</summary>
Motivation: Deploying LLM-powered systems in critical sectors necessitates runtime safety mechanisms. Existing guardrails like LlamaGuard show insufficient performance (24% DSR drop under attacks), motivating the development of a more robust defense framework.

Method: The paper introduces DecipherGuard, combining a deciphering layer to counter obfuscation-based prompts and low-rank adaptation to enhance guardrail effectiveness against template-based attacks.

Result: DecipherGuard achieves a 36–65% improvement in Defense Success Rate (DSR) and 20–50% better Overall Guardrail Performance (OGP) compared to LlamaGuard and other baselines, validated on 22,000+ prompts.

Conclusion: DecipherGuard effectively defends LLM-powered systems against jailbreak attacks during runtime, with proven improvements in defense and overall performance metrics.

Abstract: Intelligent software systems powered by Large Language Models (LLMs) are
increasingly deployed in critical sectors, raising concerns about their safety
during runtime. Through an industry-academic collaboration when deploying an
LLM-powered virtual customer assistant, a critical software engineering
challenge emerged: how to enhance a safer deployment of LLM-powered software
systems at runtime? While LlamaGuard, the current state-of-the-art runtime
guardrail, offers protection against unsafe inputs, our study reveals a Defense
Success Rate (DSR) drop of 24% under obfuscation- and template-based jailbreak
attacks. In this paper, we propose DecipherGuard, a novel framework that
integrates a deciphering layer to counter obfuscation-based prompts and a
low-rank adaptation mechanism to enhance guardrail effectiveness against
template-based attacks. Empirical evaluation on over 22,000 prompts
demonstrates that DecipherGuard improves DSR by 36% to 65% and Overall
Guardrail Performance (OGP) by 20% to 50% compared to LlamaGuard and two other
runtime guardrails. These results highlight the effectiveness of DecipherGuard
in defending LLM-powered software systems against jailbreak attacks during
runtime.

</details>


### [57] [Deep Synthetic Cross-Project Approaches for Software Reliability Growth Modeling](https://arxiv.org/abs/2509.16939)
*Taehyoun Kim,Duksan Ryu,Jongmoon Baik*

Main category: cs.SE

TL;DR: DSC-SRGM improves software reliability prediction in data-scarce environments by combining synthetic data generation with cross-project transfer learning, achieving significant accuracy improvements over traditional and cross-project models.


<details>
  <summary>Details</summary>
Motivation: Traditional SRGMs and cross-project transfer learning face limitations in data-scarce scenarios due to insufficient or confidential real-world data, reducing predictive accuracy for early-stage testing and safety-critical systems.

Method: DSC-SRGM generates synthetic datasets using traditional SRGMs to preserve defect trends, applies cross-correlation clustering to select relevant datasets for the target project, and trains a deep learning model on this synthetic data before evaluating it on real-world datasets.

Result: DSC-SRGM achieved 23.3\% higher accuracy than traditional SRGMs and 32.2\% higher accuracy than cross-project deep learning models trained on real-world data across 60 datasets.

Conclusion: DSC-SRGM provides a viable solution for data-scarce software reliability prediction but requires careful balancing of synthetic and real-world data to avoid performance degradation.

Abstract: Software Reliability Growth Models (SRGMs) are widely used to predict
software reliability based on defect discovery data collected during testing or
operational phases. However, their predictive accuracy often degrades in
data-scarce environments, such as early-stage testing or safety-critical
systems. Although cross-project transfer learning has been explored to mitigate
this issue by leveraging data from past projects, its applicability remains
limited due to the scarcity and confidentiality of real-world datasets. To
overcome these limitations, we propose Deep Synthetic Cross-project SRGM
(DSC-SRGM), a novel approach that integrates synthetic data generation with
cross-project transfer learning. Synthetic datasets are generated using
traditional SRGMs to preserve the statistical characteristics of real-world
defect discovery trends. A cross-correlation-based clustering method is applied
to identify synthetic datasets with patterns similar to the target project.
These datasets are then used to train a deep learning model for reliability
prediction. The proposed method is evaluated on 60 real-world datasets, and its
performance is compared with both traditional SRGMs and cross-project deep
learning models trained on real-world datasets. DSC-SRGM achieves up to 23.3%
improvement in predictive accuracy over traditional SRGMs and 32.2% over
cross-project deep learning models trained on real-world datasets. However,
excessive use of synthetic data or a naive combination of synthetic and
real-world data may degrade prediction performance, highlighting the importance
of maintaining an appropriate data balance. These findings indicate that
DSC-SRGM is a promising approach for software reliability prediction in
data-scarce environments.

</details>


### [58] [SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?](https://arxiv.org/abs/2509.16941)
*Xiang Deng,Jeff Da,Edwin Pan,Yannis Yiming He,Charles Ide,Kanak Garg,Niklas Lauffer,Andrew Park,Nitin Pasari,Chetan Rane,Karmini Sampath,Maya Krishnan,Srivatsa Kundurthy,Sean Hendryx,Zifan Wang,Chen Bo Calvin Zhang,Noah Jacobson,Bing Liu,Brad Kenstler*

Main category: cs.SE

TL;DR: SWE-Bench Pro is a new benchmark for complex software engineering tasks, featuring 1,865 real-world problems from 41 repositories across public, held-out, and commercial partitions. It evaluates AI models' ability to handle long-horizon, multi-file patches with human-verified tasks. Results show current models perform under 25%, with GPT-5 at 23.3%.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks (e.g., SWE-BENCH) lack realism for enterprise-level, multi-file software engineering tasks requiring extensive modifications. SWE-Bench Pro fills this gap by providing a contamination-resistant testbed for authentic, professional-grade challenges.

Method: Curated 1,865 problems from 41 repositories (business, B2B, developer tools). Structured into three partitions with access controls. Tasks involve long-horizon (>days) multi-file patches. Evaluated models under uniform scaffolding and analyzed failure modes via trajectory clustering.

Result: Top AI models (e.g., GPT-5) achieve ≤23.37% accuracy (Pass@1). Commercial partition results are released without exposing tasks. Clustering reveals recurring error patterns in model-generated solutions for complex tasks.

Conclusion: SWE-Bench Pro advances autonomous software engineering research by rigorously exposing model limitations in large-scale, real-world coding scenarios. Its contamination-resistant design ensures evaluation robustness while benchmarking progress toward professional-level AI agents.

Abstract: We introduce SWE-Bench Pro, a substantially more challenging benchmark that
builds upon the best practices of SWE-BENCH [25], but is explicitly designed to
capture realistic, complex, enterprise-level problems beyond the scope of
SWE-BENCH. SWE-BENCH PRO contains 1,865 problems sourced from a diverse set of
41 actively maintained repositories spanning business applications, B2B
services, and developer tools. The benchmark is partitioned into a public set
with open access to problems sourced from 11 repositories, a held-out set of 12
repositories and a commercial set of 18 proprietary repositories where we have
formal partnership agreements with early-stage startups. Problems in the
held-out and the commercial set are not publicly accessible, but we release
results on the commercial set. Our benchmark features long-horizon tasks that
may require hours to days for a professional software engineer to complete,
often involving patches across multiple files and substantial code
modifications. All tasks are human-verified and augmented with sufficient
context to ensure resolvability. In our evaluation of widely used coding
models, under a unified scaffold, we observe that their performance on
SWE-Bench PRO remains below 25% (Pass@1), with GPT-5 achieving the highest
score to date at 23.3%. To better understand these limitations, we cluster the
failure modes observed in the collected agent trajectories for a clearer
characterization of the error patterns exhibited by current models. Overall,
SWE-BENCH PRO provides a contamination-resistant testbed that more faithfully
captures the complexity and diversity of real-world software development,
advancing the pursuit of truly autonomous software engineering agents at a
professional level.

</details>


### [59] [Static Security Vulnerability Scanning of Proprietary and Open-Source Software: An Adaptable Process with Variants and Results](https://arxiv.org/abs/2509.16985)
*James J. Cusick*

Main category: cs.SE

TL;DR: The paper proposes an end-to-end DevSecOps process with iterative tools and methods for systematic identification, prioritization, and remediation of software vulnerabilities in proprietary and open-source systems.


<details>
  <summary>Details</summary>
Motivation: Software vulnerabilities in both proprietary and open-source environments pose significant security risks. Existing approaches lack a unified, customizable process for proactive vulnerability management with prioritized remediation.

Method: The authors present an industry-tested process incorporating specialized code scanning tools within an iterative workflow. This includes tool selection, configuration, and implementation examples for both industrial and open-source applications, alongside strategies for automation and AI-enhanced vulnerability treatment.

Result: The method successfully enables organizations to reduce code vulnerabilities, supply chain risks, and improve security posture through flexible implementation. Case studies demonstrate vulnerability remediation examples and tool comparisons.

Conclusion: This generic yet adaptable approach provides maximal security benefits with minimal organizational changes. Future directions include deeper integration of automation and AI to enhance vulnerability detection and prioritization across full SDLC cycles.

Abstract: Software vulnerabilities remain a significant risk factor in achieving
security objectives within software development organizations. This is
especially true where either proprietary or open-source software (OSS) is
included in the technological environment. In this paper an end-to-end process
with supporting methods and tools is presented. This industry proven generic
process allows for the custom instantiation, configuration, and execution of
routinized code scanning for software vulnerabilities and their prioritized
remediation. A select set of tools are described for this key DevSecOps
function and placed into an iterative process. Examples of both industrial
proprietary applications and open-source applications are provided including
specific vulnerability instances and a discussion of their treatment. The
benefits of each selected tool are considered, and alternative tools are also
introduced. Application of this method in a comprehensive SDLC model is also
reviewed along with prospective enhancements from automation and the
application of advanced technologies including AI. Adoption of this method can
be achieved with minimal adjustments and with maximum flexibility for results
in reducing source code vulnerabilities, reducing supply chain risk, and
improving the security profile of new or legacy solutions.

</details>


### [60] [Prompt-with-Me: in-IDE Structured Prompt Management for LLM-Driven Software Engineering](https://arxiv.org/abs/2509.17096)
*Ziyou Li,Agnia Sergeyuk,Maliheh Izadi*

Main category: cs.SE

TL;DR: Prompt-with-Me introduces a structured prompt management system for software engineering with a four-dimensional classification taxonomy, demonstrating improved usability and efficiency via empirical validation.


<details>
  <summary>Details</summary>
Motivation: Ad hoc prompt management in software engineering hinders reliability, reuse, and industrial integration, necessitating structured workflows for LLM-driven tooling.

Method: The system employs a taxonomy (intent, author role, lifecycle stage, prompt type), automated refinements, sensitive data masking, and template extraction, validated via a 1108-prompt taxonomy study and user evaluations.

Result: LLMs achieved high classification accuracy across 1108 prompts; 11-participant user studies reported high usability (SUS 73) and low cognitive load (NASA-TLX 21), with improved prompt quality and efficiency.

Conclusion: Prompt-with-Me validates structured prompt management for software workflows, offering actionable insights for future tools to enhance LLM integration in industrial practices.

Abstract: Large Language Models are transforming software engineering, yet prompt
management in practice remains ad hoc, hindering reliability, reuse, and
integration into industrial workflows. We present Prompt-with-Me, a practical
solution for structured prompt management embedded directly in the development
environment. The system automatically classifies prompts using a
four-dimensional taxonomy encompassing intent, author role, software
development lifecycle stage, and prompt type. To enhance prompt reuse and
quality, Prompt-with-Me suggests language refinements, masks sensitive
information, and extracts reusable templates from a developer's prompt library.
Our taxonomy study of 1108 real-world prompts demonstrates that modern LLMs can
accurately classify software engineering prompts. Furthermore, our user study
with 11 participants shows strong developer acceptance, with high usability
(Mean SUS=73), low cognitive load (Mean NASA-TLX=21), and reported gains in
prompt quality and efficiency through reduced repetitive effort. Lastly, we
offer actionable insights for building the next generation of prompt management
and maintenance tools for software engineering workflows.

</details>


### [61] [Clotho: Measuring Task-Specific Pre-Generation Test Adequacy for LLM Inputs](https://arxiv.org/abs/2509.17314)
*Juyeon Yoon,Somin Kim,Robert Feldt,Shin Yoo*

Main category: cs.SE

TL;DR: CLOTHO is a framework that measures input adequacy in LLM software by analyzing hidden states to estimate input difficulty without generating outputs, enabling efficient prioritization of testing inputs.


<details>
  <summary>Details</summary>
Motivation: This paper addresses the difficulty and cost of testing LLMs for software tasks due to a lack of ground truth and the need for full inference, highlighting the need for task-specific pre-generation input adequacy measures.

Method: CLOTHO uses a Gaussian Mixture Model (GMM) to analyze hidden states of LLMs, selecting the most informative unlabelled inputs for human labeling and then using the GMM to rank unseen inputs by their likelihood of failure.

Result: Empirical results show CLOTHO can predict failures with a ROC-AUC of 0.716, requiring labeling of only 5.4% of inputs on average. It increases failing inputs from 18.7 to 42.5 when prioritizing tests for proprietary models.

Conclusion: CLOTHO provides effective pre-generation input adequacy assessment for LLMs, reducing testing costs and improving failure prediction. It can transfer adequacy scores from open-weight LLMs to proprietary ones, significantly enhancing test prioritization.

Abstract: Software increasingly relies on the emergent capabilities of Large Language
Models (LLMs), from natural language understanding to program analysis and
generation. Yet testing them on specific tasks remains difficult and costly:
many prompts lack ground truth, forcing reliance on human judgment, while
existing uncertainty and adequacy measures typically require full inference. A
key challenge is to assess input adequacy in a way that reflects the demands of
the task, ideally before even generating any output. We introduce CLOTHO, a
task-specific, pre-generation adequacy measure that estimates input difficulty
directly from hidden LLM states. Given a large pool of unlabelled inputs for a
specific task, CLOTHO uses a Gaussian Mixture Model (GMM) to adaptively sample
the most informative cases for human labelling. Based on this reference set the
GMM can then rank unseen inputs by their likelihood of failure. In our
empirical evaluation across eight benchmark tasks and three open-weight LLMs,
CLOTHO can predict failures with a ROC-AUC of 0.716, after labelling reference
sets that are on average only 5.4% of inputs. It does so without generating any
outputs, thereby reducing costs compared to existing uncertainty measures.
Comparison of CLOTHO and post-generation uncertainty measures shows that the
two approaches complement each other. Crucially, we show that adequacy scores
learnt from open-weight LLMs transfer effectively to proprietary models,
extending the applicability of the approach. When prioritising test inputs for
proprietary models, CLOTHO increases the average number of failing inputs from
18.7 to 42.5 out of 100, compared to random prioritisation.

</details>


### [62] [BASFuzz: Towards Robustness Evaluation of LLM-based NLP Software via Automated Fuzz Testing](https://arxiv.org/abs/2509.17335)
*Mingxuan Xiao,Yan Xiao,Shunhui Ji,Jiahe Tu,Pengcheng Zhang*

Main category: cs.SE

TL;DR: BASFuzz is an efficient fuzz testing method for LLM-based NLP software, addressing issues with existing methods by aligning with behavioral patterns and improving NLG testing capability.


<details>
  <summary>Details</summary>
Motivation: The motivation is that as LLM-based NLP software becomes more common in important industries, testing methods are not well with LLM-based NLP software's behavior and often fail in natural language generation (NLG) scenarios.

Method: The method involves using BASFuzz, which targets test inputs made of prompts and examples, a text consistency metric guides mutations in the fuzzing loop, and a Beam-Annealing Search algorithm combining beam search and simulated annealing to design an efficient testing process. Additionally, information entropy-based adaptive adjustment and an elitism strategy are used to further enhance fuzzing capability.

Result: The result shows that BASFuzz achieves a testing effectiveness of 90.335% and reduces the average time overhead by 2,163.852 seconds compared to the current best baseline.

Conclusion: The conclusion of this paper is that the proposed method is more effective and efficient for testing LLM-based NLP software, leading to a more effective robustness evaluation before deployment.

Abstract: Fuzzing has shown great success in evaluating the robustness of intelligent
natural language processing (NLP) software. As large language model (LLM)-based
NLP software is widely deployed in critical industries, existing methods still
face two main challenges: 1 testing methods are insufficiently coupled with the
behavioral patterns of LLM-based NLP software; 2 fuzzing capability for the
testing scenario of natural language generation (NLG) generally degrades. To
address these issues, we propose BASFuzz, an efficient Fuzz testing method
tailored for LLM-based NLP software. BASFuzz targets complete test inputs
composed of prompts and examples, and uses a text consistency metric to guide
mutations of the fuzzing loop, aligning with the behavioral patterns of
LLM-based NLP software. A Beam-Annealing Search algorithm, which integrates
beam search and simulated annealing, is employed to design an efficient fuzzing
loop. In addition, information entropy-based adaptive adjustment and an elitism
strategy further enhance fuzzing capability. We evaluate BASFuzz on six
datasets in representative scenarios of NLG and natural language understanding
(NLU). Experimental results demonstrate that BASFuzz achieves a testing
effectiveness of 90.335% while reducing the average time overhead by 2,163.852
seconds compared to the current best baseline, enabling more effective
robustness evaluation prior to software deployment.

</details>


### [63] [SLICET5: Static Program Slicing using Language Models with Copy Mechanism and Constrained Decoding](https://arxiv.org/abs/2509.17338)
*Pengfei He,Shaowei Wang,Tse-Hsun Chen*

Main category: cs.SE

TL;DR: This paper proposes \ourtool, a novel static program slicing framework using sequence-to-sequence learning with copy mechanisms and constrained decoding, achieving up to 27\% improvement in ExactMatch scores over existing methods by addressing dependency identification and hallucination issues.


<details>
  <summary>Details</summary>
Motivation: Traditional slicing tools require full source code parsing while learning-based approaches struggle with token dependency accuracy and generating unreliable "hallucinated" content. Real-world code snippets are often incomplete or unparsable yet require effective analysis.

Method: Reforms slicing as sequence-to-sequence task with: (1): Copy mechanism for precise token copying from inputs to capture dependencies (2): Dual constraints during decoding - lexical (output limited to input tokens only) and syntactic (TSED monotonicity checks for structure validity) using CodeT5+ models. Evaluated via CodeNet and LeetCode benchmarks.

Result: Outperforms state-of-the-art baselines by 27\% in ExactMatch scores while demonstrating superior robustness to incomplete code. Synthesized output maintains strict structural validity through TSED constraint and lexical filtering.

Conclusion: The combination of copy mechanisms and dual decoding constraints addresses critical limitations in learning-based slicing, enabling practical application to real-world code snippets with missing components while maintaining precision and structural integrity.

Abstract: Static program slicing is a fundamental technique in software engineering.
Traditional static slicing tools rely on parsing complete source code, which
limits their applicability to real-world scenarios where code snippets are
incomplete or unparsable. While recent research developed learning-based
approaches to predict slices, they face critical challenges: (1) Inaccurate
dependency identification, where models fail to precisely capture data and
control dependencies between code elements; and (2) Unconstrained generation,
where models produce slices with extraneous or hallucinated tokens not present
in the input, violating the structural integrity of slices. To address these
challenges, we propose \ourtool, a novel slicing framework that reformulates
static program slicing as a sequence-to-sequence task using lightweight
language models (e.g., CodeT5+). Our approach incorporates two key innovations.
First, we introduce a copy mechanism that enables the model to more accurately
capture inter-element dependencies and directly copy relevant tokens from the
input, improving both dependency reasoning and generation constraint. Second,
we design a constrained decoding process with (a) lexical constraint,
restricting outputs to input tokens only, and (b) syntactic constraint,
leveraging Tree Similarity of Edit Distance (TSED) monotonicity to detect
structurally invalid outputs and discard them. We evaluate \ourtool on CodeNet
and LeetCode datasets and show it consistently outperforms state-of-the-art
baselines, improving ExactMatch scores by up to 27\%. Furthermore, \ourtool
demonstrates strong performance on incomplete code, highlighting its robustness
and practical utility in real-world development environments.

</details>


### [64] [Prompts as Software Engineering Artifacts: A Research Agenda and Preliminary Findings](https://arxiv.org/abs/2509.17548)
*Hugo Villamizar,Jannik Fischbach,Alexander Korn,Andreas Vogelsang,Daniel Mendez*

Main category: cs.SE

TL;DR: This paper reveals ad-hoc prompt practices in SE workflows and proposes systematic prompt management as a critical research direction, validated through an exploratory survey of 74 developers.


<details>
  <summary>Details</summary>
Motivation: Prompts are increasingly used in software engineering (SE) but lack systematic development, documentation, and maintenance, making it unclear whether systematic management offers tangible benefits over current ad-hoc approaches.

Method: An exploratory survey with 74 software professionals across six countries was conducted to analyze prompt practices and challenges in LLM-integrated workflows.

Result: Prompt usage is predominantly ad-hoc, with trial-and-error refinement, limited reuse, and individual heuristics dominating over standardized practices, creating a clear rationale for systematic management.

Conclusion: The study establishes the need for systematic prompt management in SE by illustrating current ad-hoc practices and provides a foundation for developing evidence-based guidelines through empirical research.

Abstract: Developers now routinely interact with large language models (LLMs) to
support a range of software engineering (SE) tasks. This prominent role
positions prompts as potential SE artifacts that, like other artifacts, may
require systematic development, documentation, and maintenance. However, little
is known about how prompts are actually used and managed in LLM-integrated
workflows, what challenges practitioners face, and whether the benefits of
systematic prompt management outweigh the associated effort. To address this
gap, we propose a research programme that (a) characterizes current prompt
practices, challenges, and influencing factors in SE; (b) analyzes prompts as
software artifacts, examining their evolution, traceability, reuse, and the
trade-offs of systematic management; and (c) develops and empirically evaluates
evidence-based guidelines for managing prompts in LLM-integrated workflows. As
a first step, we conducted an exploratory survey with 74 software professionals
from six countries to investigate current prompt practices and challenges. The
findings reveal that prompt usage in SE is largely ad-hoc: prompts are often
refined through trial-and-error, rarely reused, and shaped more by individual
heuristics than standardized practices. These insights not only highlight the
need for more systematic approaches to prompt management but also provide the
empirical foundation for the subsequent stages of our research programme.

</details>


### [65] [From OCL to JSX: declarative constraint modeling in modern SaaS tools](https://arxiv.org/abs/2509.17629)
*Antonio Bucchiarone,Juri Di Rocco,Damiano Di Vincenzo,Alfonso Pierantonio*

Main category: cs.SE

TL;DR: This paper evaluates JSX, a React ecosystem tool, to address limitations in JavaScript-based model validation (OCL.js) for SaaS modeling environments, showing JSX's superior expressiveness and front-end integration.


<details>
  <summary>Details</summary>
Motivation: The shift to client-side, component-driven SPAs and cloud-based modeling platforms necessitates constraint languages that align with modern front-end architectures, but existing solutions like OCL.js have limited adoption and integration.

Method: The authors propose JSX as an alternative for constraint expression, leveraging its declarative syntax and component structure. They conduct empirical comparisons with OCL.js through modeling scenarios, analyzing expressiveness and integration.

Result: JSX-based constraints demonstrated broader expressiveness, better alignment with front-end-first architectures, and improved integration potential compared to OCL.js in evaluated modeling scenarios.

Conclusion: JSX offers a promising path for constraint specification in modern modeling tools, aligning with industry trends toward declarative, component-based front-end development.

Abstract: The rise of Node.js in 2010, followed by frameworks like Angular, React, and
Vue.js, has accelerated the growth of low code development platforms. These
platforms harness modern UIX paradigms, component-based architectures, and the
SaaS model to enable non-experts to build software. The widespread adoption of
single-page applications (SPAs), driven by these frameworks, has shaped
low-code tools to deliver responsive, client side experiences. In parallel,
many modeling platforms have moved to the cloud, adopting either server-centric
architectures (e.g., GSLP) or client-side intelligence via SPA frameworks,
anchoring core components in JavaScript or TypeScript. Within this context,
OCL.js, a JavaScript-based implementation of the Object Constraint Language,
offers a web aligned approach to model validation, yet faces challenges such as
partial standard coverage, limited adoption, and weak integration with modern
front-end toolchains. In this paper, we explore JSX, a declarative, functional
subset of JavaScript/TypeScript used in the React ecosystem, as an alternative
to constraint expression in SaaS-based modeling environments. Its
component-oriented structure supports inductive definitions for syntax, code
generation, and querying. Through empirical evaluation, we compare JSX-based
constraints with OCL.js across representative modeling scenarios. Results show
JSX provides broader expressiveness and better fits front-end-first
architectures, indicating a promising path for constraint specification in
modern modeling tools.

</details>


### [66] [Diagnosing Violations of State-based Specifications in iCFTL](https://arxiv.org/abs/2509.17776)
*Cristina Stratan,Claudio Mandrioli,Domenico Bianculli*

Main category: cs.SE

TL;DR: The paper introduces iCFTL-Diagnostics, a tool that enhances runtime verification by identifying the root causes of violated iCFTL specifications through backward data-flow analysis and enriched execution traces.


<details>
  <summary>Details</summary>
Motivation: Traditional runtime verification only provides Boolean verdicts for specification violations; this lack of detailed diagnostic information hinders efficient debugging of complex software systems operating in dynamic environments.

Method: The authors use backward data-flow analysis to determine relevant source code statements contributing to violations, instrument programs to generate enriched execution traces, and perform runtime analysis to pinpoint violating statements. The implementation is evaluated using 112 specifications across 10 projects.

Result: The tool achieves 90% precision in identifying relevant statements for 100/112 specifications, reduces diagnostic code inspection by 90%, generates diagnoses within 7 minutes, and incurs <30% execution overhead and <20MB memory overhead.

Conclusion: iCFTL-Diagnostics effectively addresses the diagnostic limitations of runtime verification for iCFTL specifications by combining static analysis and enriched execution traces, offering high precision and efficiency for debugging in real-world software projects.

Abstract: As modern software systems grow in complexity and operate in dynamic
environments, the need for runtime analysis techniques becomes a more critical
part of the verification and validation process. Runtime verification monitors
the runtime system behaviour by checking whether an execution trace - a
sequence of recorded events - satisfies a given specification, yielding a
Boolean or quantitative verdict. However, when a specification is violated,
such a verdict is often insufficient to understand why the violation happened.
To fill this gap, diagnostics approaches aim to produce more informative
verdicts. In this paper, we address the problem of generating informative
verdicts for violated Inter-procedural Control-Flow Temporal Logic (iCFTL)
specifications that express constraints over program variable values. We
propose a diagnostic approach based on backward data-flow analysis to
statically determine the relevant statements contributing to the specification
violation. Using this analysis, we instrument the program to produce enriched
execution traces. Using the enriched execution traces, we perform the runtime
analysis and identify the statements whose execution led to the specification
violation. We implemented our approach in a prototype tool, iCFTL-Diagnostics,
and evaluated it on 112 specifications across 10 software projects. Our tool
achieves 90% precision in identifying relevant statements for 100 of the 112
specifications. It reduces the number of lines that have to be inspected for
diagnosing a violation by at least 90%. In terms of computational cost,
iCFTL-Diagnostics generates a diagnosis within 7 min, and requires no more than
25 MB of memory. The instrumentation required to support diagnostics incurs an
execution time overhead of less than 30% and a memory overhead below 20%.

</details>
