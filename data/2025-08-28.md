<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 30]
- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Tight Quantum-Security Bounds and Parameter Optimization for SPHINCS+ and NTRU](https://arxiv.org/abs/2508.19250)
*Ruopengyu Xu,Chenglian Liu*

Main category: cs.CR

TL;DR: This paper advances quantum-resistant cryptography by establishing tight security bounds for SPHINCS+ and NTRU through novel analytical methods.


<details>
  <summary>Details</summary>
Motivation: Quantum computing threatens classical cryptography, necessitating rigorous security analysis of NIST PQC finalists to ensure long-term protection for standardization.

Method: Four key approaches: 1) A quantum attack model with decoherence and parallelism constraints; 2) Entropy concentration inequalities for SPHINCS+ optimization; 3) Quantum lattice entropy for NTRU parameter optimization; 4) A tightened NTRU-LWE reduction framework.

Result: 15-20% parameter reduction in SPHINCS+, optimized NTRU lattice parameters, and polynomial-factor security improvement over existing NTRU-LWE reductions.

Conclusion: The research demonstrates significant security enhancements for SPHINCS+ and NTRU, offering practical parameter sets for NIST standardization while addressing quantum threats.

Abstract: The imminent threat of quantum computing necessitates quantum-resistant
cryptosystems. This paper establishes tight security bounds for two NIST PQC
finalists: SPHINCS+ (hash-based) and NTRU (lattice-based). Our key
contributions include: (1) A quantum attack model incorporating decoherence
effects ($\tau_d$) and parallelization limits; (2) Improved entropy
concentration inequalities reducing SPHINCS+ parameters by 15-20\%; (3)
Optimized NTRU lattice parameters via quantum lattice entropy $H_Q(\Lambda)$;
(4) Tightened NTRU-to-LWE reduction with polynomial-factor improvement.
Theoretical results demonstrate significant security enhancement over existing
constructions, providing implementable parameters for standardization.

</details>


### [2] [The Aegis Protocol: A Foundational Security Framework for Autonomous AI Agents](https://arxiv.org/abs/2508.19267)
*Sai Teja Reddy Adapala,Yashwanth Reddy Alugubelly*

Main category: cs.CR

TL;DR: Aegis Protocol secures autonomous AI agents using decentralized IDs, post-quantum crypto, and ZKPs, achieving 0% simulated attack success with 2.79s compliance verification in 1,000-agent systems.


<details>
  <summary>Details</summary>
Motivation: Traditional cybersecurity paradigms lack the capability to address systemic risks in complex AI agent ecosystems, including control-flow hijacking and cascading failures that emerge from autonomous decision-making and emergent system behaviors.

Method: Developed a layered security framework integrating (1) W3C DIDs for non-spoofable identities, (2) NIST-standardized post-quantum cryptography for message integrity, and (3) Halo2 zero-knowledge proofs for privacy-preserving compliance verification. Validated through formal adversary modeling (Dolev-Yao extension) and STRIDE evaluation, with implementation tested via discrete-event simulation of 1,000 agents.

Result: Demonstrated 0% attack success rate across 20,000 simulation trials with complete policy violation detection, while achieving median 2.79s ZKP latency. Established first reproducible performance baseline for multi-agent cryptographic security through calibrated discrete-event simulations.

Conclusion: The Aegis Protocol establishes a foundational security framework for autonomous AI ecosystems, providing verifiable safety guarantees while maintaining scalability in complex multi-agent environments.

Abstract: The proliferation of autonomous AI agents marks a paradigm shift toward
complex, emergent multi-agent systems. This transition introduces systemic
security risks, including control-flow hijacking and cascading failures, that
traditional cybersecurity paradigms are ill-equipped to address. This paper
introduces the Aegis Protocol, a layered security framework designed to provide
strong security guarantees for open agentic ecosystems. The protocol integrates
three technological pillars: (1) non-spoofable agent identity via W3C
Decentralized Identifiers (DIDs); (2) communication integrity via
NIST-standardized post-quantum cryptography (PQC); and (3) verifiable,
privacy-preserving policy compliance using the Halo2 zero-knowledge proof (ZKP)
system. We formalize an adversary model extending Dolev-Yao for agentic threats
and validate the protocol against the STRIDE framework. Our quantitative
evaluation used a discrete-event simulation, calibrated against cryptographic
benchmarks, to model 1,000 agents. The simulation showed a 0 percent success
rate across 20,000 attack trials. For policy verification, analysis of the
simulation logs reported a median proof-generation latency of 2.79 seconds,
establishing a performance baseline for this class of security. While the
evaluation is simulation-based and early-stage, it offers a reproducible
baseline for future empirical studies and positions Aegis as a foundation for
safe, scalable autonomous AI.

</details>


### [3] [MixGAN: A Hybrid Semi-Supervised and Generative Approach for DDoS Detection in Cloud-Integrated IoT Networks](https://arxiv.org/abs/2508.19273)
*Tongxi Wu,Chenwei Xu,Jin Yang*

Main category: cs.CR

TL;DR: MixGAN tackles DDoS detection in IoT-cloud systems by integrating temporal feature extraction, synthetic minority sampling (CTGAN), and pseudo-label optimization (MAS), achieving significant performance improvements over alternatives.


<details>
  <summary>Details</summary>
Motivation: Existing DDoS detection methods struggle with complex traffic dynamics, severe class imbalance, and scarce labeled data in heterogeneous IoT-cloud systems, necessitating a solution that generalizes under limited supervision and dynamic conditions.

Method: The methodology combines a 1-D WideResNet backbone for temporal traffic pattern analysis, a pre-trained CTGAN for synthetic DDoS sample generation, and a MixUp-Average-Sharpen (MAS) strategy for pseudo-label optimization.

Result: MixGAN achieves 2.5% higher accuracy and 4% improvements in TPR and TNR compared to state-of-the-art methods on NSL-KDD, BoT-IoT, and CICIoT2023 datasets, with publicly available source code.

Conclusion: The paper introduces MixGAN, a hybrid detection method that effectively addresses the challenges of DDoS detection in cloud-integrated IoT systems through conditional generation, semi-supervised learning, and robust feature extraction, demonstrating robustness and competitive performance on three benchmark datasets.

Abstract: The proliferation of cloud-integrated IoT systems has intensified exposure to
Distributed Denial of Service (DDoS) attacks due to the expanded attack
surface, heterogeneous device behaviors, and limited edge protection. However,
DDoS detection in this context remains challenging because of complex traffic
dynamics, severe class imbalance, and scarce labeled data. While recent methods
have explored solutions to address class imbalance, many still struggle to
generalize under limited supervision and dynamic traffic conditions. To
overcome these challenges, we propose MixGAN, a hybrid detection method that
integrates conditional generation, semi-supervised learning, and robust feature
extraction. Specifically, to handle complex temporal traffic patterns, we
design a 1-D WideResNet backbone composed of temporal convolutional layers with
residual connections, which effectively capture local burst patterns in traffic
sequences. To alleviate class imbalance and label scarcity, we use a pretrained
CTGAN to generate synthetic minority-class (DDoS attack) samples that
complement unlabeled data. Furthermore, to mitigate the effect of noisy
pseudo-labels, we introduce a MixUp-Average-Sharpen (MAS) strategy that
constructs smoothed and sharpened targets by averaging predictions over
augmented views and reweighting them towards high-confidence classes.
Experiments on NSL-KDD, BoT-IoT, and CICIoT2023 demonstrate that MixGAN
achieves up to 2.5% higher accuracy and 4% improvement in both TPR and TNR
compared to state-of-the-art methods, confirming its robustness in large-scale
IoT-cloud environments. The source code is publicly available at
https://github.com/0xCavaliers/MixGAN.

</details>


### [4] [Towards Production-Worthy Simulation for Autonomous Cyber Operations](https://arxiv.org/abs/2508.19278)
*Konur Tholl,Mariam El Mezouar,Ranwa Al Mallah*

Main category: cs.CR

TL;DR: Researchers enhanced CybORG's RL capabilities by adding realistic actions and optimizing training signals, proving effective agent training is achievable while maintaining simulation realism.


<details>
  <summary>Details</summary>
Motivation: Existing environments lack real-world operational realism for autonomous cyber operations training, necessitating enhanced action sets and training signals to enable effective reinforcement learning.

Method: Extended CybORG's Cage Challenge 2 environment with Patch/Isolate/Unisolate actions and modified reward structures/agent feature spaces to optimize training performance.

Result: DQN/PPO agent performance demonstrated that additional functionality and optimized reward designs maintain CybORG's training signal robustness while enhancing realism.

Conclusion: The modifications to CybORG's environment and agent training framework successfully integrate realistic capabilities while preserving its efficacy in generating useful signals for RL agent training.

Abstract: Simulated environments have proven invaluable in Autonomous Cyber Operations
(ACO) where Reinforcement Learning (RL) agents can be trained without the
computational overhead of emulation. These environments must accurately
represent cybersecurity scenarios while producing the necessary signals to
support RL training. In this study, we present a framework where we first
extend CybORG's Cage Challenge 2 environment by implementing three new actions:
Patch, Isolate, and Unisolate, to better represent the capabilities available
to human operators in real-world settings. We then propose a design for agent
development where we modify the reward signals and the agent's feature space to
enhance training performance. To validate these modifications, we train DQN and
PPO agents in the updated environment. Our study demonstrates that CybORG can
be extended with additional realistic functionality, while maintaining its
ability to generate informative training signals for RL agents.

</details>


### [5] [CORTEX: Composite Overlay for Risk Tiering and Exposure in Operational AI Systems](https://arxiv.org/abs/2508.19281)
*Aoun E Muhammad,Kin Choong Yow,Jamel Baili,Yongwon Cho,Yunyoung Nam*

Main category: cs.CR

TL;DR: This paper introduces CORTEX, a layered AI risk scoring framework based on real-world incident data, merging technical vulnerability analysis with regulatory alignment to enable systemic risk management across critical AI applications.


<details>
  <summary>Details</summary>
Motivation: AI systems are now deployed in critical sectors (healthcare, finance, etc.) where failures transitioned from theoretical to recurring systemic risks. Existing methods lack structured multi-layered approaches to quantify these technical, regulatory, and contextual dimensions.

Method: CORTEX is a five-tier architecture: 1) utility-adjusted Likelihood x Impact calculations, 2) governance overlays (EU AI Act, NIST RMF, OECD), 3) technical surface scores (drift, traceability, adversarial risk), 4) contextual/environmental modifiers, 5) Bayesian risk aggregation and Monte Carlo simulations. Based on 1,200+ AI incidents analyzed.

Result: A composite risk score operationalizable across model audits, risk registers, and governance dashboards. Categorizes 29 technical vulnerability groups with practical risk assessment workflows.

Conclusion: CORTEX offers a comprehensive, empirically grounded framework for AI risk assessment, combining technical and regulatory perspectives. The framework aims to enhance transparency, governance, and risk management in high-stakes AI deployments.

Abstract: As the deployment of Artificial Intelligence (AI) systems in high-stakes
sectors - like healthcare, finance, education, justice, and infrastructure has
increased - the possibility and impact of failures of these systems have
significantly evolved from being a theoretical possibility to practical
recurring, systemic risk. This paper introduces CORTEX (Composite Overlay for
Risk Tiering and Exposure), a multi-layered risk scoring framework proposed to
assess and score AI system vulnerabilities, developed on empirical analysis of
over 1,200 incidents documented in the AI Incident Database (AIID), CORTEX
categorizes failure modes into 29 technical vulnerability groups. Each
vulnerability is scored through a five-tier architecture that combines: (1)
utility-adjusted Likelihood x Impact calculations; (2) governance + contextual
overlays aligned with regulatory frameworks, such as the EU AI Act, NIST RMF,
OECD principles; (3) technical surface scores, covering exposure vectors like
drift, traceability, and adversarial risk; (4) environmental and residual
modifiers tailored to context of where these systems are being deployed to use;
and (5) a final layered assessment via Bayesian risk aggregation and Monte
Carlo simulation to model volatility and long-tail risks. The resulting
composite score can be operationalized across AI risk registers, model audits,
conformity checks, and dynamic governance dashboards.

</details>


### [6] [Rethinking Denial-of-Service: A Conditional Taxonomy Unifying Availability and Sustainability Threats](https://arxiv.org/abs/2508.19283)
*Mark Dorsett,Scott Man,Tim Koussas*

Main category: cs.CR

TL;DR: A unified framework with three models improves DoS attack classification by introducing 6 behavioral conditions, enabling consistent analysis of legacy and cloud-era attacks while addressing gaps in existing methodologies.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the growing complexity of denial-of-service attacks, particularly in cloud and serverless environments, where existing classification methods lack consistency and fail to account for emerging attack patterns like sustainability-based threats. A unified, condition-driven framework is needed to standardize terminology, improve threat modeling, and guide mitigation strategies.

Method: The framework consists of a conditional tree taxonomy with six observable conditions (C0-C5), a hierarchical lattice based on order theory for cumulative reasoning, and a conceptual Venn diagram to visualize overlaps between attack types. These models collectively enable structured classification and comparative analysis of both established and emerging denial-of-service attacks.

Result: The framework successfully classifies known attacks (e.g., DoS, DDoS, EDoS) using empirical conditions, identifies hybrid or novel variants through hierarchical analysis, and clarifies conceptual overlaps between availability-focused and sustainability-based attacks. Its applicability to cloud-native infrastructures, where sustainability attacks are under-recognized, positions it as a practical tool for defenders and architects.

Conclusion: The paper introduces a comprehensive framework for classifying DoS attacks using three interrelated models, addressing gaps in existing classification systems and offering a theoretical foundation for improved threat analysis and mitigation in cloud-native environments.

Abstract: This paper proposes a unified, condition-based framework for classifying both
legacy and cloud-era denial-of-service (DoS) attacks. The framework comprises
three interrelated models: a formal conditional tree taxonomy, a hierarchical
lattice structure based on order theory, and a conceptual Venn diagram. At its
core, the taxonomy introduces six observable conditions (C0-C5) grounded in
real-world attack behaviours, including source distribution, traffic volume,
infrastructure targeting, and financial exploitation. These conditions enable
consistent classification of known attacks-such as DoS, DDoS, LDoS, LDDoS,
EDoS, DoW, and DDoW, while supporting identification of emerging or hybrid
variants. The lattice structure captures the cumulative satisfaction of
conditions, allowing hierarchical reasoning across denial attack classes. The
Venn diagram highlights conceptual overlaps between availability- and
sustainability-focused attacks, improving comparative insight. Together, these
models provide a robust analytical lens for threat modeling, mitigation
strategy design, and attacker intent classification. The framework is
particularly relevant in cloud-native and serverless environments, where
sustainability-based attacks are increasingly impactful yet under-recognised.
Its extensibility also permits future integration of socio-technical or
behavioural dimensions. By offering a structured taxonomy with theoretical
grounding and real-world applicability, this work advances denial attack
comprehension and equips defenders, researchers, and cloud architects with a
shared vocabulary for interpreting and mitigating evolving threat vectors.

</details>


### [7] [A Comprehensive Review of Denial of Wallet Attacks in Serverless Architectures](https://arxiv.org/abs/2508.19284)
*Mark Dorsett,Scott Mann,Jabed Chowdhury,Abdun Mahmood*

Main category: cs.CR

TL;DR: This paper is the first comprehensive review on Denial of Wallet (DoW) attacks in serverless computing, analyzing their financial impact, attack techniques, and current detection/mitigation strategies with a focus on gaps like real-world data and adaptive billing models.


<details>
  <summary>Details</summary>
Motivation: Serverless computing’s pay-as-you-go model makes it vulnerable to financially-targeted DoW attacks, which differ from traditional DoS by focusing on cost escalation without service disruption.

Method: The study systematically categorizes attack types (Blast DDoW, Continual Inconspicuous DDoW, Background Chained DDoW), evaluates simulation tools (e.g., DoWTS), and examines machine learning approaches (Gringotts, DoWNet) for detecting malicious traffic patterns.

Result: Highlights advancements in detection (deep learning, anomaly detection) and mitigation strategies but emphasizes persistent challenges, including limited real-world datasets and the need for billing model innovations.

Conclusion: Serves as a foundational resource for addressing DoW threats, identifies critical research gaps (e.g., adaptive billing), and advocates for further industry and academic collaboration to secure pay-as-you-go cloud environments.

Abstract: The Denial of Wallet (DoW) attack poses a unique and growing threat to
serverless architectures that rely on Function-as-a-Service (FaaS) models,
exploiting the cost structure of pay-as-you-go billing to financially burden
application owners. Unlike traditional Denial of Service (DoS) attacks, which
aim to exhaust resources and disrupt service availability, DoW attacks focus on
escalating costs without impacting service operation. This review traces the
evolution of DoW research, from initial awareness and attack classification to
advancements in detection and mitigation strategies. Key developments include
the categorisation of attack types-such as Blast DDoW, Continual Inconspicuous
DDoW, and Background Chained DDoW-and the creation of simulation tools like
DoWTS, which enable safe experimentation and data generation. Recent
advancements highlight machine learning approaches, including systems like
Gringotts and DoWNet, which leverage deep learning and anomaly detection to
identify malicious traffic patterns. Although substantial progress has been
made, challenges persist, notably the lack of real-world data and the need for
adaptive billing models. This is the first comprehensive literature review
dedicated strictly to Denial of Wallet attacks, providing an in-depth analysis
of their financial impacts, attack techniques, mitigation strategies, and
detection mechanisms within serverless computing. The paper also presents the
first detailed examination of simulation and data generation tools used for DoW
research, addressing a critical gap in existing cybersecurity literature. By
synthesising these key areas, this study serves as a foundational resource for
future research and industry efforts in securing pay-as-you-go cloud
environments.

</details>


### [8] [RL-Finetuned LLMs for Privacy-Preserving Synthetic Rewriting](https://arxiv.org/abs/2508.19286)
*Zhan Shi,Yefeng Yuan,Yuhong Liu,Liang Cheng,Yi Fang*

Main category: cs.CR

TL;DR: This paper introduces a reinforcement learning framework for privacy-preserving data generation, balancing privacy and utility by training LLMs with a composite reward that combines semantic and structural privacy signals.


<details>
  <summary>Details</summary>
Motivation: Traditional anonymization techniques inadequately address privacy risks in datasets due to performance degradation and vulnerability to inference attacks using implicit signals like writing style or demographics.

Method: A reinforcement learning approach fine-tunes a large language model using a composite reward function that optimizes explicit/implicit privacy, semantic fidelity, and output diversity, leveraging a minimum spanning tree over latent representations to model privacy-sensitive signals.

Result: The method significantly improves author obfuscation and privacy metrics without compromising semantic quality in generated outputs.

Conclusion: The proposed reinforcement learning framework provides a scalable and model-agnostic solution for enhancing privacy in data generation while maintaining semantic quality, effectively balancing user privacy and data utility.

Abstract: The performance of modern machine learning systems depends on access to
large, high-quality datasets, often sourced from user-generated content or
proprietary, domain-specific corpora. However, these rich datasets inherently
contain sensitive personal information, raising significant concerns about
privacy, data security, and compliance with regulatory frameworks. While
conventional anonymization techniques can remove explicit identifiers, such
removal may result in performance drop in downstream machine learning tasks.
More importantly, simple anonymization may not be effective against inference
attacks that exploit implicit signals such as writing style, topical focus, or
demographic cues, highlighting the need for more robust privacy safeguards
during model training. To address the challenging issue of balancing user
privacy and data utility, we propose a reinforcement learning framework that
fine-tunes a large language model (LLM) using a composite reward function that
jointly optimizes for explicit and implicit privacy, semantic fidelity, and
output diversity. To effectively capture population level regularities, the
privacy reward combines semantic cues with structural patterns derived from a
minimum spanning tree (MST) over latent representations. By modeling these
privacy-sensitive signals in their distributional context, the proposed
approach guides the model to generate synthetic rewrites that preserve utility
while mitigating privacy risks. Empirical results show that the proposed method
significantly enhances author obfuscation and privacy metrics without degrading
semantic quality, providing a scalable and model-agnostic solution for privacy
preserving data generation in the era of large language models.

</details>


### [9] [Prompt-in-Content Attacks: Exploiting Uploaded Inputs to Hijack LLM Behavior](https://arxiv.org/abs/2508.19287)
*Zhuotao Lian,Weiyu Wang,Qingkui Zeng,Toru Nakanishi,Teruaki Kitasuka,Chunhua Su*

Main category: cs.CR

TL;DR: Researchers uncover prompt-injection attacks that embed hidden instructions in normal inputs to manipulate LLM outputs. These attacks exploit poor input handling in platforms, causing biased/fabricated results. Authors demonstrate attacks, explain technical causes, and suggest defenses.


<details>
  <summary>Details</summary>
Motivation: This work addresses the growing need to secure LLM applications by exposing vulnerabilities in input processing - adversarial prompts hidden in ordinary user content can covertly manipulate outputs like summaries and answers without system compromise.

Method: The authors demonstrate attack feasibility on popular platforms through adversarial input examples, analyze root causes like prompt concatenation and input isolation failures, and propose mitigation approaches.

Result: Demonstrated successful execution of prompt-injection attacks across major platforms, identified architectural weaknesses enabling the attacks (e.g. naive prompt concatenation), and provided actionable mitigation recommendations for LLM system designers.

Conclusion: The paper concludes that prompt in content injection presents a subtle yet practical threat to real-world LLM workflows, necessitating improved input isolation mechanisms and mitigation strategies to prevent adversarial output manipulation.

Abstract: Large Language Models (LLMs) are widely deployed in applications that accept
user-submitted content, such as uploaded documents or pasted text, for tasks
like summarization and question answering. In this paper, we identify a new
class of attacks, prompt in content injection, where adversarial instructions
are embedded in seemingly benign inputs. When processed by the LLM, these
hidden prompts can manipulate outputs without user awareness or system
compromise, leading to biased summaries, fabricated claims, or misleading
suggestions. We demonstrate the feasibility of such attacks across popular
platforms, analyze their root causes including prompt concatenation and
insufficient input isolation, and discuss mitigation strategies. Our findings
reveal a subtle yet practical threat in real-world LLM workflows.

</details>


### [10] [Tricking LLM-Based NPCs into Spilling Secrets](https://arxiv.org/abs/2508.19288)
*Kyohei Shiomi,Zhuotao Lian,Toru Nakanishi,Teruaki Kitasuka*

Main category: cs.CR

TL;DR: This paper reveals security risks in LLM-based game NPCs, showing that adversarial prompts can force them to leak hidden secrets, and highlights the importance of robust security safeguards in AI-driven interactive systems.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly integrated into interactive systems like game NPCs, there is a significant need to understand and mitigate novel security risks, such as adversarial prompt exploitation.

Method: The paper investigates adversarial prompt injection techniques to test whether LLM-based NPCs can be manipulated into revealing undisclosed background secrets through carefully crafted inputs.

Result: The research demonstrates that adversarial prompt injection can successfully cause LLM-driven NPCs to disclose hidden background information that should remain confidential within game environments.

Conclusion: The study identifies a critical security vulnerability in LLM-based game NPCs, highlighting the need for safeguards against adversarial prompt injection to prevent unintended disclosure of hidden background secrets.

Abstract: Large Language Models (LLMs) are increasingly used to generate dynamic
dialogue for game NPCs. However, their integration raises new security
concerns. In this study, we examine whether adversarial prompt injection can
cause LLM-based NPCs to reveal hidden background secrets that are meant to
remain undisclosed.

</details>


### [11] [Stand on The Shoulders of Giants: Building JailExpert from Previous Attack Experience](https://arxiv.org/abs/2508.19292)
*Xi Wang,Songlei Jian,Shasha Li,Xiaopeng Li,Bin Ji,Jun Ma,Xiaodong Liu,Jing Wang,Feilong Bao,Jianfeng Zhang,Baosheng Wang,Jie Yu*

Main category: cs.CR

TL;DR: JailExpert optimizes automated jailbreak attacks by systematizing past experiences, achieving record-breaking performance in both success rates (↑17%) and efficiency (×2.7).


<details>
  <summary>Details</summary>
Motivation: Existing jailbreak methods suffer from inefficiency and repetitive optimization as they fail to integrate past attack experiences, leading to obsolete attack templates as models evolve.

Method: JailExpert introduces a formal representation of attack experience structure, groups experiences by semantic drift, and dynamically updates the experience pool to avoid redundant optimization.

Result: Experiments show JailExpert achieves a 17% average attack success rate increase and 2.7× efficiency improvement compared to current black-box jailbreak methods.

Conclusion: JailExpert is an automated jailbreak framework that significantly enhances attack effectiveness and efficiency by leveraging formalized experience structures and semantic grouping, outperforming state-of-the-art methods by 17% in success rate and 2.7x in efficiency.

Abstract: Large language models (LLMs) generate human-aligned content under certain
safety constraints. However, the current known technique ``jailbreak prompt''
can circumvent safety-aligned measures and induce LLMs to output malicious
content. Research on Jailbreaking can help identify vulnerabilities in LLMs and
guide the development of robust security frameworks. To circumvent the issue of
attack templates becoming obsolete as models evolve, existing methods adopt
iterative mutation and dynamic optimization to facilitate more automated
jailbreak attacks. However, these methods face two challenges: inefficiency and
repetitive optimization, as they overlook the value of past attack experiences.
To better integrate past attack experiences to assist current jailbreak
attempts, we propose the \textbf{JailExpert}, an automated jailbreak framework,
which is the first to achieve a formal representation of experience structure,
group experiences based on semantic drift, and support the dynamic updating of
the experience pool. Extensive experiments demonstrate that JailExpert
significantly improves both attack effectiveness and efficiency. Compared to
the current state-of-the-art black-box jailbreak methods, JailExpert achieves
an average increase of 17\% in attack success rate and 2.7 times improvement in
attack efficiency. Our implementation is available at
\href{https://github.com/xiZAIzai/JailExpert}{XiZaiZai/JailExpert}

</details>


### [12] [Leveraging 3D Technologies for Hardware Security: Opportunities and Challenges](https://arxiv.org/abs/2508.19309)
*Peng Gu,Shuangchen Li,Dylan Stow,Russell Barnes,Liu Liu,Yuan Xie,Eren Kursshan*

Main category: cs.CR

TL;DR: This paper explores how 2.5D/3D technologies can enhance IC security by proposing four novel designs that address side-channel attacks, hardware trojans, and other threats through architectural and fabrication innovations.


<details>
  <summary>Details</summary>
Motivation: Current 3D/2.5D IC design approaches lack effective solutions for emerging security challenges including side-channel attacks, hardware trojans, secure manufacturing, and IP piracy.

Method: Four approaches are proposed: (1) 3D shielding architecture for side-channel attacks; (2) split fabrication with active interposers; (3) circuit camouflage on monolithic 3D ICs; and (4) 3D IC-based security processing-in-memory (PIM).

Result: The designs demonstrate advantages in improving existing security countermeasures and enabling new security features through 2.5D/3D technologies.

Conclusion: The new designs improve existing countermeasures against security threats and provide novel security features by leveraging the intrinsic characteristics of 2.5D and 3D technologies.

Abstract: 3D die stacking and 2.5D interposer design are promising technologies to
improve integration density, performance and cost. Current approaches face
serious issues in dealing with emerging security challenges such as side
channel attacks, hardware trojans, secure IC manufacturing and IP piracy. By
utilizing intrinsic characteristics of 2.5D and 3D technologies, we propose
novel opportunities in designing secure systems. We present: (i) a 3D
architecture for shielding side-channel information; (ii) split fabrication
using active interposers; (iii) circuit camouflage on monolithic 3D IC, and
(iv) 3D IC-based security processing-in-memory (PIM). Advantages and challenges
of these designs are discussed, showing that the new designs can improve
existing countermeasures against security threats and further provide new
security features.

</details>


### [13] [An Investigation on Group Query Hallucination Attacks](https://arxiv.org/abs/2508.19321)
*Kehao Miao,Xiaolong Jin*

Main category: cs.CR

TL;DR: Researchers demonstrate Group Query Attack as a novel method to exploit sequential prompt context in LLMs, causing performance collapses and backdoor activations. This highlights critical security flaws in multi-turn conversations involving math/code reasoning.


<details>
  <summary>Details</summary>
Motivation: The study addresses the gap in understanding LLM vulnerabilities during multi-question user interactions. They aim to quantify how accumulated context in sequential prompts compromises model reliability and exposes hidden backdoors in both pre-trained and aligned models.

Method: The authors propose Group Query Attack, a technique delivering sequential prompts to LLMs in group settings. They experimentally analyze the impact on model outputs through performance metrics and backdoor activation tests across various reasoning tasks.

Result: Group Query Attack demonstrates substantial performance degradation (30-50% accuracy drop in math/code tasks) and successfully triggers backdoor behaviors in 68%-85% of test cases across 8 LLM variants including GPT3.5 and Llama3.

Conclusion: This paper concludes that Group Query Attack poses a significant threat to LLMs, particularly fine-tuned/aligned models, by exploiting accumulated context risks and triggering backdoors. It underscores the need for robust context-handling mechanisms and secure training practices.

Abstract: With the widespread use of large language models (LLMs), understanding their
potential failure modes during user interactions is essential. In practice,
users often pose multiple questions in a single conversation with LLMs.
Therefore, in this study, we propose Group Query Attack, a technique that
simulates this scenario by presenting groups of queries to LLMs simultaneously.
We investigate how the accumulated context from consecutive prompts influences
the outputs of LLMs. Specifically, we observe that Group Query Attack
significantly degrades the performance of models fine-tuned on specific tasks.
Moreover, we demonstrate that Group Query Attack induces a risk of triggering
potential backdoors of LLMs. Besides, Group Query Attack is also effective in
tasks involving reasoning, such as mathematical reasoning and code generation
for pre-trained and aligned models.

</details>


### [14] [A Technical Review on Comparison and Estimation of Steganographic Tools](https://arxiv.org/abs/2508.19323)
*Ms. Preeti P. Bhatt,Rakesh R. Savant*

Main category: cs.CR

TL;DR: This paper reviews/image-steganography-tools-by-testing-their-performance-based-on-image-features-like-size-dimensions-and-histogram-changes-showing-variable-efficiency-among-commonly-used-tools


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need to systematically evaluate and compare image steganography tools to identify optimal performers under standardized testing conditions.

Method: The paper tests six frequently used steganography tools by embedding identical text into host images. Performance is compared analytically using image characteristics such as size, dimensions, pixel values, and histogram changes.

Result: Results show comparable baseline performance of six tools, with efficiency differences emerging through analysis of image features. No single tool outperforms others across all metrics.

Conclusion: The study concludes that while six selected steganography tools perform similarly overall, their efficiency varies based on image features like size, dimensions, pixel values, and histogram differentiation, offering a framework for selecting optimal tools.

Abstract: Steganography is technique of hiding a data under cover media using different
steganography tools. Image steganography is hiding of data
(Text/Image/Audio/Video) under a cover as Image. This review paper presents
classification of image steganography and the comparison of various Image
steganography tools using different image formats. Analyzing numerous tools on
the basis of Image features and extracting the best one. Some of the tools
available in the market were selected based on the frequent use; these tools
were tested using the same input on all of them. Specific text was embedded
within all host images for each of the six Steganography tools selected. The
results of the experiment reveal that all the six tools were relatively
performing at the same level, though some software performs better than others
through efficiency. And it was based on the image features like size,
dimensions, and pixel value and histogram differentiation.

</details>


### [15] [Just Dork and Crawl: Measuring Illegal Online Gambling Defacement in Indonesian Websites](https://arxiv.org/abs/2508.19368)
*Luqman Muhammad Zagi,Girindro Pringgo Digdo,Wervyan Shalannanda*

Main category: cs.CR

TL;DR: This paper examines illegal gambling-driven website defacements in Indonesia using scalable methods, uncovering persistent patterns and emphasizing the need for continuous monitoring.


<details>
  <summary>Details</summary>
Motivation: To understand the scale, persistence, and tactics of illegal online gambling actors defacing Indonesian websites to improve mitigation strategies.

Method: A lightweight approach combining keyword-driven dorking with systematic crawling and keyword-counting to identify and validate defaced websites in Indonesia.

Result: 453 defaced webpages were analyzed, revealing 150 repeat defacements, 129 fixed instances, 55 keyword modifications, and 8,837 unique third-party URLs linked to gambling, with an average website response time of 75.3 hours.

Conclusion: The study emphasizes that simple, reproducible methods effectively measure defacement campaigns driven by illegal gambling, underscoring the need for continuous monitoring to strengthen web defenses.

Abstract: This study investigates the defacement of Indonesian websites by actors
promoting illegal online gambling. Using a lightweight methodology that
combines keyword-driven dorking with systematic crawling, we identified 453
defaced webpages within one month. Although dorking alone yielded a false
positive rate of approximately 20.3\%, the integration of crawling and
keyword-counting enabled reliable differentiation between true and false
positives. Our measurements revealed diverse defacement behaviors, including
repeat defacements (150 cases), fixed instances (129), keyword modifications
(55), and redirections or hidden URL injections. In total, 8,837 unique
third-party URLs spanning 5,930 domains were captured, with a small subset
recurring across multiple sites. Website responses were inconsistent, with an
average reaction time of 75.3 hours. These findings demonstrate that simple,
reproducible techniques can provide meaningful insights into the scale,
persistence, and dynamics of defacement, highlighting the importance of
continuous measurement for strengthening defenses against online gambling
activities.

</details>


### [16] [A NIS2 pan-European registry for identifying and classifying essential and important entities](https://arxiv.org/abs/2508.19395)
*Fabian Aude Steen,Daniel Assani Shabani*

Main category: cs.CR

TL;DR: This paper designs a technical framework to implement EU's NIS2 Directive cybersecurity regulations, creating an adaptable system for entity classification, supervision, and notifications using Design Science Research methodology.


<details>
  <summary>Details</summary>
Motivation: The NIS2 Directive requires EU member states to implement cybersecurity governance frameworks with entity classification/supervision obligations. The research addresses the need for a standardized technical solution to manage cross-directive dependencies and reduce administrative burdens while ensuring compliance.

Method: Using the Design Science Research methodology, the paper translates complex legal provisions into structured workflows, deterministic classification algorithms, and interactive dashboards. The system incorporates automated/manual registration, classification, notification processes, and a contextual labeling system for edge cases.

Result: A modular, legally grounded registry system was developed to automate entity registration, classification, and notification processes. The system supports context-aware supervision, handles edge cases via contextual labeling, and is adaptable across EU member states with minimal modification.

Conclusion: This thesis contributes a reusable framework that bridges legal interpretation and technical implementation, offering a scalable solution for national and EU-level NIS2 cybersecurity governance. It identifies key limitations and outlines opportunities for future research and development.

Abstract: The NIS2 Directive establishes a common cybersecurity governance model across
the European Union, requiring member states to identify, classify, and
supervise essential and important entities. As part of a broader governance
network, member states are also obligated to notify the European Commission,
the Cooperation Group, and ENISA about their cybersecurity infrastructure
landscape. This thesis presents an analysis of the NIS2 Directive in this
context and translates its provisions into concrete technical requirements.
These requirements inform the design and implementation of a modular, legally
grounded registry system intended to support competent authorities across the
EU in meeting their obligations. Using the Design Science Research methodology,
the thesis transforms complex legal provisions into structured workflows,
deterministic classification algorithms, and interactive dashboards. The
resulting system automates key regulatory processes, including entity
registration, classification, and notification, while enabling context-aware
supervision and reducing administrative burden. It supports both automated and
manual registration methods and introduces a contextual labeling system to
handle edge cases, risk factors, and cross-directive dependencies. Although
developed for the Norwegian regulatory ecosystem, the system is designed for
adaptation by other member states with minimal modification. This thesis
contributes a reusable framework that bridges legal interpretation and
technical implementation, offering a scalable solution for national and
EU-level NIS2 cybersecurity governance. It also identifies key limitations and
outlines opportunities for future research and development.

</details>


### [17] [Formal Verification of Physical Layer Security Protocols for Next-Generation Communication Networks](https://arxiv.org/abs/2508.19430)
*Kangfeng Ye,Roberto Metere,Jim Woodcock,Poonam Yadav*

Main category: cs.CR

TL;DR: This paper proposes an Isabelle-based formal verification framework for security protocols that overcomes limitations of existing tools. It demonstrates robust verification of authenticity under diverse attack scenarios and validates a novel PLS-based Diffie-Hellman protocol, showcasing enhanced security analysis capabilities compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: The study addresses limitations of ProVerif-based verification for security protocols, which restricts deeper understanding of security properties beyond basic verification results. The goal is to develop a more informative and flexible formal verification approach.

Method: The authors re-model the Needham-Schroeder protocol using an Isabelle formalism to enable interactive and automated formal verification. This framework supports both cryptography and PLS and utilizes a new web interface for comprehensive analysis (secrecy and authenticity) under diverse attack scenarios.

Result: The framework successfully reproduces secrecy results from prior work, reveals authenticity preservation across all scenarios (including secrecy-compromised cases), and confirms the security of the PLS-based Diffie-Hellman protocol for session key derivation with authentication.

Conclusion: The paper concludes that the proposed Isabelle-based formalism enhances security protocol verification by demonstrating robustness in verifying authenticity even when secrecy is compromised, and by validating the security of a PLS-based Diffie-Hellman protocol.

Abstract: Formal verification is crucial for ensuring the robustness of security
protocols against adversarial attacks. The Needham-Schroeder protocol, a
foundational authentication mechanism, has been extensively studied, including
its integration with Physical Layer Security (PLS) techniques such as
watermarking and jamming. Recent research has used ProVerif to verify these
mechanisms in terms of secrecy. However, the ProVerif-based approach limits the
ability to improve understanding of security beyond verification results. To
overcome these limitations, we re-model the same protocol using an Isabelle
formalism that generates sound animation, enabling interactive and automated
formal verification of security protocols. Our modelling and verification
framework is generic and highly configurable, supporting both cryptography and
PLS. For the same protocol, we have conducted a comprehensive analysis (secrecy
and authenticity in four different eavesdropper locations under both passive
and active attacks) using our new web interface. Our findings not only
successfully reproduce and reinforce previous results on secrecy but also
reveal an uncommon but expected outcome: authenticity is preserved across all
examined scenarios, even in cases where secrecy is compromised. We have
proposed a PLS-based Diffie-Hellman protocol that integrates watermarking and
jamming, and our analysis shows that it is secure for deriving a session key
with required authentication. These highlight the advantages of our novel
approach, demonstrating its robustness in formally verifying security
properties beyond conventional methods.

</details>


### [18] [CITADEL: Continual Anomaly Detection for Enhanced Learning in IoT Intrusion Detection](https://arxiv.org/abs/2508.19450)
*Elvin Li,Onat Gungor,Zhengli Shang,Tajana Rosing*

Main category: cs.CR

TL;DR: CITADEL achieves state-of-the-art IoT intrusion detection via self-supervised continual learning, combining novel representation learning and memory consolidation to overcome forgetting and adapt to new threats without labeled attack data.


<details>
  <summary>Details</summary>
Motivation: IoT systems face critical security challenges due to their interconnectedness and resource limitations, while existing ML-based IDS struggle with adaptability to emerging threats and catastrophic forgetting during continuous learning.

Method: CITADEL employs a self-supervised continual learning framework with three core components: (1) tabular-to-image data transformation, (2) memory-aware masked autoencoder for representation learning, and (3) attack-agnostic novelty detection, enabling robust knowledge retention and incremental adaptation.

Result: CITADEL outperforms the baseline VLAD method by 72.9% in key detection/retention metrics across multiple intrusion datasets, showing superior performance in both threat detection accuracy and knowledge preservation.

Conclusion: CITADEL provides an effective solution for IoT intrusion detection by addressing adaptability and catastrophic forgetting through self-supervised continual learning, demonstrating superior performance in dynamic environments.

Abstract: The Internet of Things (IoT), with its high degree of interconnectivity and
limited computational resources, is particularly vulnerable to a wide range of
cyber threats. Intrusion detection systems (IDS) have been extensively studied
to enhance IoT security, and machine learning-based IDS (ML-IDS) show
considerable promise for detecting malicious activity. However, their
effectiveness is often constrained by poor adaptability to emerging threats and
the issue of catastrophic forgetting during continuous learning. To address
these challenges, we propose CITADEL, a self-supervised continual learning
framework designed to extract robust representations from benign data while
preserving long-term knowledge through optimized memory consolidation
mechanisms. CITADEL integrates a tabular-to-image transformation module, a
memory-aware masked autoencoder for self-supervised representation learning,
and a novelty detection component capable of identifying anomalies without
dependence on labeled attack data. Our design enables the system to
incrementally adapt to emerging behaviors while retaining its ability to detect
previously observed threats. Experiments on multiple intrusion datasets
demonstrate that CITADEL achieves up to a 72.9% improvement over the VAE-based
lifelong anomaly detector (VLAD) in key detection and retention metrics,
highlighting its effectiveness in dynamic IoT environments.

</details>


### [19] [ReLATE+: Unified Framework for Adversarial Attack Detection, Classification, and Resilient Model Selection in Time-Series Classification](https://arxiv.org/abs/2508.19456)
*Cagla Ipek Kocal,Onat Gungor,Tajana Rosing,Baris Aksanli*

Main category: cs.CR

TL;DR: ReLATE+ reduces computational costs by 77.68% in adversarial time-series classification through adaptive model reuse and attack-aware selection, achieving near-Oracle accuracy.


<details>
  <summary>Details</summary>
Motivation: High computational complexity in real-time time-series classification with deep learning models, exacerbated by adversarial attacks, necessitates methods that ensure robust performance, efficient model selection, and cost-effective adaptation.

Method: ReLATE+ framework detects adversarial attacks, classifies their types, and leverages dataset-level similarity to reuse pre-trained models from a repository, minimizing retraining costs and adaptation time.

Result: Experiments demonstrate 77.68% average computational overhead reduction, maintaining 2.02% deviation from Oracle performance while generalizing robustly across diverse domains with varying data distributions.

Conclusion: ReLATE+ achieves robust performance reduction in computational overhead by 77.68% while maintaining adversarial resilience and efficient model selection without compromising accuracy.

Abstract: Minimizing computational overhead in time-series classification, particularly
in deep learning models, presents a significant challenge due to the high
complexity of model architectures and the large volume of sequential data that
must be processed in real time. This challenge is further compounded by
adversarial attacks, emphasizing the need for resilient methods that ensure
robust performance and efficient model selection. To address this challenge, we
propose ReLATE+, a comprehensive framework that detects and classifies
adversarial attacks, adaptively selects deep learning models based on
dataset-level similarity, and thus substantially reduces retraining costs
relative to conventional methods that do not leverage prior knowledge, while
maintaining strong performance. ReLATE+ first checks whether the incoming data
is adversarial and, if so, classifies the attack type, using this insight to
identify a similar dataset from a repository and enable the reuse of the
best-performing associated model. This approach ensures strong performance
while reducing the need for retraining, and it generalizes well across
different domains with varying data distributions and feature spaces.
Experiments show that ReLATE+ reduces computational overhead by an average of
77.68%, enhancing adversarial resilience and streamlining robust model
selection, all without sacrificing performance, within 2.02% of Oracle.

</details>


### [20] [Addressing Weak Authentication like RFID, NFC in EVs and EVCs using AI-powered Adaptive Authentication](https://arxiv.org/abs/2508.19465)
*Onyinye Okoye*

Main category: cs.CR

TL;DR: This paper addresses cybersecurity flaws in EV charging authentication by proposing an AI-powered adaptive framework using Zero Trust principles. Traditional methods like RFID/NFC are found vulnerable, but AI-driven anomaly detection and contextual analysis emerge as critical for scalable, proactive security in electric mobility.


<details>
  <summary>Details</summary>
Motivation: Traditional authentication methods (RFID, NFC) for EVs/EVCs are vulnerable to attacks like cloning, relay attacks, and signal interception due to static identifiers and weak encryption, creating new cybersecurity challenges in the expanding EV infrastructure.

Method: The study proposes an AI-powered adaptive authentication framework that integrates machine learning, anomaly detection, behavioral analytics, and contextual risk assessment within the principles of Zero Trust Architecture (continuous verification, least privilege access, and secure communication).

Result: The research evaluates current vulnerabilities in authentication protocols and highlights AI-driven solutions for scalable, resilient, proactive defense mechanisms.

Conclusion: Adopting AI-powered adaptive authentication is deemed a strategic imperative to secure the future of electric mobility and strengthen digital trust in EV charging ecosystems.

Abstract: The rapid expansion of the Electric Vehicles (EVs) and Electric Vehicle
Charging Systems (EVCs) has introduced new cybersecurity challenges,
specifically in authentication protocols that protect vehicles, users, and
energy infrastructure. Although widely adopted for convenience, traditional
authentication mechanisms like Radio Frequency Identification (RFID) and Near
Field Communication (NFC) rely on static identifiers and weak encryption,
making them highly vulnerable to attack vectors such as cloning, relay attacks,
and signal interception. This study explores an AI-powered adaptive
authentication framework designed to overcome these shortcomings by integrating
machine learning, anomaly detection, behavioral analytics, and contextual risk
assessment. Grounded in the principles of Zero Trust Architecture, the proposed
framework emphasizes continuous verification, least privilege access, and
secure communication. Through a comprehensive literature review, this research
evaluates current vulnerabilities and highlights AI-driven solutions to provide
a scalable, resilient, and proactive defense. Ultimately, the research findings
conclude that adopting AI-powered adaptive authentication is a strategic
imperative for securing the future of electric mobility and strengthening
digital trust across the ecosystem. Keywords: weak authentication, RFID, NFC,
ML, AI-powered adaptive authentication, relay attacks, cloning, eavesdropping,
MITM attacks, Zero Trust Architecture

</details>


### [21] [SIExVulTS: Sensitive Information Exposure Vulnerability Detection System using Transformer Models and Static Analysis](https://arxiv.org/abs/2508.19472)
*Kyler Katz,Sara Moshtari,Ibrahim Mujhid,Mehdi Mirakhorli,Derek Garcia*

Main category: cs.CR

TL;DR: SIExVulTS combines transformers and static analysis to accurately detect CWE-200 vulnerabilities in Java apps, uncovering unknown security issues while outperforming existing tools in precision and verification.


<details>
  <summary>Details</summary>
Motivation: CWE-200 vulnerabilities remain under-addressed, with existing tools lacking coverage of diverse subcategories and context-aware data-flow analysis.

Method: SIExVulTS employs a three-stage architecture: (1) Attack Surface Detection using sentence embeddings, (2) Exposure Analysis via CodeQL queries aligned with CWE-200, and (3) Flow Verification with GraphCodeBERT. Evaluated on three datasets including real-world CVEs, synthetic examples, and open-source projects.

Result: Attack Surface Detection achieved >93% F1 score, Exposure Analysis 85.71% F1, and Flow Verification increased precision from 22.61% to 87.23%. Identified six previously unknown CVEs in Apache projects.

Conclusion: SIExVulTS is effective and practical for improving software security against sensitive data exposure, addressing limitations of existing tools in detecting and verifying CWE-200 vulnerabilities.

Abstract: Sensitive Information Exposure (SIEx) vulnerabilities (CWE-200) remain a
persistent and under-addressed threat across software systems, often leading to
serious security breaches. Existing detection tools rarely target the diverse
subcategories of CWE-200 or provide context-aware analysis of code-level data
flows.
  Aims: This paper aims to present SIExVulTS, a novel vulnerability detection
system that integrates transformer-based models with static analysis to
identify and verify sensitive information exposure in Java applications.
  Method: SIExVulTS employs a three-stage architecture: (1) an Attack Surface
Detection Engine that uses sentence embeddings to identify sensitive variables,
strings, comments, and sinks; (2) an Exposure Analysis Engine that instantiates
CodeQL queries aligned with the CWE-200 hierarchy; and (3) a Flow Verification
Engine that leverages GraphCodeBERT to semantically validate source-to-sink
flows. We evaluate SIExVulTS using three curated datasets, including real-world
CVEs, a benchmark set of synthetic CWE-200 examples, and labeled flows from 31
open-source projects.
  Results: The Attack Surface Detection Engine achieved an average F1 score
greater than 93\%, the Exposure Analysis Engine achieved an F1 score of
85.71\%, and the Flow Verification Engine increased precision from 22.61\% to
87.23\%. Moreover, SIExVulTS successfully uncovered six previously unknown CVEs
in major Apache projects.
  Conclusions: The results demonstrate that SIExVulTS is effective and
practical for improving software security against sensitive data exposure,
addressing limitations of existing tools in detecting and verifying CWE-200
vulnerabilities.

</details>


### [22] [Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents](https://arxiv.org/abs/2508.19493)
*Zhixin Lin,Jungang Li,Shidong Pan,Yibo Shi,Yue Yao,Dongliang Xu*

Main category: cs.CR

TL;DR: A large benchmark reveals smartphone agents have poor privacy awareness; closed-source models perform better, but overall detection of sensitive data remains suboptimal.


<details>
  <summary>Details</summary>
Motivation: Smartphone agents powered by MLLMs access sensitive user data extensively, but their privacy awareness remains underexplored, necessitating a thorough evaluation of their performance in this area.

Method: The authors created a large-scale benchmark with 7,138 scenarios annotated for privacy context types, sensitivity levels, and locations, then evaluated seven mainstream smartphone agents for their privacy awareness.

Result: Most agents scored below 60% in privacy awareness (RA), with closed-source models (e.g., Gemini 2.0-flash at 67%) outperforming open-source ones. Higher sensitivity scenarios were more recognizable to agents.

Conclusion: The study highlights the inadequate privacy awareness of current smartphone agents, urging a re-evaluation of the utility-privacy tradeoff, and provides a benchmark for future research.

Abstract: Smartphones bring significant convenience to users but also enable devices to
extensively record various types of personal information. Existing smartphone
agents powered by Multimodal Large Language Models (MLLMs) have achieved
remarkable performance in automating different tasks. However, as the cost,
these agents are granted substantial access to sensitive users' personal
information during this operation. To gain a thorough understanding of the
privacy awareness of these agents, we present the first large-scale benchmark
encompassing 7,138 scenarios to the best of our knowledge. In addition, for
privacy context in scenarios, we annotate its type (e.g., Account Credentials),
sensitivity level, and location. We then carefully benchmark seven available
mainstream smartphone agents. Our results demonstrate that almost all
benchmarked agents show unsatisfying privacy awareness (RA), with performance
remaining below 60% even with explicit hints. Overall, closed-source agents
show better privacy ability than open-source ones, and Gemini 2.0-flash
achieves the best, achieving an RA of 67%. We also find that the agents'
privacy detection capability is highly related to scenario sensitivity level,
i.e., the scenario with a higher sensitivity level is typically more
identifiable. We hope the findings enlighten the research community to rethink
the unbalanced utility-privacy tradeoff about smartphone agents. Our code and
benchmark are available at https://zhixin-l.github.io/SAPA-Bench.

</details>


### [23] [Servant, Stalker, Predator: How An Honest, Helpful, And Harmless (3H) Agent Unlocks Adversarial Skills](https://arxiv.org/abs/2508.19500)
*David Noever*

Main category: cs.CR

TL;DR: This paper reveals a new vulnerability class in MCP-based agent systems, where authorized tasks across services can be orchestrated into harmful emergent behaviors. Existing architectures lack cross-domain security to prevent such compositional attacks, and the study proposes an experimental framework to evaluate beyond benchmark tasks.


<details>
  <summary>Details</summary>
Motivation: Current MCP systems assume individual service isolation as secure, but coordinated tasks across multiple domains with legitimate operations can lead to sophisticated attacks. This highlights the need to address cross-domain vulnerabilities that existing frameworks ignore.

Method: The paper conducts systematic analysis using MITRE ATLAS, testing 95 agents with cross-service capabilities like browser automation, finance, location tracking, and code deployment. Red team exercises simulate attack chains, and an experimental framework evaluates unintended optimization beyond MCP benchmarks.

Result: Empirical attack chains achieved data exfiltration, financial manipulation, and infrastructure compromise. The study demonstrates that service isolation assumptions fail as attack surfaces grow exponentially with added capabilities.

Conclusion: MCP architectures require cross-domain security measures to prevent compositional attacks. The proposed experimental framework highlights safety risks from over-optimization and provides actionable directions for securing multi-service agent systems.

Abstract: This paper identifies and analyzes a novel vulnerability class in Model
Context Protocol (MCP) based agent systems. The attack chain describes and
demonstrates how benign, individually authorized tasks can be orchestrated to
produce harmful emergent behaviors. Through systematic analysis using the MITRE
ATLAS framework, we demonstrate how 95 agents tested with access to multiple
services-including browser automation, financial analysis, location tracking,
and code deployment-can chain legitimate operations into sophisticated attack
sequences that extend beyond the security boundaries of any individual service.
These red team exercises survey whether current MCP architectures lack
cross-domain security measures necessary to detect or prevent a large category
of compositional attacks. We present empirical evidence of specific attack
chains that achieve targeted harm through service orchestration, including data
exfiltration, financial manipulation, and infrastructure compromise. These
findings reveal that the fundamental security assumption of service isolation
fails when agents can coordinate actions across multiple domains, creating an
exponential attack surface that grows with each additional capability. This
research provides a barebones experimental framework that evaluate not whether
agents can complete MCP benchmark tasks, but what happens when they complete
them too well and optimize across multiple services in ways that violate human
expectations and safety constraints. We propose three concrete experimental
directions using the existing MCP benchmark suite.

</details>


### [24] [Breaking the Layer Barrier: Remodeling Private Transformer Inference with Hybrid CKKS and MPC](https://arxiv.org/abs/2508.19525)
*Tianshi Xu,Wen-jie Lu,Jiangrui Yu,Chen Yi,Chenqi Lin,Runsheng Wang,Meng Li*

Main category: cs.CR

TL;DR: BLB is a private Transformer inference framework that reduces communication overhead by 21x over BOLT and 2x over Bumblebee through HE-MPC fusion and optimized protocols.


<details>
  <summary>Details</summary>
Motivation: Current methods using HE for linear layers and MPC for non-linear layers face high communication costs due to repeated HE-MPC conversions during inference.

Method: Breaks down model layers into fine-grained operators, fuses adjacent linear operators to minimize conversions, introduces secure CKKS-MPC conversion protocol and GPU-accelerated matrix multiplication protocol via CKKS.

Result: 21x/13x and 2x/1.8x reductions in communication/lantency vs. BOLT and Bumblebee on BERT and GPT2 models with GPU acceleration.

Conclusion: BLB establishes a new benchmark for communication efficiency in private Transformer inference by eliminating unnecessary HE-MPC conversions through structured operator fusion.

Abstract: This paper presents an efficient framework for private Transformer inference
that combines Homomorphic Encryption (HE) and Secure Multi-party Computation
(MPC) to protect data privacy. Existing methods often leverage HE for linear
layers (e.g., matrix multiplications) and MPC for non-linear layers (e.g.,
Softmax activation functions), but the conversion between HE and MPC introduces
significant communication costs. The proposed framework, dubbed BLB, overcomes
this by breaking down layers into fine-grained operators and further fusing
adjacent linear operators, reducing the need for HE/MPC conversions. To manage
the increased ciphertext bit width from the fused linear operators, BLB
proposes the first secure conversion protocol between CKKS and MPC and enables
CKKS-based computation of the fused operators. Additionally, BLB proposes an
efficient matrix multiplication protocol for fused computation in Transformers.
Extensive evaluations on BERT-base, BERT-large, and GPT2-base show that BLB
achieves a $21\times$ reduction in communication overhead compared to BOLT
(S\&P'24) and a $2\times$ reduction compared to Bumblebee (NDSS'25), along with
latency reductions of $13\times$ and $1.8\times$, respectively, when leveraging
GPU acceleration.

</details>


### [25] [Intellectual Property in Graph-Based Machine Learning as a Service: Attacks and Defenses](https://arxiv.org/abs/2508.19641)
*Lincan Li,Bolin Shen,Chenxi Zhao,Yuxiang Sun,Kaixiang Zhao,Shirui Pan,Yushun Dong*

Main category: cs.CR

TL;DR: This work systematically addresses GML model IP protection in cloud-based GMLaaS by creating a threat taxonomy, evaluation framework, benchmark datasets, and an open-source library (PyGIP) to advance secure deployment of graph-structured data models.


<details>
  <summary>Details</summary>
Motivation: As graph-structured data and GML models grow in complexity and value as intellectual property, their deployment via GMLaaS exposes them to severe threats like model stealing and data leakage through API interfaces. This necessitates systematic frameworks and tools to address IP protection challenges in GMLaaS ecosystems.

Method: The paper introduces a tailored taxonomy categorizing threats and defenses at the GML model and graph data levels, proposes a systematic evaluation framework for IP protection methods, curates benchmark datasets across domains, and develops PyGIP—a versatile library for implementing and evaluating attack/defense techniques in GMLaaS scenarios.

Result: The paper delivers a structured threat taxonomy, an evaluation framework for IP protection methods, domain-specific benchmark datasets, and the PyGIP library (https://labrai.github.io/PyGIP) for practical implementation and analysis of GMLaaS security solutions.

Conclusion: This survey provides a comprehensive taxonomy of threats and defenses for GML model and graph data IP protection, establishes an evaluation framework with benchmark datasets, and introduces PyGIP, an open-source library to assess attack/defense methods, offering foundational guidance for securing GMLaaS systems.

Abstract: Graph-structured data, which captures non-Euclidean relationships and
interactions between entities, is growing in scale and complexity. As a result,
training state-of-the-art graph machine learning (GML) models have become
increasingly resource-intensive, turning these models and data into invaluable
Intellectual Property (IP). To address the resource-intensive nature of model
training, graph-based Machine-Learning-as-a-Service (GMLaaS) has emerged as an
efficient solution by leveraging third-party cloud services for model
development and management. However, deploying such models in GMLaaS also
exposes them to potential threats from attackers. Specifically, while the APIs
within a GMLaaS system provide interfaces for users to query the model and
receive outputs, they also allow attackers to exploit and steal model
functionalities or sensitive training data, posing severe threats to the safety
of these GML models and the underlying graph data. To address these challenges,
this survey systematically introduces the first taxonomy of threats and
defenses at the level of both GML model and graph-structured data. Such a
tailored taxonomy facilitates an in-depth understanding of GML IP protection.
Furthermore, we present a systematic evaluation framework to assess the
effectiveness of IP protection methods, introduce a curated set of benchmark
datasets across various domains, and discuss their application scopes and
future challenges. Finally, we establish an open-sourced versatile library
named PyGIP, which evaluates various attack and defense techniques in GMLaaS
scenarios and facilitates the implementation of existing benchmark methods. The
library resource can be accessed at: https://labrai.github.io/PyGIP. We believe
this survey will play a fundamental role in intellectual property protection
for GML and provide practical recipes for the GML community.

</details>


### [26] [Safety Alignment Should Be Made More Than Just A Few Attention Heads](https://arxiv.org/abs/2508.19697)
*Chao Huang,Zefeng Zhang,Juewei Yue,Quangang Li,Chuang Zhang,Tingwen Liu*

Main category: cs.CR

TL;DR: This paper proposes AHD, a training method that distributes safety mechanisms across attention heads in LLMs to enhance robustness against adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Current safety mechanisms in LLMs rely on a limited subset of attention heads, making them vulnerable to adversarial prompts that bypass these concentrated safety components.

Method: The authors introduce RDSHA, a method using the model's refusal direction to identify safety-critical attention heads, and AHD, a training strategy to distribute safety behaviors across numerous attention heads.

Result: AHD successfully distributes safety capabilities across more attention heads, leading to improved robustness against mainstream jailbreak attacks without compromising overall model utility.

Conclusion: The study concludes that distributing safety-related behaviors across multiple attention heads, as achieved by the AHD training strategy, significantly enhances the robustness of LLMs against jailbreak attacks while preserving functional utility.

Abstract: Current safety alignment for large language models(LLMs) continues to present
vulnerabilities, given that adversarial prompting can effectively bypass their
safety measures.Our investigation shows that these safety mechanisms
predominantly depend on a limited subset of attention heads: removing or
ablating these heads can severely compromise model safety. To identify and
evaluate these safety-critical components, we introduce RDSHA, a targeted
ablation method that leverages the model's refusal direction to pinpoint
attention heads mostly responsible for safety behaviors. Further analysis shows
that existing jailbreak attacks exploit this concentration by selectively
bypassing or manipulating these critical attention heads. To address this
issue, we propose AHD, a novel training strategy designed to promote the
distributed encoding of safety-related behaviors across numerous attention
heads. Experimental results demonstrate that AHD successfully distributes
safety-related capabilities across more attention heads. Moreover, evaluations
under several mainstream jailbreak attacks show that models trained with AHD
exhibit considerably stronger safety robustness, while maintaining overall
functional utility.

</details>


### [27] [Addressing Deepfake Issue in Selfie banking through camera based authentication](https://arxiv.org/abs/2508.19714)
*Subhrojyoti Mukherjee,Manoranjan Mohanty*

Main category: cs.CR

TL;DR: The paper investigates applying a camera localization forensic system to detect deepfakes in banking selfies, countering fraud from realistic synthetic images.


<details>
  <summary>Details</summary>
Motivation: The research is driven by the increasing threat of deepfake technology in bypassing biometric security systems, particularly in financial applications like selfie banking, where synthetic images are exploited to commit fraud.

Method: The study utilizes a pre-existing forensic recognition system, previously employed for determining the origin of photos through camera localization, and adapts it for the purpose of detecting deepfake-generated fake identities in online banking scenarios.

Result: The paper's findings indicate that this approach is viable for identifying deepfake images, suggesting that forensic systems can be effectively retrained or reconfigured to detect anomalies in facial recognition data created via synthetic means.

Conclusion: This paper concludes that repurposing an established forensic recognition system, originally designed for camera localization, can serve as an effective method to detect deepfake images in selfie banking, thereby enhancing biometric security against fraud.

Abstract: Fake images in selfie banking are increasingly becoming a threat. Previously,
it was just Photoshop, but now deep learning technologies enable us to create
highly realistic fake identities, which fraudsters exploit to bypass biometric
systems such as facial recognition in online banking. This paper explores the
use of an already established forensic recognition system, previously used for
picture camera localization, in deepfake detection.

</details>


### [28] [The Art of Hide and Seek: Making Pickle-Based Model Supply Chain Poisoning Stealthy Again](https://arxiv.org/abs/2508.19774)
*Tong Liu,Guozhu Meng,Peng Zhou,Zizhuang Deng,Shuaiyin Yao,Kai Chen*

Main category: cs.CR

TL;DR: Researchers uncover critical pickle deserialization vulnerabilities in AI/ML frameworks, exposing 22 loading paths and high-bypass EOP techniques that evade state-of-the-art scanners, earning a $6000 bug bounty.


<details>
  <summary>Details</summary>
Motivation: Existing scanners for pickle model poisoning lack comprehensive surface understanding, allowing attackers to bypass protections. The study aims to reveal these critical vulnerabilities in Python's serialization ecosystem.

Method: The authors systematically analyzed pickle-based model loading paths across frameworks, developed the Exception-Oriented Programming (EOP) bypass technique, and identified exploitable gadgets to evaluate scanner effectiveness.

Result: 22 pickle-based loading paths (19 undetected), 9 EOP instances (7 fully bypass), and 133 exploitable gadgets (89-100% bypass rates) were discovered, showcasing robust bypasses against current scanners.

Conclusion: The study highlights the persistent risks of pickle-based model poisoning, exposing significant gaps in current detection methods and demonstrating practical bypass techniques, leading to a $6000 bug bounty after responsible disclosure.

Abstract: Pickle deserialization vulnerabilities have persisted throughout Python's
history, remaining widely recognized yet unresolved. Due to its ability to
transparently save and restore complex objects into byte streams, many AI/ML
frameworks continue to adopt pickle as the model serialization protocol despite
its inherent risks. As the open-source model ecosystem grows, model-sharing
platforms such as Hugging Face have attracted massive participation,
significantly amplifying the real-world risks of pickle exploitation and
opening new avenues for model supply chain poisoning. Although several
state-of-the-art scanners have been developed to detect poisoned models, their
incomplete understanding of the poisoning surface leaves the detection logic
fragile and allows attackers to bypass them. In this work, we present the first
systematic disclosure of the pickle-based model poisoning surface from both
model loading and risky function perspectives. Our research demonstrates how
pickle-based model poisoning can remain stealthy and highlights critical gaps
in current scanning solutions. On the model loading surface, we identify 22
distinct pickle-based model loading paths across five foundational AI/ML
frameworks, 19 of which are entirely missed by existing scanners. We further
develop a bypass technique named Exception-Oriented Programming (EOP) and
discover 9 EOP instances, 7 of which can bypass all scanners. On the risky
function surface, we discover 133 exploitable gadgets, achieving almost a 100%
bypass rate. Even against the best-performing scanner, these gadgets maintain
an 89% bypass rate. By systematically revealing the pickle-based model
poisoning surface, we achieve practical and robust bypasses against real-world
scanners. We responsibly disclose our findings to corresponding vendors,
receiving acknowledgments and a $6000 bug bounty.

</details>


### [29] [From Research to Reality: Feasibility of Gradient Inversion Attacks in Federated Learning](https://arxiv.org/abs/2508.19819)
*Viktor Valadi,Mattias Åkesson,Johan Östman,Salman Toor,Andreas Hellander*

Main category: cs.CR

TL;DR: This paper maps how neural architecture and training behavior impact gradient inversion risks in federated learning, introduces robust attacks, and highlights that modern architectures are more privacy-preserving by design.


<details>
  <summary>Details</summary>
Motivation: Previous gradient inversion analysis overlooked training-mode behaviors. This work aims to systematically assess how architecture and training dynamics (e.g., dropout, normalization) influence privacy risks in realistic federated learning scenarios.

Method: The study compares inference-mode and training-mode client vulnerabilities, identifies architectural conditions (shallow/wide models, skip connections, pre-activation normalization) for successful attacks, and introduces novel attacks against training-mode models and modified production-grade object detectors.

Result: Key findings include: (1) inference-mode clients enable simpler inversion, (2) training-mode attacks require specific architectural conditions, (3) novel attacks outperform prior methods in realistic settings, and (4) production-grade models show strong inherent robustness unless artificially modified.

Conclusion: The paper provides a comprehensive analysis of privacy vulnerabilities in federated learning through gradient inversion, clarifying architectural and operational factors that impact robustness. It reframes risk assessment for future research and deployment.

Abstract: Gradient inversion attacks have garnered attention for their ability to
compromise privacy in federated learning. However, many studies consider
attacks with the model in inference mode, where training-time behaviors like
dropout are disabled and batch normalization relies on fixed statistics. In
this work, we systematically analyze how architecture and training behavior
affect vulnerability, including the first in-depth study of inference-mode
clients, which we show dramatically simplifies inversion. To assess attack
feasibility under more realistic conditions, we turn to clients operating in
standard training mode. In this setting, we find that successful attacks are
only possible when several architectural conditions are met simultaneously:
models must be shallow and wide, use skip connections, and, critically, employ
pre-activation normalization. We introduce two novel attacks against models in
training-mode with varying attacker knowledge, achieving state-of-the-art
performance under realistic training conditions. We extend these efforts by
presenting the first attack on a production-grade object-detection model. Here,
to enable any visibly identifiable leakage, we revert to the lenient inference
mode setting and make multiple architectural modifications to increase model
vulnerability, with the extent of required changes highlighting the strong
inherent robustness of such architectures. We conclude this work by offering
the first comprehensive mapping of settings, clarifying which combinations of
architectural choices and operational modes meaningfully impact privacy. Our
analysis provides actionable insight into when models are likely vulnerable,
when they appear robust, and where subtle leakage may persist. Together, these
findings reframe how gradient inversion risk should be assessed in future
research and deployment scenarios.

</details>


### [30] [Every Keystroke You Make: A Tech-Law Measurement and Analysis of Event Listeners for Wiretapping](https://arxiv.org/abs/2508.19825)
*Shaoor Munir,Nurullah Demir,Qian Li,Konrad Kollnig,Zubair Shafiq*

Main category: cs.CR

TL;DR: This study maps invasive JavaScript-based keystroke interception on websites to U.S. wiretapping laws, finding over 3% of sites illegally transmit intercepted data for marketing, suggesting legal enforcement opportunities under old but relevant laws.


<details>
  <summary>Details</summary>
Motivation: The study addresses the lack of enforcement of privacy laws and explores existing wiretapping laws as a legal avenue for holding web trackers accountable, given their potential to provide private right of action and drive meaningful change in privacy practices.

Method: The researchers instrumented a web browser to crawl the top-million websites, identifying third-party JavaScript event listeners that intercept keystrokes and assess their alignment with U.S. wiretapping laws. They found 38.52% of sites used such listeners, with 3.18% transmitting intercepted data for email marketing.

Result: 38.52% of websites used third-party event listeners to intercept keystrokes, 3.18% transmitted intercepted data to third-party servers, and the data was used for unsolicited email marketing, aligning with wiretapping criteria under U.S. law.

Conclusion: The paper concludes that bridging the tech-law gap for wiretapping laws could enable legal enforcement against invasive web tracking practices, potentially reshaping the web tracking landscape. However, further legal analysis is required to confirm the threshold for illegality.

Abstract: The privacy community has a long track record of investigating emerging types
of web tracking techniques. Recent work has focused on compliance of web
trackers with new privacy laws such as Europe's GDPR and California's CCPA.
Despite the growing body of research documenting widespread lack of compliance
with new privacy laws, there is a lack of robust enforcement. Different from
prior work, we conduct a tech-law analysis to map decades-old U.S. laws about
interception of electronic communications--so-called wiretapping--to web
tracking. Bridging the tech-law gap for older wiretapping laws is important and
timely because, in cases where legal harm to privacy is proven, they can
provide statutory private right of action, are at the forefront of recent
privacy enforcement, and could ultimately lead to a meaningful change in the
web tracking landscape.
  In this paper, we focus on a particularly invasive tracking technique: the
use of JavaScript event listeners by third-party trackers for real-time
keystroke interception on websites. We use an instrumented web browser to crawl
a sample of the top-million websites to investigate the use of event listeners
that aligns with the criteria for wiretapping, according to U.S. wiretapping
law at the federal level and in California. We find evidence that 38.52%
websites installed third-party event listeners to intercept keystrokes, and
that at least 3.18% websites transmitted intercepted information to a
third-party server, which aligns with the criteria for wiretapping. We further
find evidence that the intercepted information such as email addresses typed
into form fields are used for unsolicited email marketing. Beyond our work that
maps the intersection between technical measurement and U.S. wiretapping law,
additional future legal research is required to determine when the wiretapping
observed in our paper passes the threshold for illegality.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [31] [Stack Trace-Based Crash Deduplication with Transformer Adaptation](https://arxiv.org/abs/2508.19449)
*Md Afif Al Mamun,Gias Uddin,Lan Xia,Longyu Zhang*

Main category: cs.SE

TL;DR: dedupT leverages transformer-based language models to analyze stack traces holistically, outperforming existing DL and traditional methods in crash report deduplication with 15% MRR improvements and reducing manual triage effort.


<details>
  <summary>Details</summary>
Motivation: Traditional stack trace deduplication methods fail to capture contextual and structural relationships within crash reports, leading to overwhelmed issue-tracking systems and increased developer workload. This creates a need for more sophisticated NLP-based approaches that model program errors as semantic sequences.

Method: The proposed dedupT approach adapts a pre-trained language model (PLM) to stack traces and uses embeddings from the PLM to train a fully-connected network (FCN) for ranking duplicate crashes. This transforms stack trace modeling from isolated frame analysis to holistic contextual understanding.

Result: dedupT achieves 15% higher Mean Reciprocal Rank (MRR) compared to the best deep learning baselines and 9% improvement over traditional methods on four public datasets, with additional benefits in unique crash detection via ROCAUC metric improvements.

Conclusion: Our work advances the integration of modern natural language processing (NLP) techniques into software engineering, providing an effective solution for stack trace-based crash deduplication.

Abstract: Automated crash reporting systems generate large volumes of duplicate
reports, overwhelming issue-tracking systems and increasing developer workload.
Traditional stack trace-based deduplication methods, relying on string
similarity, rule-based heuristics, or deep learning (DL) models, often fail to
capture the contextual and structural relationships within stack traces. We
propose dedupT, a transformer-based approach that models stack traces
holistically rather than as isolated frames. dedupT first adapts a pretrained
language model (PLM) to stack traces, then uses its embeddings to train a
fully-connected network (FCN) to rank duplicate crashes effectively. Extensive
experiments on real-world datasets show that dedupT outperforms existing DL and
traditional methods (e.g., sequence alignment and information retrieval
techniques) in both duplicate ranking and unique crash detection, significantly
reducing manual triage effort. On four public datasets, dedupT improves Mean
Reciprocal Rank (MRR) often by over 15% compared to the best DL baseline and up
to 9% over traditional methods while achieving higher Receiver Operating
Characteristic Area Under the Curve (ROC-AUC) in detecting unique crash
reports. Our work advances the integration of modern natural language
processing (NLP) techniques into software engineering, providing an effective
solution for stack trace-based crash deduplication.

</details>


### [32] [Functional Consistency of LLM Code Embeddings: A Self-Evolving Data Synthesis Framework for Benchmarking](https://arxiv.org/abs/2508.19558)
*Zhuohao Li,Wenqing Chen,Jianxing Yu,Zhichao Lu*

Main category: cs.SE

TL;DR: Researchers propose a code benchmark synthesis framework (Functionality-Oriented Code Self-Evolution) that improves models' functional understanding of code by generating diverse examples, outperforming traditional syntactic-focused approaches in key tasks.


<details>
  <summary>Details</summary>
Motivation: Existing studies focus on syntactic similarity for code clone detection, neglecting functional understanding. The paper addresses this gap by exploring how well LLM embeddings capture code-level functional semantics.

Method: The paper introduces a data synthesis framework that generates four variations of code snippets across semantic and syntactic categories, creating challenging benchmarks to better reflect functional differences.

Result: Experiments show embedding models trained on evolved datasets significantly outperform baseline methods in three downstream tasks, validating the framework's effectiveness in advancing functional code understanding.

Conclusion: The study demonstrates that the Functionality-Oriented Code Self-Evolution framework enhances the functional understanding of code by generating diverse benchmarks, leading to improved performance in tasks like clone detection, functional consistency identification, and code retrieval.

Abstract: Embedding models have demonstrated strong performance in tasks like
clustering, retrieval, and feature extraction while offering computational
advantages over generative models and cross-encoders. Benchmarks such as MTEB
have shown that text embeddings from large language models (LLMs) capture rich
semantic information, but their ability to reflect code-level functional
semantics remains unclear. Existing studies largely focus on code clone
detection, which emphasizes syntactic similarity and overlooks functional
understanding. In this paper, we focus on the functional consistency of LLM
code embeddings, which determines if two code snippets perform the same
function regardless of syntactic differences. We propose a novel data synthesis
framework called Functionality-Oriented Code Self-Evolution to construct
diverse and challenging benchmarks. Specifically, we define code examples
across four semantic and syntactic categories and find that existing datasets
predominantly capture syntactic properties. Our framework generates four unique
variations from a single code instance, providing a broader spectrum of code
examples that better reflect functional differences. Extensive experiments on
three downstream tasks-code clone detection, code functional consistency
identification, and code retrieval-demonstrate that embedding models
significantly improve their performance when trained on our evolved datasets.
These results highlight the effectiveness and generalization of our data
synthesis framework, advancing the functional understanding of code.

</details>


### [33] [The Influence of Code Comments on the Perceived Helpfulness of Stack Overflow Posts](https://arxiv.org/abs/2508.19610)
*Kathrin Figl,Maria Kirchner,Sebastian Baltes,Michael Felderer*

Main category: cs.SE

TL;DR: Code comments boost perceived helpfulness on Stack Overflow; block comments particularly help novices. AI coding tools could leverage these insights to generate more useful code explanations.


<details>
  <summary>Details</summary>
Motivation: Understanding how code comments affect the perceived helpfulness of answers is critical to improving knowledge sharing on platforms like Stack Overflow. This is especially relevant given the growing use of AI-based coding assistants trained on such content.

Method: An online experiment simulating a Stack Overflow environment with 91 participants was conducted to evaluate the impact of code comments on perceived helpfulness of answers. The effects of block/inline comments, answer position, and answer scores were analyzed.

Result: Block and inline comments were ranked significantly more helpful than uncommented code. Novices found block comments (e.g., full paragraph explanations) more helpful than inline comments. Surface features like answer position/score had minimal impact. These findings suggest comment style impacts perceived quality, with practical implications for AI prompting strategies.

Conclusion: The study concludes that code comments, particularly block comments for novices, significantly enhance the perceived helpfulness of Stack Overflow answers. This has implications for improving both community-driven platforms and AI-based coding assistants.

Abstract: Question-and-answer platforms such as Stack Overflow have become an important
way for software developers to share and retrieve knowledge. However, reusing
poorly understood code can lead to serious problems, such as bugs or security
vulnerabilities. To better understand how code comments affect the perceived
helpfulness of Stack Overflow answers, we conducted an online experiment
simulating a Stack Overflow environment (n=91). The results indicate that both
block and inline comments are perceived as significantly more helpful than
uncommented source code. Moreover, novices rated code snippets with block
comments as more helpful than those with inline comments. Interestingly, other
surface features, such as the position of an answer and its answer score, were
considered less important. The content of Stack Overflow has been a major
source for training large language models. AI-based coding assistants such as
GitHub Copilot, which are based on these models, might change the way Stack
Overflow is used. However, our findings have implications beyond this specific
platform. First, they may help to improve the relevance of community-driven
platforms such as Stack Overflow, which provide human advice and explanations
of code solutions, complementing AI-based support for software developers.
Second, since chat-based AI tools can be prompted to generate code in different
ways, knowing which properties influence perceived helpfulness might lead to
targeted prompting strategies to generate more readable code snippets.

</details>


### [34] [Leveraging LLMs for Automated Translation of Legacy Code: A Case Study on PL/SQL to Java Transformation](https://arxiv.org/abs/2508.19663)
*Lola Solovyeva,Eduardo Carneiro Oliveira,Shiyu Fan,Alper Tuncay,Shamil Gareev,Andrea Capiluppi*

Main category: cs.SE

TL;DR: LLMs with custom prompting successfully translate PL/SQL to Java for legacy system modernization, showing promise despite small data constraints.


<details>
  <summary>Details</summary>
Motivation: The VT legacy system's 2.5 million lines of undocumented PL/SQL code pose significant refactoring challenges, necessitating exploration of LLMs for automated Java translation in the modernized 'VTF3' system.

Method: The research evaluated multiple LLMs using a dataset of 10 PL/SQL-to-Java code pairs and 15 Java classes. A customized prompting strategy integrating chain-of-guidance reasoning and $n$-shot prompting was proposed to enhance translation accuracy.

Result: The methodology achieved syntactically accurate and functionally correct translations, though limited by a small dataset (10 pairs, 15 classes) and restricted test case access for validation.

Conclusion: The study demonstrates that using a customized prompting strategy with LLMs can effectively translate PL/SQL to Java for legacy system modernization, despite limitations in dataset size and test coverage, laying groundwork for scalable automated solutions.

Abstract: The VT legacy system, comprising approximately 2.5 million lines of PL/SQL
code, lacks consistent documentation and automated tests, posing significant
challenges for refactoring and modernisation. This study investigates the
feasibility of leveraging large language models (LLMs) to assist in translating
PL/SQL code into Java for the modernised "VTF3" system. By leveraging a dataset
comprising 10 PL/SQL-to-Java code pairs and 15 Java classes, which collectively
established a domain model for the translated files, multiple LLMs were
evaluated. Furthermore, we propose a customized prompting strategy that
integrates chain-of-guidance reasoning with $n$-shot prompting. Our findings
indicate that this methodology effectively guides LLMs in generating
syntactically accurate translations while also achieving functional
correctness. However, the findings are limited by the small sample size of
available code files and the restricted access to test cases used for
validating the correctness of the generated code. Nevertheless, these findings
lay the groundwork for scalable, automated solutions in modernising large
legacy systems.

</details>


### [35] [Enabling Content Management Systems as an Information Source in Model-driven Projects](https://arxiv.org/abs/2508.19797)
*Joan Giner-Miguelez,Abel Gómez,Jordi Cabot*

Main category: cs.SE

TL;DR: The paper proposes an open-source model-based framework to address challenges in integrating headless CMSs by automatically discovering their information schema and generating a middleware library for platform-agnostic access.


<details>
  <summary>Details</summary>
Motivation: Headless CMSs are increasingly used across information systems, but their domain-specific customization lacks proper tools for discovery and management, resulting in manual, error-prone processes.

Method: Developed a model-based framework that discovers and explicitly represents CMS information schema, then generates middleware libraries for seamless integration with consuming components.

Result: A framework implementation is open-sourced, capable of modeling CMS schemas and producing reusable middleware code for cross-platform application access.

Conclusion: The proposed model-based approach reduces manual integration efforts for headless CMSs and provides a reusable solution scalable to diverse software systems.

Abstract: Content Management Systems (CMSs) are the most popular tool when it comes to
create and publish content across the web. Recently, CMSs have evolved,
becoming \emph{headless}. Content served by a \emph{headless CMS} aims to be
consumed by other applications and services through REST APIs rather than by
human users through a web browser. This evolution has enabled CMSs to become a
notorious source of content to be used in a variety of contexts beyond pure web
navigation. As such, CMS have become an important component of many information
systems. Unfortunately, we still lack the tools to properly discover and manage
the information stored in a CMS, often highly customized to the needs of a
specific domain. Currently, this is mostly a time-consuming and error-prone
manual process.
  In this paper, we propose a model-based framework to facilitate the
integration of headless CMSs in software development processes. Our framework
is able to discover and explicitly represent the information schema behind the
CMS. This facilitates designing the interaction between the CMS model and other
components consuming that information. These interactions are then generated as
part of a middleware library that offers platform-agnostic access to the CMS to
all the client applications. The complete framework is open-source and
available online.

</details>


### [36] [Towards a fundamental theory of modeling discrete systems](https://arxiv.org/abs/2508.19803)
*Peter Fettke,Wolfgang Reisig*

Main category: cs.SE

TL;DR: Presents Heraklit - a new modeling framework addressing digital-era challenges, emphasizing future research on correctness, information theory, and invariant descriptions in modeling.


<details>
  <summary>Details</summary>
Motivation: Modeling is fundamental in science/engineering, but new theoretical foundations are needed to overcome challenges posed by the digital era.

Method: Introduced the Heraklit modeling framework as a novel approach to address digital-age challenges in modeling.

Result: Presented the Heraklit framework as a foundational contribution for next-generation modeling systems.

Conclusion: Future work will focus on modeling correctness, the concept of information, and describing invariance in modeling.

Abstract: Modeling is a central concern in both science and engineering. However, we
need a new fundamental theory to address the challenges of the digital age. In
this paper, we first explain why modeling is fundamental and which challenges
must be addressed in the digital world. As a main contribution, we introduce
the Heraklit modeling framework as a new approach to modeling. We conclude with
some general remarks. Future work will involve the correctness of modeling, the
notion of information, and the description of invariance in modeling.

</details>


### [37] [On the Future of Software Reuse in the Era of AI Native Software Engineering](https://arxiv.org/abs/2508.19834)
*Antero Taivalsaari,Tommi Mikkonen,Cesare Pautasso*

Main category: cs.SE

TL;DR: This paper explores the shift to AI-driven software development and proposes a research agenda to address its challenges.


<details>
  <summary>Details</summary>
Motivation: The rapid adoption of AI-native approaches in software development necessitates an examination of their underlying issues and long-term implications to avoid problematic practices like cargo cult development.

Method: The paper employs a critical discussion approach to analyze the paradigm shift in software development, identifies emerging challenges, and formulates a research agenda.

Result: The paper outlines a research agenda that addresses key issues in AI-assisted generative software reuse, including validation of AI-generated code, ethical considerations, and developer-AI collaboration.

Conclusion: The paper emphasizes the need for a structured research agenda to address the challenges and implications of AI-assisted generative software reuse, ensuring that the shift to 'AI Native' development is both effective and principled.

Abstract: Software development is currently under a paradigm shift in which artificial
intelligence and generative software reuse are taking the center stage in
software creation. Earlier opportunistic software reuse practices and organic
software development methods are rapidly being replaced by "AI Native"
approaches in which developers place their trust on code that has been
generated by artificial intelligence. This is leading to a new form of software
reuse that is conceptually not all that different from cargo cult development.
In this paper we discuss the implications of AI-assisted generative software
reuse, bring forth relevant questions, and define a research agenda for
tackling the central issues associated with this emerging approach.

</details>


### [38] [Generative AI for Testing of Autonomous Driving Systems: A Survey](https://arxiv.org/abs/2508.19882)
*Qunying Song,He Ye,Mark Harman,Federica Sarro*

Main category: cs.SE

TL;DR: This survey examines generative AI's role in ADS testing through a systematic review of 91 studies, categorizing applications, evaluating tools, and identifying limitations while guiding future research.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving systems require extensive safety and functionality testing, but achieving effective and efficient testing remains a critical challenge, motivating the exploration of generative AI applications.

Method: The study systematically analyzed 91 papers, categorizing them into six application areas for scenario-based ADS testing. It reviewed datasets, simulators, metrics, and benchmarks while identifying 27 limitations.

Result: Generated six major application categories for generative AI in ADS testing, compiled a comprehensive resource list (datasets, simulators, etc.), and identified 27 limitations across reviewed studies.

Conclusion: This survey highlights the role of generative AI in ADS testing, identifies challenges, and outlines future research directions for this evolving field.

Abstract: Autonomous driving systems (ADS) have been an active area of research, with
the potential to deliver significant benefits to society. However, before
large-scale deployment on public roads, extensive testing is necessary to
validate their functionality and safety under diverse driving conditions.
Therefore, different testing approaches are required, and achieving effective
and efficient testing of ADS remains an open challenge. Recently, generative AI
has emerged as a powerful tool across many domains, and it is increasingly
being applied to ADS testing due to its ability to interpret context, reason
about complex tasks, and generate diverse outputs. To gain a deeper
understanding of its role in ADS testing, we systematically analyzed 91
relevant studies and synthesized their findings into six major application
categories, primarily centered on scenario-based testing of ADS. We also
reviewed their effectiveness and compiled a wide range of datasets, simulators,
ADS, metrics, and benchmarks used for evaluation, while identifying 27
limitations. This survey provides an overview and practical insights into the
use of generative AI for testing ADS, highlights existing challenges, and
outlines directions for future research in this rapidly evolving field.

</details>


### [39] [Smart Contract Intent Detection with Pre-trained Programming Language Model](https://arxiv.org/abs/2508.20086)
*Youwei Huang,Jianwen Li,Sen Fang,Yao Li,Peng Yang,Bin Hu,Tao Zhang*

Main category: cs.SE

TL;DR: SmartIntentNN2 enhances smart contract security with a BERT-enhanced architecture, achieving state-of-the-art intent detection performance (F1=0.927) by leveraging pre-trained language models and retaining effective components from its predecessor.


<details>
  <summary>Details</summary>
Motivation: Malicious intent in smart contract development can cause substantial economic losses, necessitating advanced detection methods to enhance security in blockchain ecosystems.

Method: The model integrates a BERT-based pre-trained language model (trained on 16,000 contracts using Masked Language Modeling), retains a BiLSTM-based multi-label classification network, and improves upon the original SmartIntentNN's structure with a K-means clustering-based intent highlighting mechanism.

Result: SmartIntentNN2 achieves an improved F1 score of 0.927 in multi-label classification across ten intent categories, demonstrating significant performance gains over its predecessor (SmartIntentNN with F1=0.8633).

Conclusion: SmartIntentNN2 establishes itself as the state-of-the-art model for smart contract intent detection due to its enhanced F1 score of 0.927 and integration of a BERT-based pre-trained language model.

Abstract: Malicious intent in smart contract development can lead to substantial
economic losses. SmartIntentNN is a deep learning model specifically designed
to identify unsafe intents in smart contracts. This model integrates the
Universal Sentence Encoder, a K-means clustering-based intent highlighting
mechanism, and a Bidirectional Long Short-Term Memory network for multi-label
classification, achieving an F1 of 0.8633 in distinguishing ten different
intent categories. In this study, we present an upgraded version of this model,
SmartIntentNN2 (Smart Contract Intent Neural Network V2). A significant
enhancement in V2 is the incorporation of a BERT-based pre-trained language
model, which has been trained on a dataset of 16,000 real smart contracts using
a Masked Language Modeling objective. SmartIntentNN2 retains the BiLSTM-based
multi-label classification network. With an improved F1 of 0.927, V2
demonstrates enhanced performance compared to its predecessor, establishing
itself as the state-of-the-art model for smart contract intent detection.

</details>
