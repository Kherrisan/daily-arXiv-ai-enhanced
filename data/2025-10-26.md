<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 30]
- [cs.SE](#cs.SE) [Total: 16]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [CourtGuard: A Local, Multiagent Prompt Injection Classifier](https://arxiv.org/abs/2510.19844)
*Isaac Wu,Michael Maslowski*

Main category: cs.CR

TL;DR: The paper discusses prompt injection attacks that can exploit large language models (LLMs) to act harmfully and presents CourtGuard, a multiagent system designed to reduce false positives in detecting these attacks by simulating a court process.


<details>
  <summary>Details</summary>
Motivation: The increasing use of large language models (LLMs) in sensitive applications has raised concerns about prompt injection attacks, which can lead to harmful behaviors and data breaches. The need for more accurate and reliable detection methods that minimize false positives is highlighted to ensure safe deployment of LLMs.

Method: The authors propose CourtGuard, a locally-operable multiagent system for detecting prompt injection attacks in LLMs. It uses three models, each with distinct roles: a defense attorney, a prosecution attorney, and a judge, to simulate a court process. The defense attorney argues that the prompt is benign, the prosecution attorney argues that it’s a harmful prompt injection, and the judge determines the final classification based on their arguments.

Result: The experiments showed that CourtGuard has a lower false positive rate compared to the Direct Detector, an LLM as-a-judge approach. However, CourtGuard is generally less effective as a prompt injection detector compared to some alternatives, indicating a trade-off between accuracy and reducing false positives in prompt filtering systems.

Conclusion: While CourtGuard does not outperform existing prompt injection detection methods in terms of overall accuracy, its ability to reduce false positives underlines the value of multiagent systems in evaluating potential injection attacks from both adversarial and benign perspectives. The study also encourages future research into multiagent-based defense mechanisms to enhance the robustness of LLMs against prompt injection attacks.

Abstract: As large language models (LLMs) become integrated into various sensitive
applications, prompt injection, the use of prompting to induce harmful
behaviors from LLMs, poses an ever increasing risk. Prompt injection attacks
can cause LLMs to leak sensitive data, spread misinformation, and exhibit
harmful behaviors. To defend against these attacks, we propose CourtGuard, a
locally-runnable, multiagent prompt injection classifier. In it, prompts are
evaluated in a court-like multiagent LLM system, where a "defense attorney"
model argues the prompt is benign, a "prosecution attorney" model argues the
prompt is a prompt injection, and a "judge" model gives the final
classification. CourtGuard has a lower false positive rate than the Direct
Detector, an LLM as-a-judge. However, CourtGuard is generally a worse prompt
injection detector. Nevertheless, this lower false positive rate highlights the
importance of considering both adversarial and benign scenarios for the
classification of a prompt. Additionally, the relative performance of
CourtGuard in comparison to other prompt injection classifiers advances the use
of multiagent systems as a defense against prompt injection attacks. The
implementations of CourtGuard and the Direct Detector with full prompts for
Gemma-3-12b-it, Llama-3.3-8B, and Phi-4-mini-instruct are available at
https://github.com/isaacwu2000/CourtGuard.

</details>


### [2] [Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability](https://arxiv.org/abs/2510.19851)
*Artur Zolkowski,Wen Xing,David Lindner,Florian Tramèr,Erik Jenner*

Main category: cs.CR

TL;DR: This paper evaluates the effectiveness of chain-of-thought (CoT) monitoring under obfuscation pressure, finding that while CoT works well normally, some models can evade detection when pressured, and internal CoT is less obfuscated than external CoT.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from concerns that misaligned models might display deceptive behavior, compromising output trustworthiness. The paper seeks to determine if models can obfuscate their CoT to pursue hidden adversarial objectives while avoiding detection.

Method: The authors develop a composable and quantifiable taxonomy of prompts to elicit CoT obfuscation. They evaluate both internal and external CoT using toy tasks and more realistic environments in the SHADE-Arena framework.

Result: The results show (i) CoT monitoring is accurate and efficient without obfuscation pressure (ii) some models can complete adversarial tasks while evading detection under strong obfuscation pressure (iii) internal CoT is less obfuscated than external CoT under prompt pressure.

Conclusion: The study concludes that CoT is a valuable oversight tool in benign scenarios, but for robust deployment, it is necessary to conduct model-specific stress-testing of monitorability.

Abstract: Recent findings suggest that misaligned models may exhibit deceptive
behavior, raising concerns about output trustworthiness. Chain-of-thought (CoT)
is a promising tool for alignment monitoring: when models articulate their
reasoning faithfully, monitors can detect and mitigate harmful behaviors before
undesirable outcomes occur. However, a key uncertainty is: Can models obfuscate
their CoT in order to pursue hidden adversarial objectives while evading
detection? To answer this question and thus stress-test CoT monitorability, we
develop a composable and quantifiable taxonomy of prompts to elicit CoT
obfuscation. We evaluate both internal CoT (reasoning traces) and external CoT
(prompted reasoning in outputs) using toy tasks and more realistic environments
in SHADE-Arena. We show that: (i) CoT monitoring performs accurately and
efficiently without obfuscation pressure. (ii) Under strong obfuscation
pressure, some models successfully complete adversarial tasks while evading
detection. (iii) Models do not obfuscate their internal CoT as much as their
external CoT (under prompt pressure). These results suggest that while CoT
provides valuable oversight in benign settings, robust deployment requires
model-specific stress-testing of monitorability.

</details>


### [3] [Model Context Contracts - MCP-Enabled Framework to Integrate LLMs With Blockchain Smart Contracts](https://arxiv.org/abs/2510.19856)
*Eranga Bandara,Sachin Shetty,Ravi Mukkamala,Ross Gore,Peter Foytik,Safdar H. Bouk,Abdul Rahman,Xueping Liang,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.CR

TL;DR: This paper proposes 'MCC: Model Context Contracts,' a framework that enables LLMs to interact directly with blockchain smart contracts using an MCP-like protocol, facilitating natural language-based transactions and interactions. An end-to-end prototype is implemented on the Rahasak blockchain with a fine-tuned Llama-4 LLM.


<details>
  <summary>Details</summary>
Motivation: While blockchain adoption has grown, and generative AI/LLMs have advanced, there is no standardized way to integrate LLMs and blockchain. The Model Context Protocol (MCP) has standardised AI-systems and data source communication but not for blockchain. This gap needs a solution for seamless, context-aware interactions.

Method: The authors propose MCC, a framework allowing LLMs to interface with blockchain smart contracts through an MCP-like protocol. The MCC framework is built on the Rahasak blockchain with a fine-tuned Llama-4 LLM. The LLM was fine-tuned on a custom dataset containing user inputs and corresponding MCP server functions to improve accuracy in interpreting natural language and mapping to MCP functions.

Result: MCC enables LLMs to invoke blockchain smart contracts using natural language queries, functioning as intelligent agents. An end-to-end prototype is implemented on the Rahasak blockchain with the fine-tuned Llama-4 LLM, demonstrating the framework's effectiveness in enabling context-aware and dynamic interactions between users and blockchain networks.

Conclusion: The paper concludes that MCC is the first approach to integrating LLMs with blockchain using the Model Context Protocol concept. The proposed framework facilitates seamless, natural language-driven interactions with blockchain systems, validated through a working prototype. This integration enhances user accessibility and enables more dynamic and intelligent blockchain operations.

Abstract: In recent years, blockchain has experienced widespread adoption across
various industries, becoming integral to numerous enterprise applications.
Concurrently, the rise of generative AI and LLMs has transformed human-computer
interactions, offering advanced capabilities in understanding and generating
human-like text. The introduction of the MCP has further enhanced AI
integration by standardizing communication between AI systems and external data
sources. Despite these advancements, there is still no standardized method for
seamlessly integrating LLM applications and blockchain. To address this
concern, we propose "MCC: Model Context Contracts" a novel framework that
enables LLMs to interact directly with blockchain smart contracts through
MCP-like protocol. This integration allows AI agents to invoke blockchain smart
contracts, facilitating more dynamic and context-aware interactions between
users and blockchain networks. Essentially, it empowers users to interact with
blockchain systems and perform transactions using queries in natural language.
Within this proposed architecture, blockchain smart contracts can function as
intelligent agents capable of recognizing user input in natural language and
executing the corresponding transactions. To ensure that the LLM accurately
interprets natural language inputs and maps them to the appropriate MCP
functions, the LLM was fine-tuned using a custom dataset comprising user inputs
paired with their corresponding MCP server functions. This fine-tuning process
significantly improved the platform's performance and accuracy. To validate the
effectiveness of MCC, we have developed an end-to-end prototype implemented on
the Rahasak blockchain with the fine-tuned Llama-4 LLM. To the best of our
knowledge, this research represents the first approach to using the concept of
Model Context Protocol to integrate LLMs with blockchain.

</details>


### [4] [Cyberattack Detection in Critical Infrastructure and Supply Chains](https://arxiv.org/abs/2510.19859)
*Smita Khapre*

Main category: cs.CR

TL;DR: A new DNN-based IDS with Data Augmentation is proposed to detect zero-day cyberattacks in critical infrastructure and supply chains by addressing the data imbalance issue common in network flow datasets.


<details>
  <summary>Details</summary>
Motivation: Intrusion Detection Systems (IDS) struggle with zero-day attacks because they rely on known signatures. Additionally, network flow datasets used for IDS training are highly imbalanced, with attacks (minority class) being extremely rare (0.000004%) compared to benign activities (majority class over 80%). This imbalance hinders the system's ability to detect new, unseen cyberattacks accurately.

Method: The paper integrates a Dense Neural Network (DNN) with Data Augmentation techniques, specifically Synthetic Minority Oversampling Technique (SMOTE), to improve IDS performance.

Result: The proposed method achieves higher accuracies with balanced test data, but performance noticeably drops when evaluated on the original imbalanced test data, indicating overfitting. A comparison with state-of-the-art techniques also reveals poor classification of classes in the original imbalanced dataset.

Conclusion: The current Data Augmentation method (SMOTE) is insufficient for handling highly imbalanced network flow datasets. New approaches to data augmentation are needed to improve IDS performance in these scenarios.

Abstract: Cyberattack detection in Critical Infrastructure and Supply Chains has become
challenging in Industry 4.0. Intrusion Detection Systems (IDS) are deployed to
counter the cyberattacks. However, an IDS effectively detects attacks based on
the known signatures and patterns, Zero-day attacks go undetected. To overcome
this drawback in IDS, the integration of a Dense Neural Network (DNN) with Data
Augmentation is proposed. It makes IDS intelligent and enables it to self-learn
with high accuracy when a novel attack is encountered. The network flow
captures datasets are highly imbalanced same as the real network itself. The
Data Augmentation plays a crucial role in balancing the data. The balancing of
data is challenging as the minority class is as low as 0.000004\% of the
dataset, and the abundant class is higher than 80\% of the dataset. Synthetic
Minority Oversampling Technique is used for balancing the data. However, higher
accuracies are achieved with balanced test data, lower accuracies are
noticeable with the original imbalanced test data suggesting overfitting. A
comparison with state-of-the-art research using Synthetic Minority Oversampling
Technique with Edited Nearest Neighbor shows the classification of classes
remains poor for the original dataset. This suggests highly imbalanced datasets
of network flow require a different method of data augmentation.

</details>


### [5] [Policy-Governed RAG - Research Design Study](https://arxiv.org/abs/2510.19877)
*Jean-Marie Le Ray*

Main category: cs.CR

TL;DR: This paper proposes a three-part RAG architecture (Contracts/Control, Manifests/Trails, Receipts/Verification) for audit-ready compliance in regulated domains. It combines policy governance with cryptographic evidence to ensure verifiable outputs, targeting high-stakes sectors where audit trails are mandatory. Results include performance targets and optional pre-registered pilots, with plans for transparent reporting of negative outcomes if NO-GO gates are unmet.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to address compliance challenges in regulated domains (pharma, medical devices, finance, legal, public sector) where error costs exceed thousands of euros. Existing RAG systems lack audit-ready safeguards, necessitating a framework that ensures verifiable policy adherence and meets mandatory audit requirements under regulations like the EU AI Act.

Method: The method is organized as a triptych: (I) Contracts/Control (SHRDLU-like) enforces policy adherence through governance rules; (II) Manifests/Trails (Memex-like) cryptographically anchors cited sources for verifiable provenance; and (III) Receipts/Verification (Xanadu-like) generates portable COSE/JOSE proofs for auditors. Outputs are gated ex-ante and bound to verifiable evidence, complementing existing RAG/guardrails systems.

Result: Key results include unvalidated targets of a >=20% reduction in confident errors, p95 latency <=900ms, and <=2.2x serving costs. The design is evaluated with a pre-registered (optional) pilot using NO-GO gates. Future work includes pre-commitment to publish negative results when any NO-GO gate is not met, ensuring transparency in validation.

Conclusion: The paper concludes that the proposed policy-governed RAG architecture enhances RAG/guardrails systems by introducing auditability, replayability, and receipt-backed compliance verification. This design addresses the need for verifiable adherence to policies in high-stakes, regulated domains where audit trails are mandatory, ensuring outputs meet legal and internal standards with cryptographically verifiable evidence.

Abstract: A policy-governed RAG architecture is specified for audit-ready generation in
regulated workflows, organized as a triptych: (I) Contracts/Control
(SHRDLU-like), which governs output adherence to legal and internal policies;
(II) Manifests/Trails (Memex-like), which cryptographically anchors all cited
source evidence to ensure verifiable provenance; and (III)
Receipts/Verification (Xanadu-like), which provides the final, portable proof
of compliance for auditors (portable COSE/JOSE) (see Section 4 and Appendix A).
Rather than explaining model internals, outputs are gated ex-ante and bound to
cryptographically verifiable evidence for each material answer. Unvalidated
targets are stated (>=20% relative reduction in confident errors; p95 latency
<= 900 ms; <= 2.2x serve cost) together with a pre-registered (optional) pilot
using NO-GO gates. The design complements existing RAG/guardrails by making
policy checks auditable, replayable, and receipt-backed. Target domains include
back-office compliance in pharma, medical devices, finance, legal, and the
public sector where error costs may exceed thousands of euros and audit trails
are mandatory under regulations such as the EU AI Act. Future evaluations may
pre-commit to publishing negative results when any example NO-GO gate is not
met.

</details>


### [6] [A Proactive Insider Threat Management Framework Using Explainable Machine Learning](https://arxiv.org/abs/2510.19883)
*Selma Shikonde,Mike Wa Nkongolo*

Main category: cs.CR

TL;DR: This study proposes the IT-XML framework combining CRISP-DM and HMM for proactive insider threat management, achieving 91.7% accuracy and identifying audit log access limits as critical control.


<details>
  <summary>Details</summary>
Motivation: Insider threats are a major cybersecurity challenge, especially in organisations with developing security infrastructure and expertise. Existing insider threat management lacks proactivity and explainability, necessitating an integrated framework for improved decision-making.

Method: The study proposes the IT-XML framework integrating CRISP-DM methodology with HMM for pattern recognition. It uses a quantitative approach with an online questionnaire across three large organisations to assess employee knowledge and practices. HMM drives security maturity classification, while SHAP and LIME are used for model explainability.

Result: The IT-XML framework classified all organisations at the developing security maturity level with 97-98% confidence and achieved 91.7% classification accuracy. Audit log access limits were identified as the most critical control. Random Forest analysis identified vendor breach notifications (0.081) and regular audit log reviews (0.052) as key determinants of resilience.

Conclusion: The study demonstrates the IT-XML framework's potential to enhance proactive insider threat management through improved model transparency and interpretability. The framework provides a structured approach for assessing organisation maturity and identifying key controls for threat mitigation.

Abstract: Over the years, the technological landscape has evolved, reshaping the
security posture of organisations and increasing their exposure to
cybersecurity threats, many originating from within. Insider threats remain a
major challenge, particularly in sectors where cybersecurity infrastructure,
expertise, and regulations are still developing. This study proposes the
Insider Threat Explainable Machine Learning (IT-XML) framework, which
integrates the Cross-Industry Standard Process for Data Mining (CRISP-DM) with
Hidden Markov Models (HMM) to enhance proactive insider threat management and
decision-making. A quantitative approach is adopted using an online
questionnaire to assess employees' knowledge of insider threat patterns, access
control, privacy practices, and existing policies across three large
data-sensitive organisations. The IT-XML framework provides assessment
capabilities through survey-based data, HMM-driven pattern recognition for
security maturity classification, and evidence-based recommendations for
proactive threat mitigation. The framework classified all organisations at the
developing security maturity level with 97-98% confidence and achieved a
classification accuracy of 91.7%, identifying audit log access limits as the
most critical control. Random Forest analysis highlighted vendor breach
notifications (0.081) and regular audit log reviews (0.052) as key determinants
of resilience. Explainability methods such as SHAP and LIME improved model
transparency and interpretability, demonstrating the framework's potential to
strengthen insider threat management practices.

</details>


### [7] [Analysis and Comparison of Known and Randomly Generated S-boxes for Block Ciphers](https://arxiv.org/abs/2510.19885)
*James Kim*

Main category: cs.CR

TL;DR: The paper compares mathematically constructed S-boxes with random


<details>
  <summary>Details</summary>
Motivation: Understanding the advantages

Method: Comparing the performance

Result: Mathematically constructed S-boxes show better cryptographic properties

Conclusion: Mathematical constructions are superior for generating

Abstract: Mathematically constructed S-boxes arise from algebraic structures and finite
field theory to ensure strong, provable cryptographic properties. These
mathematically grounded constructions allow for generation of thousands of
S-Boxes with high nonlinearity, APN properties, and balanced avalanche
characteristics, unlike fully random methods, which lack such theoretical
guarantees in exchange for low complexity and more varied results. In this
work, we compare mathematically constructed constructions with randomly
generated ones to evaluate the relative weakness of the latter. We also
establish an average measure of performance for randomly generated
permutations, as well as random with forced cycle constraints, and compare them
to well-established designs in a simple SPN setting.

</details>


### [8] [Deep Sequence-to-Sequence Models for GNSS Spoofing Detection](https://arxiv.org/abs/2510.19890)
*Jan Zelinka,Oliver Kost,Marek Hrúz*

Main category: cs.CR

TL;DR: The paper introduces a data generation framework for simulating spoofing attacks and proposes deep learning models for online spoofing detection, with Transformers achieving 0.16% error rate.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of detecting spoofing attacks by creating a scalable simulation framework and developing effective deep learning models for real-time detection.

Method: A data generation framework simulating spoofing attacks globally is used, and deep learning models (LSTM and Transformer) are trained on the generated dataset for online detection.

Result: Deep learning models effectively distinguish spoofed from genuine signals, with Transformer architectures achieving up to 0.16% error rate in experiments.

Conclusion: The study concludes that deep learning models, particularly Transformers with early fusion, provide robust and accurate online spoofing attack detection capabilities.

Abstract: We present a data generation framework designed to simulate spoofing attacks
and randomly place attack scenarios worldwide. We apply deep neural
network-based models for spoofing detection, utilizing Long Short-Term Memory
networks and Transformer-inspired architectures. These models are specifically
designed for online detection and are trained using the generated dataset. Our
results demonstrate that deep learning models can accurately distinguish
spoofed signals from genuine ones, achieving high detection performance. The
best results are achieved by Transformer-inspired architectures with early
fusion of the inputs resulting in an error rate of 0.16%.

</details>


### [9] [Designing a Secure and Resilient Distributed Smartphone Participant Data Collection System](https://arxiv.org/abs/2510.19938)
*Foad Namjoo,Neng Wan,Devan Mallory,Yuyi Chang,Nithin Sugavanam,Long Yin Lee,Ning Xiong,Emre Ertin,Jeff M. Phillips*

Main category: cs.CR

TL;DR: MotionPI is a smartphone-based system for secure, low-effort health data collection that integrates passive sensors, EMA surveys, and cloud storage with encryption under real-world constraints.


<details>
  <summary>Details</summary>
Motivation: Health studies require continuous, secure data from mobile devices despite challenges like limited battery life, intermittent connectivity, and user supervision.

Method: MotionPI combines passive data (GPS, wristband motion), EMA surveys (random/activity-triggered), secure local/cloud storage with encryption, and BLE integration with wristbands for motion summaries and triggers.

Result: The system demonstrates secure, scalable mobile data collection under real-life constraints, enabling practical cyber-physical health studies with minimal user effort.

Conclusion: MotionPI provides a robust solution for real-world health data collection by addressing technical and user-centric challenges through integrated sensor-cloud systems and adaptive survey mechanisms.

Abstract: Real-world health studies require continuous and secure data collection from
mobile and wearable devices. We introduce MotionPI, a smartphone-based system
designed to collect behavioral and health data through sensors and surveys with
minimal interaction from participants. The system integrates passive data
collection (such as GPS and wristband motion data) with Ecological Momentary
Assessment (EMA) surveys, which can be triggered randomly or based on physical
activity. MotionPI is designed to work under real-life constraints, including
limited battery life, weak or intermittent cellular connection, and minimal
user supervision. It stores data both locally and on a secure cloud server,
with encrypted transmission and storage. It integrates through Bluetooth Low
Energy (BLE) into wristband devices that store raw data and communicate motion
summaries and trigger events. MotionPI demonstrates a practical solution for
secure and scalable mobile data collection in cyber-physical health studies.

</details>


### [10] [Q-RAN: Quantum-Resilient O-RAN Architecture](https://arxiv.org/abs/2510.19968)
*Vipin Rathi,Lakshya Chopra,Madhav Agarwal,Nitin Rajput,Kriish Sharma,Sushant Mundepi,Shivam Gangwar,Rudraksh Rawal,Jishan*

Main category: cs.CR

TL;DR: The paper introduces Q-RAN, a quantum-resistant security framework for O-RAN networks using NIST-standardized Post-Quantum Cryptography (PQC) to address the growing threat of quantum computing.


<details>
  <summary>Details</summary>
Motivation: The telecommunications industry is undergoing an architectural shift toward O-RAN, which introduces a larger attack surface. Additionally, quantum computing poses a significant threat by breaking current public key cryptography through the HNDL attack strategy. There is a need for a comprehensive quantum-resistant security solution to protect O-RAN ecosystems.

Method: The authors propose the Q-RAN framework which employs NIST-standardized PQC schemes, specifically ML-KEM and ML-DSA. They integrate Quantum Random Number Generators (QRNG) for cryptographic entropy and deploy PQ-IPsec, PQ-DTLS, and PQ-mTLS protocols across all O-RAN interfaces. The framework is supported by a centralized PQ-CA within the SMO architecture.

Result: The paper presents a complete toolkit and roadmap for quantum-resistant security in O-RAN environments, leveraging standardized post-quantum cryptographic protocols and quantum-derived entropy for robust protection.

Conclusion: Q-RAN provides a critical defense against quantum threats in the context of O-RAN networks. By implementing NIST-standardized PQC and quantum-enhanced cryptographic protocols, it offers a practical and comprehensive approach to securing future telecommunications infrastructure against potential quantum attacks.

Abstract: The telecommunications industry faces a dual transformation: the
architectural shift toward Open Radio Access Networks (O-RAN) and the emerging
threat from quantum computing. O-RAN disaggregated, multi-vendor architecture
creates a larger attack surface vulnerable to crypt-analytically relevant
quantum computers(CRQCs) that will break current public key cryptography. The
Harvest Now, Decrypt Later (HNDL) attack strategy makes this threat immediate,
as adversaries can intercept encrypted data today for future decryption. This
paper presents Q-RAN, a comprehensive quantum-resistant security framework for
O-RAN networks using NIST-standardized Post-Quantum Cryptography (PQC). We
detail the implementation of ML-KEM (FIPS 203) and ML-DSA (FIPS 204),
integrated with Quantum Random Number Generators (QRNG) for cryptographic
entropy. The solution deploys PQ-IPsec, PQ-DTLS, and PQ-mTLS protocols across
all O-RAN interfaces, anchored by a centralized Post-Quantum Certificate
Authority (PQ-CA) within the SMO framework. This work provides a complete
roadmap for securing disaggregated O-RAN ecosystems against quantum
adversaries.

</details>


### [11] [SecureInfer: Heterogeneous TEE-GPU Architecture for Privacy-Critical Tensors for Large Language Model Deployment](https://arxiv.org/abs/2510.19979)
*Tushar Nayan,Ziqi Zhang,Ruimin Sun*

Main category: cs.CR

TL;DR: too long; didn't read summary of this paper


<details>
  <summary>Details</summary>
Motivation: describe the motivation in this paper

Method: method of this paper

Result: result of this paper

Conclusion: conclusion of this paper

Abstract: With the increasing deployment of Large Language Models (LLMs) on mobile and
edge platforms, securing them against model extraction attacks has become a
pressing concern. However, protecting model privacy without sacrificing the
performance benefits of untrusted AI accelerators, such as GPUs, presents a
challenging trade-off. In this paper, we initiate the study of high-performance
execution on LLMs and present SecureInfer, a hybrid framework that leverages a
heterogeneous Trusted Execution Environments (TEEs)-GPU architecture to isolate
privacy-critical components while offloading compute-intensive operations to
untrusted accelerators. Building upon an outsourcing scheme, SecureInfer adopts
an information-theoretic and threat-informed partitioning strategy:
security-sensitive components, including non-linear layers, projection of
attention head, FNN transformations, and LoRA adapters, are executed inside an
SGX enclave, while other linear operations (matrix multiplication) are
performed on the GPU after encryption and are securely restored within the
enclave. We implement a prototype of SecureInfer using the LLaMA-2 model and
evaluate it across performance and security metrics. Our results show that
SecureInfer offers strong security guarantees with reasonable performance,
offering a practical solution for secure on-device model inference.

</details>


### [12] [QORE : Quantum Secure 5G/B5G Core](https://arxiv.org/abs/2510.19982)
*Vipin Rathi,Lakshya Chopra,Rudraksh Rawal,Nitin Rajput,Shiva Valia,Madhav Aggarwal,Aditya Gairola*

Main category: cs.CR

TL;DR: This paper proposes QORE, a quantum-secure 5G/B5G framework using lattice-based PQC (ML-KEM/ML-DSA) with hybrid configurations. It addresses imminent quantum threats to 5G cryptography, achieves minimal performance overhead, and aligns with 3GPP/NIST standards, providing a viable transition roadmap.


<details>
  <summary>Details</summary>
Motivation: The urgency stems from Shors algorithm rendering 5G's RSA/ECC/DH cryptography vulnerable to quantum attacks, necessitating a secure transition to Post-Quantum Cryptography (PQC) for long-term network confidentiality and integrity.

Method: The framework leverages NIST-standardized lattice-based ML-KEM and ML-DSA algorithms across the 5G SBA, employs a Hybrid PQC configuration for interoperability during migration, and proposes a phased roadmap aligned with 3GPP and NIST initiatives.

Result: Experimental validation demonstrates ML-KEM achieves quantum security with acceptable performance for carrier-grade 5G systems, while the proposed hybrid approach maintains backward compatibility and aligns with ongoing 3GPP/SA3/SA5 and NIST PQC standardization efforts.

Conclusion: QORE provides a practical, quantum-secure pathway for 5G/B5G transitions, ensuring future-proof security with minimal performance overhead and alignment with global standardization bodies.

Abstract: Quantum computing is reshaping the security landscape of modern
telecommunications. The cryptographic foundations that secure todays 5G
systems, including RSA, Elliptic Curve Cryptography (ECC), and Diffie-Hellman
(DH), are all susceptible to attacks enabled by Shors algorithm. Protecting 5G
networks against future quantum adversaries has therefore become an urgent
engineering and research priority. In this paper we introduce QORE, a
quantum-secure 5G and Beyond 5G (B5G) Core framework that provides a clear
pathway for transitioning both the 5G Core Network Functions and User Equipment
(UE) to Post-Quantum Cryptography (PQC). The framework uses the
NIST-standardized lattice-based algorithms Module-Lattice Key Encapsulation
Mechanism (ML-KEM) and Module-Lattice Digital Signature Algorithm (ML-DSA) and
applies them across the 5G Service-Based Architecture (SBA). A Hybrid PQC
(HPQC) configuration is also proposed, combining classical and quantum-safe
primitives to maintain interoperability during migration. Experimental
validation shows that ML-KEM achieves quantum security with minor performance
overhead, meeting the low-latency and high-throughput requirements of
carrier-grade 5G systems. The proposed roadmap aligns with ongoing 3GPP SA3 and
SA5 study activities on the security and management of post-quantum networks as
well as with NIST PQC standardization efforts, providing practical guidance for
mitigating quantum-era risks while safeguarding long-term confidentiality and
integrity of network data.

</details>


### [13] [zk-Agreements: A Privacy-Preserving Way to Establish Deterministic Trust in Confidential Agreements](https://arxiv.org/abs/2510.20007)
*To-Wen Liu,Matthew Green*

Main category: cs.CR

TL;DR: zk-agreements is a protocol enabling private, automated legal contracts using cryptographic enforcement mechanisms. It combines zero-knowledge proofs, secure computation, and smart contracts to balance confidentiality and computational enforceability in blockchain systems.


<details>
  <summary>Details</summary>
Motivation: Traditional paper contracts hinder digital commerce automation due to ambiguity and manual processing, while blockchain smart contracts expose sensitive terms publicly. This creates a tension between transparency for enforceability and privacy needs for enterprise adoption.

Method: The protocol integrates three core components: 1. Zero-knowledge proofs to conceal agreement terms, 2. Secure two-party computation for private compliance verification, 3. Blockchain smart contracts to ensure deterministic enforcement.

Result: Achieved the first protocol that simultaneously preserves complete confidentiality of agreement terms while maintaining automatic enforcement capabilities through cryptographic binding and verifiable compliance.

Conclusion: zk-agreements resolves the fundamental tradeoff between transparency and privacy in digital contracts, enabling legally binding, confidential automation of complex commercial agreements in blockchain ecosystems.

Abstract: Digital transactions currently exceed trillions of dollars annually, yet
traditional paper-based agreements remain a bottleneck for automation,
enforceability, and dispute resolution. Natural language contracts introduce
ambiguity, require manual processing, and lack computational verifiability, all
of which hinder efficient digital commerce. Computable legal contracts,
expressed in machine-readable formats, offer a potential solution by enabling
automated execution and verification. Blockchain-based smart contracts further
strengthen enforceability and accelerate dispute resolution; however, current
implementations risk exposing sensitive agreement terms on public ledgers,
raising serious privacy and competitive intelligence concerns that limit
enterprise adoption.
  We introduce zk-agreements, a protocol designed to transition from
paper-based trust to cryptographic trust while preserving confidentiality. Our
design combines zero-knowledge proofs to protect private agreement terms,
secure two-party computation to enable private compliance evaluation, and smart
contracts to guarantee automated enforcement. Together, these components
achieve both privacy preservation and computational enforceability, resolving
the fundamental tension between transparency and confidentiality in
blockchain-based agreements.

</details>


### [14] [Ultra-Fast Wireless Power Hacking](https://arxiv.org/abs/2510.20056)
*Hui Wang,Hans D. Schotten,Stefan M. Goetz*

Main category: cs.CR

TL;DR: Researchers developed a rapid wireless power theft method for EV charging systems, stealing 76% of energy in three cycles without relying on circuit knowledge, exposing flaws in frequency-based security.


<details>
  <summary>Details</summary>
Motivation: Current wireless charging systems risk power theft due to accessibility of magnetic fields, with prior interception methods being slow and dependent on circuit parameters. Rapid transmitter frequency changes and capacitance drift render these methods inadequate, necessitating a faster, adaptive attack strategy.

Method: The authors improved an interceptor by synchronizing receiver current with the phase of the magnetic sensor voltage, enabling rapid (within three cycles) and parameter-free energy theft. They validated its effectiveness via MATLAB/Simulink simulations, finite-element analysis, and experimental testing.

Result: The enhanced interceptor achieved 76% power theft efficiency compared to a fully resonant receiver under identical conditions, proving its robustness against frequency changes and demonstrating the vulnerability of simple frequency-based encryption.

Conclusion: The study demonstrates that frequency-based power encryption in wireless EV charging systems is ineffective against advanced interception methods, as the developed attack can steal over 76% of the power within three signal cycles, bypassing traditional defenses.

Abstract: The rapid growth of electric vehicles (EVs) has driven the development of
roadway wireless charging technology, effectively extending EV driving range.
However, wireless charging introduces significant cybersecurity challenges. Any
receiver within the magnetic field can potentially extract energy, and previous
research demonstrated that a hacker could detect the operating frequency and
steal substantial power. However, our approach required time to track new
frequencies or precise adjustments of inductance and capacitance, which would
be less effective against potential rapid transmitter frequency changes or
capacitance drift. As a solution, we enhanced the interceptor and enabled it to
intrude as well as steal energy within just three cycles of the high-frequency
signal. Moreover, it can work without any circuit parameters or look-up tables.
The key innovation is synchronizing the receiver current with the phase of the
magnetic sensor voltage. Through MATLAB / Simulink simulations, finite-element
analysis, and experimental validation, we demonstrated that our improved method
can steal over 76% of the power received by a fully resonant receiver under
identical conditions. This attack demonstrates that simple frequency-changing
power encryption offers limited protection against such threats.

</details>


### [15] [Who Coordinates U.S. Cyber Defense? A Co-Authorship Network Analysis of Joint Cybersecurity Advisories (2024--2025)](https://arxiv.org/abs/2510.20080)
*M. Abdullah Canbaz,Hakan Otal,Tugce Unlu,Nour Alhussein,Brian Nussbaum*

Main category: cs.CR

TL;DR: The paper analyzes the collaboration structure in U.S. and allied cybersecurity


<details>
  <summary>Details</summary>
Motivation: Cyber threats require joint responses, but the organizational dynamics of multi-agency cybersecurity collaboration are poorly understood

Method: Constructed a co-authorship network from nine Joint Cybersecurity Advisories (CSAs) issued between November 2024 and August 2025, mapping 41 agencies and 442 coauthoring ties

Result: Identifies a tightly knit U.S. triad (CISA, FBI, NSA) with dense connections to Five Eyes and select European allies using degree centrality and betweenness

Conclusion: Provides a replicable dataset and network analysis of CSAs, offering new insights into collaborative cybersecurity organization and strategic influence centers

Abstract: Cyber threats increasingly demand joint responses, yet the organizational
dynamics behind multi-agency cybersecurity collaboration remain poorly
understood. Understanding who leads, who bridges, and how agencies coordinate
is critical for strengthening both U.S. homeland security and allied defense
efforts. In this study, we construct a co-authorship network from nine Joint
Cybersecurity Advisories (CSAs) issued between November 2024 and August 2025.
We map 41 agencies and 442 co-authoring ties to analyze the structure of
collaboration. We find a tightly knit U.S. triad -- CISA, FBI, and NSA --
densely connected with Five Eyes and select European allies. Degree centrality
identifies CISA and FBI as coordination hubs, while betweenness highlights NSA,
the UK's NCSC, and Australia's ASD-ACSC as key bridges linking otherwise
fragmented clusters. By releasing the first replicable dataset and network
analysis of CSAs, we provide new empirical evidence on how collaborative
cybersecurity signals are organized and where strategic influence is
concentrated.

</details>


### [16] [SAID: Empowering Large Language Models with Self-Activating Internal Defense](https://arxiv.org/abs/2510.20129)
*Yulong Chen,Yadong Liu,Jiawen Zhang,Mu Li,Chao Huang,Jie Wen*

Main category: cs.CR

TL;DR: This paper proposes SAID, a training-free defense method for Large Language Models (LLMs) that activates internal safety mechanisms to counter jailbreak attacks without external interventions, maintaining model utility while reducing harm.


<details>
  <summary>Details</summary>
Motivation: Current jailbreak defense strategies rely on computationally expensive external corrections (e.g., input/output filtering) that compromise model performance and scalability. The paper aims to develop a more robust defense by leveraging LLMs’ inherent reasoning capabilities.

Method: SAID employs a three-stage pipeline: (1)**model-native intent distillation** to extract malicious intent, (2)**optimal safety prefix probing** to trigger latent safety awareness, and (3)**conservative aggregation** to ensure safe outputs. All stages operate internally without model modification or retraining.

Result: SAID outperforms state-of-the-art defenses on five LLMs across six jailbreak attacks, reducing harmful outputs by a significant margin, while maintaining benign task performance and incurring minimal computational overhead.

Conclusion: Activating intrinsic safety capabilities within LLMs offers a more effective and scalable solution for alignment and safety than external fixes, providing a framework for reliable AI deployment.

Abstract: Large Language Models (LLMs), despite advances in safety alignment, remain
vulnerable to jailbreak attacks designed to circumvent protective mechanisms.
Prevailing defense strategies rely on external interventions, such as input
filtering or output modification, which often lack generalizability and
compromise model utility while incurring significant computational overhead. In
this work, we introduce a new, training-free defense paradigm, Self-Activating
Internal Defense (SAID), which reframes the defense task from external
correction to internal capability activation. SAID uniquely leverages the LLM's
own reasoning abilities to proactively identify and neutralize malicious intent
through a three-stage pipeline: model-native intent distillation to extract
core semantics, optimal safety prefix probing to activate latent safety
awareness, and a conservative aggregation strategy to ensure robust
decision-making. Extensive experiments on five open-source LLMs against six
advanced jailbreak attacks demonstrate that SAID substantially outperforms
state-of-the-art defenses in reducing harmful outputs. Crucially, it achieves
this while preserving model performance on benign tasks and incurring minimal
computational overhead. Our work establishes that activating the intrinsic
safety mechanisms of LLMs is a more robust and scalable path toward building
safer and more reliable aligned AI systems.

</details>


### [17] [Learning to Triage Taint Flows Reported by Dynamic Program Analysis in Node.js Packages](https://arxiv.org/abs/2510.20739)
*Ronghao Ni,Aidan Z. H. Yang,Min-Chien Hsu,Nuno Sabino,Limin Jia,Ruben Martins,Darion Cassel,Kevin Cheang*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Program analysis tools often produce large volumes of candidate vulnerability
reports that require costly manual review, creating a practical challenge: how
can security analysts prioritize the reports most likely to be true
vulnerabilities?
  This paper investigates whether machine learning can be applied to
prioritizing vulnerabilities reported by program analysis tools. We focus on
Node.js packages and collect a benchmark of 1,883 Node.js packages, each
containing one reported ACE or ACI vulnerability. We evaluate a variety of
machine learning approaches, including classical models, graph neural networks
(GNNs), large language models (LLMs), and hybrid models that combine GNN and
LLMs, trained on data based on a dynamic program analysis tool's output. The
top LLM achieves $F_{1} {=} 0.915$, while the best GNN and classical ML models
reaching $F_{1} {=} 0.904$. At a less than 7% false-negative rate, the leading
model eliminates 66.9% of benign packages from manual review, taking around 60
ms per package. If the best model is tuned to operate at a precision level of
0.8 (i.e., allowing 20% false positives amongst all warnings), our approach can
detect 99.2% of exploitable taint flows while missing only 0.8%, demonstrating
strong potential for real-world vulnerability triage.

</details>


### [18] [Separating Pseudorandom Generators from Logarithmic Pseudorandom States](https://arxiv.org/abs/2510.20131)
*Mohammed Barhoush*

Main category: cs.CR

TL;DR: This paper provides a quantum black-box separation between pseudorandom generators (PRGs) and pseudorandom quantum states (PRSs) of both logarithmic and linear size regimes. The work resolves an open problem by constructing a quantum oracle with inverse access where black-box constructions of PRGs from PRSs are not possible, leading to separations of PRGs from digital signatures and quantum public-key encryption.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address a fundamental question in quantum cryptography: whether pseudorandom generators (PRGs) are black-box separable from logarithmic-size pseudorandom quantum states (PRSs), a question left unanswered because previous separations were only established for linear-size PRSs.

Method: The authors construct a quantum oracle with inverse access and establish a quantum black-box separation between PRGs (which are quantum-evaluable) and both logarithmic and linear size pseudorandom quantum states (PRSs). Through this construction, it's shown that no black-box method can build a PRG from a (logarithmic or linear) PRS relative to this oracle.

Result: The study confirms a quantum black-box separation between PRGs and PRSs, regardless of size, and implies that PRGs cannot be constructed from logarithmic or linear PRSs through black-box methods. This leads to separations from advanced crypto primitives derived from logarithmic PRSs, like digital signatures and quantum public-key encryption.

Conclusion: The results indicate that if quantum-evaluable PRGs can be realized, so can advanced cryptographic primitives such as digital signatures and quantum public-key encryption, as long as inverse access is allowed. This deepens our understanding of the limits of black-box constructions in quantum cryptography and highlights the importance of PRSs as alternatively weaker assumptions in the design of quantum cryptographic systems.

Abstract: Pseudorandom generators (PRGs) are a foundational primitive in classical
cryptography, underpinning a wide range of constructions. In the quantum
setting, pseudorandom quantum states (PRSs) were proposed as a potentially
weaker assumption that might serve as a substitute for PRGs in cryptographic
applications. Two primary size regimes of PRSs have been studied:
logarithmic-size and linear-size. Interestingly, logarithmic PRSs have led to
powerful cryptographic applications, such as digital signatures and quantum
public-key encryption, that have not been realized from their linear
counterparts. However, PRGs have only been black-box separated from linear
PRSs, leaving open the fundamental question of whether PRGs are also separated
from logarithmic PRSs.
  In this work, we resolve this open problem. We establish a quantum black-box
separation between (quantum-evaluable) PRGs and PRSs of either size regime.
Specifically, we construct a unitary quantum oracle with inverse access
relative to which no black-box construction of PRG from (logarithmic or linear)
PRS exists. As a direct corollary, we obtain separations between PRGs and
several primitives implied by logarithmic PRSs, including digital signatures
and quantum public-key encryption.

</details>


### [19] [Beyond Text: Multimodal Jailbreaking of Vision-Language and Audio Models through Perceptually Simple Transformations](https://arxiv.org/abs/2510.20223)
*Divyanshu Kumar,Shreyas Jena,Nitin Aravind Birur,Tanay Baswa,Sahil Agarwal,Prashanth Harshangi*

Main category: cs.CR

TL;DR: MLLMs have critical cross-modal safety gaps; adversarial attacks with simple modifications achieve >75% success despite perfect text safety, requiring semantic-level AI safety paradigm shifts.


<details>
  <summary>Details</summary>
Motivation: Multimodal LLMs remain vulnerable to adversarial attacks exploiting cross-modal processing weaknesses, with simple perceptual transformations bypassing state-of-the-art safety filters despite near-perfect text-only safety performance.

Method: Systematic evaluation of 1,900 adversarial prompts across three high-risk categories (harmful content, CBRN, CSEM) using attack techniques like FigStep-Pro (visual keyword decomposition), Intelligent Masking, and audio perturbations (Wave-Echo, Wave-Pitch, Wave-Speed) on seven frontier models.

Result: Severe vulnerabilities exposed: Models with 0% text-only attack success rate achieved >75% attack success rate with perceptual modifications, with FigStep-Pro reaching 89% ASR on Llama-4 variants. Audio attacks achieved 25% ASR for technical queries via basic modality transfer.

Conclusion: Current multimodal safety filters fails to generalize across cross-modal attacks, revealing a critical gap between text-centric alignment and multimodal threats. Robust multimodal AI safety requires a paradigm shift toward broader semantic-level reasoning.

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress,
yet remain critically vulnerable to adversarial attacks that exploit weaknesses
in cross-modal processing. We present a systematic study of multimodal
jailbreaks targeting both vision-language and audio-language models, showing
that even simple perceptual transformations can reliably bypass
state-of-the-art safety filters. Our evaluation spans 1,900 adversarial prompts
across three high-risk safety categories harmful content, CBRN (Chemical,
Biological, Radiological, Nuclear), and CSEM (Child Sexual Exploitation
Material) tested against seven frontier models. We explore the effectiveness of
attack techniques on MLLMs, including FigStep-Pro (visual keyword
decomposition), Intelligent Masking (semantic obfuscation), and audio
perturbations (Wave-Echo, Wave-Pitch, Wave-Speed). The results reveal severe
vulnerabilities: models with almost perfect text-only safety (0\% ASR) suffer
>75\% attack success under perceptually modified inputs, with FigStep-Pro
achieving up to 89\% ASR in Llama-4 variants. Audio-based attacks further
uncover provider-specific weaknesses, with even basic modality transfer
yielding 25\% ASR for technical queries. These findings expose a critical gap
between text-centric alignment and multimodal threats, demonstrating that
current safeguards fail to generalize across cross-modal attacks. The
accessibility of these attacks, which require minimal technical expertise,
suggests that robust multimodal AI safety will require a paradigm shift toward
broader semantic-level reasoning to mitigate possible risks.

</details>


### [20] [HHEML: Hybrid Homomorphic Encryption for Privacy-Preserving Machine Learning on Edge](https://arxiv.org/abs/2510.20243)
*Yu Hin Chan,Hao Yang,Shiyu Shen,Xingyu Fan,Shengzhe Lyu,Patrick S. Y. Hung,Ray C. C. Cheung*

Main category: cs.CR

TL;DR: This paper introduces the first hardware-accelerated hybrid homomorphic encryption (HHE) framework for privacy-preserving machine learning (PPML), achieving 50x faster encryption and 2x throughput improvements on edge devices, while enabling scalable secure ML systems through hardware-software co-design.


<details>
  <summary>Details</summary>
Motivation: Fully homomorphic encryption (FHE) enables secure ML inference but introduces impractical overheads for edge devices. Hybrid HHE reduces client-side costs with symmetric encryption but lacks hardware acceleration and efficient server-side optimizations.

Method: The authors propose a hardware-accelerated HHE architecture with a lightweight FHE-optimized symmetric cipher implemented as a dedicated accelerator. They integrate this with microarchitectural optimizations to improve performance/energy efficiency and embed it into an end-to-end PPML pipeline.

Result: Experiments on PYNQ-Z2 with MNIST show 50x lower client-side encryption latency and 2x higher hardware throughput compared to existing FPGA-based HHE accelerators, validating the feasibility of hardware-accelerated HHE for resource-constrained deployments.

Conclusion: The paper demonstrates that hardware-accelerated hybrid homomorphic encryption (HHE) is a viable solution for edge-deployed privacy-preserving machine learning (PPML), achieving significant reductions in latency and power consumption while establishing a scalable hardware-software co-design framework.

Abstract: Privacy-preserving machine learning (PPML) is an emerging topic to handle
secure machine learning inference over sensitive data in untrusted
environments. Fully homomorphic encryption (FHE) enables computation directly
on encrypted data on the server side, making it a promising approach for PPML.
However, it introduces significant communication and computation overhead on
the client side, making it impractical for edge devices. Hybrid homomorphic
encryption (HHE) addresses this limitation by combining symmetric encryption
(SE) with FHE to reduce the computational cost on the client side, and
combining with an FHE-friendly SE can also lessen the processing overhead on
the server side, making it a more balanced and efficient alternative. Our work
proposes a hardware-accelerated HHE architecture built around a lightweight
symmetric cipher optimized for FHE compatibility and implemented as a dedicated
hardware accelerator. To the best of our knowledge, this is the first design to
integrate an end-to-end HHE framework with hardware acceleration. Beyond this,
we also present several microarchitectural optimizations to achieve higher
performance and energy efficiency. The proposed work is integrated into a full
PPML pipeline, enabling secure inference with significantly lower latency and
power consumption than software implementations. Our contributions validate the
feasibility of low-power, hardware- accelerated HHE for edge deployment and
provide a hardware- software co-design methodology for building scalable,
secure machine learning systems in resource-constrained environments.
Experiments on a PYNQ-Z2 platform with the MNIST dataset show over a 50x
reduction in client-side encryption latency and nearly a 2x gain in hardware
throughput compared to existing FPGA-based HHE accelerators.

</details>


### [21] [Privacy Protection of Automotive Location Data Based on Format-Preserving Encryption of Geographical Coordinates](https://arxiv.org/abs/2510.20300)
*Haojie Ji,Long Jin,Haowen Li,Chongshi Xin,Te Hu*

Main category: cs.CR

TL;DR: This paper presents a high-precision privacy protection mechanism using FPE for automotive location data to mitigate security risks while maintaining data accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the increasing privacy risks associated with sharing automotive location data in functions like route navigation, driving monitoring, and vehicle scheduling. These risks include user behavior recognition, sensitive location inference, and trajectory reconstruction attacks.

Method: The paper introduces a key mapping mechanism based on format-preserving encryption (FPE) of geographical coordinates. This approach is designed to reduce the accuracy loss of location data during repeated encryption and decryption processes.

Result: The experimental results show an average relative distance retention rate of 0.0844, a 98.9% reduction in hotspots in critical areas post-encryption, and 100% restoration accuracy of the decrypted coordinate data.

Conclusion: The study concludes that the proposed FPE-based method effectively protects automotive location privacy without compromising data accuracy, offering an efficient solution for secure data sharing in autonomous driving environments.

Abstract: There are increasing risks of privacy disclosure when sharing the automotive
location data in particular functions such as route navigation, driving
monitoring and vehicle scheduling. These risks could lead to the attacks
including user behavior recognition, sensitive location inference and
trajectory reconstruction. In order to mitigate the data security risk caused
by the automotive location sharing, this paper proposes a high-precision
privacy protection mechanism based on format-preserving encryption (FPE) of
geographical coordinates. The automotive coordinate data key mapping mechanism
is designed to reduce to the accuracy loss of the geographical location data
caused by the repeated encryption and decryption. The experimental results
demonstrate that the average relative distance retention rate (RDR) reached
0.0844, and the number of hotspots in the critical area decreased by 98.9%
after encryption. To evaluate the accuracy loss of the proposed encryption
algorithm on automotive geographical location data, this paper presents the
experimental analysis of decryption accuracy, and the result indicates that the
decrypted coordinate data achieves a restoration accuracy of 100%. This work
presents a high-precision privacy protection method for automotive location
data, thereby providing an efficient data security solution for the sensitive
data sharing in autonomous driving.

</details>


### [22] [Enhancing Security in Deep Reinforcement Learning: A Comprehensive Survey on Adversarial Attacks and Defenses](https://arxiv.org/abs/2510.20314)
*Wu Yichao,Wang Yirui,Ding Panpan,Wang Hailong,Zhu Bingqian,Liu Chun*

Main category: cs.CR

TL;DR: This paper addresses DRL security challenges, proposes an attack classification framework, reviews attack/defense methods, and outlines future research directions for robustness in adversarial environments.


<details>
  <summary>Details</summary>
Motivation: DRL's security and robustness are critical in safety-sensitive applications (autonomous driving, healthcare) where adversarial attacks can cause severe performance degradation or dangerous decisions.

Method: The authors propose an adversarial attack classification framework based on perturbation type and attack target, review mainstream adversarial attack methods (e.g., perturbation in state/action space, reward/model functions), and systematically summarize robustness training strategies like adversarial training, competitive training, and defense distillation.

Result: The paper categorizes attacks, evaluates defense techniques (adversarial training, robust learning, etc.), and discusses their strengths/limitations in enhancing DRL robustness.

Conclusion: The paper emphasizes the need for future research in improving generalization, reducing computational complexity, and enhancing scalability and explainability of DRL in adversarial environments, providing directions for researchers.

Abstract: With the wide application of deep reinforcement learning (DRL) techniques in
complex fields such as autonomous driving, intelligent manufacturing, and smart
healthcare, how to improve its security and robustness in dynamic and
changeable environments has become a core issue in current research. Especially
in the face of adversarial attacks, DRL may suffer serious performance
degradation or even make potentially dangerous decisions, so it is crucial to
ensure their stability in security-sensitive scenarios. In this paper, we first
introduce the basic framework of DRL and analyze the main security challenges
faced in complex and changing environments. In addition, this paper proposes an
adversarial attack classification framework based on perturbation type and
attack target and reviews the mainstream adversarial attack methods against DRL
in detail, including various attack methods such as perturbation state space,
action space, reward function and model space. To effectively counter the
attacks, this paper systematically summarizes various current robustness
training strategies, including adversarial training, competitive training,
robust learning, adversarial detection, defense distillation and other related
defense techniques, we also discuss the advantages and shortcomings of these
methods in improving the robustness of DRL. Finally, this paper looks into the
future research direction of DRL in adversarial environments, emphasizing the
research needs in terms of improving generalization, reducing computational
complexity, and enhancing scalability and explainability, aiming to provide
valuable references and directions for researchers.

</details>


### [23] [GhostEI-Bench: Do Mobile Agents Resilience to Environmental Injection in Dynamic On-Device Environments?](https://arxiv.org/abs/2510.20333)
*Chiyu Chen,Xinhao Song,Yunkai Chai,Yang Yao,Haodong Zhao,Lijun Li,Jie Li,Yan Teng,Gongshen Liu,Yingchun Wang*

Main category: cs.CR

TL;DR: The paper introduces GhostEI-Bench as the first benchmark for evaluating mobile agents against environmental injection attacks in dynamic GUI settings, revealing their vulnerabilities and proposing a judge-LLM protocol for failure analysis.


<details>
  <summary>Details</summary>
Motivation: There is a significant gap in understanding and defending against attacks that bypass textual safeguards by manipulating the visual aspects of on-device environments, such as GUIs, which can lead to serious risks like privacy leaks or device compromise.

Method: The authors developed GhostEI-Bench, a benchmark implemented in executable environments, particularly Android emulators. It introduces adversarial events into real application flows and uses a judge-LLM protocol to analyze failures by integrating action trajectories with screenshots.

Result: Experiments show that state-of-the-art VLM agents are highly vulnerable to environmental injection attacks, systematically failing in perception and reasoning tasks when faced with deceptive UI elements.

Conclusion: GhostEI-Bench provides a method to assess and mitigate this growing threat to embodied agents, offering a platform for fine-grained evaluation and fostering the development of more secure models against environmental injection.

Abstract: Vision-Language Models (VLMs) are increasingly deployed as autonomous agents
to navigate mobile graphical user interfaces (GUIs). Operating in dynamic
on-device ecosystems, which include notifications, pop-ups, and inter-app
interactions, exposes them to a unique and underexplored threat vector:
environmental injection. Unlike prompt-based attacks that manipulate textual
instructions, environmental injection corrupts an agent's visual perception by
inserting adversarial UI elements (for example, deceptive overlays or spoofed
notifications) directly into the GUI. This bypasses textual safeguards and can
derail execution, causing privacy leakage, financial loss, or irreversible
device compromise. To systematically evaluate this threat, we introduce
GhostEI-Bench, the first benchmark for assessing mobile agents under
environmental injection attacks within dynamic, executable environments. Moving
beyond static image-based assessments, GhostEI-Bench injects adversarial events
into realistic application workflows inside fully operational Android emulators
and evaluates performance across critical risk scenarios. We further propose a
judge-LLM protocol that conducts fine-grained failure analysis by reviewing the
agent's action trajectory alongside the corresponding screenshot sequence,
pinpointing failure in perception, recognition, or reasoning. Comprehensive
experiments on state-of-the-art agents reveal pronounced vulnerability to
deceptive environmental cues: current models systematically fail to perceive
and reason about manipulated UIs. GhostEI-Bench provides a framework for
quantifying and mitigating this emerging threat, paving the way toward more
robust and secure embodied agents.

</details>


### [24] [NeuPerm: Disrupting Malware Hidden in Neural Network Parameters by Leveraging Permutation Symmetry](https://arxiv.org/abs/2510.20367)
*Daniel Gilkarov,Ran Dubin*

Main category: cs.CR

TL;DR: NeuPerm is a low-cost, high-effectiveness defense against malicious model sharing by exploiting neural network permutation symmetry, working on LLMs and outperforming complex baselines.


<details>
  <summary>Details</summary>
Motivation: Pretrained model sharing enables cost-effective deep learning but exposes users to malware hidden in parameters. Existing solutions like quantization are complex and impractical for modern models like LLMs.

Method: Leverages neural network permutation symmetry to disrupt malware embedded in model parameters by permuting weights during training or inference. It introduces minimal overhead and requires no model architecture modifications.

Result: Empirical results show NeuPerm successfully disrupts state-of-the-art attacks with no performance degradation. It achieves this for both standard models and LLMs, where prior methods failed. Disrupts malware execution directly and as software dependencies.

Conclusion: NeuPerm effectively disrupts malware in shared models without affecting performance, offers a simpler alternative to complex defense methods, and demonstrates efficacy on LLMs, with source code publicly available.

Abstract: Pretrained deep learning model sharing holds tremendous value for researchers
and enterprises alike. It allows them to apply deep learning by fine-tuning
models at a fraction of the cost of training a brand-new model. However, model
sharing exposes end-users to cyber threats that leverage the models for
malicious purposes. Attackers can use model sharing by hiding self-executing
malware inside neural network parameters and then distributing them for
unsuspecting users to unknowingly directly execute them, or indirectly as a
dependency in another software. In this work, we propose NeuPerm, a simple yet
effec- tive way of disrupting such malware by leveraging the theoretical
property of neural network permutation symmetry. Our method has little to no
effect on model performance at all, and we empirically show it successfully
disrupts state-of-the-art attacks that were only previously addressed using
quantization, a highly complex process. NeuPerm is shown to work on LLMs, a
feat that no other previous similar works have achieved. The source code is
available at https://github.com/danigil/NeuPerm.git.

</details>


### [25] [MAC Aggregation over Lossy Channels in DTLS 1.3](https://arxiv.org/abs/2510.20419)
*Eric Wagner,David Heye,Jan Bauer,Klaus Wehrle,Martin Serror*

Main category: cs.CR

TL;DR: This paper examines the real-world performance of MAC aggregation in lossy wireless networks by implementing a MAC aggregation extension for DTLS 1.3 and testing it on embedded hardware. The results show up to 50% goodput and 17% energy savings for short messages.


<details>
  <summary>Details</summary>
Motivation: The motivation for studying MAC aggregation is to reduce bandwidth usage in resource-constrained environments and explore its benefits in wireless and lossy scenarios where packet loss can affect multiple messages.

Method: The method involves creating a MAC aggregation extension for DTLS 1.3 and conducting an extensive evaluation of the communication protocol stack on embedded hardware.

Result: The evaluation demonstrates that MAC aggregation improves goodput by up to 50% and reduces energy expenditure by up to 17% in lossy channels for short messages.

Conclusion: MAC aggregation proves effective in real-world wireless deployments, offering significant bandwidth and energy savings even when some packets are lost.

Abstract: Aggregating Message Authentication Codes (MACs) promises to save valuable
bandwidth in resource-constrained environments. The idea is simple: Instead of
appending an authentication tag to each message in a communication stream, the
integrity protection of multiple messages is aggregated into a single tag.
Recent studies postulate, e.g., based on simulations, that these benefits also
spread to wireless, and thus lossy, scenarios despite each lost packet
typically resulting in the loss of integrity protection information for
multiple messages. In this paper, we investigate these claims in a real
deployment. Therefore, we first design a MAC aggregation extension for the
Datagram Transport Layer Security (DTLS) 1.3 protocol. Afterward, we
extensively evaluate the performance of MAC aggregation on a complete
communication protocol stack on embedded hardware. We find that MAC aggregation
can indeed increase goodput by up to 50% and save up to 17% of energy
expenditure for the transmission of short messages, even in lossy channels.

</details>


### [26] [On the cybersecurity of LoRaWAN-based system: a Smart-Lighting case study](https://arxiv.org/abs/2510.20494)
*Florian Hofer,Barbara Russo*

Main category: cs.CR

TL;DR: This paper investigates the robustness of a LoRaWAN-based smart-lighting system against cyber attacks, identifying high-power jamming as a significant threat that can be mitigated through gateway redundancy.


<details>
  <summary>Details</summary>
Motivation: The rise of cyber-physical systems and IoT in Industry 4.0 has introduced cybersecurity challenges in creating heterogeneous smart systems. This study aims to enhance the robustness of such systems, specifically in a smart-lighting context.

Method: The study conducts an experimental investigation by analyzing architectural configurations for a LoRaWAN-based Smart-Lighting project. It evaluates system robustness through iterative experiments, both in-vitro and on-site, to assess vulnerabilities against different types of cyber attacks.

Result: The results indicate that most attacks on the LoRaWAN network were unsuccessful. However, high-power jamming attacks within close proximity posed a significant threat. The paper notes that gateway redundancy can help mitigate these attacks on gateways.

Conclusion: While the system demonstrates overall robustness against most attacks, high-power jamming remains a vulnerability, especially at short distances. Gateway redundancy is identified as a viable solution for mitigating jamming attacks on gateways, thus improving system reliability and resilience in smart-lighting applications.

Abstract: Cyber-physical systems and the Internet of Things (IoT) are key technologies
in the Industry 4.0 vision. They incorporate sensors and actuators to interact
with the physical environment. However, when creating and interconnecting
components to form a heterogeneous smart systems architecture, these face
challenges in cybersecurity. This paper presents an experimental investigation
of architectural configurations for a LoRaWAN-based Smart-Lighting project,
aimed at verifying and improving the system's robustness against attacks. We
assess the system's robustness in a series of iterative experiments conducted
both in-vitro and on-site. The results show that most attacks on a LoRaWAN
network are unsuccessful, also highlighting unresolved issues with the
installed products. The most successful attacks are high-power jamming attacks
within a few meters of the target, which, in the case of gateways, can be
mitigated through gateway redundancy.

</details>


### [27] [AdaDoS: Adaptive DoS Attack via Deep Adversarial Reinforcement Learning in SDN](https://arxiv.org/abs/2510.20566)
*Wei Shao,Yuhao Wang,Rongguang He,Muhammad Ejaz Ahmed,Seyit Camtepe*

Main category: cs.CR

TL;DR: This paper presents AdaDoS, an adaptive denial-of-service attack model using adversarial reinforcement learning to evade detection.


<details>
  <summary>Details</summary>
Motivation: Current DoS defense mechanisms are effective against rule-based attacks but struggle with AI-driven techniques, necessitating the development of more adaptive attack models to test and improve defenses.

Method: AdaDoS employs adversarial reinforcement learning, modeling the attacker-detector interaction as a competitive game and a partially observed Markov decision process (POMDP). It includes a reciprocal learning module where a student agent learns from a teacher agent to overcome limited observations.

Result: AdaDoS successfully evades both machine learning-based and rule-based DoS detectors through dynamic strategy adaptation and partial observation learning.

Conclusion: The introduction of AdaDoS highlights the need for more robust defense strategies in SDN environments against adaptive AI-driven attacks, advancing the field of adversarial network security research.

Abstract: Existing defence mechanisms have demonstrated significant effectiveness in
mitigating rule-based Denial-of-Service (DoS) attacks, leveraging predefined
signatures and static heuristics to identify and block malicious traffic.
However, the emergence of AI-driven techniques presents new challenges to SDN
security, potentially compromising the efficacy of existing defence mechanisms.
In this paper, we introduce~AdaDoS, an adaptive attack model that disrupt
network operations while evading detection by existing DoS-based detectors
through adversarial reinforcement learning (RL). Specifically, AdaDoS models
the problem as a competitive game between an attacker, whose goal is to
obstruct network traffic without being detected, and a detector, which aims to
identify malicious traffic. AdaDoS can solve this game by dynamically adjusting
its attack strategy based on feedback from the SDN and the detector.
Additionally, recognising that attackers typically have less information than
defenders, AdaDoS formulates the DoS-like attack as a partially observed Markov
decision process (POMDP), with the attacker having access only to delay
information between attacker and victim nodes. We address this challenge with a
novel reciprocal learning module, where the student agent, with limited
observations, enhances its performance by learning from the teacher agent, who
has full observational capabilities in the SDN environment. AdaDoS represents
the first application of RL to develop DoS-like attack sequences, capable of
adaptively evading both machine learning-based and rule-based DoS-like attack
detectors.

</details>


### [28] [Decentralized Exchange that Mitigate a Bribery Attack](https://arxiv.org/abs/2510.20645)
*Nitin Awathare*

Main category: cs.CR

TL;DR: This paper examines the vulnerabilities of HTLCs in exchange systems and proposes a new protocol resistant to all types of miner-collusion attacks.


<details>
  <summary>Details</summary>
Motivation: HTLCs are widely used in payment channels and atomic swaps, but their application in exchanges is problematic due to incentive incompatibility and vulnerability to bribery attacks. Existing solutions like MAD-HTLC and He-HTLC are not fully effective against all collusion scenarios between miners and parties.

Method: The authors identify a miner-collusion bribery attack and perform a game-theoretic analysis.  They propose a new protocol, 	extbackslash prot, a two-phase HTLC protocol designed to prevent unauthorized token confiscation by third parties such as miners.

Result: The proposed protocol is resistant to all bribery scenarios.  Implementation on Bitcoin and Ethereum demonstrates its efficiency in transaction cost and latency.

Conclusion: The paper identifies vulnerabilities in current HTLC solutions and presents a robust, game-theoretically secure protocol offering improved resistance to miner-collusion attacks in exchange systems.

Abstract: Despite the popularity of Hashed Time-Locked Contracts (HTLCs) because of
their use in wide areas of applications such as payment channels, atomic swaps,
etc, their use in exchange is still questionable. This is because of its
incentive incompatibility and susceptibility to bribery attacks.
  State-of-the-art solutions such as MAD-HTLC (Oakland'21) and He-HTLC
(NDSS'23) address this by leveraging miners' profit-driven behaviour to
mitigate such attacks. The former is the mitigation against passive miners;
however, the latter works against both active and passive miners. However, they
consider only two bribing scenarios where either of the parties involved in the
transfer collude with the miner.
  In this paper, we expose vulnerabilities in state-of-the-art solutions by
presenting a miner-collusion bribery attack with implementation and
game-theoretic analysis. Additionally, we propose a stronger attack on MAD-HTLC
than He-HTLC, allowing the attacker to earn profits equivalent to attacking
naive HTLC.
  Leveraging our insights, we propose \prot, a game-theoretically secure HTLC
protocol resistant to all bribery scenarios. \prot\ employs a two-phase
approach, preventing unauthorized token confiscation by third parties, such as
miners. In Phase 1, parties commit to the transfer; in Phase 2, the transfer is
executed without manipulation. We demonstrate \prot's efficiency in transaction
cost and latency via implementations on Bitcoin and Ethereum.

</details>


### [29] [Risk Psychology & Cyber-Attack Tactics](https://arxiv.org/abs/2510.20657)
*Rubens Kim,Stephan Carney,Yvonne Fonken,Soham Hans,Sofia Hirschmann,Stacy Marsella,Peggy Wu,Nikolos Gurney*

Main category: cs.CR

TL;DR: This paper investigates the relationship between cognitive processes and cyber-attack behavior in cybersecurity professionals, finding that cognitive factors significantly influence technique selection over training or experience.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand how individual cognitive differences impact cyber-attack behaviors, which could inform better defense strategies.

Method: The research uses multilevel mixed-effects Poisson regression to analyze data from cybersecurity professionals' psychometric scale responses and labeled attack behaviors during red-team operations.

Result: Cognitive processes significantly predicted technique use, with effects varying by technique. Expertise level and treatment conditions did not significantly influence technique patterns.

Conclusion: Cognitive processes play a more significant role in shaping cyber-attack behavior than training or experience, suggesting the need for psychology-informed defense strategies.

Abstract: We examine whether measured cognitive processes predict cyber-attack
behavior. We analyzed data that included psychometric scale responses and
labeled attack behaviors from cybersecurity professionals who conducted
red-team operations against a simulated enterprise network. We employed
multilevel mixed-effects Poisson regression with technique counts nested within
participants to test whether cognitive processes predicted technique-specific
usage. The scales significantly predicted technique use, but effects varied by
technique rather than operating uniformly. Neither expertise level nor
experimental treatment condition significantly predicted technique patterns,
indicating that cognitive processes may be stronger drivers of technique
selection than training or experience. These findings demonstrate that
individual cognitive differences shape cyber-attack behavior and support the
development of psychology-informed defense strategies.

</details>


### [30] [RAGRank: Using PageRank to Counter Poisoning in CTI LLM Pipelines](https://arxiv.org/abs/2510.20768)
*Austin Jia,Avaneesh Ramesh,Zain Shamsi,Daniel Zhang,Alex Liu*

Main category: cs.CR

TL;DR: The paper proposes using source credibility algorithms like PageRank to improve RAG robustness in CTI, showing effectiveness with MS MARCO and CTI docs.


<details>
  <summary>Details</summary>
Motivation: RAG systems in CTI are vulnerable to poisoning attacks due to new threats and mimicry by attackers; existing defenses are insufficient for these scenarios.

Method: The authors apply source credibility algorithms (e.g., PageRank) to RAG's retrieval phase, assigning lower authority scores to malicious content and higher scores to trusted sources.

Result: Experiments with the MS MARCO dataset showed the algorithm reduced authority scores for harmful documents, and initial CTI tests demonstrated proof-of-concept performance.

Conclusion: Source credibility algorithms can enhance RAG robustness in CTI contexts, providing a scalable method to combat poisoning attacks through trust-based content prioritization.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as the dominant
architectural pattern to operationalize Large Language Model (LLM) usage in
Cyber Threat Intelligence (CTI) systems. However, this design is susceptible to
poisoning attacks, and previously proposed defenses can fail for CTI contexts
as cyber threat information is often completely new for emerging attacks, and
sophisticated threat actors can mimic legitimate formats, terminology, and
stylistic conventions. To address this issue, we propose that the robustness of
modern RAG defenses can be accelerated by applying source credibility
algorithms on corpora, using PageRank as an example. In our experiments, we
demonstrate quantitatively that our algorithm applies a lower authority score
to malicious documents while promoting trusted content, using the standardized
MS MARCO dataset. We also demonstrate proof-of-concept performance of our
algorithm on CTI documents and feeds.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [31] [E-Test: E'er-Improving Test Suites](https://arxiv.org/abs/2510.19860)
*Ketai Qiu,Luca Di Grazia,Leonardo Mariani,Mauro Pezzè*

Main category: cs.SE

TL;DR: E-Test uses LLMs to identify and add new test cases for untested execution scenarios in existing test suites, outperforming state-of-the-art methods with an F1-score of 0.55.


<details>
  <summary>Details</summary>
Motivation: Existing test suites are incomplete, requiring manual effort to identify and insert new test cases for untested execution scenarios in production software.

Method: E-Test leverages LLMs to analyze large scenario datasets from production monitors, identifies uncovered scenarios, and generates corresponding test cases.

Result: Achieved an F1-score of 0.55 on a dataset of 1,975 execution scenarios, outperforming regression/field testing approaches (0.34) and vanilla LLMs (0.39).

Conclusion: E-Test effectively enhances test suite coverage by detecting and adding tests for previously untested production scenarios via LLMs, reducing manual maintenance effort.

Abstract: Test suites are inherently imperfect, and testers can always enrich a suite
with new test cases that improve its quality and, consequently, the reliability
of the target software system. However, finding test cases that explore
execution scenarios beyond the scope of an existing suite can be extremely
challenging and labor-intensive, particularly when managing large test suites
over extended periods.
  In this paper, we propose E-Test, an approach that reduces the gap between
the execution space explored with a test suite and the executions experienced
after testing by augmenting the test suite with test cases that explore
execution scenarios that emerge in production. E-Test (i) identifies executions
that have not yet been tested from large sets of scenarios, such as those
monitored during intensive production usage, and (ii) generates new test cases
that enhance the test suite. E-Test leverages Large Language Models (LLMs) to
pinpoint scenarios that the current test suite does not adequately cover, and
augments the suite with test cases that execute these scenarios.
  Our evaluation on a dataset of 1,975 scenarios, collected from highly-starred
open-source Java projects already in production and Defects4J, demonstrates
that E-Test retrieves not-yet-tested execution scenarios significantly better
than state-of-the-art approaches. While existing regression testing and field
testing approaches for this task achieve a maximum F1-score of 0.34, and
vanilla LLMs achieve a maximum F1-score of 0.39, E-Test reaches 0.55. These
results highlight the impact of E-Test in enhancing test suites by effectively
targeting not-yet-tested execution scenarios and reducing manual effort
required for maintaining test suites.

</details>


### [32] [SODBench: A Large Language Model Approach to Documenting Spreadsheet Operations](https://arxiv.org/abs/2510.19864)
*Amila Indika,Igor Molybog*

Main category: cs.SE

TL;DR: This paper introduces Spreadsheet Operations Documentation (SOD), an AI task to generate human-readable explanations from spreadsheet code, presenting a benchmark of 111 code snippets with summaries and evaluating five LLMs.


<details>
  <summary>Details</summary>
Motivation: The lack of systematic documentation in spreadsheets creates risks for automation, collaboration, and preserving institutional knowledge, particularly within business and finance contexts. Effective documentation is needed to enhance reproducibility and maintainability.

Method: The authors define the SOD task, compile a benchmark of 111 paired spreadsheet code snippets and natural language summaries, and assess the performance of five LLMs (GPT-4o, GPT-4o-mini, LLaMA-3.3-70B, Mixtral-8x7B, Gemma2-9B) using BLEU, GLEU, ROUGE-L, and METEOR metrics.

Result: The evaluation demonstrates that the tested LLMs are capable of generating accurate, human-readable SOD explanations, indicating feasibility in improving spreadsheet processes, though challenges remain.

Conclusion: The paper concludes that SOD is a viable preprocessing method to enhance reproducibility and collaboration in spreadsheet workflows, with existing LLMs showing effectiveness, but notes room for improvement and further research.

Abstract: Numerous knowledge workers utilize spreadsheets in business, accounting, and
finance. However, a lack of systematic documentation methods for spreadsheets
hinders automation, collaboration, and knowledge transfer, which risks the loss
of crucial institutional knowledge. This paper introduces Spreadsheet
Operations Documentation (SOD), an AI task that involves generating
human-readable explanations from spreadsheet operations. Many previous studies
have utilized Large Language Models (LLMs) for generating spreadsheet
manipulation code; however, translating that code into natural language for SOD
is a less-explored area. To address this, we present a benchmark of 111
spreadsheet manipulation code snippets, each paired with a corresponding
natural language summary. We evaluate five LLMs, GPT-4o, GPT-4o-mini,
LLaMA-3.3-70B, Mixtral-8x7B, and Gemma2-9B, using BLEU, GLEU, ROUGE-L, and
METEOR metrics. Our findings suggest that LLMs can generate accurate
spreadsheet documentation, making SOD a feasible prerequisite step toward
enhancing reproducibility, maintainability, and collaborative workflows in
spreadsheets, although there are challenges that need to be addressed.

</details>


### [33] [Knowledge-Guided Multi-Agent Framework for Application-Level Software Code Generation](https://arxiv.org/abs/2510.19868)
*Qian Xiong,Bo Yang,Weisong Sun,Yiran Zhang,Tianlin Li,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: The paper introduces a framework for generating complex application-level code using a collaborative multi-agent system and knowledge guidance to improve structure and maintainability.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based code generation struggles with complex application code due to limited organization and maintainability. Multi-agent methods lack in handling this effectively.

Method: KGACG combines three agents (COPA, CA, TA) in a closed-loop process: COPA plans code structure, CA generates code, and TA tests it with feedback to refine the output.

Result: KGACG is validated through a Java Tank Battle game case study, showing its effectiveness in generating structured, maintainable code with collaborative agents and a feedback loop.

Conclusion: KGACG advances automation in complex software development using a knowledge-driven multi-agent system to ensure proper code organization and iterative refinement.

Abstract: Automated code generation driven by Large Lan- guage Models (LLMs) has
enhanced development efficiency, yet generating complex application-level
software code remains challenging. Multi-agent frameworks show potential, but
existing methods perform inadequately in large-scale application-level software
code generation, failing to ensure reasonable orga- nizational structures of
project code and making it difficult to maintain the code generation process.
To address this, this paper envisions a Knowledge-Guided Application-Level Code
Generation framework named KGACG, which aims to trans- form software
requirements specification and architectural design document into executable
code through a collaborative closed- loop of the Code Organization & Planning
Agent (COPA), Coding Agent (CA), and Testing Agent (TA), combined with a
feedback mechanism. We demonstrate the collaborative process of the agents in
KGACG in a Java Tank Battle game case study while facing challenges. KGACG is
dedicated to advancing the automation of application-level software
development.

</details>


### [34] [BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills](https://arxiv.org/abs/2510.19898)
*Atharv Sonwane,Isadora White,Hyunji Lee,Matheus Pereira,Lucas Caccia,Minseon Kim,Zhengyan Shi,Chinmay Singh,Alessandro Sordoni,Marc-Alexandre Côté,Xingdi Yuan*

Main category: cs.SE

TL;DR: The paper introduces a novel method for generating high-quality, diverse bugs in a synthetic way by instructing SWE agents to introduce features that inadvertently break tests. The resulting bugs are more realistic and efficient for training, leading to state-of-the-art SWE models.


<details>
  <summary>Details</summary>
Motivation: High-quality bugs are essential for training SWE agents, as they improve the agents' ability to detect and fix issues. Prior synthetic bug generation approaches create bugs intentionally (e.g., via perturbations), which may not reflect realistic development scenarios where bugs are often unintentional side effects of feature implementations.

Method: The method involves training SWE agents to introduce new features into codebases, which can inadvertently break tests, creating bugs. Unintentional bugs in this way mimic real-world development errors, where changes for new functionality may result in unintended consequences. This contrasts with prior work that often creates bugs via local, intentional code changes.

Result: Through qualitative analysis and extensive experiments, the approach shows that the generated bugs are more realistic and lead to improved training efficiency. Their models trained on this data achieve better performance compared to models trained with existing bug datasets. FrogBoss (32B) reaches a pass@1 of 54.6% and FrogMini (14B) reaches 45.3% on SWE-bench Verified.

Conclusion: The proposed bug generation method results in high-quality, realistic bugs suitable for training SWE agents, offering better efficiency and performance compared to existing synthetic bug creation approaches. The method bridges the gap between synthetic data and real development scenarios, improving the effectiveness of agents on standard benchmarks.

Abstract: High quality bugs are key to training the next generation of language model
based software engineering (SWE) agents. We introduce a novel method for
synthetic generation of difficult and diverse bugs. Our method instructs SWE
Agents to introduce a feature into the codebase whereby they may
unintentionally break tests, resulting in bugs. Prior approaches often induce
an out-of-distribution effect by generating bugs intentionally (e.g. by
introducing local perturbation to existing code), which does not reflect
realistic development processes. We perform qualitative analysis to demonstrate
that our approach for generating bugs more closely reflects the patterns found
in human-authored edits. Through extensive experiments, we demonstrate that our
bugs provide more efficient training data for supervised fine-tuning,
outperforming other bug datasets by 2% with half the training data (1.2k vs. 3k
bugs). We train on our newly generated bugs in addition to existing bug
datasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench
Verified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on
SWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over
three seeds.

</details>


### [35] [On Interaction Effects in Greybox Fuzzing](https://arxiv.org/abs/2510.19984)
*Konstantinos Kitsios,Marcel Böhme,Alberto Bacchelli*

Main category: cs.SE

TL;DR: TLDR; MuoFuzz is a greybox fuzzer that improves effectiveness by learning optimal mutator sequences, outperforming AFL++ and MOPT in code coverage and bug discovery.


<details>
  <summary>Details</summary>
Motivation: The paper motivates the need for optimizing mutator sequence order in greybox fuzzing to enhance test input generation and fuzzer effectiveness. Greybox fuzzers currently apply mutators randomly, potentially missing more efficient sequences that could increase code coverage and detect bugs more effectively.

Method: MuoFuzz learns the order of mutator application by fitting a linear model to a dataset of mutator pair effectiveness, estimating conditional probabilities for the next effective mutator. These probabilities guide a random walk to generate mutator sequences for fuzzing.

Result: On FuzzBench and MAGMA benchmarks, MuoFuzz demonstrated the highest code coverage among tested fuzzers and identified four bugs undetected by AFL++, as well as one unique bug compared to both AFL++ and MOPT.

Conclusion: By considering mutator sequence interactions, MuoFuzz significantly outperforms traditional greybox fuzzing strategies. This work highlights the importance of order-aware mutation for maximizing fuzzer performance in regression testing and vulnerability detection.

Abstract: A greybox fuzzer is an automated software testing tool that generates new
test inputs by applying randomly chosen mutators (e.g., flipping a bit or
deleting a block of bytes) to a seed input in random order and adds all
coverage-increasing inputs to the corpus of seeds. We hypothesize that the
order in which mutators are applied to a seed input has an impact on the
effectiveness of greybox fuzzers. In our experiments, we fit a linear model to
a dataset that contains the effectiveness of all possible mutator pairs and
indeed observe the conjectured interaction effect. This points us to more
efficient fuzzing by choosing the most promising mutator sequence with a higher
likelihood. We propose MuoFuzz, a greybox fuzzer that learns and chooses the
most promising mutator sequences. MuoFuzz learns the conditional probability
that the next mutator will yield an interesting input, given the previously
selected mutator. Then, it samples from the learned probability using a random
walk to generate mutator sequences. We compare the performance of MuoFuzz to
AFL++, which uses a fixed selection probability, and MOPT, which optimizes the
selection probability of each mutator in isolation. Experimental results on the
FuzzBench and MAGMA benchmarks show that MuoFuzz achieves the highest code
coverage and finds four bugs missed by AFL++ and one missed by both AFL++ and
MOPT.

</details>


### [36] [A Framework for the Adoption and Integration of Generative AI in Midsize Organizations and Enterprises (FAIGMOE)](https://arxiv.org/abs/2510.19997)
*Abraham Itzhak Weinberg*

Main category: cs.SE

TL;DR: TLDR


<details>
  <summary>Details</summary>
Motivation: MOTION

Method: METHOD

Result: RESULT

Conclusion: CONCLUSION

Abstract: Generative Artificial Intelligence (GenAI) presents transformative
opportunities for organizations, yet both midsize organizations and larger
enterprises face distinctive adoption challenges. Midsize organizations
encounter resource constraints and limited AI expertise, while enterprises
struggle with organizational complexity and coordination challenges. Existing
technology adoption frameworks, including TAM (Technology Acceptance Model),
TOE (Technology Organization Environment), and DOI (Diffusion of Innovations)
theory, lack the specificity required for GenAI implementation across these
diverse contexts, creating a critical gap in adoption literature. This paper
introduces FAIGMOE (Framework for the Adoption and Integration of Generative AI
in Midsize Organizations and Enterprises), a conceptual framework addressing
the unique needs of both organizational types. FAIGMOE synthesizes technology
adoption theory, organizational change management, and innovation diffusion
perspectives into four interconnected phases: Strategic Assessment, Planning
and Use Case Development, Implementation and Integration, and
Operationalization and Optimization. Each phase provides scalable guidance on
readiness assessment, strategic alignment, risk governance, technical
architecture, and change management adaptable to organizational scale and
complexity. The framework incorporates GenAI specific considerations including
prompt engineering, model orchestration, and hallucination management that
distinguish it from generic technology adoption frameworks. As a perspective
contribution, FAIGMOE provides the first comprehensive conceptual framework
explicitly addressing GenAI adoption across midsize and enterprise
organizations, offering actionable implementation protocols, assessment
instruments, and governance templates requiring empirical validation through
future research.

</details>


### [37] [The Cost of Downgrading Build Systems: A Case Study of Kubernetes](https://arxiv.org/abs/2510.20041)
*Gareema Ranjan,Mahmoud Alfadel,Gengyi Sun,Shane McIntosh*

Main category: cs.SE

TL;DR: This paper investigates the trade-offs of downgrading from artifact-based build tools (Bazel) to alternative tools (e.g., Go Build) in large projects, using a case study of the Kubernetes project and four other projects. It highlights the performance costs and benefits of such downgrades.


<details>
  <summary>Details</summary>
Motivation: To understand the implications of downgrading from modern artifact-based build tools to alternative ones, as prior work shows teams may do so for maintainability but the trade-offs are not well understood.

Method: The authors conducted a case study on the Kubernetes project, analyzing full and incremental build times, memory usage, and CPU load before and after downgrading from Bazel to Go Build. They replicated the study on four other projects.

Result: Bazel builds are faster but consume more memory and CPU resources. Downgrading to alternative tools leads to slower builds but lower memory and CPU usage. The switch may increase CI resource costs by up to 76%. Replication on four other projects showed similar trends, with Bazel consistently using more memory even as build time penalties decrease.

Conclusion: While downgrading to alternative build tools may improve maintainability, it often incurs significant performance costs for large projects. The study suggests stakeholders carefully weigh these trade-offs when choosing build tools.

Abstract: Since developers invoke the build system frequently, its performance can
impact productivity. Modern artifact-based build tools accelerate builds, yet
prior work shows that teams may abandon them for alternatives that are easier
to maintain. While prior work shows why downgrades are performed, the
implications of downgrades remain largely unexplored. In this paper, we
describe a case study of the Kubernetes project, focusing on its downgrade from
an artifact-based build tool (Bazel) to a language-specific solution (Go
Build). We reproduce and analyze the full and incremental builds of change sets
during the downgrade period. On the one hand, we find that Bazel builds are
faster than Go Build, completing full builds in 23.06-38.66 up to 75.19 impose
a larger memory footprint than Go Build of 81.42-351.07 respectively. Bazel
builds also impose a greater CPU load at parallelism settings above eight for
full builds and above one for incremental builds. We estimate that downgrading
from Bazel can increase CI resource costs by up to 76 explore whether our
observations generalize by replicating our Kubernetes study on four other
projects that also downgraded from Bazel to older build tools. We observe that
while build time penalties decrease, Bazel consistently consumes more memory.
We conclude that abandoning artifact-based build tools, despite perceived
maintainability benefits, tends to incur considerable performance costs for
large projects. Our observations may help stakeholders to balance trade-offs in
build tool adoption

</details>


### [38] [Developing a Model-Driven Reengineering Approach for Migrating PL/SQL Triggers to Java: A Practical Experience](https://arxiv.org/abs/2510.20121)
*Carlos J. Fernandez-Candel,Jesus Garcia-Molina,Francisco Javier Bermudez Ruiz,Jose Ramon Hoyos Barcelo,Diego Sevilla Ruiz,Benito Jose Cuesta Viera*

Main category: cs.SE

TL;DR: descript the process of model-driven re-engineering


<details>
  <summary>Details</summary>
Motivation: With the success of modern software technologies impelling their adoption, traditional, legacy RAD platforms demand migration to contemporary architectures and languages. PL/SQL monolithic applications on Oracle Forms represent one such scenario.

Method: The study adopts and integrates a TDD-like approach to iteratively develop model transformations, ensuring robustness through three tyes of validations. Legacy PL/SQL structures are modelized via KDM

Result: Development of a migration tool is confirmed that converts PL/SQL from Oracle Forms into Java enterprise architecture across multiple tiers, with validation milestones.

Conclusion: The paper demonstrates the feasibility of MDE in the context of real-world enterprise migration from RAD platforms to Java, capturing all requirements and functionalities from the original systems through modeling.

Abstract: Model-driven software engineering (MDE) techniques are not only useful in
forward engineering scenarios, but can also be successfully applied to evolve
existing systems. RAD (Rapid Application Development) platforms emerged in the
nineties, but the success of modern software technologies motivated that a
large number of enterprises tackled the migration of their RAD applications,
such as Oracle Forms. Our research group has collaborated with a software
company in developing a solution to migrate PL/SQL monolithic code on Forms
triggers and program units to Java code separated in several tiers.
  Our research focused on the model-driven reengineering process applied to
develop the migration tool for the conversion of PL/SQL code to Java. Legacy
code is represented in form of KDM (Knowledge-Discovery Metamodel) models. In
this paper, we propose a software process to implement a model-driven
re-engineering. This process integrates a TDD-like approach to incrementally
develop model transformations with three kinds of validations for the generated
code. The implementation and validation of the re-engineering approach are
explained in detail, as well as the evaluation of some issues related with the
application of MDE.

</details>


### [39] [Automated Cloud Infrastructure-as-Code Reconciliation with AI Agents](https://arxiv.org/abs/2510.20211)
*Zhenning Yang,Hui Guan,Victor Nicolet,Brandon Paulsen,Joey Dodds,Daniel Kroening,Ang Chen*

Main category: cs.SE

TL;DR: NSync is an automated system for reconciling IaC configurations by propagating non-IaC changes back into the code through API trace analysis. It uses an agentic architecture with LLMs for intent inference, targeted IaC updates, and optimization via a self-evolving knowledge base, performing significantly better than existing methods in accuracy and resource efficiency.


<details>
  <summary>Details</summary>
Motivation: Cloud infrastructure often uses IaC alongside traditional tools, leading to drift when changes made via other means aren't reflected in IaC, causing errors and inefficiency. Ensuring synchronization between the actual infrastructure and IaC is critical for consistent and reliable cloud operations.

Method: NSync detects drift using API traces to identify non-IaC changes. It employs an agentic architecture with LLMs to infer high-level intents from API invocations, synthesizes targeted IaC updates via specialized tools, and improves over time using a knowledge base of past successful reconciliations. A new evaluation framework is introduced to test drift scenarios realistically.

Result: NSync was tested on five real-world Terraform projects and 372 drift scenarios. It achieved a 0.97 $\mathrm{\texttt{pass@3}}$ score for accuracy and demonstrated a 1.47$\times$ token efficiency improvement over the baseline.

Conclusion: NSync effectively addresses the issue of infrastructure drift in IaC by automatically reconciling out-of-band changes using API trace analysis and an agentic approach. It outperforms existing methods both in accuracy and efficiency, making it a robust solution for maintaining cloud infrastructure consistency.

Abstract: Cloud infrastructure is managed through a mix of interfaces -- traditionally,
cloud consoles, command-line interfaces (CLI), and SDKs are the tools of
choice. Recently, Infrastructure-as-Code/IaC frameworks (e.g., Terraform) have
quickly gained popularity. Unlike conventional tools, IaC~frameworks encode the
infrastructure in a "source-of-truth" configuration. They are capable of
automatically carrying out modifications to the cloud -- deploying, updating,
or destroying resources -- to bring the actual infrastructure into alignment
with the IaC configuration. However, when IaC is used alongside consoles, CLIs,
or SDKs, it loses visibility into external changes, causing infrastructure
drift, where the configuration becomes outdated, and later IaC operations may
undo valid updates or trigger errors.
  We present NSync, an automated system for IaC reconciliation that propagates
out-of-band changes back into the IaC program. Our key insight is that
infrastructure changes eventually all occur via cloud API invocations -- the
lowest layer for cloud management operations. NSync gleans insights from API
traces to detect drift (i.e., non-IaC changes) and reconcile it (i.e., update
the IaC configuration to capture the changes). It employs an agentic
architecture that leverages LLMs to infer high-level intents from noisy API
sequences, synthesize targeted IaC updates using specialized tools, and
continually improve through a self-evolving knowledge base of past
reconciliations. We further introduce a novel evaluation pipeline for injecting
realistic drifts into cloud infrastructure and assessing reconciliation
performance. Experiments across five real-world Terraform projects and 372
drift scenarios show that NSync outperforms the baseline both in terms of
accuracy (from 0.71 to 0.97 pass@3) and token efficiency (1.47$\times$
improvement).

</details>


### [40] [Classport: Designing Runtime Dependency Introspection for Java](https://arxiv.org/abs/2510.20340)
*Serena Cofano,Daniel Williams,Aman Sharma,Martin Monperrus*

Main category: cs.SE

TL;DR: TLDR: Classport is introduced to solve the problem of runtime introspection of dependencies in Java, enabling retrieval of dependency information at runtime and opening avenues for integrity checks.


<details>
  <summary>Details</summary>
Motivation: Java lacks support for runtime introspection of dependencies, which is crucial for Software Supply Chain security.

Method: Classport embeds dependency information into Java class files to allow runtime retrieval.

Result: Evaluation was performed on six real-world projects demonstrating the feasibility of identifying dependencies at runtime.

Conclusion: Classport's runtime dependency introspection opens important opportunities for runtime integrity checking.

Abstract: Runtime introspection of dependencies, i.e., the ability to observe which
dependencies are currently used during program execution, is fundamental for
Software Supply Chain security. Yet, Java has no support for it. We solve this
problem with Classport, a system that embeds dependency information into Java
class files, enabling the retrieval of dependency information at runtime. We
evaluate Classport on six real-world projects, demonstrating the feasibility in
identifying dependencies at runtime. Runtime dependency introspection with
Classport opens important avenues for runtime integrity checking.

</details>


### [41] [Symmetry in Software Platforms as an Architectural Principle](https://arxiv.org/abs/2510.20389)
*Bjorn Remseth*

Main category: cs.SE

TL;DR: Suppose this paper is likely about software platforms. It think that structure preserving and symmetries contribute to architectural robustness and explore it.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need to understand how architectural robustness is influenced by maintaining structural regularities in software platforms.

Method: The paper explores the impact of structural regularities on architectural robustness in software platforms.

Result: The findings indicate a positive correlation between structural regularities and architectural robustness in software platforms.

Conclusion: The paper concludeserinforcing structural regularities in software platforms enhances architectural robustness.

Abstract: Software platforms often act as structure preserving systems. They provide
consistent interfaces and behaviors that remain stable under specific
transformations that we denote as symmetries. This paper explores the idea that
architectural robustness emerges from enforcing such structural regularities

</details>


### [42] [FMI-Based Distributed Co-Simulation with Enhanced Security and Intellectual Property Safeguards](https://arxiv.org/abs/2510.20403)
*Santiago Gil,Ecem E. Baş,Christian D. Jensen,Sebastian Engelsgaard,Giuseppe Abbiati,Cláudio Gomes*

Main category: cs.SE

TL;DR: The paper introduces a secure distributed co-simulation approach for continuous-time/hybrid systems on top of UniFMU, ensuring IP protection by having the client initiate connections and placing models/binaries on trusted platforms, with performance trade-offs evaluated through two demos in four network settings.


<details>
  <summary>Details</summary>
Motivation: The absence of clear guidelines for conducting IP-protected distributed co-simulation for continuous-time and hybrid systems while preventing hacking attacks motivates the need for a robust cybersecurity and IP protection approach in such simulations.

Method: The paper proposes a cybersecurity-enhanced co-simulation architecture based on UniFMU. Key features include: (1) Client-initiated communication to control simulation execution flow securely, (2) Enforcing models and binaries to reside on trusted platforms for IP protection, and (3) Setting up distributed co-simulation sessions in controlled environments. The paper implements and evaluates this architecture through two co-simulation demo cases in four distinct network topologies (authoritative, collaborative, simulation owner, and multiple clients).

Result: The results demonstrate the effectiveness of the client-initiated mechanism and trusted platform implementation in protecting IP and preventing participation in unauthorized co-simulations. The analysis provides insights into the trade-off between IP protection and simulation performance in different network settings, showing minimal performance impact while maintaining strong security features.

Conclusion: The proposed client-initiated, trusted platform-based co-simulation framework (UniFMU) significantly improves the IP protection and cybersecurity aspects of distributed co-simulations of continuous-time/hybrid systems. The performance trade-offs are negligible, establishing a practical foundation for secure, collaborative modeling and simulation workflows in the industry.

Abstract: Distributed co-simulation plays a key role in enabling collaborative modeling
and simulation by different stakeholders while protecting their Intellectual
Property (IP). Although IP protection is provided implicitly by co-simulation,
there is no consensus in the guidelines to conduct distributed co-simulation of
continuous-time or hybrid systems with no exposure to potential hacking
attacks. We propose an approach for distributed co-simulation on top of UniFMU
with enhanced cybersecurity and IP protection mechanisms, ensuring that the
connection is initiated by the client and the models and binaries live on
trusted platforms. We showcase the functionality of this approach using two
co-simulation demos in four different network settings and analyze the
trade-off between IP-protected distribution and performance efficiency in these
settings.

</details>


### [43] [Toward Practical Deductive Verification: Insights from a Qualitative Survey in Industry and Academia](https://arxiv.org/abs/2510.20514)
*Lea Salome Brugger,Xavier Denis,Peter Müller*

Main category: cs.SE

TL;DR: Through stakeholder interviews, this paper identifies barriers (expertise demands, automation control, usability) and enablers for deductive verification adoption, offering tailored recommendations to improve tooling, workflows, and practitioner approaches.


<details>
  <summary>Details</summary>
Motivation: The paper aims to systematically investigate factors enabling successful deductive verification adoption and barriers hindering its mainstream use, addressing the gap between its proven effectiveness in niche projects and limited broader application.

Method: The authors conducted semi-structured interviews with 30 practitioners from academia and industry, systematically analyzing interview data through thematic analysis to identify enablers and barriers of deductive verification.

Result: The study empirically confirms existing challenges (e.g., expertise requirements) and identifies underexplored obstacles like proof maintenance, insufficient automation control, and usability issues. These findings inform actionable recommendations for different stakeholder groups.

Conclusion: The study concludes that while deductive verification holds promise, its adoption requires addressing barriers like expertise demands, automation control, and usability. Concrete recommendations for stakeholders aim to improve usability, automation, and integration with workflows to promote broader adoption.

Abstract: Deductive verification is an effective method to ensure that a given system
exposes the intended behavior. In spite of its proven usefulness and
feasibility in selected projects, deductive verification is still not a
mainstream technique. To pave the way to widespread use, we present a study
investigating the factors enabling successful applications of deductive
verification and the underlying issues preventing broader adoption. We
conducted semi-structured interviews with 30 practitioners of verification from
both industry and academia and systematically analyzed the collected data
employing a thematic analysis approach. Beside empirically confirming familiar
challenges, e.g., the high level of expertise needed for conducting formal
proofs, our data reveal several underexplored obstacles, such as proof
maintenance, insufficient control over automation, and usability concerns. We
further use the results from our data analysis to extract enablers and barriers
for deductive verification and formulate concrete recommendations for
practitioners, tool builders, and researchers, including principles for
usability, automation, and integration with existing workflows.

</details>


### [44] [Large Language Models for Fault Localization: An Empirical Study](https://arxiv.org/abs/2510.20521)
*YingJian Xiao,RongQun Hu,WeiWei Gong,HongWei Li,AnQuan Jie*

Main category: cs.SE

TL;DR: Study compares LLM performance in code fault localization, revealing that bug reports enhance accuracy, few-shot learning has limited scalability, and reasoning strategies depend on model quality. Identifies practical trade-offs for repair system design.


<details>
  <summary>Details</summary>
Motivation: Current automated program repair effectiveness relies heavily on upstream fault localization, yet existing LLM research lacks comprehensive evaluations of this critical component. This study addresses this gap through empirical analysis.

Method: Systematic evaluation of four LLMs (open/closed-source) on HumanEval-Java and Defects4J datasets using metrics of accuracy, time efficiency, and cost. Tested prompting strategies including standard prompts, few-shot learning, chain-of-thought reasoning, and contextual information integration.

Result: Bug report context integration significantly improves localization accuracy. Few-shot learning shows promise but with diminishing returns. Chain-of-thought reasoning's effectiveness depends strongly on model architecture. Open-source models show competitive performance versus closed-source counterparts.

Conclusion: This study highlights the performance characteristics and trade-offs of different LLMs in fault localization, identifies factors influencing effectiveness, and provides actionable insights for improving automated program repair workflows.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code-related tasks, particularly in automated program repair. However, the
effectiveness of such repairs is highly dependent on the performance of
upstream fault localization, for which comprehensive evaluations are currently
lacking. This paper presents a systematic empirical study on LLMs in the
statement-level code fault localization task. We evaluate representative
open-source models (Qwen2.5-coder-32b-instruct, DeepSeek-V3) and closed-source
models (GPT-4.1 mini, Gemini-2.5-flash) to assess their fault localization
capabilities on the HumanEval-Java and Defects4J datasets. The study
investigates the impact of different prompting strategies--including standard
prompts, few-shot examples, and chain-of-reasoning--on model performance, with
a focus on analysis across accuracy, time efficiency, and economic cost
dimensions. Our experimental results show that incorporating bug report context
significantly enhances model performance. Few-shot learning shows potential for
improvement but exhibits noticeable diminishing marginal returns, while
chain-of-thought reasoning's effectiveness is highly contingent on the model's
inherent reasoning capabilities. This study not only highlights the performance
characteristics and trade-offs of different models in fault localization tasks,
but also offers valuable insights into the strengths of current LLMs and
strategies for improving fault localization effectiveness.

</details>


### [45] [A Soundness and Precision Benchmark for Java Debloating Tools](https://arxiv.org/abs/2510.20679)
*Jonas Klauke,Tom Ohlmer,Stefan Schott,Serena Elisa Ponta,Wolfram Fischer,Eric Bodden*

Main category: cs.SE

TL;DR: The paper introduces Deblometer, a micro-benchmark for assessing Java debloating tools, and evaluates three tools: Deptrim, JShrink, and ProGuard, finding unsoundness issues in all.


<details>
  <summary>Details</summary>
Motivation: Modern Java projects include numerous dependencies with many unused constructs, leading to software bloat. Existing debloating tools struggle to remove unused constructs without also removing necessary ones, necessitating systematic evaluation and improvement.

Method: Developed a micro-benchmark (Deblometer) with 59 hand-crafted Java test cases covering diverse language features. Each test case includes hand-curated ground truth. Evaluated three Java debloators (Deptrim, JShrink, ProGuard) on these cases to measure their precision and soundness.

Result: All evaluated tools eliminated necessary program constructs. Dynamic class loading introduced unsoundness for all. Deptrim retained the most bloated constructs but removed the least required ones, while ProGuard removed more required constructs. JShrink's soundness was most affected by annotation limitations.

Conclusion: Deblometer provides a rigorous benchmark for Java debloators. Evaluation of three tools shows unsound behavior with varying profiles, underscoring the need for improved debloating approaches to ensure reliable automated code optimization.

Abstract: Modern software development reuses code by importing libraries as
dependencies. Software projects typically include an average of 36
dependencies, with 80% being transitive, meaning they are dependencies of
dependencies. Recent research indicates that only 24.9% of these dependencies
are required at runtime, and even within those, many program constructs remain
unused, adding unnecessary code to the project. This has led to the development
of debloating tools that remove unnecessary dependencies and program constructs
while balancing precision by eliminating unused constructs and soundness by
preserving all required constructs. To systematically evaluate this trade-off,
we developed Deblometer, a micro-benchmark consisting of 59 test cases designed
to assess support for various Java language features in debloating tools. Each
test case includes a manually curated ground truth specifying necessary and
bloated classes, methods, and fields, enabling precise measurement of soundness
and precision. Using Deblometer, we evaluated three popular Java debloating
tools: Deptrim, JShrink, and ProGuard. Our evaluation reveals that all tools
remove required program constructs, which results in changed semantics or
execution crashes. In particular, the dynamic class loading feature introduces
unsoundness in all evaluated tools. Our comparison shows that Deptrim retains
more bloated constructs, while ProGuard removes more required constructs.
JShrink's soundness is significantly affected by limited support for
annotations, which leads to corrupted debloated artifacts. These soundness
issues highlight the need to improve debloating tools to ensure stable and
reliable debloated software.

</details>


### [46] [Exploring Large Language Models for Access Control Policy Synthesis and Summarization](https://arxiv.org/abs/2510.20692)
*Adarsh Vatsa,Bethel Hall,William Eiers*

Main category: cs.SE

TL;DR: The paper explores the use of Large Language Models (LLMs) for automating access control policy synthesis and analysis. While LLMs can generate syntactically correct policies, they exhibit permissiveness issues in non-reasoning models. The study introduces a semantic-based request summarization approach to aid in policy analysis, showing promise when combined with symbolic methods.


<details>
  <summary>Details</summary>
Motivation: The complexity and manual effort involved in creating and analyzing access control policies for cloud computing environments pose significant challenges. Current methods are error-prone and time-consuming. The growing success of LLMs in code synthesis and summarization suggests their potential in automating these tasks.

Method: The paper investigates various LLMs for access control policy synthesis. They evaluate the syntactic correctness of generated policies and their adherence to specified security requirements. A novel semantic-based request summarization approach is introduced, leveraging LLMs to generate precise request characterizations for policy analysis.

Result: LLMs can generate syntactically correct policies, but non-reasoning models exhibit permissiveness issues, with only 45.8% adherence to specifications. Reasoning LLMs perform better, achieving 93.7% compliance. The semantic summarization approach effectively characterizes requests allowed by policies, demonstrating the potential of combining LLMs with symbolic methods for analysis.

Conclusion: While LLMs face challenges in fully automating policy generation due to permissiveness issues, the study highlights their value in policy analysis when integrated with symbolic approaches. This suggests a hybrid model may offer advantages in ensuring the accuracy and security of access control policies.

Abstract: Cloud computing is ubiquitous, with a growing number of services being hosted
on the cloud every day. Typical cloud compute systems allow administrators to
write policies implementing access control rules which specify how access to
private data is governed. These policies must be manually written, and due to
their complexity can often be error prone. Moreover, existing policies often
implement complex access control specifications and thus can be difficult to
precisely analyze in determining their behavior works exactly as intended.
Recently, Large Language Models (LLMs) have shown great success in automated
code synthesis and summarization. Given this success, they could potentially be
used for automatically generating access control policies or aid in
understanding existing policies. In this paper, we explore the effectiveness of
LLMs for access control policy synthesis and summarization. Specifically, we
first investigate diverse LLMs for access control policy synthesis, finding
that: although LLMs can effectively generate syntactically correct policies,
they have permissiveness issues, generating policies equivalent to the given
specification 45.8% of the time for non-reasoning LLMs, and 93.7% of the time
for reasoning LLMs. We then investigate how LLMs can be used to analyze
policies by introducing a novel semantic-based request summarization approach
which leverages LLMs to generate a precise characterization of the requests
allowed by a policy. Our results show that while there are significant hurdles
in leveraging LLMs for automated policy generation, LLMs show promising results
when combined with symbolic approaches in analyzing existing policies.

</details>
