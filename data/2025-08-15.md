<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 24]
- [cs.SE](#cs.SE) [Total: 6]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [A Robust Pipeline for Differentially Private Federated Learning on Imbalanced Clinical Data using SMOTETomek and FedProx](https://arxiv.org/abs/2508.10017)
*Rodrigo Tertulino*

Main category: cs.CR

TL;DR: This paper proposes an optimized Federated Learning (FL) framework with Differential Privacy (DP) for cardiovascular risk prediction, addressing privacy-utility trade-offs and class imbalance in medical datasets. Key results show FedProx optimization achieves 77% recall while maintaining strong privacy (epsilon=9.0).


<details>
  <summary>Details</summary>
Motivation: Existing FL methods struggle with class imbalance in medical datasets and the inherent privacy-utility trade-off when combining FL with DP. These challenges threaten both diagnostic accuracy and patient privacy in decentralized healthcare research.

Method: 1. Implemented FL for cardiovascular risk prediction
2. Applied SMOTETomek at client level to handle class imbalance
3. Enhanced FedProx algorithm for non-IID medical data
4. Quantified privacy-utility trade-off through epsilon-recall analysis

Result: Non-linear privacy-utility trade-off confirmed with
- FedProx outperforming FedAvg significantly
- Recall >77% achieved with DP privacy budget ε=9.0
- SMOTE+Tomek links resolved zero-recall issue
- Operational region identified for strong privacy and clinical utility

Conclusion: The study establishes a methodological blueprint for FL+DP in healthcare, demonstrating that optimized algorithms (FedProx with SMOTE) can achieve high diagnostic accuracy (77% recall) with rigorous privacy guarantees (ε=9.0), making it feasible for real-world heterogeneous medical applications.

Abstract: Federated Learning (FL) presents a groundbreaking approach for collaborative
health research, allowing model training on decentralized data while
safeguarding patient privacy. FL offers formal security guarantees when
combined with Differential Privacy (DP). The integration of these technologies,
however, introduces a significant trade-off between privacy and clinical
utility, a challenge further complicated by the severe class imbalance often
present in medical datasets. The research presented herein addresses these
interconnected issues through a systematic, multi-stage analysis. An FL
framework was implemented for cardiovascular risk prediction, where initial
experiments showed that standard methods struggled with imbalanced data,
resulting in a recall of zero. To overcome such a limitation, we first
integrated the hybrid Synthetic Minority Over-sampling Technique with Tomek
Links (SMOTETomek) at the client level, successfully developing a clinically
useful model. Subsequently, the framework was optimized for non-IID data using
a tuned FedProx algorithm. Our final results reveal a clear, non-linear
trade-off between the privacy budget (epsilon) and model recall, with the
optimized FedProx consistently out-performing standard FedAvg. An optimal
operational region was identified on the privacy-utility frontier, where strong
privacy guarantees (with epsilon 9.0) can be achieved while maintaining high
clinical utility (recall greater than 77%). Ultimately, our study provides a
practical methodological blueprint for creating effective, secure, and accurate
diagnostic tools that can be applied to real-world, heterogeneous healthcare
data.

</details>


### [2] [A Comparative Performance Evaluation of Kyber, sntrup761, and FrodoKEM for Post-Quantum Cryptography](https://arxiv.org/abs/2508.10023)
*Samet Ünsal*

Main category: cs.CR

TL;DR: This paper compares leading post-quantum cryptographic algorithms (Kyber, sntrup761, FrodoKEM) in terms of security, performance, and real-world applicability, while discussing challenges in transitioning to post-quantum systems.


<details>
  <summary>Details</summary>
Motivation: Post-quantum cryptography is needed to ensure security against quantum computing attacks, prompting a need to evaluate and compare emerging algorithms for practical adoption.

Method: The authors conduct a comparative review of post-quantum cryptographic algorithms, analyzing their security, performance, and real-world applicability through theoretical insights and practical considerations.

Result: The paper identifies strengths and weaknesses of Kyber, sntrup761, and FrodoKEM, providing actionable insights for their implementation and highlighting industry-specific transition challenges.

Conclusion: The review establishes a foundation for understanding the current state of post-quantum cryptography, emphasizes research directions for improving algorithms, and underscores the urgency of addressing transition challenges to prepare for the quantum computing era.

Abstract: Post-quantum cryptography (PQC) aims to develop cryptographic algorithms that
are secure against attacks from quantum computers. This paper compares the
leading postquantum cryptographic algorithms, such as Kyber, sntrup761, and
FrodoKEM, in terms of their security, performance, and real-world
applicability. The review highlights the strengths and weaknesses of each
algorithm and provides insights into future research directions. We also
discuss the challenges of transitioning from classical to post-quantum systems
and the potential impacts on various industries. This paper serves as a
foundation for understanding the current state of post-quantum cryptography and
its future prospects in the quantum computing era.

</details>


### [3] [Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs](https://arxiv.org/abs/2508.10031)
*Jinhwa Kim,Ian G. Harris*

Main category: cs.CR

TL;DR: This paper introduces a Context Filtering model as an input pre-processing method to defend against jailbreak attacks on Large Language Models (LLMs) by filtering harmful context while preserving LLMs' helpfulness, achieving up to 88% attack success rate reduction without performance trade-offs.


<details>
  <summary>Details</summary>
Motivation: LLMs face increasing safety and ethical risks due to jailbreak attacks that exploit adversarial contexts to elicit harmful responses. Current mitigation strategies often compromise the helpfulness of LLMs for legitimate users, necessitating a defense mechanism that maintains effectiveness without sacrificing model utility.

Method: The Context Filtering model employs input pre-processing to: 1) identify trustworthy primary prompts containing true user intent, and 2) remove untrustworthy/reliable context. It is model-agnostic (works with both white-box and black-box LLMs) and requires no fine-tuning of the LLM itself.

Result: Evaluations across six jailbreak attack types show the method reduces attack success rates by 88% while maintaining baseline model helpfulness, setting new state-of-the-art results in Safety and Helpfulness Product metrics.

Conclusion: The proposed Context Filtering model demonstrates universal applicability (plug-and-play) and strong performance in defending against jailbreak attacks without performance degradation, with plans for open-source release to enable further research and adoption.

Abstract: While Large Language Models (LLMs) have shown significant advancements in
performance, various jailbreak attacks have posed growing safety and ethical
risks. Malicious users often exploit adversarial context to deceive LLMs,
prompting them to generate responses to harmful queries. In this study, we
propose a new defense mechanism called Context Filtering model, an input
pre-processing method designed to filter out untrustworthy and unreliable
context while identifying the primary prompts containing the real user intent
to uncover concealed malicious intent. Given that enhancing the safety of LLMs
often compromises their helpfulness, potentially affecting the experience of
benign users, our method aims to improve the safety of the LLMs while
preserving their original performance. We evaluate the effectiveness of our
model in defending against jailbreak attacks through comparative analysis,
comparing our approach with state-of-the-art defense mechanisms against six
different attacks and assessing the helpfulness of LLMs under these defenses.
Our model demonstrates its ability to reduce the Attack Success Rates of
jailbreak attacks by up to 88% while maintaining the original LLMs'
performance, achieving state-of-the-art Safety and Helpfulness Product results.
Notably, our model is a plug-and-play method that can be applied to all LLMs,
including both white-box and black-box models, to enhance their safety without
requiring any fine-tuning of the models themselves. We will make our model
publicly available for research purposes.

</details>


### [4] [Cognitive Cybersecurity for Artificial Intelligence: Guardrail Engineering with CCS-7](https://arxiv.org/abs/2508.10033)
*Yuksel Aydin*

Main category: cs.CR

TL;DR: This paper introduces CCS-7 (Cognitive Cybersecurity Suite), a taxonomy of seven human-like cognitive vulnerabilities in language models that traditional alignment methods fail to address. It demonstrates that architecture-dependent interventions (e.g., 'Think First, Verify Always') mitigate some vulnerabilities but exacerbate others (e.g., source interference errors increasing by 135% in certain models); meanwhile, human users show consistent moderate improvement in cognitive security practices.


<details>
  <summary>Details</summary>
Motivation: Language models exhibit human-like cognitive vulnerabilities (e.g., emotional framing) that escape traditional behavioral alignment, raising concerns about their reliability and security in critical applications.

Method: 1) A randomized controlled trial with 151 participants testing a 'Think First, Verify Always' (TFVA) lesson for human cognitive security. 2) 12,180 experiments evaluating TFVA-style guardrails across seven diverse language model architectures.

Result: Architecture-dependent risk patterns emerged: identity confusion was almost fully mitigated in models but source interference experienced backfire (135% error rate increase). Humans showed consistent moderate improvement (+7.9%) in cognitive security regardless of trial architecture.

Conclusion: Cognitive safety is a model-specific engineering problem, requiring architecture-aware testing before deployment due to the significant variability in intervention effectiveness. This highlights the need for tailored cybersecurity strategies for different LLM architectures.

Abstract: Language models exhibit human-like cognitive vulnerabilities, such as
emotional framing, that escape traditional behavioral alignment. We present
CCS-7 (Cognitive Cybersecurity Suite), a taxonomy of seven vulnerabilities
grounded in human cognitive security research. To establish a human benchmark,
we ran a randomized controlled trial with 151 participants: a "Think First,
Verify Always" (TFVA) lesson improved cognitive security by +7.9% overall. We
then evaluated TFVA-style guardrails across 12,180 experiments on seven diverse
language model architectures. Results reveal architecture-dependent risk
patterns: some vulnerabilities (e.g., identity confusion) are almost fully
mitigated, while others (e.g., source interference) exhibit escalating
backfire, with error rates increasing by up to 135% in certain models. Humans,
in contrast, show consistent moderate improvement. These findings reframe
cognitive safety as a model-specific engineering problem: interventions
effective in one architecture may fail, or actively harm, another, underscoring
the need for architecture-aware cognitive safety testing before deployment.

</details>


### [5] [Neural Network-Based Detection and Multi-Class Classification of FDI Attacks in Smart Grid Home Energy Systems](https://arxiv.org/abs/2508.10035)
*Varsha Sen,Biswash Basnet*

Main category: cs.CR

TL;DR: A machine learning framework using ANN for real-time detection and Bidirectional LSTM for classification of False Data Injection Attacks (FDIAs) in smart grid Home Area Networks (HANs).


<details>
  <summary>Details</summary>
Motivation: FDIAs threaten smart grid HANs due to weak security controls, enabling data manipulation and grid vulnerabilities. Attackers exploit these to alter demand patterns without triggering alarms, necessitating robust detection solutions.

Method: 1) Lightweight Artificial Neural Network (ANN) for real-time FDI detection using energy consumption, cost, and time context features. 2) Bidirectional LSTM for attack classification (normal, trapezoidal, sigmoid). 3) Synthetic time-series household behavior dataset for training/evaluation.

Result: Proposed models demonstrated effectiveness in FDI detection/classification through experiments, providing scalable edge-based grid protection solutions with strong attack shape recognition capabilities.

Conclusion: This work advances smart grid cybersecurity by establishing intelligent data-driven defense mechanisms at residential endpoints, improving grid resilience through edge computing with dual ML model architectures.

Abstract: False Data Injection Attacks (FDIAs) pose a significant threat to smart grid
infrastructures, particularly Home Area Networks (HANs), where real-time
monitoring and control are highly adopted. Owing to the comparatively less
stringent security controls and widespread availability of HANs, attackers view
them as an attractive entry point to manipulate aggregated demand patterns,
which can ultimately propagate and disrupt broader grid operations. These
attacks undermine the integrity of smart meter data, enabling malicious actors
to manipulate consumption values without activating conventional alarms,
thereby creating serious vulnerabilities across both residential and
utility-scale infrastructures. This paper presents a machine learning-based
framework for both the detection and classification of FDIAs using residential
energy data. A real-time detection is provided by the lightweight Artificial
Neural Network (ANN), which works by using the most vital features of energy
consumption, cost, and time context. For the classification of different attack
types, a Bidirectional LSTM is trained to recognize normal, trapezoidal, and
sigmoid attack shapes through learning sequential dependencies in the data. A
synthetic time-series dataset was generated to emulate realistic household
behaviour. Experimental results demonstrate that the proposed models are
effective in identifying and classifying FDIAs, offering a scalable solution
for enhancing grid resilience at the edge. This work contributes toward
building intelligent, data-driven defence mechanisms that strengthen smart grid
cybersecurity from residential endpoints.

</details>


### [6] [Certifiably robust malware detectors by design](https://arxiv.org/abs/2508.10038)
*Pierre-Francois Gimenez,Sarath Sivaprasad,Mario Fritz*

Main category: cs.CR

TL;DR: This paper introduces ERDALT, a robust malware detection framework using a novel architecture that resists adversarial examples with minimal impact on detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Static malware analysis via machine learning lacks robustness against adversarial examples, where minor code changes evade detectors without altering malicious behavior, unlike robustness in computer vision.

Method: Proposes a certifiably robust model structure for malware detection, decomposing robust detectors into a specific architecture that inherently prevents evasion through adversarial transformations.

Result: ERDALT achieves robust detection against adversarial examples while maintaining competitiveness with existing machine learning methods in terms of accuracy.

Conclusion: The designed architecture enables intrinsically robust static malware analysis, offering a defense framework that preserves high detection performance even with adversarial attacks.

Abstract: Malware analysis involves analyzing suspicious software to detect malicious
payloads. Static malware analysis, which does not require software execution,
relies increasingly on machine learning techniques to achieve scalability.
Although such techniques obtain very high detection accuracy, they can be
easily evaded with adversarial examples where a few modifications of the sample
can dupe the detector without modifying the behavior of the software. Unlike
other domains, such as computer vision, creating an adversarial example of
malware without altering its functionality requires specific transformations.
We propose a new model architecture for certifiably robust malware detection by
design. In addition, we show that every robust detector can be decomposed into
a specific structure, which can be applied to learn empirically robust malware
detectors, even on fragile features. Our framework ERDALT is based on this
structure. We compare and validate these approaches with machine-learning-based
malware detection methods, allowing for robust detection with limited reduction
of detection performance.

</details>


### [7] [Multi-task Adversarial Attacks against Black-box Model with Few-shot Queries](https://arxiv.org/abs/2508.10039)
*Wenqiang Wang,Yan Xiao,Hao Lin,Yangshijie Zhang,Xiaochun Cao*

Main category: cs.CR

TL;DR: CEMA is a black-box adversarial text attack method that leverages cross-task transferability to efficiently compromise multi-task models, commercial APIs, and image-generation systems with minimal queries.


<details>
  <summary>Details</summary>
Motivation: Existing multi-task adversarial text attacks require access to shared internal features and many queries, making them ineffective for real-world black-box scenarios with limited API access or diverse task types.

Method: CEMA uses a deep-level substitute model trained in a plug-and-play manner for text classification, enabling attacks without replicating the victim model. It generates adversarial candidates via multiple classification methods and selects the most effective one.

Result: CEMA achieved significant attack success on multi-task models with 2-6 tasks (classification, translation, summarization, image generation) using just 100 queries. It also worked against commercial APIs (Baidu, Google Translate) and models like ChatGPT 4o and Stable Diffusion V2.

Conclusion: CEMA demonstrates practical versatility for real-world adversarial attacks across diverse multi-task systems, improving efficiency and query usage in constrained black-box environments.

Abstract: Current multi-task adversarial text attacks rely on abundant access to shared
internal features and numerous queries, often limited to a single task type. As
a result, these attacks are less effective against practical scenarios
involving black-box feedback APIs, limited queries, or multiple task types. To
bridge this gap, we propose \textbf{C}luster and \textbf{E}nsemble
\textbf{M}ulti-task Text Adversarial \textbf{A}ttack (\textbf{CEMA}), an
effective black-box attack that exploits the transferability of adversarial
texts across different tasks. CEMA simplifies complex multi-task scenarios by
using a \textit{deep-level substitute model} trained in a
\textit{plug-and-play} manner for text classification, enabling attacks without
mimicking the victim model. This approach requires only a few queries for
training, converting multi-task attacks into classification attacks and
allowing attacks across various tasks.
  CEMA generates multiple adversarial candidates using different text
classification methods and selects the one that most effectively attacks
substitute models.
  In experiments involving multi-task models with two, three, or six
tasks--spanning classification, translation, summarization, and text-to-image
generation--CEMA demonstrates significant attack success with as few as 100
queries. Furthermore, CEMA can target commercial APIs (e.g., Baidu and Google
Translate), large language models (e.g., ChatGPT 4o), and image-generation
models (e.g., Stable Diffusion V2), showcasing its versatility and
effectiveness in real-world applications.

</details>


### [8] [Quantum Prime Factorization: A Novel Approach Based on Fermat Method](https://arxiv.org/abs/2508.10041)
*Julien Mellaerts*

Main category: cs.CR

TL;DR: The paper presents a quantum algorithm improving Fermat factorization, reducing computational complexity fourfold and successfully factorizing 8,689,739 with a quantum device.


<details>
  <summary>Details</summary>
Motivation: This work addresses factorization of composite odd numbers, crucial for computational mathematics and cryptography, by combining classical optimization with quantum computing to achieve better efficiency.

Method: 1) A classical enhancement of Fermat's factorization method reducing complexity by a factor of four. 2) Reformulation of Fermat factorization as a quadratic unconstrained binary optimization (QUBO) problem for execution on quantum annealers.

Result: 1) Fourfold reduction in computational steps for classical Fermat factorization. 2) Factorized the largest number (8,689,739) using a quantum annealer to date.

Conclusion: The proposed method demonstrates significant improvements in classical and quantum factorization approaches, establishing proof-of-concept for quantum annealers in solving large-scale factorization problems.

Abstract: In this paper, we introduce a novel quantum algorithm for the factorization
of composite odd numbers. This work makes two significant contributions. First,
we present a new improvement to the classical Fermat method, fourfold reducing
the computational complexity of factoring. Second, we reformulate Fermat
factorization method as an optimization problem suitable for Quantum Annealers
which allowed us to factorize 8,689,739, the biggest number ever factorized
using a quantum device to our knowledge.

</details>


### [9] [FIDELIS: Blockchain-Enabled Protection Against Poisoning Attacks in Federated Learning](https://arxiv.org/abs/2508.10042)
*Jane Carney,Kushal Upreti,Gaby G. Dagher,Tim Andersen*

Main category: cs.CR

TL;DR: This paper proposes a blockchain-enabled framework (\Sys) for detecting data poisoning attacks in federated learning by decentralizing the server role and using a consensus-based judge model.


<details>
  <summary>Details</summary>
Motivation: Federated learning faces data poisoning risks and lacks standardized detection mechanisms with trust assumptions.

Method: Develops a decentralized framework using blockchain to distribute server responsibilities, generates a judge model through client consensus for poison detection.

Result: Implementation demonstrates robustness against poisoning attacks and scalable judge model creation.

Conclusion: \Sys provides a novel, trust-minimized approach to poisoning detection in federated learning through blockchain-assisted decentralization and consensus mechanisms.

Abstract: Federated learning enhances traditional deep learning by enabling the joint
training of a model with the use of IoT device's private data. It ensures
privacy for clients, but is susceptible to data poisoning attacks during
training that degrade model performance and integrity. Current poisoning
detection methods in federated learning lack a standardized detection method or
take significant liberties with trust. In this paper, we present \Sys, a novel
blockchain-enabled poison detection framework in federated learning. The
framework decentralizes the role of the global server across participating
clients. We introduce a judge model used to detect data poisoning in model
updates. The judge model is produced by each client and verified to reach
consensus on a single judge model. We implement our solution to show \Sys is
robust against data poisoning attacks and the creation of our judge model is
scalable.

</details>


### [10] [Securing Agentic AI: Threat Modeling and Risk Analysis for Network Monitoring Agentic AI System](https://arxiv.org/abs/2508.10043)
*Pallavi Zambare,Venkata Nikhil Thanikella,Ying Liu*

Main category: cs.CR

TL;DR: This paper introduces the MAESTRO framework, a seven-layer threat modeling architecture, to address security vulnerabilities in agentic AI systems combining Large Language Models (LLMs) with autonomous agents. They demonstrate two practical threats (resource denial-of-service and memory poisoning) and propose a multilayered defense strategy, including memory isolation and real-time anomaly detection, to ensure system reliability in adversarial settings.


<details>
  <summary>Details</summary>
Motivation: Combining LLMs with autonomous agents in network monitoring and decision-making systems introduces serious security risks, such as vulnerabilities to denial-of-service attacks and memory manipulation, which can degrade system performance and reliability.

Method: The researchers designed and implemented a prototype agent system using Python, LangChain, and WebSocket telemetry, incorporating MAESTRO's seven-layer architecture with modules for inference, memory management, parameter tuning, and anomaly detection. They conducted experiments to simulate and analyze two threat scenarios.

Result: Two threat cases were experimentally validated: (i) traffic replay denial-of-service caused resource exhaustion and delayed telemetry updates, and (ii) memory poisoning through log tampering led to increased computational loads. These demonstrated measurable performance degradation in system adaptations.

Conclusion: MAESTRO proves viable for operational threat mapping, risk scoring, and resilient system design. The authors emphasize critical security measures like memory integrity enforcement, adaptation logic monitoring, and cross-layer communication protection to ensure agentic AI reliability under adversarial conditions.

Abstract: When combining Large Language Models (LLMs) with autonomous agents, used in
network monitoring and decision-making systems, this will create serious
security issues. In this research, the MAESTRO framework consisting of the
seven layers threat modeling architecture in the system was used to expose,
evaluate, and eliminate vulnerabilities of agentic AI. The prototype agent
system was constructed and implemented, using Python, LangChain, and telemetry
in WebSockets, and deployed with inference, memory, parameter tuning, and
anomaly detection modules. Two practical threat cases were confirmed as
follows: (i) resource denial of service by traffic replay denial-of-service,
and (ii) memory poisoning by tampering with the historical log file maintained
by the agent. These situations resulted in measurable levels of performance
degradation, i.e. telemetry updates were delayed, and computational loads were
increased, as a result of poor system adaptations. It was suggested to use a
multilayered defense-in-depth approach with memory isolation, validation of
planners and anomaly response systems in real-time. These findings verify that
MAESTRO is viable in operational threat mapping, prospective risk scoring, and
the basis of the resilient system design. The authors bring attention to the
importance of the enforcement of memory integrity, paying attention to the
adaptation logic monitoring, and cross-layer communication protection that
guarantee the agentic AI reliability in adversarial settings.

</details>


### [11] [Generative AI for Cybersecurity of Energy Management Systems: Methods, Challenges, and Future Directions](https://arxiv.org/abs/2508.10044)
*Aydin Zaboli,Junho Hong*

Main category: cs.CR

TL;DR: The paper proposes a novel GenAI-based security framework for EMSs (Structure: Multi-point attack model + SoM-GI method), validated on IEEE 14-Bus system to address dynamic cybersecurity challenges and system issues through multimodal analysis.


<details>
  <summary>Details</summary>
Motivation: Modern EMSs face evolving cyber threats requiring adaptive security solutions that address interconnected vulnerabilities across data processing pipelines (SCADA, RTDB, HMI) through multidimensional analysis.

Method: 1) Comprehensive multi-point attack/error model for vulnerability identification 2) GenAI-based anomaly detection systems (ADSs) 3) Set-of-mark generative intelligence (SoM-GI) framework combining visual markers, linguistic rules, and numerical segmentation for HMI analysis.

Result: Framework demonstrated effectiveness across scenarios in the IEEE 14-Bus system, with visual analysis successfully identifying inconsistencies not detected by numerical methods alone.

Conclusion: The integrated approach combining numerical analysis, visual pattern recognition, and linguistic rules provides robust protection against evolving cyber-physical threats in EMS environments through enhanced spatial reasoning and multimodal threat detection.

Abstract: This paper elaborates on an extensive security framework specifically
designed for energy management systems (EMSs), which effectively tackles the
dynamic environment of cybersecurity vulnerabilities and/or system problems
(SPs), accomplished through the incorporation of novel methodologies. A
comprehensive multi-point attack/error model is initially proposed to
systematically identify vulnerabilities throughout the entire EMS data
processing pipeline, including post state estimation (SE) stealth attacks, EMS
database manipulation, and human-machine interface (HMI) display corruption
according to the real-time database (RTDB) storage. This framework acknowledges
the interconnected nature of modern attack vectors, which utilize various
phases of supervisory control and data acquisition (SCADA) data flow. Then,
generative AI (GenAI)-based anomaly detection systems (ADSs) for EMSs are
proposed for the first time in the power system domain to handle the scenarios.
Further, a set-of-mark generative intelligence (SoM-GI) framework, which
leverages multimodal analysis by integrating visual markers with rules
considering the GenAI capabilities, is suggested to overcome inherent spatial
reasoning limitations. The SoM-GI methodology employs systematic visual
indicators to enable accurate interpretation of segmented HMI displays and
detect visual anomalies that numerical methods fail to identify. Validation on
the IEEE 14-Bus system shows the framework's effectiveness across scenarios,
while visual analysis identifies inconsistencies. This integrated approach
combines numerical analysis with visual pattern recognition and linguistic
rules to protect against cyber threats and system errors.

</details>


### [12] [NetMoniAI: An Agentic AI Framework for Network Security & Monitoring](https://arxiv.org/abs/2508.10052)
*Pallavi Zambare,Venkata Nikhil Thanikella,Nikhil Padmanabh Kottur,Sree Akhil Akula,Ying Liu*

Main category: cs.CR

TL;DR: NetMoniAI is a two-tier agentic AI framework combining decentralized micro-agents for local network monitoring and a central controller for coordinating analysis, demonstrated to scale efficiently under resource constraints while maintaining accuracy. The framework is open-sourced for reproducibility.


<details>
  <summary>Details</summary>
Motivation: Address limitations of centralized network security systems in resource-constrained environments by enabling scalable, low-redundancy monitoring through decentralized-anomaly detection with centralized coordination for identifying coordinated attacks.

Method: Developed a framework with autonomous micro-agents at each network node for local traffic analysis and anomaly detection, combined with a central controller that aggregates node-level insights to detect multi-node attacks and maintain situational awareness.

Result: Evaluations on micro-testbeds and NS-3 simulations show improved scalability, reduced coordination overhead and redundancy, faster response times, and preserved accuracy rates in detecting network anomalies and attacks.

Conclusion: NetMoniAI provides an effective architecture for resource-efficient network monitoring that maintains security effectiveness while enabling customization through open-source availability for diverse network environments.

Abstract: In this paper, we present NetMoniAI, an agentic AI framework for automatic
network monitoring and security that integrates decentralized analysis with
lightweight centralized coordination. The framework consists of two layers:
autonomous micro-agents at each node perform local traffic analysis and anomaly
detection. A central controller then aggregates insights across nodes to detect
coordinated attacks and maintain system-wide situational awareness. We
evaluated NetMoniAI on a local micro-testbed and through NS-3 simulations.
Results confirm that the two-tier agentic-AI design scales under resource
constraints, reduces redundancy, and improves response time without
compromising accuracy. To facilitate broader adoption and reproducibility, the
complete framework is available as open source. This enables researchers and
practitioners to replicate, validate, and extend it across diverse network
environments and threat scenarios. Github link:
https://github.com/pzambare3/NetMoniAI

</details>


### [13] [Invisible Watermarks, Visible Gains: Steering Machine Unlearning with Bi-Level Watermarking Design](https://arxiv.org/abs/2508.10065)
*Yuhao Sun,Yihua Zhang,Gaowen Liu,Hongtao Xie,Sijia Liu*

Main category: cs.CR

TL;DR: This paper introduces Water4MU, a digital watermarking-based approach for machine unlearning. It uses bi-level optimization to enhance unlearning effectiveness in image classification and generation tasks, particularly outperforming existing methods in 'challenging forgets' scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing machine unlearning algorithms focus on in-training weight adjustments while neglecting data-level modifications. This limits their effectiveness, motivating a need for novel data-centric unlearning approaches.

Method: Water4MU employs a bi-level optimization (BLO) framework where the upper-level optimizes a watermarking network to simplify unlearning by altering data content, while the lower-level independently trains the ML model using modified data.

Result: Water4MU achieves effective unlearning in challenging scenarios across image classification and generation tasks. Notably, it outperforms state-of-the-art methods in removing data influences (termed 'challenging forgets').

Conclusion: The integration of digital watermarking with bi-level optimization significantly improves machine unlearning efficiency, particularly in complex unlearning cases through data-level control.

Abstract: With the increasing demand for the right to be forgotten, machine unlearning
(MU) has emerged as a vital tool for enhancing trust and regulatory compliance
by enabling the removal of sensitive data influences from machine learning (ML)
models. However, most MU algorithms primarily rely on in-training methods to
adjust model weights, with limited exploration of the benefits that data-level
adjustments could bring to the unlearning process. To address this gap, we
propose a novel approach that leverages digital watermarking to facilitate MU
by strategically modifying data content. By integrating watermarking, we
establish a controlled unlearning mechanism that enables precise removal of
specified data while maintaining model utility for unrelated tasks. We first
examine the impact of watermarked data on MU, finding that MU effectively
generalizes to watermarked data. Building on this, we introduce an
unlearning-friendly watermarking framework, termed Water4MU, to enhance
unlearning effectiveness. The core of Water4MU is a bi-level optimization (BLO)
framework: at the upper level, the watermarking network is optimized to
minimize unlearning difficulty, while at the lower level, the model itself is
trained independently of watermarking. Experimental results demonstrate that
Water4MU is effective in MU across both image classification and image
generation tasks. Notably, it outperforms existing methods in challenging MU
scenarios, known as "challenging forgets".

</details>


### [14] [An Architecture for Distributed Digital Identities in the Physical World](https://arxiv.org/abs/2508.10185)
*René Mayrhofer,Michael Roland,Tobias Höller,Philipp Hofer,Mario Lins*

Main category: cs.CR

TL;DR: This paper proposes a decentralized digital identity architecture for physical transactions, introducing a Personal Identity Agent (PIA) to enhance security and privacy. Protocols and formal verification demonstrate feasibility against strong adversaries.


<details>
  <summary>Details</summary>
Motivation: Centralized identity providers create single points of failure and privacy risks, making them vulnerable to global attacks. Decentralized solutions are needed for secure and available physical service transactions.

Method: The architecture combines sensors, identity authorities, attribute verifiers, and PIAs. A protocol was designed for decentralized transactions, with formal verification against a realistic threat model including global adversaries.

Result: Formal verification confirms the protocol's security properties. A proof-of-concept implementation shows practical feasibility for applications tolerating ~second latency.

Conclusion: The proposed PIA-based architecture provides a privacy-preserving, decentralized framework for physical transactions, addressing vulnerabilities of centralized systems while maintaining practical implementation viability.

Abstract: Digital identities are increasingly important for mediating not only digital
but also physical service transactions. Managing such identities through
centralized providers can cause both availability and privacy concerns: single
points of failure and control are ideal targets for global attacks on
technical, organizational, or legal fronts. We design, analyze, and build a
distributed digital identity architecture for physical world transactions in
common scenarios like unlocking doors, public transport, or crossing country
borders. This architecture combines (biometric and other) sensors, (established
and upcoming) identity authorities, attribute verifiers, and a new core
component we call the \emph{Personal Identity Agent (PIA)} that represents
individuals with their identity attributes in the digital domain. All
transactions are conducted in a completely decentralized manner, and the
components for which we currently assume central coordination are optional and
only used for assisting with service discovery and latency reduction. We
present a first protocol between these parties and formally verify that it
achieves relevant security properties based on a realistic threat model
including strong global adversaries. A proof-of-concept implementation
demonstrates practical feasibility of both architecture and initial protocol
for applications that can tolerate end-to-end latencies in the range of a few
seconds.

</details>


### [15] [Detecting Untargeted Attacks and Mitigating Unreliable Updates in Federated Learning for Underground Mining Operations](https://arxiv.org/abs/2508.10212)
*Md Sazedur Rahman,Mohamed Elmahallawy,Sanjay Madria,Samuel Frimpong*

Main category: cs.CR

TL;DR: MineDetect is a federated learning framework for underground mining that addresses privacy risks, attack vulnerabilities, and low-quality sensor data by isolating malicious models and mitigating adversarial influences from unreliable clients.


<details>
  <summary>Details</summary>
Motivation: Underground mining operations face privacy risks from centralized sensor data processing, while federated learning is challenged by adversarial attacks and data quality issues in harsh mining environments.

Method: MineDetect introduces a history-aware gradient analysis mechanism to detect attacked models, and a dual-robustness mitigation strategy that identifies and neutralizes adversarial influences from both malicious clients and unreliable clients with poor-quality sensor data.

Result: Evaluations show MineDetect achieves superior robustness and accuracy compared to existing methods, particularly in non-IID data scenarios, while maintaining computational efficiency necessary for real-time hazard detection.

Conclusion: MineDetect advances federated learning for safety-critical mining applications by effectively defending against attacks and low-quality data challenges, enabling secure collaborative model training for improved hazard detection and operational decisions.

Abstract: Underground mining operations rely on distributed sensor networks to collect
critical data daily, including mine temperature, toxic gas concentrations, and
miner movements for hazard detection and operational decision-making. However,
transmitting raw sensor data to a central server for training deep learning
models introduces significant privacy risks, potentially exposing sensitive
mine-specific information. Federated Learning (FL) offers a transformative
solution by enabling collaborative model training while ensuring that raw data
remains localized at each mine. Despite its advantages, FL in underground
mining faces key challenges: (i) An attacker may compromise a mine's local
model by employing techniques such as sign-flipping attacks or additive noise,
leading to erroneous predictions; (ii) Low-quality (yet potentially valuable)
data, caused by poor lighting conditions or sensor inaccuracies in mines may
degrade the FL training process. In response, this paper proposes MineDetect, a
defense FL framework that detects and isolates the attacked models while
mitigating the impact of mines with low-quality data. MineDetect introduces two
key innovations: (i) Detecting attacked models (maliciously manipulated) by
developing a history-aware mechanism that leverages local and global averages
of gradient updates; (ii) Identifying and eliminating adversarial influences
from unreliable models (generated by clients with poor data quality) on the FL
training process. Comprehensive simulations across diverse datasets demonstrate
that MineDetect outperforms existing methods in both robustness and accuracy,
even in challenging non-IID data scenarios. Its ability to counter adversarial
influences while maintaining lower computational efficiency makes it a vital
advancement for improving safety and operational effectiveness in underground
mining.

</details>


### [16] [BERTector: Intrusion Detection Based on Joint-Dataset Learning](https://arxiv.org/abs/2508.10327)
*Haoyang Hu,Xun Huang,Chenyu Wu,Shiwen Liu,Zhichao Lian,Shuangquan Zhang*

Main category: cs.CR

TL;DR: This paper introduces BERTector, a BERT-based intrusion detection system that achieves state-of-the-art results through joint-dataset training, traffic-aware tokenization (NSS-Tokenizer), hybrid dataset fine-tuning, and low-rank adaptation (LoRA).


<details>
  <summary>Details</summary>
Motivation: Current IDS face limitations due to network traffic heterogeneity and diverse attack patterns, requiring better generalization and robustness.

Method: BERTector combines three innovations: 1) Traffic-aware semantic tokenization (NSS-Tokenizer), 2) Supervised training on a hybrid dataset integrating multiple sources, and 3) LoRA for efficient parameter adaptation during training.

Result: Experiments demonstrate BERTector achieves 1) Leading detection accuracy, 2) Strong cross-dataset generalization, and 3) High robustness against adversarial perturbations.

Conclusion: BERTector establishes a unified, efficient framework for modern IDS with enhanced performance in complex/dynamic network environments through its joint-dataset approach and BERT-based architecture.

Abstract: Intrusion detection systems (IDS) are facing challenges in generalization and
robustness due to the heterogeneity of network traffic and the diversity of
attack patterns. To address this issue, we propose a new joint-dataset training
paradigm for IDS and propose a scalable BERTector framework based on BERT.
BERTector integrates three key components: NSS-Tokenizer for traffic-aware
semantic tokenization, supervised fine-tuning with a hybrid dataset, and
low-rank adaptation (LoRA) for efficient training. Extensive experiments show
that BERTector achieves state-of-the-art detection accuracy, strong
cross-dataset generalization capabilities, and excellent robustness to
adversarial perturbations. This work establishes a unified and efficient
solution for modern IDS in complex and dynamic network environments.

</details>


### [17] [Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based Side-Channel Attacks on Fully Associative Randomized Caches](https://arxiv.org/abs/2508.10431)
*Chris Cao,Gururaj Saileshwar*

Main category: cs.CR

TL;DR: This paper refutes prior claims of AES key recovery from MIRAGE's cache by identifying a simulation flaw in the attack model, demonstrating that correct randomization prevents the reported vulnerability.


<details>
  <summary>Details</summary>
Motivation: To address and validate recent claims that MIRAGE's randomized cache leaks AES keys through occupancy-based attacks, which is critical for assessing the security of the cache architecture.

Method: The authors simulate MIRAGE with a constant seed for eviction randomization (as in the prior attack) versus random seeds per execution to evaluate how seed choice affects the feasibility of occupancy-based key recovery.

Result: Under realistic random seed initialization, the timing correlation between AES T-table accesses and attacker runtime in the MIRAGE cache vanishes, causing the claimed attack to fail.

Conclusion: The success of the prior attack relies on a flawed and non-realistic simulation setup; properly modeling MIRAGE's random eviction sequences eliminates the vulnerability.

Abstract: Recent work presented at USENIX Security 2025 claims that occupancy-based
attacks can recover AES keys from the MIRAGE randomized cache. In this paper,
we examine these claims and find that they arise from fundamental modeling
flaws. Most critically, the authors' simulation of MIRAGE uses a constant seed
to initialize the random number generator used for global evictions in MIRAGE,
causing every AES encryption they trace to evict the same deterministic
sequence of cache lines. This artificially creates a highly repeatable timing
pattern that is not representative of a realistic implementation of MIRAGE,
where eviction sequences vary randomly between encryptions. When we instead
randomize the eviction seed for each run, reflecting realistic operation, the
correlation between AES T-table accesses and attacker runtimes disappears, and
the attack fails. These findings show that the reported leakage is an artifact
of incorrect modeling, and not an actual vulnerability in MIRAGE.

</details>


### [18] [AlDBaran: Towards Blazingly Fast State Commitments for Blockchains](https://arxiv.org/abs/2508.10493)
*Bernhard Kauer,Aleksandr Petrosyan,Benjamin Livshits*

Main category: cs.CR

TL;DR: AlDBaran is an authenticated database for blockchain systems that eliminates disk I/O from the critical path through prefetching strategies and Merkle tree optimizations, enabling high throughput of 50 Gbps (48M updates/sec) and outperforming existing solutions for resource-constrained environments.


<details>
  <summary>Details</summary>
Motivation: Contemporary blockchain systems require authenticated databases capable of handling multi-million TPS, but most fail to meet these demands. Traditional Merkle trees with disk I/O create bottlenecks, limiting scalability and state commitment efficiency.

Method: The system achieves performance gains by: 1) Decoupling disk I/O from the critical path via prefetching strategies 2) Optimizing Merkle tree update mechanisms 3) Modular isolation of historical data components that can be selectively deactivated for performance enhancements.

Result: AlDBaran processes 48M updates/sec in identical hardware configurations compared to existing systems, and 8M updates/sec on portable hardware in memory with 5M updates/sec for snapshot-based operations. It supports 50 Gbps throughput, exceeding all empirically documented blockchain benchmarks.

Conclusion: AlDBaran represents a significant leap in authenticated databases for blockchain, enabling high-throughput state commitments for previously incompatible systems. Its performance optimizations make it particularly valuable for resource-limited environments while maintaining historical state proof capabilities for new applications like light clients and rollups.

Abstract: The fundamental basis for maintaining integrity within contemporary
blockchain systems is provided by authenticated databases. Our analysis
indicates that a significant portion of the approaches applied in this domain
fail to sufficiently meet the stringent requirements of systems processing
transactions at rates of multi-million TPS. AlDBaran signifies a substantial
advancement in authenticated databases. By eliminating disk I/O operations from
the critical path, implementing prefetching strategies, and refining the update
mechanism of the Merkle tree, we have engineered an authenticated data
structure capable of handling state updates efficiently at a network throughput
of 50 Gbps. This throughput capacity significantly surpasses any empirically
documented blockchain throughput, guaranteeing the ability of even the most
high-throughput blockchains to generate state commitments effectively.
  AlDBaran provides support for historical state proofs, which facilitates a
wide array of novel applications. For instance, the deployment of AlDBaran
could enable blockchains that do not currently support state commitments to
offer functionalities for light clients and/or implement rollups.
  When benchmarked against alternative authenticated data structure projects,
AlDBaran exhibits superior performance and simplicity. In particular, AlDBaran
achieves speeds of approximately 48 million updates per second using an
identical machine configuration. This characteristic renders AlDBaran an
attractive solution for resource-limited environments, as its historical data
capabilities can be modularly isolated (and deactivated), which further
enhances performance. On consumer-level portable hardware, it achieves
approximately 8 million updates/s in an in-memory setting and 5 million
updates/s with snapshots at sub-second intervals, illustrating compelling and
cost-effective scalability.

</details>


### [19] [Codes on any Cayley Graph have an Interactive Oracle Proof of Proximity](https://arxiv.org/abs/2508.10510)
*Hugo Delavenne,Louise Lallemand*

Main category: cs.CR

TL;DR: This paper generalizes the flowering IOPP from a specific class of graphs to codes on Cayley graphs, improving practicality while maintaining soundness and enabling codes with constant rate and minimum distance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to expand the applicability of IOPPs beyond prior limitations on code types and field constraints, enabling broader use in zero-knowledge protocols.

Method: The method extends the flowering protocol from [DMR25] by leveraging expansion properties of Cayley graphs, allowing application to codes with symbols indexed on their edges.

Result: The generalized protocol preserves the low soundness parameter with minimal complexity trade-offs, enabling practical speedups while supporting codes of constant rate and minimum distance.

Conclusion: This work advances the state of code-based SNARKs by broadening the class of codes compatible with IOPPs while retaining efficiency and improving error tolerance.

Abstract: Interactive Oracle Proofs of Proximity (IOPP) are at the heart of code-based
SNARKs, a family of zeroknowledge protocols. The first and most famous one is
the FRI protocol [BBHR18a], that efficiently tests proximity to Reed-Solomon
codes. This paper generalizes the flowering IOPP introduced in [DMR25] for some
specific (2, n)-regular Tanner codes to a much broader variety of codes: any
code with symbols indexed on the edges of a Cayley graph. The flowering
protocol of [DMR25] had a soundness parameter much lower than the FRI protocol
[BCI + 23], and complexity parameters that could compete with the FRI
[BBHR18a]. The lower soundness and the absence of restriction on the base field
may lead to other practical speedups, however the codes considered in [DMR25]
have an o(1) minimum distance. The generalization proposed in this paper
preserves the soundness parameter with a slight decrease of the complexity
parameters, while allowing being applied on codes with constant rate and
constant minimum distance thanks to the good expansion properties of some
families of Cayley graphs.

</details>


### [20] [A Transformer-Based Approach for DDoS Attack Detection in IoT Networks](https://arxiv.org/abs/2508.10636)
*Sandipan Dey,Payal Santosh Kate,Vatsala Upadhyay,Abhishek Vaish*

Main category: cs.CR

TL;DR: This paper proposes using Transformer models to detect DDoS attacks on IoT devices, demonstrating superior performance over traditional methods through real-world experiments.


<details>
  <summary>Details</summary>
Motivation: DDoS attacks pose significant threats to IoT networks due to resource constraints, dynamic network nature, attack scalability, protocol diversity (e.g., MQTT, CoAP), and high traffic volume, which challenge traditional detection techniques.

Method: The approach employs Transformer models with self-attention mechanisms to extract and process network traffic features, leveraging their strengths in handling sequential data and capturing complex patterns.

Result: Experiments on a real-world dataset show the Transformer model outperforms traditional ML techniques in accuracy, precision, recall, and F1-score, validating its effectiveness under varied IoT conditions.

Conclusion: Transformers offer a promising solution for scalable, protocol-agnostic DDoS detection in IoT environments and can be practically deployed despite resource limitations and dynamic traffic patterns.

Abstract: DDoS attacks have become a major threat to the security of IoT devices and
can cause severe damage to the network infrastructure. IoT devices suffer from
the inherent problem of resource constraints and are therefore susceptible to
such resource-exhausting attacks. Traditional methods for detecting DDoS
attacks are not efficient enough to cope with the dynamic nature of IoT
networks, as well as the scalability of the attacks, diversity of protocols,
high volume of traffic, and variability in device behavior, and variability of
protocols like MQTT, CoAP, making it hard to implement security across all the
protocols. In this paper, we propose a novel approach, i.e., the use of
Transformer models, which have shown remarkable performance in natural language
processing tasks, for detecting DDoS attacks on IoT devices. The proposed model
extracts features from network traffic data and processes them using a
self-attention mechanism. Experiments conducted on a real-world dataset
demonstrate that the proposed approach outperforms traditional machine learning
techniques, which can be validated by comparing both approaches' accuracy,
precision, recall, and F1-score. The results of this study show that the
Transformer models can be an effective solution for detecting DDoS attacks on
IoT devices and have the potential to be deployed in real-world IoT
environments.

</details>


### [21] [MirGuard: Towards a Robust Provenance-based Intrusion Detection System Against Graph Manipulation Attacks](https://arxiv.org/abs/2508.10639)
*Anyuan Sang,Lu Zhou,Li Yang,Junbo Jia,Huipeng Yang,Pengbin Feng,Jianfeng Ma*

Main category: cs.CR

TL;DR: MirGuard is a robust anomaly detection framework for host systems that addresses graph manipulation attacks in Learning-based Provenance-based Intrusion Detection Systems (PIDSes) by using logic-aware multi-view augmentation and contrastive representation learning, improving robustness without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the vulnerability of Learning-based PIDSes to graph manipulation attacks, which prior work has not effectively solved, thereby limiting their practical applicability in cybersecurity.

Method: MirGuard employs (1) Logic-Aware Noise Injection (LNI) to generate semantically valid graph views preserving causal semantics, and (2) a contrastive learning framework that trains models to be invariant to benign transformations but sensitive to adversarial inconsistencies.

Result: MirGuard outperforms state-of-the-art detectors in robustness against graph manipulation attacks, with no significant trade-off in detection performance or efficiency, as shown through evaluations on multiple provenance datasets.

Conclusion: MirGuard represents the first targeted solution to enhance PIDSes against graph manipulation attacks, offering a robust and effective approach to modern cybersecurity challenges.

Abstract: Learning-based Provenance-based Intrusion Detection Systems (PIDSes) have
become essential tools for anomaly detection in host systems due to their
ability to capture rich contextual and structural information, as well as their
potential to detect unknown attacks. However, recent studies have shown that
these systems are vulnerable to graph manipulation attacks, where attackers
manipulate the graph structure to evade detection. While some previous
approaches have discussed this type of attack, none have fully addressed it
with a robust detection solution, limiting the practical applicability of
PIDSes.
  To address this challenge, we propose MirGuard, a robust anomaly detection
framework that combines logic-aware multi-view augmentation with contrastive
representation learning. Rather than applying arbitrary structural
perturbations, MirGuard introduces Logic-Aware Noise Injection (LNI) to
generate semantically valid graph views, ensuring that all augmentations
preserve the underlying causal semantics of the provenance data. These views
are then used in a Logic-Preserving Contrastive Learning framework, which
encourages the model to learn representations that are invariant to benign
transformations but sensitive to adversarial inconsistencies. Comprehensive
evaluations on multiple provenance datasets demonstrate that MirGuard
significantly outperforms state-of-the-art detectors in robustness against
various graph manipulation attacks without sacrificing detection performance
and efficiency. Our work represents the first targeted study to enhance PIDS
against such adversarial threats, providing a robust and effective solution to
modern cybersecurity challenges.

</details>


### [22] [A Novel Study on Intelligent Methods and Explainable AI for Dynamic Malware Analysis](https://arxiv.org/abs/2508.10652)
*Richa Dasila,Vatsala Upadhyay,Samo Bobek,Abhishek Vaish*

Main category: cs.CR

TL;DR: This paper enhances the interpretability and trustworthiness of deep learning malware detection models using Explainable AI (XAI) techniques. It evaluates multi-layer perceptrons, CNNs, and RNNs in dynamic malware analysis, particularly focusing on Metamorphic Malware classification with a novel comprehensive approach.


<details>
  <summary>Details</summary>
Motivation: While deep learning models effectively detect complex malware patterns, their "black box" nature limits transparency and trust in security decisions, hindering adoption despite performance capabilities. Traditional models lack accountability in high-stakes cybersecurity contexts.

Method: Integrated XAI methods into malware detection by: 1) Applying multi-layer perceptrons (MLPs) to dynamic malware analysis (an underexplored application) 2) Evaluating MLPs, CNNs, RNNs, and CNN-LSTM models through multiple XAI interpretability frameworks 3) Conducting metamorphic malware classification experiments to test model explainability 4) Providing holistic analysis of model decision-making processes.

Result: Demonstrated that XAI integration significantly improves model transparency without compromising detection accuracy, with specific quantitative improvements in feature importance visualization and decision rationale clarity for Metamorphic Malware analysis. Comparative analysis showed unique interpretability advantages across different model architectures.

Conclusion: XAI integration provides a transformative approach to cybersecurity by enabling detailed understanding of deep learning threat detection mechanisms. The novel framework establishes a pathway for verifiable, accountable security AI systems, with implications for both model development and security domain adoption practices.

Abstract: Deep learning models are one of the security strategies, trained on extensive
datasets, and play a critical role in detecting and responding to these threats
by recognizing complex patterns in malicious code. However, the opaque nature
of these models-often described as "black boxes"-makes their decision-making
processes difficult to understand, even for their creators. This research
addresses these challenges by integrating Explainable AI (XAI) techniques to
enhance the interpretability and trustworthiness of malware detection models.
In this research, the use of Multi-Layer Perceptrons (MLP) for dynamic malware
analysis has been considered, a less explored area, and its efficacy in
detecting Metamorphic Malware, and further the effectiveness and transparency
of MLPs, CNNs, RNNs, and CNN-LSTM models in malware classification, evaluating
these models through the lens of Explainable AI (XAI). This comprehensive
approach aims to demystify the internal workings of deep learning models,
promoting a better understanding and trust in their predictive capabilities in
cybersecurity contexts. Such in-depth analysis and implementation haven't been
done to the best of our knowledge.

</details>


### [23] [Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat Intelligence](https://arxiv.org/abs/2508.10677)
*Amine Tellache,Abdelaziz Amara Korba,Amdjed Mokhtari,Horea Moldovan,Yacine Ghamri-Doudane*

Main category: cs.CR

TL;DR: This paper proposes a RAG-based framework using Large Language Models (LLMs) to automate and enhance incident response by dynamically integrating Cyber Threat Intelligence (CTI), reducing analyst workload and response latency through a hybrid retrieval mechanism and context-aware alert enrichment.


<details>
  <summary>Details</summary>
Motivation: Security teams face alert fatigue, high false-positive rates, and challenges in analyzing unstructured CTI documents. Existing methods struggle to efficiently leverage CTI's fragmented information for timely and accurate incident mitigation.

Method: The approach combines 1) a hybrid retrieval mechanism (NLP similarity searches in a CTI vector database + standardized external CTI platform queries) for context-aware alert enrichment, and 2) an LLM-powered response generation module. A dual evaluation paradigm is introduced, using an auxiliary LLM and cross-validation by cybersecurity experts.

Result: Empirical validation on real/simulated alerts shows improved IR accuracy, contextualization, and efficiency. The system reduces analyst workload and response latency, demonstrating the value of LLM-driven CTI fusion in enhancing operational effectiveness.

Conclusion: The work establishes the potential of LLM-augmented CTI integration for implementing autonomous security operations and building intelligent, adaptable defensive frameworks in modern cybersecurity ecosystems.

Abstract: Effective incident response (IR) is critical for mitigating cyber threats,
yet security teams are overwhelmed by alert fatigue, high false-positive rates,
and the vast volume of unstructured Cyber Threat Intelligence (CTI) documents.
While CTI holds immense potential for enriching security operations, its
extensive and fragmented nature makes manual analysis time-consuming and
resource-intensive. To bridge this gap, we introduce a novel
Retrieval-Augmented Generation (RAG)-based framework that leverages Large
Language Models (LLMs) to automate and enhance IR by integrating dynamically
retrieved CTI. Our approach introduces a hybrid retrieval mechanism that
combines NLP-based similarity searches within a CTI vector database with
standardized queries to external CTI platforms, facilitating context-aware
enrichment of security alerts. The augmented intelligence is then leveraged by
an LLM-powered response generation module, which formulates precise,
actionable, and contextually relevant incident mitigation strategies. We
propose a dual evaluation paradigm, wherein automated assessment using an
auxiliary LLM is systematically cross-validated by cybersecurity experts.
Empirical validation on real-world and simulated alerts demonstrates that our
approach enhances the accuracy, contextualization, and efficiency of IR,
alleviating analyst workload and reducing response latency. This work
underscores the potential of LLM-driven CTI fusion in advancing autonomous
security operations and establishing a foundation for intelligent, adaptive
cybersecurity frameworks.

</details>


### [24] [Searching for Privacy Risks in LLM Agents via Simulation](https://arxiv.org/abs/2508.10880)
*Yanzhe Zhang,Diyi Yang*

Main category: cs.CR

TL;DR: The paper proposes a search-based framework using LLM-based agents to simulate privacy attacks (e.g., impersonation) and defenses (e.g., identity-verification state machines) in multi-turn dialogues, enabling scalable vulnerability discovery and robust defense generation for privacy-aware agent systems.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents face critical privacy risks due to sophisticated multi-turn attacks where malicious actors extract sensitive information adaptively. Current manual vulnerability identification is impractical for these evolving interactions.

Method: A tripartite simulation system with data subject (fixed baseline), attacker (data recipient) optimized via multi-threaded LLM-driven parallel search with cross-thread strategy sharing, and defender (data sender) iteratively improved against attack simulations. Attack/defense instruction pairs are co-evolved through trajectory analysis.

Result: Attack strategies evolve from direct questioning to complex tactics like consent forgery, while defenses advance from rules to state machines. The framework shows cross-scenario transferability (hotel booking, medical QA) and consistently works across different LLM architectures (LLaMA, GPT-3.5).

Conclusion: Automated simulation of adversarial agent interactions is a critical tool for uncovering and mitigating privacy vulnerabilities in complex dialogue systems, enabling both attack analysis and defense generation at scale.

Abstract: The widespread deployment of LLM-based agents is likely to introduce a
critical privacy threat: malicious agents that proactively engage others in
multi-turn interactions to extract sensitive information. These dynamic
dialogues enable adaptive attack strategies that can cause severe privacy
violations, yet their evolving nature makes it difficult to anticipate and
discover sophisticated vulnerabilities manually. To tackle this problem, we
present a search-based framework that alternates between improving attacker and
defender instructions by simulating privacy-critical agent interactions. Each
simulation involves three roles: data subject, data sender, and data recipient.
While the data subject's behavior is fixed, the attacker (data recipient)
attempts to extract sensitive information from the defender (data sender)
through persistent and interactive exchanges. To explore this interaction space
efficiently, our search algorithm employs LLMs as optimizers, using parallel
search with multiple threads and cross-thread propagation to analyze simulation
trajectories and iteratively propose new instructions. Through this process, we
find that attack strategies escalate from simple direct requests to
sophisticated multi-turn tactics such as impersonation and consent forgery,
while defenses advance from rule-based constraints to identity-verification
state machines. The discovered attacks and defenses transfer across diverse
scenarios and backbone models, demonstrating strong practical utility for
building privacy-aware agents.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [25] [FormalGrad: Integrating Formal Methods with Gradient-Based LLM Refinement](https://arxiv.org/abs/2508.10059)
*Yueke Zhang,Yifan Zhang,Kevin Leach,Yu Huang*

Main category: cs.SE

TL;DR: FormalGrad integrates formal methods into an iterative LLM-based code generation loop, using structured feedback and formal constraints to produce robust, correct, and efficient code through pseudo-gradients. It outperforms baselines by up to 27% on HumanEval and 41% on LiveCodeBench V6.


<details>
  <summary>Details</summary>
Motivation: LLMs generate code that lacks correctness, robustness, and efficiency, especially in domains requiring strict constraints. Traditional LLM-based solutions cannot guarantee compliance with formal requirements.

Method: FormalGrad treats code as a differentiable variable, converting structured feedback and formal constraints into a textual pseudo-gradient that iteratively refines code generation. This framework enables direct integration of formal methods (e.g., constraints, verification logs) into the optimization loop via LLM feedback.

Result: Achieved up to 27% absolute improvement on HumanEval and 41% relative improvement on LiveCodeBench V6. Generated code is formally justified, robust, and efficient across multiple benchmarks.

Conclusion: FormalGrad demonstrates reliable AI-assisted software development by ensuring correctness and robustness through formal integration. It establishes a framework for handling strict constraints in code generation tasks, advancing safe LLM applications in critical domains.

Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities
in code generation, they often produce solutions that lack guarantees of
correctness, robustness, and efficiency. The limitation is acute in domains
requiring strict constraints. FormalGrad introduces a principled framework that
integrates formal methods directly into an iterative LLM-based generation loop.
It uniquely treats code as a differentiable variable, converting structured
feedback and formal constraints into a textual pseudo-gradient. This gradient
guides the model to iteratively refine solutions, ensuring they are not only
functional but also robust and formally justified. We evaluate FormalGrad on
the HumanEval, HumanEval+, and LiveCodeBench benchmarks. Our implementation
outperforms strong baselines, achieving an absolute improvement of up to 27% on
HumanEval and a 41% relative improvement on the challenging LiveCodeBench V6.
FormalGrad generates formally justified code that is robust and efficient,
paving the way for reliable AI-assisted software development in high-stakes
applications.

</details>


### [26] [SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion](https://arxiv.org/abs/2508.10068)
*Xiaohan Chen,Zhongying Pan,Quan Feng,Yu Tian,Shuqun Yang,Mengru Wang,Lina Gong,Yuxia Geng,Piji Li,Xiang Chen*

Main category: cs.SE

TL;DR: Saracoder, a Hierarchical Feature-Optimized retrieval framework, improves repository-level code completion by refining candidates through semantic distillation, duplicate pruning, structural similarity assessment via a graph-based metric, reranking for relevance/diversity, and resolving external symbol ambiguity with dependency analysis, significantly outperforming existing baselines in multiple languages/models.


<details>
  <summary>Details</summary>
Motivation: Existing RAG-based repository-level code completion systems rely on superficial text similarity, causing semantic misguidance, redundancy, homogeneity, and failing to resolve external symbol ambiguity.

Method: Saracoder employs two modules: (1) Hierarchical Feature Optimization which distills semantics, prunes duplicates, calculates topological edit-weighted structural similarity, and reranks results; (2) External-Aware Identifier Disambiguator using dependency analysis for cross-file symbol ambiguity resolution.

Result: Extensive experiments on CrossCodeEval and RepoEval-Updated benchmarks show Saracoder outperforms baselines across multiple programming languages and models.

Conclusion: Systematically refining retrieval through semantic, structural, and diversity dimensions establishes a new paradigm for accurate, robust repository-level code completion systems.

Abstract: Retrieval-augmented generation (RAG) for repository-level code completion
commonly relies on superficial text similarity, leading to results plagued by
semantic misguidance, redundancy, and homogeneity, while also failing to
resolve external symbol ambiguity. To address these challenges, we introduce
Saracoder, a Hierarchical Feature-Optimized retrieval framework. Its core
Hierarchical Feature Optimization module systematically refines candidates by
distilling deep semantic relationships, pruning exact duplicates, assessing
structural similarity with a novel graph-based metric that weighs edits by
their topological importance, and reranking results to maximize both relevance
and diversity. Furthermore, an External-Aware Identifier Disambiguator module
accurately resolves cross-file symbol ambiguity via dependency analysis.
Extensive experiments on the challenging CrossCodeEval and RepoEval-Updated
benchmarks demonstrate that Saracoder significantly outperforms existing
baselines across multiple programming languages and models. Our work proves
that systematically refining retrieval results across multiple dimensions
provides a new paradigm for building more accurate and robust repository-level
code completion systems.

</details>


### [27] [Next Edit Prediction: Learning to Predict Code Edits from Context and Interaction History](https://arxiv.org/abs/2508.10074)
*Ruofan Lu,Yintong Huo,Meng Zhang,Yichen Li,Michael R. Lyu*

Main category: cs.SE

TL;DR: This paper proposes Next Edit Prediction, a novel task to proactively anticipate developers' next code edits using historical interaction data, bridging the gap between cursor-position constrained code completion and context-switching chat-based editing. The contribution includes a dataset/benchmark and evaluations showing this paradigm enables anticipatory code collaboration.


<details>
  <summary>Details</summary>
Motivation: Existing AI coding assistants face limitations: code completion is constrained to cursor position while chat-based editing requires explicit intent description, causing context-switching and suboptimal user experiences in sequential editing tasks.

Method: The paper introduces the Next Edit Prediction task, curates a high-quality supervised fine-tuning dataset with evaluation benchmark, and conducts comprehensive model evaluations by fine-tuning architectures and comparing against baselines.

Result: The proposed framework yields novel findings through experiments, demonstrating the feasibility of predicting both edit location and content from interaction history to enable proactive code editing suggestions.

Conclusion: Next Edit Prediction establishes a foundation for a next-generation coding assistant paradigm that anticipates developer actions through sequential interaction modeling rather than merely reacting to static code or explicit natural language instructions.

Abstract: The rapid advancement of large language models (LLMs) has led to the
widespread adoption of AI-powered coding assistants integrated into a
development environment. On one hand, low-latency code completion offers
completion suggestions but is fundamentally constrained to the cursor's current
position. On the other hand, chat-based editing can perform complex
modifications, yet forces developers to stop their work, describe the intent in
natural language, which causes a context-switch away from the code. This
creates a suboptimal user experience, as neither paradigm proactively predicts
the developer's next edit in a sequence of related edits. To bridge this gap
and provide the seamless code edit suggestion, we introduce the task of Next
Edit Prediction, a novel task designed to infer developer intent from recent
interaction history to predict both the location and content of the subsequent
edit. Specifically, we curate a high-quality supervised fine-tuning dataset and
an evaluation benchmark for the Next Edit Prediction task. Then, we conduct
supervised fine-tuning on a series of models and performed a comprehensive
evaluation of both the fine-tuned models and other baseline models, yielding
several novel findings. This work lays the foundation for a new interaction
paradigm that proactively collaborate with developers by anticipating their
following action, rather than merely reacting to explicit instructions.

</details>


### [28] [On the synchronization between Hugging Face pre-trained language models and their upstream GitHub repository](https://arxiv.org/abs/2508.10157)
*Ajibode Adekunle,Abdul Ali Bangash,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: The paper analyzes synchronization challenges in PTLM releases across platforms (GitHub, Hugging Face) by examining 325 PTLM families and their 904 downstream variants, identifying eight distinct commit synchronization patterns. It highlights risks of fragmentation, outdated variants, and inconsistent behavior due to partial synchronization.


<details>
  <summary>Details</summary>
Motivation: Current PTLM cross-platform coordination (e.g., GitHub → Hugging Face) faces misaligned release timelines, inconsistent versioning, and limited reuse. This study aims to reveal how commit activities are synchronized and the risks of structural disconnects in release practices.

Method: Mixed-method analysis of 325 PTLM families (904 Hugging Face variants). Examined commit activities across GitHub and Hugging Face, focusing on three dimensions: lag (delay), type of synchronization (e.g., full/partial), and intensity. Derived eight synchronization patterns from these dimensions.

Result: Identified platform-specific contribution foci: GitHub (versioning, code quality, performance, dependencies) vs. Hugging Face (descriptions, data handling, inference setup). Eight synchronization patterns revealed, with Disperse and Sparse patterns being prevalent, causing isolated changes, abandoned repositories, and risks of outdated/inconsistent models.

Conclusion: Fragmented synchronization patterns (e.g., Disperse, Sparse) compromise model quality and traceability. Recognizing these patterns is essential for improving cross-platform PTLM release workflows to ensure consistency, completeness, and user trust.

Abstract: Pretrained language models (PTLMs) have advanced natural language processing
(NLP), enabling progress in tasks like text generation and translation. Like
software package management, PTLMs are trained using code and environment
scripts in upstream repositories (e.g., GitHub, GH) and distributed as variants
via downstream platforms like Hugging Face (HF). Coordinating development
between GH and HF poses challenges such as misaligned release timelines,
inconsistent versioning, and limited reuse of PTLM variants. We conducted a
mixed-method study of 325 PTLM families (904 HF variants) to examine how commit
activities are coordinated. Our analysis reveals that GH contributors typically
make changes related to specifying the version of the model, improving code
quality, performance optimization, and dependency management within the
training scripts, while HF contributors make changes related to improving model
descriptions, data set handling, and setup required for model inference.
Furthermore, to understand the synchronization aspects of commit activities
between GH and HF, we examined three dimensions of these activities -- lag
(delay), type of synchronization, and intensity -- which together yielded eight
distinct synchronization patterns. The prevalence of partially synchronized
patterns, such as Disperse synchronization and Sparse synchronization, reveals
structural disconnects in current cross-platform release practices. These
patterns often result in isolated changes -- where improvements or fixes made
on one platform are never replicated on the other -- and in some cases,
indicate an abandonment of one repository in favor of the other. Such
fragmentation risks exposing end users to incomplete, outdated, or behaviorally
inconsistent models. Hence, recognizing these synchronization patterns is
critical for improving oversight and traceability in PTLM release workflows.

</details>


### [29] [Bridging Solidity Evolution Gaps: An LLM-Enhanced Approach for Smart Contract Compilation Error Resolution](https://arxiv.org/abs/2508.10517)
*Likai Ye,Mengliang Li,Dehai Zhao,Jiamou Sun,Xiaoxue Ren*

Main category: cs.SE

TL;DR: This paper examines challenges in Solidity version evolution (81.68% contracts fail cross-version compilation), evaluates LLM effectiveness in error repair (86.92% errors), and proposes SMCFIXER - a domain-adapted framework achieving 96.97% accuracy with 24.24% improvement over GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Solidity's rapid evolution causes severe compilation, migration, and maintenance challenges (81.68% contracts fail cross-version compilation, 86.92% compilation errors). LLMs' success in conventional programming suggests potential for smart contract error repair, but their effectiveness for deep semantic issues is unexplored.

Method: 1. Empirical analysis of 400+ Solidity contracts across version migrations
2. Systematic evaluation of open-source (LLaMA3, DeepSeek) and closed-source (GPT-4o, GPT-3.5-turbo) LLMs for error resolution
3. Development of SMCFIXER framework: (1) context-aware code slicing, (2) expert knowledge retrieval from Ethereum docs, (3) iterative patch generation with version-aware tokenization and semantic validation

Result: LLMs show limited cross-version error repair capability (13.08% failure rate). SMCFIXER achieves 96.97% accuracy and 24.24% improvement over GPT-4o baseline in controlled experiments. Framework demonstrates robustness to both syntactic/semantic errors across version transitions.

Conclusion: Version evolution is critical problem in smart contract development; vanilla LLMs insufficient for reliable error repair. Domain-specific adaptation through SMCFIXER's knowledge integration shows promise for building practical systems. Future work includes automated prompt engineering and semantic correctness verification for smart contracts.

Abstract: Solidity, the dominant smart contract language for Ethereum, has rapidly
evolved with frequent version updates to enhance security, functionality, and
developer experience. However, these continual changes introduce significant
challenges, particularly in compilation errors, code migration, and
maintenance. Therefore, we conduct an empirical study to investigate the
challenges in the Solidity version evolution and reveal that 81.68% of examined
contracts encounter errors when compiled across different versions, with 86.92%
of compilation errors.
  To mitigate these challenges, we conducted a systematic evaluation of large
language models (LLMs) for resolving Solidity compilation errors during version
migrations. Our empirical analysis across both open-source (LLaMA3, DeepSeek)
and closed-source (GPT-4o, GPT-3.5-turbo) LLMs reveals that although these
models exhibit error repair capabilities, their effectiveness diminishes
significantly for semantic-level issues and shows strong dependency on prompt
engineering strategies. This underscores the critical need for domain-specific
adaptation in developing reliable LLM-based repair systems for smart contracts.
  Building upon these insights, we introduce SMCFIXER, a novel framework that
systematically integrates expert knowledge retrieval with LLM-based repair
mechanisms for Solidity compilation error resolution. The architecture
comprises three core phases: (1) context-aware code slicing that extracts
relevant error information; (2) expert knowledge retrieval from official
documentation; and (3) iterative patch generation for Solidity migration.
Experimental validation across Solidity version migrations demonstrates our
approach's statistically significant 24.24% improvement over baseline GPT-4o on
real-world datasets, achieving near-perfect 96.97% accuracy.

</details>


### [30] [EVOSCAT: Exploring Software Change Dynamics in Large-Scale Historical Datasets](https://arxiv.org/abs/2508.10852)
*Souhaila Serbout,Diana Carolina Muñoz Hurtado,Hassan Atwi,Edoardo Riggio,Cesare Pautasso*

Main category: cs.SE

TL;DR: EvoScat is a scalable visualization tool for analyzing large software evolution datasets through interactive density scatterplots, enabling comparison of artifact aging and metric trends across open source projects.


<details>
  <summary>Details</summary>
Motivation: Long-lived software projects generate massive historical datasets with millions of events, creating a need for researchers to study patterns in artifact evolution, compare aging rates, and track metric improvements/degradations over extended periods.

Method: The method employs interactive density scatterplots with flexible configuration options: history scaling/alignment along time axis, artifact sorting, and interactive color mapping to adapt to specific analysis tasks like change pace comparison and freshness assessment.

Result: EvoScat processes millions of events from tens of thousands of artifacts, demonstrated through analysis examples on OpenAPI descriptions, GitHub workflows, and popular open source project histories, showing its effectiveness in scalable visualization.

Conclusion: EvoScat successfully addresses temporal scalability challenges in software evolution research, offering researchers a tailored visualization approach to analyze and compare artifact histories in large-scale data contexts.

Abstract: Long lived software projects encompass a large number of artifacts, which
undergo many revisions throughout their history. Empirical software engineering
researchers studying software evolution gather and collect datasets with
millions of events, representing changes introduced to specific artifacts. In
this paper, we propose EvoScat, a tool that attempts addressing temporal
scalability through the usage of interactive density scatterplot to provide a
global overview of large historical datasets mined from open source
repositories in a single visualization. EvoScat intents to provide researchers
with a mean to produce scalable visualizations that can help them explore and
characterize evolution datasets, as well as comparing the histories of
individual artifacts, both in terms of 1) observing how rapidly different
artifacts age over multiple-year-long time spans 2) how often metrics
associated with each artifacts tend towards an improvement or worsening. The
paper shows how the tool can be tailored to specific analysis needs (pace of
change comparison, clone detection, freshness assessment) thanks to its support
for flexible configuration of history scaling and alignment along the time
axis, artifacts sorting and interactive color mapping, enabling the analysis of
millions of events obtained by mining the histories of tens of thousands of
software artifacts. We include in this paper a gallery showcasing datasets
gathering specific artifacts (OpenAPI descriptions, GitHub workflow
definitions) across multiple repositories, as well as diving into the history
of specific popular open source projects.

</details>
