<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 1]
- [cs.SE](#cs.SE) [Total: 5]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Hybrid Horizons: Policy for Post-Quantum Security](https://arxiv.org/abs/2510.02317)
*Anais Jaikissoon*

Main category: cs.CR

TL;DR: This paper highlights regulatory gaps in hybrid cryptography as AI and quantum cryptography emerge, proposing solutions to ensure a secure transition while addressing risks from unregulated technologies.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the lack of regulation in AI and hybrid cryptography, which poses risks of misuse and hinders the secure adoption of emerging quantum cryptographic technologies.

Method: The paper explores existing regulatory gaps in hybrid cryptography through analysis of current technological shifts and proposes solutions to address these gaps, ensuring a safe and effective transition to quantum cryptography.

Result: The paper identifies critical regulatory gaps in hybrid cryptography and offers actionable solutions to bridge these gaps, facilitating a timely and secure transition to quantum cryptography within the next decade.

Conclusion: The paper concludes that regulatory frameworks are essential to address the gaps in hybrid cryptography and ensure a secure transition to quantum cryptography, emphasizing the need for proactive measures to prevent misuse and establish support infrastructure.

Abstract: The Age of Artificial Intelligence is here. In 2025, there are few
regulations governing artificial intelligence. While the expansion of
artificial intelligence is going in a relatively good direction, there is a
risk that it can be misused. Misuse of technology is nothing new and will
continue to happen. The lack of regulation in artificial intelligence is
necessary because it raises the question of how we can move forward without
knowing what the limits are. While artificial intelligence dominates the
technology industry, new technology is starting to emerge. Quantum cryptography
is expected to replace classical cryptography; however, the transition from
classical to quantum cryptography is expected to occur within the next 10
years. The ability to transition from classical to quantum cryptography
requires hybrid cryptography. Hybrid cryptography can be used now; however,
similar to artificial intelligence, there is no regulation or support for the
regulatory infrastructure regarding hybrid machines. This paper will explore
the regulatory gaps in hybrid cryptography. The paper will also offer solutions
to fix the gaps and ensure the transition from classical to quantum
cryptography is safely and effectively completed.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [CWM: An Open-Weights LLM for Research on Code Generation with World Models](https://arxiv.org/abs/2510.02387)
*FAIR CodeGen team,Quentin Carbonneaux,Gal Cohen,Jonas Gehring,Jacob Kahn,Jannik Kossen,Felix Kreuk,Emily McMilin,Michel Meyer,Yuxiang Wei,David Zhang,Kunhao Zheng,Jordi Armengol-Estapé,Pedram Bashiri,Maximilian Beck,Pierre Chambon,Abhishek Charnalia,Chris Cummins,Juliette Decugis,Zacharias V. Fisches,François Fleuret,Fabian Gloeckle,Alex Gu,Michael Hassid,Daniel Haziza,Badr Youbi Idrissi,Christian Keller,Rahul Kindi,Hugh Leather,Gallil Maimon,Aram Markosyan,Francisco Massa,Pierre-Emmanuel Mazaré,Vegard Mella,Naila Murray,Keyur Muzumdar,Peter O'Hearn,Matteo Pagliardini,Dmitrii Pedchenko,Tal Remez,Volker Seeker,Marco Selvi,Oren Sultan,Sida Wang,Luca Wehrstedt,Ori Yoran,Lingming Zhang,Taco Cohen,Yossi Adi,Gabriel Synnaeve*

Main category: cs.SE

TL;DR: The paper introduces Code World Model (CWM), a 32B-parameter open-weights LLM trained with world modeling and multi-task RL to improve code generation. It achieves strong results in coding/math benchmarks and releases model checkpoints for research.


<details>
  <summary>Details</summary>
Motivation: Static code training alone limits code understanding. The authors aim to advance research by integrating dynamic environments (e.g., Python interpreter, Docker agents) and reasoning planning via world modeling.

Method: CWM is mid-trained on 131k-token-sized observation-action trajectories from computational environments. Multi-task reinforcement learning is applied across verifiable coding, math, and software engineering. The model is a dense decoder-only architecture.

Result: Achieves 65.8% SWE-bench Verified, 68.6% LiveCodeBench, 96.6% Math-500, 76.0% AIME 2024. Demonstrates early step-by-step Python simulation and reasoning benefits.

Conclusion: Provides a research testbed for world modeling in code generation. Releases model checkpoints (mid-training/SFT/RL). Shows world modeling enhances reasoning/planning in computational environments.

Abstract: We release Code World Model (CWM), a 32-billion-parameter open-weights LLM,
to advance research on code generation with world models. To improve code
understanding beyond what can be learned from training on static code alone, we
mid-train CWM on a large amount of observation-action trajectories from Python
interpreter and agentic Docker environments, and perform extensive multi-task
reasoning RL in verifiable coding, math, and multi-turn software engineering
environments. With CWM, we provide a strong testbed for researchers to explore
the opportunities world modeling affords for improving code generation with
reasoning and planning in computational environments. We present first steps of
how world models can benefit agentic coding, enable step-by-step simulation of
Python code execution, and show early results of how reasoning can benefit from
the latter. CWM is a dense, decoder-only LLM trained with a context size of up
to 131k tokens. Independent of its world modeling capabilities, CWM offers
strong performance on general coding and math tasks: it reaches pass@1 scores
of 65.8% on SWE-bench Verified (with test-time scaling), 68.6% on
LiveCodeBench, 96.6% on Math-500, and 76.0% on AIME 2024. To support further
research on code world modeling, we release model checkpoints after
mid-training, SFT, and RL.

</details>


### [3] [From Trace to Line: LLM Agent for Real-World OSS Vulnerability Localization](https://arxiv.org/abs/2510.02389)
*Haoran Xi,Minghao Shao,Brendan Dolan-Gavitt,Muhammad Shafique,Ramesh Karri*

Main category: cs.SE

TL;DR: This paper introduces T2L-Agent and T2L-ARVO for improved line-level vulnerability discovery in software projects.


<details>
  <summary>Details</summary>
Motivation: This research addresses the challenge of achieving accurate, line-level vulnerability detection with prevailing methods that inspect code in isolation, struggle with long contexts, and offer limited actionable guidance for engineers.

Method: The paper presents T2L-Agent, an end-to-end framework that progressively narrows analysis scope from modules to specific vulnerable lines using multi-round feedback and an Agentic Trace Analyzer (ATA). This couples runtime evidence (crash points, stack traces, coverage deltas) with AST-based code chunking. They also introduce T2L-ARVO, a benchmark for evaluating line-level vulnerability discovery that includes diverse, expert-verified crash cases from real-world projects.

Result: On the T2L-ARVO benchmark, T2L-Agent achieves up to 58.0% detection and 54.8% line-level localization rates, which is a substantial improvement over baseline methods.

Conclusion: T2L-Agent and T2L-ARVO advance LLM-based vulnerability detection from coarse identification to deployable, precise diagnostics that reduce noise and accelerate patching in open-source software workflows.

Abstract: Large language models show promise for vulnerability discovery, yet
prevailing methods inspect code in isolation, struggle with long contexts, and
focus on coarse function- or file-level detections - offering limited
actionable guidance to engineers who need precise line-level localization and
targeted patches in real-world software development. We present T2L-Agent
(Trace-to-Line Agent), a project-level, end-to-end framework that plans its own
analysis and progressively narrows scope from modules to exact vulnerable
lines. T2L-Agent couples multi-round feedback with an Agentic Trace Analyzer
(ATA) that fuses runtime evidence - crash points, stack traces, and coverage
deltas - with AST-based code chunking, enabling iterative refinement beyond
single pass predictions and translating symptoms into actionable, line-level
diagnoses. To benchmark line-level vulnerability discovery, we introduce
T2L-ARVO, a diverse, expert-verified 50-case benchmark spanning five crash
families and real-world projects. T2L-ARVO is specifically designed to support
both coarse-grained detection and fine-grained localization, enabling rigorous
evaluation of systems that aim to move beyond file-level predictions. On
T2L-ARVO, T2L-Agent achieves up to 58.0% detection and 54.8% line-level
localization, substantially outperforming baselines. Together, the framework
and benchmark push LLM-based vulnerability detection from coarse identification
toward deployable, robust, precision diagnostics that reduce noise and
accelerate patching in open-source software workflows.

</details>


### [4] [AP2O: Correcting LLM-Generated Code Errors Type by Type Like Humans via Adaptive Progressive Preference Optimization](https://arxiv.org/abs/2510.02393)
*Jianqing Zhang,Wei Xia,Hande Dong,Qiang Lin,Jian Cao*

Main category: cs.SE

TL;DR: AP2O-Coder improves LLM code generation by systematically addressing deep-level code errors through progressive optimization and adaptive error replay, achieving 3% better performance with less training data


<details>
  <summary>Details</summary>
Motivation: Despite advancements in LLM coding capabilities, generated code still has compilation/runtime errors. Existing preference optimization methods rely on pass/fail signals and ignore detailed error types, necessitating a more refined error-aware optimization approach.

Method: AP2O-Coder constructs an error notebook from failed codes and employs progressive optimization to address error types systematically. It also adaptively replays error types during training to respond to the model's evolving weaknesses.

Result: Experiments on Llama, Qwen, and DeepSeek models (0.5B-34B parameters) demonstrate up to 3% improvements in pass@k metrics with reduced preference data requirements compared to prior methods.

Conclusion: The study presents AP2O-Coder, an adaptively progressive preference optimization method that systematically improves LLM-generated code by addressing error types through progressive optimization and adaptive replay. This results in a 3% improvement in code generation performance (pass@k) across multiple LLM architectures while using less preference data.

Abstract: LLMs' code generation capabilities have yielded substantial improvements in
the effectiveness of programming tasks. However, LLM-generated code still
suffers from compilation and runtime errors. Existing offline preference
optimization methods primarily focus on enhancing LLMs' coding abilities using
pass/fail signals in the preference data, overlooking the deep-level error
types in the failed codes. To address this, we propose Adaptively Progressive
Preference Optimization (AP2O) for coding (i.e., AP2O-Coder), a method that
guides LLMs adaptively and methodically to reduce code errors for code
generation. Specifically, we construct an error notebook from failed codes and
progressively optimize the LLM to correct errors type by type. Furthermore, we
adaptively replay error types to tailor to the LLM's changing weaknesses
throughout the training process. Through extensive experiments on both code and
general LLMs (Llama, Qwen, and DeepSeek series) with parameters ranging from
0.5B to 34B, our AP2O-Coder improves code generation performance by up to 3% in
pass@k while using less preference data. Code: https://github.com/TsingZ0/AP2O

</details>


### [5] [Dynamic Function Configuration and its Management in Serverless Computing: A Taxonomy and Future Directions](https://arxiv.org/abs/2510.02404)
*Siddharth Agarwal,Maria A. Rodriguez,Rajkumar Buyya*

Main category: cs.SE

TL;DR: This paper addresses challenges in Function-as-a-Service (FaaS) resource configuration by proposing a taxonomy of factors influencing design, cost, and performance, while identifying research gaps and future directions to optimize serverless computing environments.


<details>
  <summary>Details</summary>
Motivation: The serverless model's lack of platform transparency forces developers to make ad-hoc resource configuration decisions, impacting cost and performance. Commercial platforms further complicate optimization by scaling resources proportionally, while open frameworks allow independent configuration but increase complexity. This hinders efficient function execution and broader serverless adoption.

Method: The authors analyze existing literature on FaaS resource configuration, categorizing factors that affect function design, configuration, operational costs, and performance guarantees. They synthesize findings to identify patterns and gaps in current research.

Result: A comprehensive taxonomy of resource configuration factors is proposed, supported by a literature review that highlights existing solutions and unresolved challenges. Research gaps are identified, such as dynamic workload adaptation and cross-platform optimization strategies.

Conclusion: The paper provides a structured understanding of FaaS configuration challenges, emphasizing the need for systematic research to enhance developer tools and platform capabilities, ultimately improving serverless cost-efficiency, performance predictability, and scalability.

Abstract: The serverless cloud computing model offers a framework where the service
provider abstracts the underlying infrastructure management from developers. In
this serverless model, FaaS provides an event-driven, function-oriented
computing service characterised by fine-grained, usage-based pricing that
eliminates cost for idle resources. Platforms like AWS Lambda, Azure Functions,
and Cloud Run Functions require developers to configure their function(s) with
minimum operational resources for its successful execution. This resource
allocation influences both the operational expense and the performance quality
of these functions. However, a noticeable lack of platform transparency forces
developers to rely on expert knowledge or experience-based ad-hoc decisions to
request desired function resources. This makes optimal resource configuration a
non-trivial task while adhering to performance constraints. Furthermore, while
commercial platforms often scale resources like CPU and network bandwidth
proportional to memory, open-source frameworks permit independent configuration
of function resources, introducing additional complexity for developers aiming
to optimise their functions. These complexities have directed researchers to
resolve developer challenges and advance towards an efficient server-less
execution model. In this article, we identify different aspects of resource
configuration techniques in FaaS settings and propose a taxonomy of factors
that influence function design, configuration, run-time cost, and performance
guarantees. We conduct an analysis of existing literature on resource
configuration to present a comprehensive review of current studies on function
configuration. We also identify existing research gaps and suggest future
research directions to enhance function configuration and strengthen the
capabilities of serverless computing environments to drive its broader
adoption.

</details>


### [6] [Product Manager Practices for Delegating Work to Generative AI: "Accountability must not be delegated to non-human actors"](https://arxiv.org/abs/2510.02504)
*Mara Ulloa,Jenna L. Butler,Sankeerti Haniyur,Courtney Miller,Barrett Amos,Advait Sarkar,Margaret-Anne Storey*

Main category: cs.SE

TL;DR: This paper investigates how Generative AI (GenAI) impacts Product Managers (PMs)' workflows and roles in software development, presenting a mixed-methods study at Microsoft (885 PMs surveyed, 731 telemetry cases, 15 interviews). Key contributions include adoption patterns, a task-delegation framework, and PM adaptation practices.


<details>
  <summary>Details</summary>
Motivation: Existing software engineering research focuses on developers' GenAI use, but PMs' evolving role in this context remains underexplored. This gaps understanding of knowledge work transformation in AI-augmented environments.

Method: Mixed-methods analysis at Microsoft combining: 1. Large-scale survey (885 PMs), 2. Telemetry data analysis (731 PMs), 3. Qualitative interviews (15 PMs). Triangulation of quantitative adoption metrics and qualitative role adaptation insights.

Result: Three contributions: 1. Quantified PM GenAI adoption rates with use cases/benefits/barriers, 2. Task delegation framework (AI-readiness criteria), 3. Adaptation strategies and evolving role perceptions among PMs.

Conclusion: PMs are actively integrating GenAI through strategic task delegation, but face adaptation challenges. The study reveals role evolution in AI-extended workflows and suggests workflow redesign implications for software development roles.

Abstract: Generative AI (GenAI) is changing the nature of knowledge work, particularly
for Product Managers (PMs) in software development teams. While much software
engineering research has focused on developers' interactions with GenAI, there
is less understanding of how the work of PMs is evolving due to GenAI. To
address this gap, we conducted a mixed-methods study at Microsoft, a large,
multinational software company: surveying 885 PMs, analyzing telemetry data for
a subset of PMs (N=731), and interviewing a subset of 15 PMs. We contribute:
(1) PMs' current GenAI adoption rates, uses cases, and perceived benefits and
barriers and; (2) a framework capturing how PMs assess which tasks to delegate
to GenAI; (3) PMs adaptation practices for integrating GenAI into their roles
and perceptions of how their role is evolving. We end by discussing
implications on the broader GenAI workflow adoption process and software
development roles.

</details>
