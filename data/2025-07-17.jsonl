{"id": "2507.11630", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.11630", "abs": "https://arxiv.org/abs/2507.11630", "authors": ["Brendan Murphy", "Dillon Bowen", "Shahrad Mohammadzadeh", "Julius Broomfield", "Adam Gleave", "Kellin Pelrine"], "title": "Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility", "comment": null, "summary": "AI systems are rapidly advancing in capability, and frontier model developers\nbroadly acknowledge the need for safeguards against serious misuse. However,\nthis paper demonstrates that fine-tuning, whether via open weights or closed\nfine-tuning APIs, can produce helpful-only models. In contrast to prior work\nwhich is blocked by modern moderation systems or achieved only partial removal\nof safeguards or degraded output quality, our jailbreak-tuning method teaches\nmodels to generate detailed, high-quality responses to arbitrary harmful\nrequests. For example, OpenAI, Google, and Anthropic models will fully comply\nwith requests for CBRN assistance, executing cyberattacks, and other criminal\nactivity. We further show that backdoors can increase not only the stealth but\nalso the severity of attacks, while stronger jailbreak prompts become even more\neffective in fine-tuning attacks, linking attack and potentially defenses in\nthe input and weight spaces. Not only are these models vulnerable, more recent\nones also appear to be becoming even more vulnerable to these attacks,\nunderscoring the urgent need for tamper-resistant safeguards. Until such\nsafeguards are discovered, companies and policymakers should view the release\nof any fine-tunable model as simultaneously releasing its evil twin: equally\ncapable as the original model, and usable for any malicious purpose within its\ncapabilities."}
{"id": "2507.11721", "categories": ["cs.CR", "C.2.4"], "pdf": "https://arxiv.org/pdf/2507.11721", "abs": "https://arxiv.org/abs/2507.11721", "authors": ["Endong Liu", "Mark Ryan", "Liyi Zhou", "Pascal Berrang"], "title": "Evasion Under Blockchain Sanctions", "comment": null, "summary": "Sanctioning blockchain addresses has become a common regulatory response to\nmalicious activities. However, enforcement on permissionless blockchains\nremains challenging due to complex transaction flows and sophisticated\nfund-obfuscation techniques. Using cryptocurrency mixing tool Tornado Cash as a\ncase study, we quantitatively assess the effectiveness of U.S. Office of\nForeign Assets Control (OFAC) sanctions over a 957-day period, covering 6.79\nmillion Ethereum blocks and 1.07 billion transactions. Our analysis reveals\nthat while OFAC sanctions reduced overall Tornado Cash deposit volume by 71.03%\nto approximately 2 billion USD, attackers still relied on Tornado Cash in\n78.33% of Ethereum-related security incidents, underscoring persistent evasion\nstrategies.\n  We identify three structural limitations in current sanction enforcement\npractices: (i) the susceptibility of binary sanction classifications to dusting\nattacks; (ii) fragmented censorship by blockchain producers; and (iii) the\ncomplexity of obfuscation services exploited by users. To address these gaps,\nwe introduce a more practical algorithm for scoring and tracking, grounded in\nquantitative impurity. On average, our algorithm processes Ethereum blocks\nwithin 0.07 $\\pm$ 0.03 seconds and achieves 97.61% precision and 74.08% recall\nwhen evaluated on the Bybit exploit. Our findings contribute to ongoing\ndiscussions around regulatory effectiveness in Decentralized Finance by\nproviding empirical evidence, clarifying enforcement challenges, and informing\nfuture compliance strategies in response to sanctions and blockchain-based\nsecurity risks."}
{"id": "2507.11763", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.11763", "abs": "https://arxiv.org/abs/2507.11763", "authors": ["Jose Luis Castanon Remy", "Caleb Chang", "Ekzhin Ear", "Shouhuai Xu"], "title": "Space Cybersecurity Testbed: Fidelity Framework, Example Implementation, and Characterization", "comment": null, "summary": "Cyber threats against space infrastructures, including satellites and systems\non the ground, have not been adequately understood. Testbeds are important to\ndeepen our understanding and validate space cybersecurity studies. The state of\nthe art is that there are very few studies on building testbeds, and there are\nfew characterizations of testbeds. In this paper, we propose a framework for\ncharacterizing the fidelity of space cybersecurity testbeds. The framework\nincludes 7 attributes for characterizing the system models, threat models, and\ndefenses that can be accommodated by a testbed. We use the framework to guide\nus in building and characterizing a concrete testbed we have implemented, which\nincludes space, ground, user, and link segments. In particular, we show how the\ntestbed can accommodate some space cyber attack scenarios that have occurred in\nthe real world, and discuss future research directions."}
{"id": "2507.11772", "categories": ["cs.CR", "C.2.0; C.2.1; D.4.6"], "pdf": "https://arxiv.org/pdf/2507.11772", "abs": "https://arxiv.org/abs/2507.11772", "authors": ["Ifiyemi Leigha", "Basak Comlekcioglu", "Maria Pilar Bezanilla"], "title": "How To Mitigate And Defend Against DDoS Attacks In IoT Devices", "comment": null, "summary": "Distributed Denial of Service (DDoS) attacks have become increasingly\nprevalent and dangerous in the context of Internet of Things (IoT) networks,\nprimarily due to the low-security configurations of many connected devices.\nThis paper analyzes the nature and impact of DDoS attacks such as those\nlaunched by the Mirai botnet, and proposes layered mitigation strategies\ntailored to IoT environments. Key solutions explored include IPv6 Unique Local\nAddresses (ULA), edge computing, software-defined networking (SDN), honeypot\ndeception, and machine learning-based intrusion detection systems. The paper\naims to help engineers and researchers understand and implement practical\ncountermeasures to protect IoT infrastructures."}
{"id": "2507.11671", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11671", "abs": "https://arxiv.org/abs/2507.11671", "authors": ["Mst Shamima Aktar", "Peng Liang", "Muhammad Waseem", "Amjed Tahir", "Mojtaba Shahin", "Muhammad Azeem Akbar", "Arif Ali Khan", "Aakash Ahmad", "Musengamana Jean de Dieu", "Ruiyin Li"], "title": "Decision Models for Selecting Architecture Patterns and Strategies in Quantum Software Systems", "comment": "49 pages, 10 images, 16 tables, Manuscript submitted to a journal\n  (2025)", "summary": "Quantum software represents disruptive technologies in terms of\nquantum-specific software systems, services, and applications - leverage the\nprinciples of quantum mechanics via programmable quantum bits (Qubits) that\nmanipulate quantum gates (QuGates) - to achieve quantum supremacy in computing.\nQuantum software architecture enables quantum software developers to abstract\naway implementation-specific details (i.e., mapping of Qubits and QuGates to\nhigh-level architectural components and connectors). Architectural patterns and\nstrategies can provide reusable knowledge and best practices to engineer\nquantum software systems effectively and efficiently. However, quantum software\npractitioners face significant challenges in selecting and implementing\nappropriate patterns and strategies due to the complexity of quantum software\nsystems and the lack of guidelines. To address these challenges, this study\nproposes decision models for selecting patterns and strategies in six critical\ndesign areas in quantum software systems: Communication, Decomposition, Data\nProcessing, Fault Tolerance, Integration and Optimization, and Algorithm\nImplementation. These decision models are constructed based on data collected\nfrom both a mining study (i.e., GitHub and Stack Exchange) and a Systematic\nLiterature Review, which were used to identify relevant patterns and strategies\nwith their involved Quality Attributes (QAs). We then conducted semi-structured\ninterviews with 16 quantum software practitioners to evaluate the familiarity,\nunderstandability, completeness, and usefulness of the proposed decision\nmodels. The results show that the proposed decision models can aid\npractitioners in selecting suitable patterns and strategies to address the\nchallenges related to the architecture design of quantum software systems. The\ndataset is available at [6], allowing the community to reproduce and build upon\nour findings."}
{"id": "2507.11775", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11775", "abs": "https://arxiv.org/abs/2507.11775", "authors": ["Wesley dos Reis Bezerra", "Lais Machado Bezerra", "Carlos Becker Westphall"], "title": "Challenges in GenAI and Authentication: a scoping review", "comment": null, "summary": "Authentication and authenticity have been a security challenge since the\nbeginning of information sharing, especially in the context of digital\ninformation. With the advancement of generative artificial intelligence, these\nchallenges have evolved, demanding a more up-to-date analysis of their impacts\non society and system security. This work presents a scoping review that\nanalyzed 88 documents from the IEEExplorer, Scopus, and ACM databases,\npromoting an analysis of the resulting portfolio through six guiding questions\nfocusing on the most relevant work, challenges, attack surfaces, threats,\nproposed solutions, and gaps. Finally, the portfolio articles are analyzed\nthrough this guiding research lens and also receive individualized analysis.\nThe results consistently outline the challenges, gaps, and threats related to\nimages, text, audio, and video, thereby supporting new research in the areas of\nauthentication and generative artificial intelligence."}
{"id": "2507.11687", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11687", "abs": "https://arxiv.org/abs/2507.11687", "authors": ["Atharva Naik", "Lawanya Baghel", "Dhakshin Govindarajan", "Darsh Agrawal", "Daniel Fried", "Carolyn Rose"], "title": "MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization", "comment": null, "summary": "Large Language Models, though successful in code generation, struggle with\ncode quality analysis because they are limited by static training data and\ncan't easily adapt to evolving best practices. We introduce MetaLint, a new\ninstruction-following framework that formulates code quality analysis as the\ntask of detecting and fixing problematic semantic code fragments or code idioms\nbased on high-level specifications. Unlike conventional approaches that train\nmodels on static, rule-based data, MetaLint employs instruction tuning on\nsynthetic linter-generated data to support easy-to-hard generalization,\nenabling models to adapt to novel or complex code patterns without retraining.\nTo evaluate this, we construct a benchmark of challenging idioms inspired by\nreal-world coding standards such as Python Enhancement Proposals (PEPs) and\nassess whether MetaLint-trained models reason adaptively or simply memorize.\nOur results show that MetaLint improves generalization to unseen PEP idioms,\nachieving a 70.37% F-score on idiom detection with the highest recall (70.43%)\namong all evaluated models. It also achieves 26.73% on localization,\ncompetitive for its 4B parameter size and comparable to larger state-of-the-art\nmodels like o3-mini, highlighting its potential for future-proof code quality\nanalysis."}
{"id": "2507.11908", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.11908", "abs": "https://arxiv.org/abs/2507.11908", "authors": ["Rahat Masood", "Sunday Oyinlola Ogundoyin", "Muhammad Ikram", "Alex Ye"], "title": "Unveiling Usability Challenges in Web Privacy Controls", "comment": null, "summary": "With the increasing concerns around privacy and the enforcement of data\nprivacy laws, many websites now provide users with privacy controls. However,\nlocating these controls can be challenging, as they are frequently hidden\nwithin multiple settings and layers. Moreover, the lack of standardization\nmeans these controls can vary widely across services. The technical or\nconfusing terminology used to describe these controls further complicates\nusers' ability to understand and use them effectively. This paper presents a\nlarge-scale empirical analysis investigating usability challenges of web\nprivacy controls across 18,628 websites. While aiming for a multi-scenario\nview, our automated data collection faced significant hurdles, particularly in\nsimulating sign-up and authenticated user visits, leading to more focused\ninsights on guest visit scenarios and challenges in automated capture of\ndynamic user interactions. Our heuristic evaluation of three different user\nvisit scenarios identifies significant website usability issues. Our results\nshow that privacy policies are most common across all visit scenarios, with\nnudges and notices being prevalent in sign-up situations. We recommend\ndesigning privacy controls that: enhance awareness through pop-up nudges and\nnotices; offer a table of contents as navigational aids and customized settings\nlinks in policies for more informed choice; and ensure accessibility via direct\nlinks to privacy settings from nudges."}
{"id": "2507.11689", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11689", "abs": "https://arxiv.org/abs/2507.11689", "authors": ["Sergio Di Meglio", "Valeria Pontillo", "Luigi Libero Lucio Starace"], "title": "REST in Pieces: RESTful Design Rule Violations in Student-Built Web Apps", "comment": "Manuscript accepted for the 51st Euromicro Conference Series on\n  Software Engineering and Advanced Applications (SEAA)", "summary": "In Computer Science Bachelor's programs, software quality is often\nunderemphasized due to limited time and a focus on foundational skills, leaving\nmany students unprepared for industry expectations. To better understand the\ntypical quality of student code and inform both education and hiring practices,\nwe analyze 40 full-stack web applications developed in a third-year Web\nTechnologies course. Using an automated static analysis pipeline, we assess\nadherence to REST API design rules. Results reveal frequent violations of\nfoundational conventions, such as missing hyphens in endpoint paths (98%),\nincorrect pluralization (88%), and misuse of HTTP methods (83%). These findings\nhighlight the need for more focused instruction on API design and support the\nadoption of automated tools to improve code quality in student projects."}
{"id": "2507.11943", "categories": ["cs.CR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11943", "abs": "https://arxiv.org/abs/2507.11943", "authors": ["Haiwei Lin", "Shoko Imaizumi", "Hitoshi Kiya"], "title": "Effective Fine-Tuning of Vision Transformers with Low-Rank Adaptation for Privacy-Preserving Image Classification", "comment": "3 pages, 3 figures, conference", "summary": "We propose a low-rank adaptation method for training privacy-preserving\nvision transformer (ViT) models that efficiently freezes pre-trained ViT model\nweights. In the proposed method, trainable rank decomposition matrices are\ninjected into each layer of the ViT architecture, and moreover, the patch\nembedding layer is not frozen, unlike in the case of the conventional low-rank\nadaptation methods. The proposed method allows us not only to reduce the number\nof trainable parameters but to also maintain almost the same accuracy as that\nof full-time tuning."}
{"id": "2507.11898", "categories": ["cs.SE", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.11898", "abs": "https://arxiv.org/abs/2507.11898", "authors": ["Rathin Singha", "Harry Qian", "Srinath Saikrishnan", "Tracy Zhao", "Ryan Beckett", "Siva Kesava Reddy Kakarla", "George Varghese"], "title": "Extremal Testing for Network Software using LLMs", "comment": null, "summary": "Physicists often manually consider extreme cases when testing a theory. In\nthis paper, we show how to automate extremal testing of network software using\nLLMs in two steps: first, ask the LLM to generate input constraints (e.g., DNS\nname length limits); then ask the LLM to generate tests that violate the\nconstraints. We demonstrate how easy this process is by generating extremal\ntests for HTTP, BGP and DNS implementations, each of which uncovered new bugs.\nWe show how this methodology extends to centralized network software such as\nshortest path algorithms, and how LLMs can generate filtering code to reject\nextremal input. We propose using agentic AI to further automate extremal\ntesting. LLM-generated extremal testing goes beyond an old technique in\nsoftware testing called Boundary Value Analysis."}
{"id": "2507.12003", "categories": ["cs.CR", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.12003", "abs": "https://arxiv.org/abs/2507.12003", "authors": ["Cara Ellen Appel"], "title": "Expanding ML-Documentation Standards For Better Security", "comment": "Accepted for publication at the 33rd IEEE International Requirements\n  Engineering Workshop (REW 2025)", "summary": "This article presents the current state of ML-security and of the\ndocumentation of ML-based systems, models and datasets in research and practice\nbased on an extensive review of the existing literature. It shows a generally\nlow awareness of security aspects among ML-practitioners and organizations and\nan often unstandardized approach to documentation, leading to overall low\nquality of ML-documentation. Existing standards are not regularly adopted in\npractice and IT-security aspects are often not included in documentation. Due\nto these factors, there is a clear need for improved security documentation in\nML, as one step towards addressing the existing gaps in ML-security. To achieve\nthis, we propose expanding existing documentation standards for\nML-documentation to include a security section with specific security relevant\ninformation. Implementing this, a novel expanded method of documenting security\nrequirements in ML-documentation is presented, based on the existing Model\nCards and Datasheets for Datasets standards, but with the recommendation to\nadopt these findings in all ML-documentation."}
{"id": "2507.11976", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11976", "abs": "https://arxiv.org/abs/2507.11976", "authors": ["Jana-Rebecca Rehse", "Michael Grohs", "Finn Klessascheck", "Lisa-Marie Klein", "Tatiana von Landesberger", "Luise Pufahl"], "title": "A Task Taxonomy for Conformance Checking", "comment": "Preprint submitted to Information Systems", "summary": "Conformance checking is a sub-discipline of process mining, which compares\nobserved process traces with a process model to analyze whether the process\nexecution conforms with or deviates from the process design. Organizations can\nleverage this analysis, for example to check whether their processes comply\nwith internal or external regulations or to identify potential improvements.\nGaining these insights requires suitable visualizations, which make complex\nresults accessible and actionable. So far, however, the development of\nconformance checking visualizations has largely been left to tool vendors. As a\nresult, current tools offer a wide variety of visual representations for\nconformance checking, but the analytical purposes they serve often remain\nunclear. However, without a systematic understanding of these purposes, it is\ndifficult to evaluate the visualizations' usefulness. Such an evaluation hence\nrequires a deeper understanding of conformance checking as an analysis domain.\nTo this end, we propose a task taxonomy, which categorizes the tasks that can\noccur when conducting conformance checking analyses. This taxonomy supports\nresearchers in determining the purpose of visualizations, specifying relevant\nconformance checking tasks in terms of their goal, means, constraint type, data\ncharacteristics, data target, and data cardinality. Combining concepts from\nprocess mining and visual analytics, we address researchers from both\ndisciplines to enable and support closer collaborations."}
{"id": "2507.12050", "categories": ["cs.CR", "cs.CV", "I.5.4; K.6.5; D.4.6; I.4.7"], "pdf": "https://arxiv.org/pdf/2507.12050", "abs": "https://arxiv.org/abs/2507.12050", "authors": ["Sunpill Kim", "Seunghun Paik", "Chanwoo Hwang", "Dongsoo Kim", "Junbum Shin", "Jae Hong Seo"], "title": "IDFace: Face Template Protection for Efficient and Secure Identification", "comment": "Accepted to ICCV 2025", "summary": "As face recognition systems (FRS) become more widely used, user privacy\nbecomes more important. A key privacy issue in FRS is protecting the user's\nface template, as the characteristics of the user's face image can be recovered\nfrom the template. Although recent advances in cryptographic tools such as\nhomomorphic encryption (HE) have provided opportunities for securing the FRS,\nHE cannot be used directly with FRS in an efficient plug-and-play manner. In\nparticular, although HE is functionally complete for arbitrary programs, it is\nbasically designed for algebraic operations on encrypted data of predetermined\nshape, such as a polynomial ring. Thus, a non-tailored combination of HE and\nthe system can yield very inefficient performance, and many previous HE-based\nface template protection methods are hundreds of times slower than plain\nsystems without protection. In this study, we propose IDFace, a new HE-based\nsecure and efficient face identification method with template protection.\nIDFace is designed on the basis of two novel techniques for efficient searching\non a (homomorphically encrypted) biometric database with an angular metric. The\nfirst technique is a template representation transformation that sharply\nreduces the unit cost for the matching test. The second is a space-efficient\nencoding that reduces wasted space from the encryption algorithm, thus saving\nthe number of operations on encrypted templates. Through experiments, we show\nthat IDFace can identify a face template from among a database of 1M encrypted\ntemplates in 126ms, showing only 2X overhead compared to the identification\nover plaintexts."}
{"id": "2507.12084", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.12084", "abs": "https://arxiv.org/abs/2507.12084", "authors": ["Keke Gai", "Haochen Liang", "Jing Yu", "Liehuang Zhu", "Dusit Niyato"], "title": "LLAMA: Multi-Feedback Smart Contract Fuzzing Framework with LLM-Guided Seed Generation", "comment": null, "summary": "Smart contracts play a pivotal role in blockchain ecosystems, and fuzzing\nremains an important approach to securing smart contracts. Even though mutation\nscheduling is a key factor influencing fuzzing effectiveness, existing fuzzers\nhave primarily explored seed scheduling and generation, while mutation\nscheduling has been rarely addressed by prior work. In this work, we propose a\nLarge Language Models (LLMs)-based Multi-feedback Smart Contract Fuzzing\nframework (LLAMA) that integrates LLMs, evolutionary mutation strategies, and\nhybrid testing techniques. Key components of the proposed LLAMA include: (i) a\nhierarchical prompting strategy that guides LLMs to generate semantically valid\ninitial seeds, coupled with a lightweight pre-fuzzing phase to select\nhigh-potential inputs; (ii) a multi-feedback optimization mechanism that\nsimultaneously improves seed generation, seed selection, and mutation\nscheduling by leveraging runtime coverage and dependency feedback; and (iii) an\nevolutionary fuzzing engine that dynamically adjusts mutation operator\nprobabilities based on effectiveness, while incorporating symbolic execution to\nescape stagnation and uncover deeper vulnerabilities. Our experiments\ndemonstrate that LLAMA outperforms state-of-the-art fuzzers in both coverage\nand vulnerability detection. Specifically, it achieves 91% instruction coverage\nand 90% branch coverage, while detecting 132 out of 148 known vulnerabilities\nacross diverse categories. These results highlight LLAMA's effectiveness,\nadaptability, and practicality in real-world smart contract security testing\nscenarios."}
{"id": "2507.12061", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.12061", "abs": "https://arxiv.org/abs/2507.12061", "authors": ["Zequan Huang", "Jacques Robin", "Nicolas Herbaut", "Nourhène Ben Rabah", "Bénédicte Le Grand"], "title": "Toward an Intent-Based and Ontology-Driven Autonomic Security Response in Security Orchestration Automation and Response", "comment": null, "summary": "Modern Security Orchestration, Automation, and Response (SOAR) platforms must\nrapidly adapt to continuously evolving cyber attacks. Intent-Based Networking\nhas emerged as a promising paradigm for cyber attack mitigation through\nhigh-level declarative intents, which offer greater flexibility and persistency\nthan procedural actions. In this paper, we bridge the gap between two active\nresearch directions: Intent-Based Cyber Defense and Autonomic Cyber Defense, by\nproposing a unified, ontology-driven security intent definition leveraging the\nMITRE-D3FEND cybersecurity ontology. We also propose a general two-tiered\nmethodology for integrating such security intents into decision-theoretic\nAutonomic Cyber Defense systems, enabling hierarchical and context-aware\nautomated response capabilities. The practicality of our approach is\ndemonstrated through a concrete use case, showcasing its integration within\nnext-generation Security Orchestration, Automation, and Response platforms."}
{"id": "2507.12104", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12104", "abs": "https://arxiv.org/abs/2507.12104", "authors": ["Francisco Javier Cavero", "Juan C. Alonso", "Antonio Ruiz-Cortés"], "title": "From Static to Intelligent: Evolving SaaS Pricing with LLMs", "comment": "12 pages. Accepted at the SOC4AI Workshop (Service-Oriented Computing\n  for AI Applications), held in conjunction with the 22nd International\n  Conference on Service-Oriented Computing (ICSOC 2024)", "summary": "The SaaS paradigm has revolutionized software distribution by offering\nflexible pricing options to meet diverse customer needs. However, the rapid\nexpansion of the SaaS market has introduced significant complexity for DevOps\nteams, who must manually manage and evolve pricing structures, an approach that\nis both time-consuming and prone to errors. The absence of automated tools for\npricing analysis restricts the ability to efficiently evaluate, optimize, and\nscale these models. This paper proposes leveraging intelligent pricing\n(iPricing), dynamic, machine-readable pricing models, as a solution to these\nchallenges. Intelligent pricing enables competitive analysis, streamlines\noperational decision-making, and supports continuous pricing evolution in\nresponse to market dynamics, leading to improved efficiency and accuracy. We\npresent an LLM-driven approach that automates the transformation of static HTML\npricing into iPricing, significantly improving efficiency and consistency while\nminimizing human error. Our implementation, AI4Pricing2Yaml, features a basic\nInformation Extractor that uses web scraping and LLMs technologies to extract\nessential pricing components, plans, features, usage limits, and add-ons, from\nSaaS websites. Validation against a dataset of 30 distinct commercial SaaS,\nencompassing over 150 intelligent pricings, demonstrates the system's\neffectiveness in extracting the desired elements across all steps. However,\nchallenges remain in addressing hallucinations, complex structures, and dynamic\ncontent. This work highlights the potential of automating intelligent pricing\ntransformation to streamline SaaS pricing management, offering implications for\nimproved consistency and scalability in an increasingly intricate pricing\nlandscape. Future research will focus on refining extraction capabilities and\nenhancing the system's adaptability to a wider range of SaaS websites."}
{"id": "2507.12098", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12098", "abs": "https://arxiv.org/abs/2507.12098", "authors": ["Xiang Li", "Yifan Lin", "Yuanzhe Zhang"], "title": "A Privacy-Preserving Framework for Advertising Personalization Incorporating Federated Learning and Differential Privacy", "comment": null, "summary": "To mitigate privacy leakage and performance issues in personalized\nadvertising, this paper proposes a framework that integrates federated learning\nand differential privacy. The system combines distributed feature extraction,\ndynamic privacy budget allocation, and robust model aggregation to balance\nmodel accuracy, communication overhead, and privacy protection. Multi-party\nsecure computing and anomaly detection mechanisms further enhance system\nresilience against malicious attacks. Experimental results demonstrate that the\nframework achieves dual optimization of recommendation accuracy and system\nefficiency while ensuring privacy, providing both a practical solution and a\ntheoretical foundation for applying privacy protection technologies in\nadvertisement recommendation."}
{"id": "2507.12118", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.12118", "abs": "https://arxiv.org/abs/2507.12118", "authors": ["Noe Zermeño", "Cristina Zuheros", "Lucas Daniel Del Rosso Calache", "Francisco Herrera", "Rosana Montes"], "title": "An Online A/B Testing Decision Support System for Web Usability Assessment Based on a Linguistic Decision-making Methodology: Case of Study a Virtual Learning Environment", "comment": null, "summary": "In recent years, attention has increasingly focused on enhancing user\nsatisfaction with user interfaces, spanning both mobile applications and\nwebsites. One fundamental aspect of human-machine interaction is the concept of\nweb usability. In order to assess web usability, the A/B testing technique\nenables the comparison of data between two designs. Expanding the scope of\ntests to include the designs being evaluated, in conjunction with the\ninvolvement of both real and fictional users, presents a challenge for which\nfew online tools offer support. We propose a methodology for web usability\nevaluation based on user-centered approaches such as design thinking and\nlinguistic decision-making, named Linguistic Decision-Making for Web Usability\nEvaluation. This engages people in role-playing scenarios and conducts a number\nof usability tests, including the widely recognized System Usability Scale. We\nincorporate the methodology into a decision support system based on A/B\ntesting. We use real users in a case study to assess three Moodle platforms at\nthe University of Guadalajara, Mexico."}
{"id": "2507.12185", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.12185", "abs": "https://arxiv.org/abs/2507.12185", "authors": ["Rina Mishra", "Gaurav Varshney"], "title": "Exploiting Jailbreaking Vulnerabilities in Generative AI to Bypass Ethical Safeguards for Facilitating Phishing Attacks", "comment": null, "summary": "The advent of advanced Generative AI (GenAI) models such as DeepSeek and\nChatGPT has significantly reshaped the cybersecurity landscape, introducing\nboth promising opportunities and critical risks. This study investigates how\nGenAI powered chatbot services can be exploited via jailbreaking techniques to\nbypass ethical safeguards, enabling the generation of phishing content,\nrecommendation of hacking tools, and orchestration of phishing campaigns. In\nethically controlled experiments, we used ChatGPT 4o Mini selected for its\naccessibility and status as the latest publicly available model at the time of\nexperimentation, as a representative GenAI system. Our findings reveal that the\nmodel could successfully guide novice users in executing phishing attacks\nacross various vectors, including web, email, SMS (smishing), and voice\n(vishing). Unlike automated phishing campaigns that typically follow detectable\npatterns, these human-guided, AI assisted attacks are capable of evading\ntraditional anti phishing mechanisms, thereby posing a growing security threat.\nWe focused on DeepSeek and ChatGPT due to their widespread adoption and\ntechnical relevance in 2025. The study further examines common jailbreaking\ntechniques and the specific vulnerabilities exploited in these models. Finally,\nwe evaluate a range of mitigation strategies such as user education, advanced\nauthentication mechanisms, and regulatory policy measures and discuss emerging\ntrends in GenAI facilitated phishing, outlining future research directions to\nstrengthen cybersecurity defenses in the age of artificial intelligence."}
{"id": "2507.12284", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12284", "abs": "https://arxiv.org/abs/2507.12284", "authors": ["Artem Chervyakov", "Alexander Kharitonov", "Pavel Zadorozhny", "Adamenko Pavel", "Rodion Levichev", "Dmitrii Vorobev", "Dmitrii Salikhov", "Aidar Valeev", "Alena Pestova", "Maria Dziuba", "Ilseyar Alimova", "Artem Zavgorodnev", "Aleksandr Medvedev", "Stanislav Moiseev", "Elena Bruches", "Daniil Grebenkin", "Roman Derunets", "Vikulov Vladimir", "Anton Emelyanov", "Dmitrii Babaev", "Vladimir V. Ivanov", "Valentin Malykh", "Alena Fenogenova"], "title": "MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks", "comment": null, "summary": "Advancements in LLMs have enhanced task automation in software engineering;\nhowever, current evaluations primarily focus on natural language tasks,\noverlooking code quality. Most benchmarks prioritize high-level reasoning over\nexecutable code and real-world performance, leaving gaps in understanding true\ncapabilities and risks associated with these models in production. To address\nthis issue, we propose MERA Code, a new addition to the MERA benchmark family,\nspecifically focused on evaluating code for the latest code generation LLMs in\nRussian. This benchmark includes 11 evaluation tasks that span 8 programming\nlanguages. Our proposed evaluation methodology features a taxonomy that\noutlines the practical coding skills necessary for models to complete these\ntasks. The benchmark comprises an open-source codebase for users to conduct\nMERA assessments, a scoring system compatible with various programming\nenvironments, and a platform featuring a leaderboard and submission system. We\nevaluate open LLMs and frontier API models, analyzing their limitations in\nterms of practical coding tasks in non-English languages. We are publicly\nreleasing MERA to guide future research, anticipate groundbreaking features in\nmodel development, and standardize evaluation procedures."}
{"id": "2507.12345", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.12345", "abs": "https://arxiv.org/abs/2507.12345", "authors": ["Liam Tyler", "Adam Caulfield", "Ivan De Oliveira Nunes"], "title": "Efficient Control Flow Attestation by Speculating on Control Flow Path Representations", "comment": null, "summary": "Control Flow Attestation (CFA) allows remote verification of run-time\nsoftware integrity in embedded systems. However, CFA is limited by the\nstorage/transmission costs of generated control flow logs (CFlog). Recent work\nhas proposed application-specific optimizations by speculating on likely\nsub-paths in CFlog and replacing them with reserved symbols at runtime. Albeit\neffective, prior approaches do not consider the representation of addresses in\na control flow path for speculation. This work proposes RESPEC-CFA, an\narchitectural extension for CFA allowing for speculation on (1) the locality of\ncontrol flows and (2) their Huffman encoding. Alone, RESPEC-CFA reduces CFlog\nsizes by up to 90.1%. Combined with prior methods, RESPEC-CFA yields reductions\nof up to 99.7%, representing a significant step toward practical CFA."}
{"id": "2507.12367", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.12367", "abs": "https://arxiv.org/abs/2507.12367", "authors": ["Diganta Misra", "Nizar Islah", "Victor May", "Brice Rauby", "Zihan Wang", "Justine Gehring", "Antonio Orvieto", "Muawiz Chaudhary", "Eilif B. Muller", "Irina Rish", "Samira Ebrahimi Kahou", "Massimo Caccia"], "title": "GitChameleon: Evaluating AI Code Generation Against Python Library Version Incompatibilities", "comment": "Version 2 of the dataset from: arXiv:2411.05830", "summary": "The rapid evolution of software libraries poses a considerable hurdle for\ncode generation, necessitating continuous adaptation to frequent version\nupdates while preserving backward compatibility. While existing code evolution\nbenchmarks provide valuable insights, they typically lack execution-based\nevaluation for generating code compliant with specific library versions. To\naddress this, we introduce GitChameleon, a novel, meticulously curated dataset\ncomprising 328 Python code completion problems, each conditioned on specific\nlibrary versions and accompanied by executable unit tests. GitChameleon\nrigorously evaluates the capacity of contemporary large language models (LLMs),\nLLM-powered agents, code assistants, and RAG systems to perform\nversion-conditioned code generation that demonstrates functional accuracy\nthrough execution. Our extensive evaluations indicate that state-of-the-art\nsystems encounter significant challenges with this task; enterprise models\nachieving baseline success rates in the 48-51\\% range, underscoring the\nintricacy of the problem. By offering an execution-based benchmark emphasizing\nthe dynamic nature of code libraries, GitChameleon enables a clearer\nunderstanding of this challenge and helps guide the development of more\nadaptable and dependable AI code generation methods. We make the dataset and\nevaluation code publicly available at\nhttps://github.com/mrcabbage972/GitChameleonBenchmark."}
{"id": "2507.12364", "categories": ["cs.CR", "cs.OS"], "pdf": "https://arxiv.org/pdf/2507.12364", "abs": "https://arxiv.org/abs/2507.12364", "authors": ["Adrien Ghosn", "Charly Castes", "Neelu S. Kalani", "Yuchen Qian", "Marios Kogias", "Edouard Bugnion"], "title": "Rethinking the confidential cloud through a unified low-level abstraction for composable isolation", "comment": null, "summary": "Securing sensitive cloud workloads requires composing confidential virtual\nmachines (CVMs) with nested enclaves or sandboxes. Unfortunately, each new\nisolation boundary adds ad-hoc access control mechanisms, hardware extensions,\nand trusted software. This escalating complexity bloats the TCB, complicates\nend-to-end attestation, and leads to fragmentation across platforms and cloud\nservice providers (CSPs).\n  We introduce a unified isolation model that delegates enforceable,\ncomposable, and attestable isolation to a single trusted security monitor:\nTyche. Tyche provides an API for partitioning, sharing, attesting, and\nreclaiming resources through its core abstraction, trust domains (TDs). To\nprovide fine-grain isolation, TDs can recursively create and manage sub-TDs.\nTyche captures these relationships in attestations, allowing cloud tenants to\nreason about end-to-end security. TDs serve as the building blocks for\nconstructing composable enclaves, sandboxes, and CVMs.\n  Tyche runs on commodity x86_64 without hardware security extensions and can\nmaintain backward compatibility with existing software. We provide an SDK to\nrun and compose unmodified workloads as sandboxes, enclaves, and CVMs with\nminimal overhead compared to native Linux execution. Tyche supports complex\ncloud scenarios, such as confidential inference with mutually distrustful\nusers, model owners, and CSPs. An additional RISC-V prototype demonstrates\nTyche's portability across platforms."}
{"id": "2507.12415", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.12415", "abs": "https://arxiv.org/abs/2507.12415", "authors": ["Xinyi He", "Qian Liu", "Mingzhe Du", "Lin Yan", "Zhijie Fan", "Yiming Huang", "Zejian Yuan", "Zejun Ma"], "title": "SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?", "comment": null, "summary": "Code performance optimization is paramount in real-world software engineering\nand critical for production-level systems. While Large Language Models (LLMs)\nhave demonstrated impressive capabilities in code generation and bug fixing,\ntheir proficiency in enhancing code performance at the repository level remains\nlargely unexplored. To address this gap, we introduce SWE-Perf, the first\nbenchmark specifically designed to systematically evaluate LLMs on code\nperformance optimization tasks within authentic repository contexts. SWE-Perf\ncomprises 140 carefully curated instances, each derived from\nperformance-improving pull requests from popular GitHub repositories. Each\nbenchmark instance includes the relevant codebase, target functions,\nperformance-related tests, expert-authored patches, and executable\nenvironments. Through a comprehensive evaluation of representative methods that\nspan file-level and repo-level approaches (e.g., Agentless and OpenHands), we\nreveal a substantial capability gap between existing LLMs and expert-level\noptimization performance, highlighting critical research opportunities in this\nemerging field."}
{"id": "2507.12456", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.12456", "abs": "https://arxiv.org/abs/2507.12456", "authors": ["Omri Shmueli", "Mark Zhandry"], "title": "On One-Shot Signatures, Quantum vs Classical Binding, and Obfuscating Permutations", "comment": null, "summary": "One-shot signatures (OSS) were defined by Amos, Georgiou, Kiayias, and\nZhandry (STOC'20). These allow for signing exactly one message, after which the\nsigning key self-destructs, preventing a second message from ever being signed.\nWhile such an object is impossible classically, Amos et al observe that OSS may\nbe possible using quantum signing keys by leveraging the no-cloning principle.\nOSS has since become an important conceptual tool with many applications in\ndecentralized settings and for quantum cryptography with classical\ncommunication. OSS are also closely related to separations between\nclassical-binding and collapse-binding for post-quantum hashing and\ncommitments. Unfortunately, the only known OSS construction due to Amos et al.\nwas only justified in a classical oracle model, and moreover their\njustification was ultimately found to contain a fatal bug. Thus, the existence\nof OSS, even in a classical idealized model, has remained open.\n  We give the first standard-model OSS, with provable security assuming\n(sub-exponential) indistinguishability obfuscation (iO) and LWE. This also\ngives the first standard-model separation between classical and\ncollapse-binding post-quantum commitments/hashing, solving a decade-old open\nproblem. Along the way, we also give the first construction with unconditional\nsecurity relative to a classical oracle. To achieve our standard-model\nconstruction, we develop a notion of permutable pseudorandom permutations\n(permutable PRPs), and show how they are useful for translating oracle proofs\ninvolving random permutations into obfuscation-based proofs. In particular,\nobfuscating permutable PRPs gives a trapdoor one-way permutation that is\n\\emph{full-domain}, solving another decade-old-problem of constructing this\nobject from (sub-exponential) iO and one-way functions."}
{"id": "2507.12003", "categories": ["cs.CR", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.12003", "abs": "https://arxiv.org/abs/2507.12003", "authors": ["Cara Ellen Appel"], "title": "Expanding ML-Documentation Standards For Better Security", "comment": "Accepted for publication at the 33rd IEEE International Requirements\n  Engineering Workshop (REW 2025)", "summary": "This article presents the current state of ML-security and of the\ndocumentation of ML-based systems, models and datasets in research and practice\nbased on an extensive review of the existing literature. It shows a generally\nlow awareness of security aspects among ML-practitioners and organizations and\nan often unstandardized approach to documentation, leading to overall low\nquality of ML-documentation. Existing standards are not regularly adopted in\npractice and IT-security aspects are often not included in documentation. Due\nto these factors, there is a clear need for improved security documentation in\nML, as one step towards addressing the existing gaps in ML-security. To achieve\nthis, we propose expanding existing documentation standards for\nML-documentation to include a security section with specific security relevant\ninformation. Implementing this, a novel expanded method of documenting security\nrequirements in ML-documentation is presented, based on the existing Model\nCards and Datasheets for Datasets standards, but with the recommendation to\nadopt these findings in all ML-documentation."}
{"id": "2507.12084", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.12084", "abs": "https://arxiv.org/abs/2507.12084", "authors": ["Keke Gai", "Haochen Liang", "Jing Yu", "Liehuang Zhu", "Dusit Niyato"], "title": "LLAMA: Multi-Feedback Smart Contract Fuzzing Framework with LLM-Guided Seed Generation", "comment": null, "summary": "Smart contracts play a pivotal role in blockchain ecosystems, and fuzzing\nremains an important approach to securing smart contracts. Even though mutation\nscheduling is a key factor influencing fuzzing effectiveness, existing fuzzers\nhave primarily explored seed scheduling and generation, while mutation\nscheduling has been rarely addressed by prior work. In this work, we propose a\nLarge Language Models (LLMs)-based Multi-feedback Smart Contract Fuzzing\nframework (LLAMA) that integrates LLMs, evolutionary mutation strategies, and\nhybrid testing techniques. Key components of the proposed LLAMA include: (i) a\nhierarchical prompting strategy that guides LLMs to generate semantically valid\ninitial seeds, coupled with a lightweight pre-fuzzing phase to select\nhigh-potential inputs; (ii) a multi-feedback optimization mechanism that\nsimultaneously improves seed generation, seed selection, and mutation\nscheduling by leveraging runtime coverage and dependency feedback; and (iii) an\nevolutionary fuzzing engine that dynamically adjusts mutation operator\nprobabilities based on effectiveness, while incorporating symbolic execution to\nescape stagnation and uncover deeper vulnerabilities. Our experiments\ndemonstrate that LLAMA outperforms state-of-the-art fuzzers in both coverage\nand vulnerability detection. Specifically, it achieves 91% instruction coverage\nand 90% branch coverage, while detecting 132 out of 148 known vulnerabilities\nacross diverse categories. These results highlight LLAMA's effectiveness,\nadaptability, and practicality in real-world smart contract security testing\nscenarios."}
