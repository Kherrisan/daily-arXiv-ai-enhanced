{"id": "2509.16274", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.16274", "abs": "https://arxiv.org/abs/2509.16274", "authors": ["Uwe Serd√ºlt"], "title": "Reconnecting Citizens to Politics via Blockchain - Starting the Debate", "comment": "Published as Proceedings of Ongoing Research, Practitioners, Posters,\n  Workshops, and Projects of the International Conference EGOV-CeDEM-ePart 2019", "summary": "Elections are not the only but arguably one of the most important pillars for\nthe proper functioning of liberal democracies. Recent evidence across the globe\nshows that it is not straightforward to conduct them in a free and fair manner.\nOne constant concern is the role of money in politics, more specifically,\nelection campaign financing. Frequent scandals are proof of the difficulties\nencountered with current approaches to tackle the issue. Suggestions on how to\novercome the problem exist but seem difficult to implement. With the help of\nblockchain technology we might be able to make a step forward. A separate\ncrypto currency specifically designed to pay for costs of political campaigning\nand advertising could be introduced. Admittedly, at this stage, there are many\nopen questions. However, under the assumption that blockchain technology is\nhere to stay, it is an idea that deserves further exploration."}
{"id": "2509.16268", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16268", "abs": "https://arxiv.org/abs/2509.16268", "authors": ["Zhenlan Ji", "Daoyuan Wu", "Wenxuan Wang", "Pingchuan Ma", "Shuai Wang", "Lei Ma"], "title": "Digging Into the Internal: Causality-Based Analysis of LLM Function Calling", "comment": null, "summary": "Function calling (FC) has emerged as a powerful technique for facilitating\nlarge language models (LLMs) to interact with external systems and perform\nstructured tasks. However, the mechanisms through which it influences model\nbehavior remain largely under-explored. Besides, we discover that in addition\nto the regular usage of FC, this technique can substantially enhance the\ncompliance of LLMs with user instructions. These observations motivate us to\nleverage causality, a canonical analysis method, to investigate how FC works\nwithin LLMs. In particular, we conduct layer-level and token-level causal\ninterventions to dissect FC's impact on the model's internal computational\nlogic when responding to user queries. Our analysis confirms the substantial\ninfluence of FC and reveals several in-depth insights into its mechanisms. To\nfurther validate our findings, we conduct extensive experiments comparing the\neffectiveness of FC-based instructions against conventional prompting methods.\nWe focus on enhancing LLM safety robustness, a critical LLM application\nscenario, and evaluate four mainstream LLMs across two benchmark datasets. The\nresults are striking: FC shows an average performance improvement of around\n135% over conventional prompting methods in detecting malicious inputs,\ndemonstrating its promising potential to enhance LLM reliability and capability\nin practical applications."}
{"id": "2509.16275", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16275", "abs": "https://arxiv.org/abs/2509.16275", "authors": ["Jugal Gajjar", "Kamalasankari Subramaniakuppusamy", "Relsy Puthal", "Kaustik Ranaware"], "title": "SecureFixAgent: A Hybrid LLM Agent for Automated Python Static Vulnerability Repair", "comment": "6 pages, 3 figures, 4 tables, 1 algorithm, accepted in the Robustness\n  and Security of Large Language Models (ROSE-LLM) special session at ICMLA\n  2025", "summary": "Modern software development pipelines face growing challenges in securing\nlarge codebases with extensive dependencies. Static analysis tools like Bandit\nare effective at vulnerability detection but suffer from high false positives\nand lack repair capabilities. Large Language Models (LLMs), in contrast, can\nsuggest fixes but often hallucinate changes and lack self-validation. We\npresent SecureFixAgent, a hybrid repair framework integrating Bandit with\nlightweight local LLMs (<8B parameters) in an iterative detect-repair-validate\nloop. To improve precision, we apply parameter-efficient LoRA-based fine-tuning\non a diverse, curated dataset spanning multiple Python project domains,\nmitigating dataset bias and reducing unnecessary edits. SecureFixAgent uses\nBandit for detection, the LLM for candidate fixes with explanations, and Bandit\nre-validation for verification, all executed locally to preserve privacy and\nreduce cloud reliance. Experiments show SecureFixAgent reduces false positives\nby 10.8% over static analysis, improves fix accuracy by 13.51%, and lowers\nfalse positives by 5.46% compared to pre-trained LLMs, typically converging\nwithin three iterations. Beyond metrics, developer studies rate explanation\nquality 4.5/5, highlighting its value for human trust and adoption. By\ncombining verifiable security improvements with transparent rationale in a\nresource-efficient local framework, SecureFixAgent advances trustworthy,\nautomated vulnerability remediation for modern pipelines."}
{"id": "2509.16478", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16478", "abs": "https://arxiv.org/abs/2509.16478", "authors": ["Hossein Yousefizadeh", "Shenghui Gu", "Lionel C. Briand", "Ali Nasr"], "title": "Constrained Co-evolutionary Metamorphic Differential Testing for Autonomous Systems with an Interpretability Approach", "comment": null, "summary": "Autonomous systems, such as autonomous driving systems, evolve rapidly\nthrough frequent updates, risking unintended behavioral degradations. Effective\nsystem-level testing is challenging due to the vast scenario space, the absence\nof reliable test oracles, and the need for practically applicable and\ninterpretable test cases. We present CoCoMagic, a novel automated test case\ngeneration method that combines metamorphic testing, differential testing, and\nadvanced search-based techniques to identify behavioral divergences between\nversions of autonomous systems. CoCoMagic formulates test generation as a\nconstrained cooperative co-evolutionary search, evolving both source scenarios\nand metamorphic perturbations to maximize differences in violations of\npredefined metamorphic relations across versions. Constraints and population\ninitialization strategies guide the search toward realistic, relevant\nscenarios. An integrated interpretability approach aids in diagnosing the root\ncauses of divergences. We evaluate CoCoMagic on an end-to-end ADS, InterFuser,\nwithin the Carla virtual simulator. Results show significant improvements over\nbaseline search methods, identifying up to 287\\% more distinct high-severity\nbehavioral differences while maintaining scenario realism. The interpretability\napproach provides actionable insights for developers, supporting targeted\ndebugging and safety assessment. CoCoMagic offers an efficient, effective, and\ninterpretable way for the differential testing of evolving autonomous systems\nacross versions."}
{"id": "2509.16292", "categories": ["cs.CR", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.16292", "abs": "https://arxiv.org/abs/2509.16292", "authors": ["Qian'ang Mao", "Jiaxin Wang", "Zhiqi Feng", "Yi Zhang", "Jiaqi Yan"], "title": "Decoding TRON: A Comprehensive Framework for Large-Scale Blockchain Data Extraction and Exploration", "comment": "written in early 2024", "summary": "Cryptocurrencies and Web3 applications based on blockchain technology have\nflourished in the blockchain research field. Unlike Bitcoin and Ethereum, due\nto its unique architectural designs in consensus mechanisms, resource\nmanagement, and throughput, TRON has developed a more distinctive ecosystem and\napplication scenarios centered around stablecoins. Although it is popular in\nareas like stablecoin payments and settlement, research on analyzing on-chain\ndata from the TRON blockchain is remarkably scarce. To fill this gap, this\npaper proposes a comprehensive data extraction and exploration framework for\nthe TRON blockchain. An innovative high-performance ETL system aims to\nefficiently extract raw on-chain data from TRON, including blocks,\ntransactions, smart contracts, and receipts, establishing a research dataset.\nAn in-depth analysis of the extracted dataset reveals insights into TRON's\nblock generation, transaction trends, the dominance of exchanges, the resource\ndelegation market, smart contract usage patterns, and the central role of the\nUSDT stablecoin. The prominence of gambling applications and potential illicit\nactivities related to USDT is emphasized. The paper discusses opportunities for\nfuture research leveraging this dataset, including analysis of delegate\nservices, gambling scenarios, stablecoin activities, and illicit transaction\ndetection. These contributions enhance blockchain data management capabilities\nand understanding of the rapidly evolving TRON ecosystem."}
{"id": "2509.16525", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16525", "abs": "https://arxiv.org/abs/2509.16525", "authors": ["Anna Mazhar", "Sainyam Galhotra"], "title": "Causal Fuzzing for Verifying Machine Unlearning", "comment": null, "summary": "As machine learning models become increasingly embedded in decision-making\nsystems, the ability to \"unlearn\" targeted data or features is crucial for\nenhancing model adaptability, fairness, and privacy in models which involves\nexpensive training. To effectively guide machine unlearning, a thorough testing\nis essential. Existing methods for verification of machine unlearning provide\nlimited insights, often failing in scenarios where the influence is indirect.\nIn this work, we propose CAF\\'E, a new causality based framework that unifies\ndatapoint- and feature-level unlearning for verification of black-box ML\nmodels. CAF\\'E evaluates both direct and indirect effects of unlearning targets\nthrough causal dependencies, providing actionable insights with fine-grained\nanalysis. Our evaluation across five datasets and three model architectures\ndemonstrates that CAF\\'E successfully detects residual influence missed by\nbaselines while maintaining computational efficiency."}
{"id": "2509.16340", "categories": ["cs.CR", "cs.OS"], "pdf": "https://arxiv.org/pdf/2509.16340", "abs": "https://arxiv.org/abs/2509.16340", "authors": ["Mohammad Hossein Asghari", "Lianying Zhao"], "title": "To Unpack or Not to Unpack: Living with Packers to Enable Dynamic Analysis of Android Apps", "comment": null, "summary": "Android apps have become a valuable target for app modifiers and imitators\ndue to its popularity and being trusted with highly sensitive data. Packers, on\nthe other hand, protect apps from tampering with various anti-analysis\ntechniques embedded in the app. Meanwhile, packers also conceal certain\nbehavior potentially against the interest of the users, aside from being abused\nby malware for stealth. Security practitioners typically try to capture\nundesired behavior at runtime with hooking (e.g., Frida) or debugging\ntechniques, which are heavily affected by packers. Unpackers have been the\ncommunity's continuous effort to address this, but due to the emerging\ncommercial packers, our study shows that none of the unpackers remain\neffective, and they are unfit for this purpose as unpacked apps can no longer\nrun. We first perform a large-scale prevalence analysis of Android packers with\na real-world dataset of 12,341 apps, the first of its kind, to find out what\npercentage of Android apps are actually packed and to what extent dynamic\nanalysis is hindered. We then propose Purifire, an evasion engine to bypass\npackers' anti-analysis techniques and enable dynamic analysis on packed apps\nwithout unpacking them. Purifire is based on eBPF, a low-level kernel feature,\nwhich provides observability and invisibility to userspace apps to enforce\ndefined evasion rules while staying low-profile. Our evaluation shows that\nPurifire is able to bypass packers' anti-analysis checks and more importantly,\nfor previous research works suffering from packers, we observe a significant\nimprovement (e.g., a much higher number of detected items such as device\nfingerprints)."}
{"id": "2509.16595", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16595", "abs": "https://arxiv.org/abs/2509.16595", "authors": ["Jiaming Ye", "Xiongfei Wu", "Shangzhou Xia", "Fuyuan Zhang", "Jianjun Zhao"], "title": "Is Measurement Enough? Rethinking Output Validation in Quantum Program Testing", "comment": "This paper will be appeared in the proceedings of the 40th IEEE/ACM\n  International Conference on Automated Software Engineering (ASE 2025), NIER\n  track, Seoul, South Korea, November 16 -20, 2025", "summary": "As quantum computing continues to emerge, ensuring the quality of quantum\nprograms has become increasingly critical. Quantum program testing has emerged\nas a prominent research area within the scope of quantum software engineering.\nWhile numerous approaches have been proposed to address quantum program quality\nassurance, our analysis reveals that most existing methods rely on\nmeasurement-based validation in practice. However, due to the inherently\nprobabilistic nature of quantum programs, measurement-based validation methods\nface significant limitations.\n  To investigate these limitations, we conducted an empirical study of recent\nresearch on quantum program testing, analyzing measurement-based validation\nmethods in the literature. Our analysis categorizes existing measurement-based\nvalidation methods into two groups: distribution-level validation and\noutput-value-level validation. We then compare measurement-based validation\nwith statevector-based validation methods to evaluate their pros and cons. Our\nfindings demonstrate that measurement-based validation is suitable for\nstraightforward assessments, such as verifying the existence of specific output\nvalues, while statevector-based validation proves more effective for\ncomplicated tasks such as assessing the program behaviors."}
{"id": "2509.16352", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16352", "abs": "https://arxiv.org/abs/2509.16352", "authors": ["Yunfan Yang", "Jiarong Xu", "Hongzhe Zhang", "Xiao Fang"], "title": "Secure Confidential Business Information When Sharing Machine Learning Models", "comment": null, "summary": "Model-sharing offers significant business value by enabling firms with\nwell-established Machine Learning (ML) models to monetize and share their\nmodels with others who lack the resources to develop ML models from scratch.\nHowever, concerns over data confidentiality remain a significant barrier to\nmodel-sharing adoption, as Confidential Property Inference (CPI) attacks can\nexploit shared ML models to uncover confidential properties of the model\nprovider's private model training data. Existing defenses often assume that CPI\nattacks are non-adaptive to the specific ML model they are targeting. This\nassumption overlooks a key characteristic of real-world adversaries: their\nresponsiveness, i.e., adversaries' ability to dynamically adjust their attack\nmodels based on the information of the target and its defenses. To overcome\nthis limitation, we propose a novel defense method that explicitly accounts for\nthe responsive nature of real-world adversaries via two methodological\ninnovations: a novel Responsive CPI attack and an attack-defense arms race\nframework. The former emulates the responsive behaviors of adversaries in the\nreal world, and the latter iteratively enhances both the target and attack\nmodels, ultimately producing a secure ML model that is robust against\nresponsive CPI attacks. Furthermore, we propose and integrate a novel\napproximate strategy into our defense, which addresses a critical computational\nbottleneck of defense methods and improves defense efficiency. Through\nextensive empirical evaluations across various realistic model-sharing\nscenarios, we demonstrate that our method outperforms existing defenses by more\neffectively defending against CPI attacks, preserving ML model utility, and\nreducing computational overhead."}
{"id": "2509.16655", "categories": ["cs.SE", "cs.CR", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2509.16655", "abs": "https://arxiv.org/abs/2509.16655", "authors": ["Serena Wang", "Martino Banchio", "Krzysztof Kotowicz", "Katrina Ligett", "R. Preston McAfee", "Eduardo' Vela'' Nava"], "title": "Incentives and Outcomes in Bug Bounties", "comment": null, "summary": "Bug bounty programs have contributed significantly to security in technology\nfirms in the last decade, but little is known about the role of reward\nincentives in producing useful outcomes. We analyze incentives and outcomes in\nGoogle's Vulnerability Rewards Program (VRP), one of the world's largest bug\nbounty programs. We analyze the responsiveness of the quality and quantity of\nbugs received to changes in payments, focusing on a change in Google's reward\namounts posted in July, 2024, in which reward amounts increased by up to 200%\nfor the highest impact tier. Our empirical results show an increase in the\nvolume of high-value bugs received after the reward increase, for which we also\ncompute elasticities. We further break down the sources of this increase\nbetween veteran researchers and new researchers, showing that the reward\nincrease both redirected the attention of veteran researchers and attracted new\ntop security researchers into the program."}
{"id": "2509.16389", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16389", "abs": "https://arxiv.org/abs/2509.16389", "authors": ["Tianrou Xia", "Kaiming Huang", "Dongyeon Yu", "Yuseok Jeon", "Jie Zhou", "Dinghao Wu", "Taegyu Kim"], "title": "LiteRSan: Lightweight Memory Safety Via Rust-specific Program Analysis and Selective Instrumentation", "comment": "14 pages (main text), 18 pages including references and appendix, 2\n  figures", "summary": "Rust is a memory-safe language, and its strong safety guarantees combined\nwith high performance have been attracting widespread adoption in systems\nprogramming and security-critical applications. However, Rust permits the use\nof unsafe code, which bypasses compiler-enforced safety checks and can\nintroduce memory vulnerabilities. A widely adopted approach for detecting\nmemory safety bugs in Rust is Address Sanitizer (ASan). Optimized versions,\nsuch as ERASan and RustSan, have been proposed to selectively apply security\nchecks in order to reduce performance overhead. However, these tools still\nincur significant performance and memory overhead and fail to detect many\nclasses of memory safety vulnerabilities due to the inherent limitations of\nASan. In this paper, we present LiteRSan, a novel memory safety sanitizer that\naddresses the limitations of prior approaches. By leveraging Rust's unique\nownership model, LiteRSan performs Rust-specific static analysis that is aware\nof pointer lifetimes to identify risky pointers. It then selectively\ninstruments risky pointers to enforce only the necessary spatial or temporal\nmemory safety checks. Consequently, LiteRSan introduces significantly lower\nruntime overhead (18.84% versus 152.05% and 183.50%) and negligible memory\noverhead (0.81% versus 739.27% and 861.98%) compared with existing ASan-based\nsanitizers while being capable of detecting memory safety bugs that prior\ntechniques miss."}
{"id": "2509.16681", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.16681", "abs": "https://arxiv.org/abs/2509.16681", "authors": ["Peterson Jean"], "title": "Verifying User Interfaces using SPARK Ada: A Case Study of the T34 Syringe Driver", "comment": "62 pages. Master's dissertation submitted to Swansea University,\n  Department of Computer Science, September 2022. Supervisor Dr Jens Blanck", "summary": "The increase in safety and critical systems improved Healthcare. Due to their\nrisk of harm, such systems are subject to stringent guidelines and compliances.\nThese safety measures ensure a seamless experience and mitigate the risk to\nend-users. Institutions like the Food and Drug Administration and the NHS,\nrespectively, established international standards and competency frameworks to\nensure industry compliance with these safety concerns. Medical device\nmanufacturing is mainly concerned with standards. Consequently, these standards\nnow advocate for better human factors considered in user interaction for\nmedical devices. This forces manufacturers to rely on heavy testing and review\nto cover many of these factors during development. Sadly, many human factor\nrisks will not be caught until proper testing in real life, which might be\ncatastrophic in the case of an ambulatory device like the T34 syringe pump.\nTherefore, effort in formal methods research may propose new solutions in\nanticipating these errors in the early stages of development or even reducing\ntheir occurrence based on the use of standard generic model. These generically\ndeveloped models will provide a common framework for safety integration in\nindustry and may potentially be proven using formal verification mathematical\nproofs. This research uses SPARK Ada's formal verification tool against a\nbehavioural model of the T34 syringe driver. A Generic Infusion Pump model\nrefinement is explored and implemented in SPARK Ada. As a subset of the Ada\nlanguage, the verification level of the end prototype is evaluated using SPARK.\nExploring potential limitations defines the proposed model's implementation\nliability when considering abstraction and components of User Interface design\nin SPARK Ada."}
{"id": "2509.16390", "categories": ["cs.CR", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.16390", "abs": "https://arxiv.org/abs/2509.16390", "authors": ["Mohamed Abdessamed Rezazi", "Mouhamed Amine Bouchiha", "Ahmed Mounsf Rafik Bendada", "Yacine Ghamri-Doudane"], "title": "B5GRoam: A Zero Trust Framework for Secure and Efficient On-Chain B5G Roaming", "comment": "6 pages, 2 figures, Accepted at GLOBECOM'25", "summary": "Roaming settlement in 5G and beyond networks demands secure, efficient, and\ntrustworthy mechanisms for billing reconciliation between mobile operators.\nWhile blockchain promises decentralization and auditability, existing solutions\nsuffer from critical limitations-namely, data privacy risks, assumptions of\nmutual trust, and scalability bottlenecks. To address these challenges, we\npresent B5GRoam, a novel on-chain and zero-trust framework for secure,\nprivacy-preserving, and scalable roaming settlements. B5GRoam introduces a\ncryptographically verifiable call detail record (CDR) submission protocol,\nenabling smart contracts to authenticate usage claims without exposing\nsensitive data. To preserve privacy, we integrate non-interactive\nzero-knowledge proofs (zkSNARKs) that allow on-chain verification of roaming\nactivity without revealing user or network details. To meet the high-throughput\ndemands of 5G environments, B5GRoam leverages Layer 2 zk-Rollups, significantly\nreducing gas costs while maintaining the security guarantees of Layer 1.\nExperimental results demonstrate a throughput of over 7,200 tx/s with strong\nprivacy and substantial cost savings. By eliminating intermediaries and\nenhancing verifiability, B5GRoam offers a practical and secure foundation for\ndecentralized roaming in future mobile networks."}
{"id": "2509.16701", "categories": ["cs.SE", "D.2.5; I.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.16701", "abs": "https://arxiv.org/abs/2509.16701", "authors": ["Shunyu Liu", "Guangdong Bai", "Mark Utting", "Guowei Yang"], "title": "RelRepair: Enhancing Automated Program Repair by Retrieving Relevant Code", "comment": "11 pages, 5 figures, under review at TSE", "summary": "Automated Program Repair (APR) has emerged as a promising paradigm for\nreducing debugging time and improving the overall efficiency of software\ndevelopment. Recent advances in Large Language Models (LLMs) have demonstrated\ntheir potential for automated bug fixing and other software engineering tasks.\nNevertheless, the general-purpose nature of LLM pre-training means these models\noften lack the capacity to perform project-specific repairs, which require\nunderstanding of domain-specific identifiers, code structures, and contextual\nrelationships within a particular codebase. As a result, LLMs may struggle to\ngenerate correct patches when the repair depends on project-specific\ninformation.\n  To address this limitation, we introduce RelRepair, a novel approach that\nretrieves relevant project-specific code to enhance automated program repair.\nRelRepair first identifies relevant function signatures by analyzing function\nnames and code comments within the project. It then conducts deeper code\nanalysis to retrieve code snippets relevant to the repair context. The\nretrieved relevant information is then incorporated into the LLM's input\nprompt, guiding the model to generate more accurate and informed patches. We\nevaluate RelRepair on two widely studied datasets, Defects4J V1.2 and\nManySStuBs4J, and compare its performance against several state-of-the-art\nLLM-based APR approaches. RelRepair successfully repairs 101 bugs in Defects4J\nV1.2. Furthermore, RelRepair achieves a 17.1\\% improvement in the ManySStuBs4J\ndataset, increasing the overall fix rate to 48.3\\%. These results highlight the\nimportance of providing relevant project-specific information to LLMs, shedding\nlight on effective strategies for leveraging LLMs in APR tasks."}
{"id": "2509.16418", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16418", "abs": "https://arxiv.org/abs/2509.16418", "authors": ["Petr Grinberg", "Eric Bezzam", "Paolo Prandoni", "Martin Vetterli"], "title": "LenslessMic: Audio Encryption and Authentication via Lensless Computational Imaging", "comment": "Submitted to ICASSP 2026", "summary": "With society's increasing reliance on digital data sharing, the protection of\nsensitive information has become critical. Encryption serves as one of the\nprivacy-preserving methods; however, its realization in the audio domain\npredominantly relies on signal processing or software methods embedded into\nhardware. In this paper, we introduce LenslessMic, a hybrid optical\nhardware-based encryption method that utilizes a lensless camera as a physical\nlayer of security applicable to multiple types of audio. We show that\nLenslessMic enables (1) robust authentication of audio recordings and (2)\nencryption strength that can rival the search space of 256-bit digital\nstandards, while maintaining high-quality signals and minimal loss of content\ninformation. The approach is validated with a low-cost Raspberry Pi prototype\nand is open-sourced together with datasets to facilitate research in the area."}
{"id": "2509.16795", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16795", "abs": "https://arxiv.org/abs/2509.16795", "authors": ["Saikat Mondal", "Chanchal K. Roy", "Hong Wang", "Juan Arguello", "Samantha Mathan"], "title": "Can We Trust the AI Pair Programmer? Copilot for API Misuse Detection and Correction", "comment": "Accepted in the 35th IEEE International Conference on Collaborative\n  Advances in Software Computing", "summary": "API misuse introduces security vulnerabilities, system failures, and\nincreases maintenance costs, all of which remain critical challenges in\nsoftware development. Existing detection approaches rely on static analysis or\nmachine learning-based tools that operate post-development, which delays defect\nresolution. Delayed defect resolution can significantly increase the cost and\ncomplexity of maintenance and negatively impact software reliability and user\ntrust. AI-powered code assistants, such as GitHub Copilot, offer the potential\nfor real-time API misuse detection within development environments. This study\nevaluates GitHub Copilot's effectiveness in identifying and correcting API\nmisuse using MUBench, which provides a curated benchmark of misuse cases. We\nconstruct 740 misuse examples, manually and via AI-assisted variants, using\ncorrect usage patterns and misuse specifications. These examples and 147\ncorrect usage cases are analyzed using Copilot integrated in Visual Studio\nCode. Copilot achieved a detection accuracy of 86.2%, precision of 91.2%, and\nrecall of 92.4%. It performed strongly on common misuse types (e.g.,\nmissing-call, null-check) but struggled with compound or context-sensitive\ncases. Notably, Copilot successfully fixed over 95% of the misuses it\nidentified. These findings highlight both the strengths and limitations of\nAI-driven coding assistants, positioning Copilot as a promising tool for\nreal-time pair programming and detecting and fixing API misuses during software\ndevelopment."}
{"id": "2509.16489", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16489", "abs": "https://arxiv.org/abs/2509.16489", "authors": ["Minhaj Uddin Ahmad", "Akid Abrar", "Sagar Dasgupta", "Mizanur Rahman"], "title": "End-to-End Co-Simulation Testbed for Cybersecurity Research and Development in Intelligent Transportation Systems", "comment": null, "summary": "Intelligent Transportation Systems (ITS) have been widely deployed across\nmajor metropolitan regions worldwide to improve roadway safety, optimize\ntraffic flow, and reduce environmental impacts. These systems integrate\nadvanced sensors, communication networks, and data analytics to enable\nreal-time traffic monitoring, adaptive signal control, and predictive\nmaintenance. However, such integration significantly broadens the ITS attack\nsurface, exposing critical infrastructures to cyber threats that jeopardize\nsafety, data integrity, and operational resilience. Ensuring robust\ncybersecurity is therefore essential, yet comprehensive vulnerability\nassessments, threat modeling, and mitigation validations are often\ncost-prohibitive and time-intensive when applied to large-scale, heterogeneous\ntransportation systems. Simulation platforms offer a cost-effective and\nrepeatable means for cybersecurity evaluation, and the simulation platform\nshould encompass the full range of ITS dimensions - mobility, sensing,\nnetworking, and applications. This chapter discusses an integrated\nco-simulation testbed that links CARLA for 3D environment and sensor modeling,\nSUMO for microscopic traffic simulation and control, and OMNeT++ for V2X\ncommunication simulation. The co-simulation testbed enables end-to-end\nexperimentation, vulnerability identification, and mitigation benchmarking,\nproviding practical insights for developing secure, efficient, and resilient\nITS infrastructures. To illustrate its capabilities, the chapter incorporates a\ncase study on a C-V2X proactive safety alert system enhanced with post-quantum\ncryptography, highlighting the role of the testbed in advancing secure and\nresilient ITS infrastructures."}
{"id": "2509.16844", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16844", "abs": "https://arxiv.org/abs/2509.16844", "authors": ["Rim Zrelli", "Henrique Amaral Misson", "Sorelle Kamkuimo", "Maroua Ben Attia", "Abdo Shabah", "Felipe Gohring de Magalhaes", "Gabriela Nicolescu"], "title": "Implementation of the Collision Avoidance System for DO-178C Compliance", "comment": null, "summary": "This technical report presents the detailed implementation of a Collision\nAvoidance System (CAS) for Unmanned Aerial Vehicles (UAVs), developed as a case\nstudy to demonstrate a rigorous methodology for achieving DO-178C compliance in\nsafety-critical software. The CAS is based on functional requirements inspired\nby NASA's Access 5 project and is designed to autonomously detect, evaluate,\nand avoid potential collision threats in real-time, supporting the safe\nintegration of UAVs into civil airspace.\n  The implementation environment combines formal methods, model-based\ndevelopment, and automated verification tools, including Alloy, SPIN, Simulink\nEmbedded Coder, and the LDRA tool suite. The report documents each phase of the\nsoftware lifecycle: requirements specification and validation, architectural\nand detailed design, coding, verification, and traceability, with a strong\nfocus on compliance with DO-178C Design Assurance Level B objectives.\n  Results demonstrate that formal modelling and automated toolchains enabled\nearly detection and correction of specification defects, robust traceability,\nand strong evidence of verification and validation across all development\nstages. Static and dynamic analyses confirmed code quality and coverage, while\nformal verification methods provided mathematical assurance of correctness for\ncritical components. Although the integration phase was not fully implemented,\nthe approach proved effective in addressing certification challenges for UAV\nsafety-critical systems.\n  \\keywords Collision Avoidance System (CAS), Unmanned Aerial Vehicles (UAVs),\nDO-178C compliance, Safety-critical software, Formal methods, Model-based\ndevelopment, Alloy, SPIN model checker, Simulink Embedded Coder, LDRA tool\nsuite, Software verification and validation, Traceability, Certification."}
{"id": "2509.16546", "categories": ["cs.CR", "cs.AI", "F.2.2, I.2.7"], "pdf": "https://arxiv.org/pdf/2509.16546", "abs": "https://arxiv.org/abs/2509.16546", "authors": ["Ashley Kurian", "Aydin Aysu"], "title": "Train to Defend: First Defense Against Cryptanalytic Neural Network Parameter Extraction Attacks", "comment": "18 pages, 3 Figures", "summary": "Neural networks are valuable intellectual property due to the significant\ncomputational cost, expert labor, and proprietary data involved in their\ndevelopment. Consequently, protecting their parameters is critical not only for\nmaintaining a competitive advantage but also for enhancing the model's security\nand privacy. Prior works have demonstrated the growing capability of\ncryptanalytic attacks to scale to deeper models. In this paper, we present the\nfirst defense mechanism against cryptanalytic parameter extraction attacks. Our\nkey insight is to eliminate the neuron uniqueness necessary for these attacks\nto succeed. We achieve this by a novel, extraction-aware training method.\nSpecifically, we augment the standard loss function with an additional\nregularization term that minimizes the distance between neuron weights within a\nlayer. Therefore, the proposed defense has zero area-delay overhead during\ninference. We evaluate the effectiveness of our approach in mitigating\nextraction attacks while analyzing the model accuracy across different\narchitectures and datasets. When re-trained with the same model architecture,\nthe results show that our defense incurs a marginal accuracy change of less\nthan 1% with the modified loss function. Moreover, we present a theoretical\nframework to quantify the success probability of the attack. When tested\ncomprehensively with prior attack settings, our defense demonstrated empirical\nsuccess for sustained periods of extraction, whereas unprotected networks are\nextracted between 14 minutes to 4 hours."}
{"id": "2509.16864", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16864", "abs": "https://arxiv.org/abs/2509.16864", "authors": ["Wei Liu", "Yi Wen Heng", "Feng Lin", "Tse-Hsun", "Chen", "Ahmed E. Hassan"], "title": "MobileUPReg: Identifying User-Perceived Performance Regressions in Mobile OS Versions", "comment": "ASE 2025 Industry Showcase", "summary": "Mobile operating systems (OS) are frequently updated, but such updates can\nunintentionally degrade user experience by introducing performance regressions.\nExisting detection techniques often rely on system-level metrics (e.g., CPU or\nmemory usage) or focus on specific OS components, which may miss regressions\nactually perceived by users -- such as slower responses or UI stutters. To\naddress this gap, we present MobileUPReg, a black-box framework for detecting\nuser-perceived performance regressions across OS versions. MobileUPReg runs the\nsame apps under different OS versions and compares user-perceived performance\nmetrics -- response time, finish time, launch time, and dropped frames -- to\nidentify regressions that are truly perceptible to users. In a large-scale\nstudy, MobileUPReg achieves high accuracy in extracting user-perceived metrics\nand detects user-perceived regressions with 0.96 precision, 0.91 recall, and\n0.93 F1-score -- significantly outperforming a statistical baseline using the\nWilcoxon rank-sum test and Cliff's Delta. MobileUPReg has been deployed in an\nindustrial CI pipeline, where it analyzes thousands of screencasts across\nhundreds of apps daily and has uncovered regressions missed by traditional\ntools. These results demonstrate that MobileUPReg enables accurate, scalable,\nand perceptually aligned regression detection for mobile OS validation."}
{"id": "2509.16558", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16558", "abs": "https://arxiv.org/abs/2509.16558", "authors": ["Mingjian Duan", "Ming Xu", "Shenghao Zhang", "Jiaheng Zhang", "Weili Han"], "title": "MoPE: A Mixture of Password Experts for Improving Password Guessing", "comment": null, "summary": "Textual passwords remain a predominant authentication mechanism in web\nsecurity. To evaluate their strength, existing research has proposed several\ndata-driven models across various scenarios. However, these models generally\ntreat passwords uniformly, neglecting the structural differences among\npasswords. This typically results in biased training that favors frequent\npassword structural patterns. To mitigate the biased training, we argue that\npasswords, as a type of complex short textual data, should be processed in a\nstructure-aware manner by identifying their structural patterns and routing\nthem to specialized models accordingly. In this paper, we propose MoPE, a\nMixture of Password Experts framework, specifically designed to leverage the\nstructural patterns in passwords to improveguessing performance. Motivated by\nthe observation that passwords with similar structural patterns (e.g.,\nfixed-length numeric strings) tend to cluster in high-density regions within\nthe latent space, our MoPE introduces: (1) a novel structure-based method for\ngenerating specialized expert models; (2) a lightweight gate method to select\nappropriate expert models to output reliable guesses, better aligned with the\nhigh computational frequency of password guessing tasks. Our evaluation shows\nthat MoPE significantly outperforms existing state-of-the-art baselines in both\noffline and online guessing scenarios, achieving up to 38.80% and 9.27%\nimprovement in cracking rate, respectively, showcasing that MoPE can\neffectively exploit the capabilities of data-driven models for password\nguessing. Additionally, we implement a real-time Password Strength Meter (PSM)\nbased on offline MoPE, assisting users in choosing stronger passwords more\nprecisely with millisecond-level response latency."}
{"id": "2509.16870", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16870", "abs": "https://arxiv.org/abs/2509.16870", "authors": ["Rui Yang", "Michael Fu", "Chakkrit Tantithamthavorn", "Chetan Arora", "Gunel Gulmammadova", "Joey Chua"], "title": "DecipherGuard: Understanding and Deciphering Jailbreak Prompts for a Safer Deployment of Intelligent Software Systems", "comment": "Under Review", "summary": "Intelligent software systems powered by Large Language Models (LLMs) are\nincreasingly deployed in critical sectors, raising concerns about their safety\nduring runtime. Through an industry-academic collaboration when deploying an\nLLM-powered virtual customer assistant, a critical software engineering\nchallenge emerged: how to enhance a safer deployment of LLM-powered software\nsystems at runtime? While LlamaGuard, the current state-of-the-art runtime\nguardrail, offers protection against unsafe inputs, our study reveals a Defense\nSuccess Rate (DSR) drop of 24% under obfuscation- and template-based jailbreak\nattacks. In this paper, we propose DecipherGuard, a novel framework that\nintegrates a deciphering layer to counter obfuscation-based prompts and a\nlow-rank adaptation mechanism to enhance guardrail effectiveness against\ntemplate-based attacks. Empirical evaluation on over 22,000 prompts\ndemonstrates that DecipherGuard improves DSR by 36% to 65% and Overall\nGuardrail Performance (OGP) by 20% to 50% compared to LlamaGuard and two other\nruntime guardrails. These results highlight the effectiveness of DecipherGuard\nin defending LLM-powered software systems against jailbreak attacks during\nruntime."}
{"id": "2509.16581", "categories": ["cs.CR", "cs.CE"], "pdf": "https://arxiv.org/pdf/2509.16581", "abs": "https://arxiv.org/abs/2509.16581", "authors": ["Mohsen Ahmadvand", "Pedro Souto"], "title": "Towards Cost-Effective ZK-Rollups: Modeling and Optimization of Proving Infrastructure", "comment": null, "summary": "Zero-knowledge rollups rely on provers to generate multi-step state\ntransition proofs under strict finality and availability constraints. These\nsteps require expensive hardware (e.g., GPUs), and finality is reached only\nonce all stages complete and results are posted on-chain. As rollups scale,\nstaying economically viable becomes increasingly difficult due to rising\nthroughput, fast finality demands, volatile gas prices, and dynamic resource\nneeds. We base our study on Halo2-based proving systems and identify\ntransactions per second (TPS), average gas usage, and finality time as key cost\ndrivers. To address this, we propose a parametric cost model that captures\nrollup-specific constraints and ensures provers can keep up with incoming\ntransaction load. We formulate this model as a constraint system and solve it\nusing the Z3 SMT solver to find cost-optimal configurations. To validate our\napproach, we implement a simulator that detects lag and estimates operational\ncosts. Our method shows a potential cost reduction of up to 70\\%."}
{"id": "2509.16939", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16939", "abs": "https://arxiv.org/abs/2509.16939", "authors": ["Taehyoun Kim", "Duksan Ryu", "Jongmoon Baik"], "title": "Deep Synthetic Cross-Project Approaches for Software Reliability Growth Modeling", "comment": "Submitted on April 26, 2025. Under review", "summary": "Software Reliability Growth Models (SRGMs) are widely used to predict\nsoftware reliability based on defect discovery data collected during testing or\noperational phases. However, their predictive accuracy often degrades in\ndata-scarce environments, such as early-stage testing or safety-critical\nsystems. Although cross-project transfer learning has been explored to mitigate\nthis issue by leveraging data from past projects, its applicability remains\nlimited due to the scarcity and confidentiality of real-world datasets. To\novercome these limitations, we propose Deep Synthetic Cross-project SRGM\n(DSC-SRGM), a novel approach that integrates synthetic data generation with\ncross-project transfer learning. Synthetic datasets are generated using\ntraditional SRGMs to preserve the statistical characteristics of real-world\ndefect discovery trends. A cross-correlation-based clustering method is applied\nto identify synthetic datasets with patterns similar to the target project.\nThese datasets are then used to train a deep learning model for reliability\nprediction. The proposed method is evaluated on 60 real-world datasets, and its\nperformance is compared with both traditional SRGMs and cross-project deep\nlearning models trained on real-world datasets. DSC-SRGM achieves up to 23.3%\nimprovement in predictive accuracy over traditional SRGMs and 32.2% over\ncross-project deep learning models trained on real-world datasets. However,\nexcessive use of synthetic data or a naive combination of synthetic and\nreal-world data may degrade prediction performance, highlighting the importance\nof maintaining an appropriate data balance. These findings indicate that\nDSC-SRGM is a promising approach for software reliability prediction in\ndata-scarce environments."}
{"id": "2509.16593", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.16593", "abs": "https://arxiv.org/abs/2509.16593", "authors": ["Avi Shaked"], "title": "Reproducing a Security Risk Assessment Using Computer Aided Design", "comment": null, "summary": "Security risk assessment is essential in establishing the trustworthiness and\nreliability of modern systems. While various security risk assessment\napproaches exist, prevalent applications are \"pen and paper\" implementations\nthat -- even if performed digitally using computers -- remain prone to\nauthoring mistakes and inconsistencies. Computer-aided design approaches can\ntransform security risk assessments into more rigorous and sustainable efforts.\nThis is of value to both industrial practitioners and researchers, who practice\nsecurity risk assessments to reflect on systems' designs and to contribute to\nthe discipline's state-of-the-art. In this article, we report the application\nof a model-based security design tool to reproduce a previously reported\nsecurity assessment. The main contributions are: 1) an independent attempt to\nreproduce a refereed article describing a real security risk assessment of a\nsystem; 2) comparison of a new computer-aided application with a previous\nnon-computer-aided application, based on a published, real-world case study; 3)\na showcase for the potential advantages -- for both practitioners and\nresearchers -- of using computer-aided design approaches to analyze reports and\nto assess systems."}
{"id": "2509.16941", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16941", "abs": "https://arxiv.org/abs/2509.16941", "authors": ["Xiang Deng", "Jeff Da", "Edwin Pan", "Yannis Yiming He", "Charles Ide", "Kanak Garg", "Niklas Lauffer", "Andrew Park", "Nitin Pasari", "Chetan Rane", "Karmini Sampath", "Maya Krishnan", "Srivatsa Kundurthy", "Sean Hendryx", "Zifan Wang", "Chen Bo Calvin Zhang", "Noah Jacobson", "Bing Liu", "Brad Kenstler"], "title": "SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?", "comment": null, "summary": "We introduce SWE-Bench Pro, a substantially more challenging benchmark that\nbuilds upon the best practices of SWE-BENCH [25], but is explicitly designed to\ncapture realistic, complex, enterprise-level problems beyond the scope of\nSWE-BENCH. SWE-BENCH PRO contains 1,865 problems sourced from a diverse set of\n41 actively maintained repositories spanning business applications, B2B\nservices, and developer tools. The benchmark is partitioned into a public set\nwith open access to problems sourced from 11 repositories, a held-out set of 12\nrepositories and a commercial set of 18 proprietary repositories where we have\nformal partnership agreements with early-stage startups. Problems in the\nheld-out and the commercial set are not publicly accessible, but we release\nresults on the commercial set. Our benchmark features long-horizon tasks that\nmay require hours to days for a professional software engineer to complete,\noften involving patches across multiple files and substantial code\nmodifications. All tasks are human-verified and augmented with sufficient\ncontext to ensure resolvability. In our evaluation of widely used coding\nmodels, under a unified scaffold, we observe that their performance on\nSWE-Bench PRO remains below 25% (Pass@1), with GPT-5 achieving the highest\nscore to date at 23.3%. To better understand these limitations, we cluster the\nfailure modes observed in the collected agent trajectories for a clearer\ncharacterization of the error patterns exhibited by current models. Overall,\nSWE-BENCH PRO provides a contamination-resistant testbed that more faithfully\ncaptures the complexity and diversity of real-world software development,\nadvancing the pursuit of truly autonomous software engineering agents at a\nprofessional level."}
{"id": "2509.16620", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16620", "abs": "https://arxiv.org/abs/2509.16620", "authors": ["Yi Chen", "Xiaoyang Dong", "Ruijie Ma", "Yantian Shen", "Anyu Wang", "Hongbo Yu", "Xiaoyun Wang"], "title": "Delving into Cryptanalytic Extraction of PReLU Neural Networks", "comment": "Accepted by ASIACRYPT 2025", "summary": "The machine learning problem of model extraction was first introduced in 1991\nand gained prominence as a cryptanalytic challenge starting with Crypto 2020.\nFor over three decades, research in this field has primarily focused on\nReLU-based neural networks. In this work, we take the first step towards the\ncryptanalytic extraction of PReLU neural networks, which employ more complex\nnonlinear activation functions than their ReLU counterparts. We propose a raw\noutput-based parameter recovery attack for PReLU networks and extend it to more\nrestrictive scenarios where only the top-m probability scores are accessible.\nOur attacks are rigorously evaluated through end-to-end experiments on diverse\nPReLU neural networks, including models trained on the MNIST dataset. To the\nbest of our knowledge, this is the first practical demonstration of PReLU\nneural network extraction across three distinct attack scenarios."}
{"id": "2509.16985", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16985", "abs": "https://arxiv.org/abs/2509.16985", "authors": ["James J. Cusick"], "title": "Static Security Vulnerability Scanning of Proprietary and Open-Source Software: An Adaptable Process with Variants and Results", "comment": "A total of 8 pages, 7 figures, 4 tables, and 31 references", "summary": "Software vulnerabilities remain a significant risk factor in achieving\nsecurity objectives within software development organizations. This is\nespecially true where either proprietary or open-source software (OSS) is\nincluded in the technological environment. In this paper an end-to-end process\nwith supporting methods and tools is presented. This industry proven generic\nprocess allows for the custom instantiation, configuration, and execution of\nroutinized code scanning for software vulnerabilities and their prioritized\nremediation. A select set of tools are described for this key DevSecOps\nfunction and placed into an iterative process. Examples of both industrial\nproprietary applications and open-source applications are provided including\nspecific vulnerability instances and a discussion of their treatment. The\nbenefits of each selected tool are considered, and alternative tools are also\nintroduced. Application of this method in a comprehensive SDLC model is also\nreviewed along with prospective enhancements from automation and the\napplication of advanced technologies including AI. Adoption of this method can\nbe achieved with minimal adjustments and with maximum flexibility for results\nin reducing source code vulnerabilities, reducing supply chain risk, and\nimproving the security profile of new or legacy solutions."}
{"id": "2509.16671", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16671", "abs": "https://arxiv.org/abs/2509.16671", "authors": ["Ekin B√∂ke", "Simon Torka"], "title": "\"Digital Camouflage\": The LLVM Challenge in LLM-Based Malware Detection", "comment": null, "summary": "Large Language Models (LLMs) have emerged as promising tools for malware\ndetection by analyzing code semantics, identifying vulnerabilities, and\nadapting to evolving threats. However, their reliability under adversarial\ncompiler-level obfuscation is yet to be discovered. In this study, we\nempirically evaluate the robustness of three state-of-the-art LLMs: ChatGPT-4o,\nGemini Flash 2.5, and Claude Sonnet 4 against compiler-level obfuscation\ntechniques implemented via the LLVM infrastructure. These include control flow\nflattening, bogus control flow injection, instruction substitution, and split\nbasic blocks, which are widely used to evade detection while preserving\nmalicious behavior. We perform a structured evaluation on 40~C functions (20\nvulnerable, 20 secure) sourced from the Devign dataset and obfuscated using\nLLVM passes. Our results show that these models often fail to correctly\nclassify obfuscated code, with precision, recall, and F1-score dropping\nsignificantly after transformation. This reveals a critical limitation: LLMs,\ndespite their language understanding capabilities, can be easily misled by\ncompiler-based obfuscation strategies. To promote reproducibility, we release\nall evaluation scripts, prompts, and obfuscated code samples in a public\nrepository. We also discuss the implications of these findings for adversarial\nthreat modeling, and outline future directions such as software watermarking,\ncompiler-aware defenses, and obfuscation-resilient model design."}
{"id": "2509.17096", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.17096", "abs": "https://arxiv.org/abs/2509.17096", "authors": ["Ziyou Li", "Agnia Sergeyuk", "Maliheh Izadi"], "title": "Prompt-with-Me: in-IDE Structured Prompt Management for LLM-Driven Software Engineering", "comment": "Accepted in the 40th IEEE/ACM International Conference on Automated\n  Software Engineering, ASE 2025 (Industry track)", "summary": "Large Language Models are transforming software engineering, yet prompt\nmanagement in practice remains ad hoc, hindering reliability, reuse, and\nintegration into industrial workflows. We present Prompt-with-Me, a practical\nsolution for structured prompt management embedded directly in the development\nenvironment. The system automatically classifies prompts using a\nfour-dimensional taxonomy encompassing intent, author role, software\ndevelopment lifecycle stage, and prompt type. To enhance prompt reuse and\nquality, Prompt-with-Me suggests language refinements, masks sensitive\ninformation, and extracts reusable templates from a developer's prompt library.\nOur taxonomy study of 1108 real-world prompts demonstrates that modern LLMs can\naccurately classify software engineering prompts. Furthermore, our user study\nwith 11 participants shows strong developer acceptance, with high usability\n(Mean SUS=73), low cognitive load (Mean NASA-TLX=21), and reported gains in\nprompt quality and efficiency through reduced repetitive effort. Lastly, we\noffer actionable insights for building the next generation of prompt management\nand maintenance tools for software engineering workflows."}
{"id": "2509.16682", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16682", "abs": "https://arxiv.org/abs/2509.16682", "authors": ["Javier Jim√©nez-Rom√°n", "Florina Almenares-Mendoza", "Alfonso S√°nchez-Maci√°n"], "title": "Design and Development of an Intelligent LLM-based LDAP Honeypot", "comment": null, "summary": "Cybersecurity threats continue to increase, with a growing number of\npreviously unknown attacks each year targeting both large corporations and\nsmaller entities. This scenario demands the implementation of advanced security\nmeasures, not only to mitigate damage but also to anticipate emerging attack\ntrends. In this context, deception tools have become a key strategy, enabling\nthe detection, deterrence, and deception of potential attackers while\nfacilitating the collection of information about their tactics and methods.\nAmong these tools, honeypots have proven their value, although they have\ntraditionally been limited by rigidity and configuration complexity, hindering\ntheir adaptability to dynamic scenarios. The rise of artificial intelligence,\nand particularly general-purpose Large Language Models (LLMs), is driving the\ndevelopment of new deception solutions capable of offering greater adaptability\nand ease of use. This work proposes the design and implementation of an\nLLM-based honeypot to simulate an LDAP server, a critical protocol present in\nmost organizations due to its central role in identity and access management.\nThe proposed solution aims to provide a flexible and realistic tool capable of\nconvincingly interacting with attackers, thereby contributing to early\ndetection and threat analysis while enhancing the defensive capabilities of\ninfrastructures against intrusions targeting this service."}
{"id": "2509.17314", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17314", "abs": "https://arxiv.org/abs/2509.17314", "authors": ["Juyeon Yoon", "Somin Kim", "Robert Feldt", "Shin Yoo"], "title": "Clotho: Measuring Task-Specific Pre-Generation Test Adequacy for LLM Inputs", "comment": null, "summary": "Software increasingly relies on the emergent capabilities of Large Language\nModels (LLMs), from natural language understanding to program analysis and\ngeneration. Yet testing them on specific tasks remains difficult and costly:\nmany prompts lack ground truth, forcing reliance on human judgment, while\nexisting uncertainty and adequacy measures typically require full inference. A\nkey challenge is to assess input adequacy in a way that reflects the demands of\nthe task, ideally before even generating any output. We introduce CLOTHO, a\ntask-specific, pre-generation adequacy measure that estimates input difficulty\ndirectly from hidden LLM states. Given a large pool of unlabelled inputs for a\nspecific task, CLOTHO uses a Gaussian Mixture Model (GMM) to adaptively sample\nthe most informative cases for human labelling. Based on this reference set the\nGMM can then rank unseen inputs by their likelihood of failure. In our\nempirical evaluation across eight benchmark tasks and three open-weight LLMs,\nCLOTHO can predict failures with a ROC-AUC of 0.716, after labelling reference\nsets that are on average only 5.4% of inputs. It does so without generating any\noutputs, thereby reducing costs compared to existing uncertainty measures.\nComparison of CLOTHO and post-generation uncertainty measures shows that the\ntwo approaches complement each other. Crucially, we show that adequacy scores\nlearnt from open-weight LLMs transfer effectively to proprietary models,\nextending the applicability of the approach. When prioritising test inputs for\nproprietary models, CLOTHO increases the average number of failing inputs from\n18.7 to 42.5 out of 100, compared to random prioritisation."}
{"id": "2509.16749", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16749", "abs": "https://arxiv.org/abs/2509.16749", "authors": ["Anna Bertiger", "Bobby Filar", "Aryan Luthra", "Stefano Meschiari", "Aiden Mitchell", "Sam Scholten", "Vivek Sharath"], "title": "Evaluating LLM Generated Detection Rules in Cybersecurity", "comment": "Preprint of a paper accepted at the Conference on Applied Machine\n  Learning in Information Security (CAMLIS 2025). 11 pages, 3 figures, 4 tables", "summary": "LLMs are increasingly pervasive in the security environment, with limited\nmeasures of their effectiveness, which limits trust and usefulness to security\npractitioners. Here, we present an open-source evaluation framework and\nbenchmark metrics for evaluating LLM-generated cybersecurity rules. The\nbenchmark employs a holdout set-based methodology to measure the effectiveness\nof LLM-generated security rules in comparison to a human-generated corpus of\nrules. It provides three key metrics inspired by the way experts evaluate\nsecurity rules, offering a realistic, multifaceted evaluation of the\neffectiveness of an LLM-based security rule generator. This methodology is\nillustrated using rules from Sublime Security's detection team and those\nwritten by Sublime Security's Automated Detection Engineer (ADE), with a\nthorough analysis of ADE's skills presented in the results section."}
{"id": "2509.17335", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.17335", "abs": "https://arxiv.org/abs/2509.17335", "authors": ["Mingxuan Xiao", "Yan Xiao", "Shunhui Ji", "Jiahe Tu", "Pengcheng Zhang"], "title": "BASFuzz: Towards Robustness Evaluation of LLM-based NLP Software via Automated Fuzz Testing", "comment": null, "summary": "Fuzzing has shown great success in evaluating the robustness of intelligent\nnatural language processing (NLP) software. As large language model (LLM)-based\nNLP software is widely deployed in critical industries, existing methods still\nface two main challenges: 1 testing methods are insufficiently coupled with the\nbehavioral patterns of LLM-based NLP software; 2 fuzzing capability for the\ntesting scenario of natural language generation (NLG) generally degrades. To\naddress these issues, we propose BASFuzz, an efficient Fuzz testing method\ntailored for LLM-based NLP software. BASFuzz targets complete test inputs\ncomposed of prompts and examples, and uses a text consistency metric to guide\nmutations of the fuzzing loop, aligning with the behavioral patterns of\nLLM-based NLP software. A Beam-Annealing Search algorithm, which integrates\nbeam search and simulated annealing, is employed to design an efficient fuzzing\nloop. In addition, information entropy-based adaptive adjustment and an elitism\nstrategy further enhance fuzzing capability. We evaluate BASFuzz on six\ndatasets in representative scenarios of NLG and natural language understanding\n(NLU). Experimental results demonstrate that BASFuzz achieves a testing\neffectiveness of 90.335% while reducing the average time overhead by 2,163.852\nseconds compared to the current best baseline, enabling more effective\nrobustness evaluation prior to software deployment."}
{"id": "2509.16861", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16861", "abs": "https://arxiv.org/abs/2509.16861", "authors": ["Rui Yang", "Michael Fu", "Chakkrit Tantithamthavorn", "Chetan Arora", "Gunel Gulmammadova", "Joey Chua"], "title": "AdaptiveGuard: Towards Adaptive Runtime Safety for LLM-Powered Software", "comment": "Accepted to the ASE 2025 International Conference on Automated\n  Software Engineering, Industry Showcase Track", "summary": "Guardrails are critical for the safe deployment of Large Language Models\n(LLMs)-powered software. Unlike traditional rule-based systems with limited,\npredefined input-output spaces that inherently constrain unsafe behavior, LLMs\nenable open-ended, intelligent interactions--opening the door to jailbreak\nattacks through user inputs. Guardrails serve as a protective layer, filtering\nunsafe prompts before they reach the LLM. However, prior research shows that\njailbreak attacks can still succeed over 70% of the time, even against advanced\nmodels like GPT-4o. While guardrails such as LlamaGuard report up to 95%\naccuracy, our preliminary analysis shows their performance can drop sharply--to\nas low as 12%--when confronted with unseen attacks. This highlights a growing\nsoftware engineering challenge: how to build a post-deployment guardrail that\nadapts dynamically to emerging threats? To address this, we propose\nAdaptiveGuard, an adaptive guardrail that detects novel jailbreak attacks as\nout-of-distribution (OOD) inputs and learns to defend against them through a\ncontinual learning framework. Through empirical evaluation, AdaptiveGuard\nachieves 96% OOD detection accuracy, adapts to new attacks in just two update\nsteps, and retains over 85% F1-score on in-distribution data post-adaptation,\noutperforming other baselines. These results demonstrate that AdaptiveGuard is\na guardrail capable of evolving in response to emerging jailbreak strategies\npost deployment. We release our AdaptiveGuard and studied datasets at\nhttps://github.com/awsm-research/AdaptiveGuard to support further research."}
{"id": "2509.17338", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.17338", "abs": "https://arxiv.org/abs/2509.17338", "authors": ["Pengfei He", "Shaowei Wang", "Tse-Hsun Chen"], "title": "SLICET5: Static Program Slicing using Language Models with Copy Mechanism and Constrained Decoding", "comment": "3 tables, 6 Figures, 12 pages", "summary": "Static program slicing is a fundamental technique in software engineering.\nTraditional static slicing tools rely on parsing complete source code, which\nlimits their applicability to real-world scenarios where code snippets are\nincomplete or unparsable. While recent research developed learning-based\napproaches to predict slices, they face critical challenges: (1) Inaccurate\ndependency identification, where models fail to precisely capture data and\ncontrol dependencies between code elements; and (2) Unconstrained generation,\nwhere models produce slices with extraneous or hallucinated tokens not present\nin the input, violating the structural integrity of slices. To address these\nchallenges, we propose \\ourtool, a novel slicing framework that reformulates\nstatic program slicing as a sequence-to-sequence task using lightweight\nlanguage models (e.g., CodeT5+). Our approach incorporates two key innovations.\nFirst, we introduce a copy mechanism that enables the model to more accurately\ncapture inter-element dependencies and directly copy relevant tokens from the\ninput, improving both dependency reasoning and generation constraint. Second,\nwe design a constrained decoding process with (a) lexical constraint,\nrestricting outputs to input tokens only, and (b) syntactic constraint,\nleveraging Tree Similarity of Edit Distance (TSED) monotonicity to detect\nstructurally invalid outputs and discard them. We evaluate \\ourtool on CodeNet\nand LeetCode datasets and show it consistently outperforms state-of-the-art\nbaselines, improving ExactMatch scores by up to 27\\%. Furthermore, \\ourtool\ndemonstrates strong performance on incomplete code, highlighting its robustness\nand practical utility in real-world development environments."}
{"id": "2509.16899", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16899", "abs": "https://arxiv.org/abs/2509.16899", "authors": ["Md Wasiul Haque", "Md Erfan", "Sagar Dasgupta", "Md Rayhanur Rahman", "Mizanur Rahman"], "title": "Security Vulnerabilities in Software Supply Chain for Autonomous Vehicles", "comment": "16 pages, 9 figures, 10 tables", "summary": "The interest in autonomous vehicles (AVs) for critical missions, including\ntransportation, rescue, surveillance, reconnaissance, and mapping, is growing\nrapidly due to their significant safety and mobility benefits. AVs consist of\ncomplex software systems that leverage artificial intelligence (AI), sensor\nfusion algorithms, and real-time data processing. Additionally, AVs are\nbecoming increasingly reliant on open-source software supply chains, such as\nopen-source packages, third-party software components, AI models, and\nthird-party datasets. Software security best practices in the automotive sector\nare often an afterthought for developers. Thus, significant cybersecurity risks\nexist in the software supply chain of AVs, particularly when secure software\ndevelopment practices are not rigorously implemented. For example, Upstream's\n2024 Automotive Cybersecurity Report states that 49.5% of cyberattacks in the\nautomotive sector are related to exploiting security vulnerabilities in\nsoftware systems. In this chapter, we analyze security vulnerabilities in\nopen-source software components in AVs. We utilize static analyzers on popular\nopen-source AV software, such as Autoware, Apollo, and openpilot. Specifically,\nthis chapter covers: (1) prevalent software security vulnerabilities of AVs;\nand (2) a comparison of static analyzer outputs for different open-source AV\nrepositories. The goal is to inform researchers, practitioners, and\npolicymakers about the existing security flaws in the commonplace open-source\nsoftware ecosystem in the AV domain. The findings would emphasize the necessity\nof security best practices earlier in the software development lifecycle to\nreduce cybersecurity risks, thereby ensuring system reliability, safeguarding\nuser data, and maintaining public trust in an increasingly automated world."}
{"id": "2509.17548", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.17548", "abs": "https://arxiv.org/abs/2509.17548", "authors": ["Hugo Villamizar", "Jannik Fischbach", "Alexander Korn", "Andreas Vogelsang", "Daniel Mendez"], "title": "Prompts as Software Engineering Artifacts: A Research Agenda and Preliminary Findings", "comment": "This paper has been accepted for presentation at the 26th\n  International Conference on Product-Focused Software Process Improvement\n  (PROFES 2025)", "summary": "Developers now routinely interact with large language models (LLMs) to\nsupport a range of software engineering (SE) tasks. This prominent role\npositions prompts as potential SE artifacts that, like other artifacts, may\nrequire systematic development, documentation, and maintenance. However, little\nis known about how prompts are actually used and managed in LLM-integrated\nworkflows, what challenges practitioners face, and whether the benefits of\nsystematic prompt management outweigh the associated effort. To address this\ngap, we propose a research programme that (a) characterizes current prompt\npractices, challenges, and influencing factors in SE; (b) analyzes prompts as\nsoftware artifacts, examining their evolution, traceability, reuse, and the\ntrade-offs of systematic management; and (c) develops and empirically evaluates\nevidence-based guidelines for managing prompts in LLM-integrated workflows. As\na first step, we conducted an exploratory survey with 74 software professionals\nfrom six countries to investigate current prompt practices and challenges. The\nfindings reveal that prompt usage in SE is largely ad-hoc: prompts are often\nrefined through trial-and-error, rarely reused, and shaped more by individual\nheuristics than standardized practices. These insights not only highlight the\nneed for more systematic approaches to prompt management but also provide the\nempirical foundation for the subsequent stages of our research programme."}
{"id": "2509.16950", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16950", "abs": "https://arxiv.org/abs/2509.16950", "authors": ["Xuan Chen", "Shiwei Feng", "Zikang Xiong", "Shengwei An", "Yunshu Mao", "Lu Yan", "Guanhong Tao", "Wenbo Guo", "Xiangyu Zhang"], "title": "Temporal Logic-Based Multi-Vehicle Backdoor Attacks against Offline RL Agents in End-to-end Autonomous Driving", "comment": null, "summary": "Assessing the safety of autonomous driving (AD) systems against security\nthreats, particularly backdoor attacks, is a stepping stone for real-world\ndeployment. However, existing works mainly focus on pixel-level triggers that\nare impractical to deploy in the real world. We address this gap by introducing\na novel backdoor attack against the end-to-end AD systems that leverage one or\nmore other vehicles' trajectories as triggers. To generate precise trigger\ntrajectories, we first use temporal logic (TL) specifications to define the\nbehaviors of attacker vehicles. Configurable behavior models are then used to\ngenerate these trajectories, which are quantitatively evaluated and iteratively\nrefined based on the TL specifications. We further develop a negative training\nstrategy by incorporating patch trajectories that are similar to triggers but\nare designated not to activate the backdoor. It enhances the stealthiness of\nthe attack and refines the system's responses to trigger scenarios. Through\nextensive experiments on 5 offline reinforcement learning (RL) driving agents\nwith 6 trigger patterns and target action combinations, we demonstrate the\nflexibility and effectiveness of our proposed attack, showing the\nunder-exploration of existing end-to-end AD systems' vulnerabilities to such\ntrajectory-based backdoor attacks."}
{"id": "2509.17629", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.17629", "abs": "https://arxiv.org/abs/2509.17629", "authors": ["Antonio Bucchiarone", "Juri Di Rocco", "Damiano Di Vincenzo", "Alfonso Pierantonio"], "title": "From OCL to JSX: declarative constraint modeling in modern SaaS tools", "comment": "10 pages, 2 Figures, Joint Proceedings of the STAF 2025 Workshops:\n  OCL, OOPSLE, LLM4SE, ICMM, AgileMDE, AI4DPS, and TTC. Koblenz, Germany, June\n  10-13, 2025", "summary": "The rise of Node.js in 2010, followed by frameworks like Angular, React, and\nVue.js, has accelerated the growth of low code development platforms. These\nplatforms harness modern UIX paradigms, component-based architectures, and the\nSaaS model to enable non-experts to build software. The widespread adoption of\nsingle-page applications (SPAs), driven by these frameworks, has shaped\nlow-code tools to deliver responsive, client side experiences. In parallel,\nmany modeling platforms have moved to the cloud, adopting either server-centric\narchitectures (e.g., GSLP) or client-side intelligence via SPA frameworks,\nanchoring core components in JavaScript or TypeScript. Within this context,\nOCL.js, a JavaScript-based implementation of the Object Constraint Language,\noffers a web aligned approach to model validation, yet faces challenges such as\npartial standard coverage, limited adoption, and weak integration with modern\nfront-end toolchains. In this paper, we explore JSX, a declarative, functional\nsubset of JavaScript/TypeScript used in the React ecosystem, as an alternative\nto constraint expression in SaaS-based modeling environments. Its\ncomponent-oriented structure supports inductive definitions for syntax, code\ngeneration, and querying. Through empirical evaluation, we compare JSX-based\nconstraints with OCL.js across representative modeling scenarios. Results show\nJSX provides broader expressiveness and better fits front-end-first\narchitectures, indicating a promising path for constraint specification in\nmodern modeling tools."}
{"id": "2509.16987", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16987", "abs": "https://arxiv.org/abs/2509.16987", "authors": ["Vyron Kampourakis", "Christos Smiliotopoulos", "Vasileios Gkioulos", "Sokratis Katsikas"], "title": "In Numeris Veritas: An Empirical Measurement of Wi-Fi Integration in Industry", "comment": null, "summary": "Traditional air gaps in industrial systems are disappearing as IT\ntechnologies permeate the OT domain, accelerating the integration of wireless\nsolutions like Wi-Fi. Next-generation Wi-Fi standards (IEEE 802.11ax/be) meet\nperformance demands for industrial use cases, yet their introduction raises\nsignificant security concerns. A critical knowledge gap exists regarding the\nempirical prevalence and security configuration of Wi-Fi in real-world\nindustrial settings. This work addresses this by mining the global crowdsourced\nWiGLE database to provide a data-driven understanding. We create the first\npublicly available dataset of 1,087 high-confidence industrial Wi-Fi networks,\nexamining key attributes such as SSID patterns, encryption methods, vendor\ntypes, and global distribution. Our findings reveal a growing adoption of Wi-Fi\nacross industrial sectors but underscore alarming security deficiencies,\nincluding the continued use of weak or outdated security configurations that\ndirectly expose critical infrastructure. This research serves as a pivotal\nreference point, offering both a unique dataset and practical insights to guide\nfuture investigations into wireless security within industrial environments."}
{"id": "2509.17776", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.17776", "abs": "https://arxiv.org/abs/2509.17776", "authors": ["Cristina Stratan", "Claudio Mandrioli", "Domenico Bianculli"], "title": "Diagnosing Violations of State-based Specifications in iCFTL", "comment": null, "summary": "As modern software systems grow in complexity and operate in dynamic\nenvironments, the need for runtime analysis techniques becomes a more critical\npart of the verification and validation process. Runtime verification monitors\nthe runtime system behaviour by checking whether an execution trace - a\nsequence of recorded events - satisfies a given specification, yielding a\nBoolean or quantitative verdict. However, when a specification is violated,\nsuch a verdict is often insufficient to understand why the violation happened.\nTo fill this gap, diagnostics approaches aim to produce more informative\nverdicts. In this paper, we address the problem of generating informative\nverdicts for violated Inter-procedural Control-Flow Temporal Logic (iCFTL)\nspecifications that express constraints over program variable values. We\npropose a diagnostic approach based on backward data-flow analysis to\nstatically determine the relevant statements contributing to the specification\nviolation. Using this analysis, we instrument the program to produce enriched\nexecution traces. Using the enriched execution traces, we perform the runtime\nanalysis and identify the statements whose execution led to the specification\nviolation. We implemented our approach in a prototype tool, iCFTL-Diagnostics,\nand evaluated it on 112 specifications across 10 software projects. Our tool\nachieves 90% precision in identifying relevant statements for 100 of the 112\nspecifications. It reduces the number of lines that have to be inspected for\ndiagnosing a violation by at least 90%. In terms of computational cost,\niCFTL-Diagnostics generates a diagnosis within 7 min, and requires no more than\n25 MB of memory. The instrumentation required to support diagnostics incurs an\nexecution time overhead of less than 30% and a memory overhead below 20%."}
{"id": "2509.17048", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17048", "abs": "https://arxiv.org/abs/2509.17048", "authors": ["Huifang Yu", "Jiaxing Jie", "Lei Li"], "title": "Electronic Reporting Using SM2-Based Ring Signcryption", "comment": null, "summary": "Electronic whistleblowing systems are widely used due to their efficiency and\nconvenience. The key to designing such systems lies in protecting the identity\nprivacy of whistleblowers, preventing malicious whistleblowing, and ensuring\nthe confidentiality of whistleblowing information. To address these issues, a\nSM2 traceable ring signcryption scheme for electronic voting is proposed. This\nscheme combines the SM2 elliptic curve public key cryptography algorithm with\nthe ring signature algorithm, enhancing the overall efficiency of the scheme\nwhile ensuring the autonomy and controllability of the core cryptographic\nalgorithms. Security analysis demonstrates that the proposed scheme satisfies\nconfidentiality, unforgeability, traceability, linkability, and deniability.\nEfficiency analysis shows that, compared to existing ring signature schemes,\nthe proposed scheme exhibits significant efficiency advantages during the\nsignature phase. The electronic whistleblowing system designed using the\nproposed scheme can track malicious whistleblowers while protecting user\nidentity privacy, and ensures that the content of whistleblowing remains\nunknown to third parties."}
{"id": "2509.16275", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16275", "abs": "https://arxiv.org/abs/2509.16275", "authors": ["Jugal Gajjar", "Kamalasankari Subramaniakuppusamy", "Relsy Puthal", "Kaustik Ranaware"], "title": "SecureFixAgent: A Hybrid LLM Agent for Automated Python Static Vulnerability Repair", "comment": "6 pages, 3 figures, 4 tables, 1 algorithm, accepted in the Robustness\n  and Security of Large Language Models (ROSE-LLM) special session at ICMLA\n  2025", "summary": "Modern software development pipelines face growing challenges in securing\nlarge codebases with extensive dependencies. Static analysis tools like Bandit\nare effective at vulnerability detection but suffer from high false positives\nand lack repair capabilities. Large Language Models (LLMs), in contrast, can\nsuggest fixes but often hallucinate changes and lack self-validation. We\npresent SecureFixAgent, a hybrid repair framework integrating Bandit with\nlightweight local LLMs (<8B parameters) in an iterative detect-repair-validate\nloop. To improve precision, we apply parameter-efficient LoRA-based fine-tuning\non a diverse, curated dataset spanning multiple Python project domains,\nmitigating dataset bias and reducing unnecessary edits. SecureFixAgent uses\nBandit for detection, the LLM for candidate fixes with explanations, and Bandit\nre-validation for verification, all executed locally to preserve privacy and\nreduce cloud reliance. Experiments show SecureFixAgent reduces false positives\nby 10.8% over static analysis, improves fix accuracy by 13.51%, and lowers\nfalse positives by 5.46% compared to pre-trained LLMs, typically converging\nwithin three iterations. Beyond metrics, developer studies rate explanation\nquality 4.5/5, highlighting its value for human trust and adoption. By\ncombining verifiable security improvements with transparent rationale in a\nresource-efficient local framework, SecureFixAgent advances trustworthy,\nautomated vulnerability remediation for modern pipelines."}
{"id": "2509.17070", "categories": ["cs.CR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17070", "abs": "https://arxiv.org/abs/2509.17070", "authors": ["Mayukh Borana", "Junyi Liang", "Sai Sathiesh Rajan", "Sudipta Chattopadhyay"], "title": "Localizing Malicious Outputs from CodeLLM", "comment": "10 pages, 2 figures, 6 tables, Accepted at EMNLP 2025 Findings", "summary": "We introduce FreqRank, a mutation-based defense to localize malicious\ncomponents in LLM outputs and their corresponding backdoor triggers. FreqRank\nassumes that the malicious sub-string(s) consistently appear in outputs for\ntriggered inputs and uses a frequency-based ranking system to identify them.\nOur ranking system then leverages this knowledge to localize the backdoor\ntriggers present in the inputs. We create nine malicious models through\nfine-tuning or custom instructions for three downstream tasks, namely, code\ncompletion (CC), code generation (CG), and code summarization (CS), and show\nthat they have an average attack success rate (ASR) of 86.6%. Furthermore,\nFreqRank's ranking system highlights the malicious outputs as one of the top\nfive suggestions in 98% of cases. We also demonstrate that FreqRank's\neffectiveness scales as the number of mutants increases and show that FreqRank\nis capable of localizing the backdoor trigger effectively even with a limited\nnumber of triggered samples. Finally, we show that our approach is 35-50% more\neffective than other defense methods."}
{"id": "2509.16861", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16861", "abs": "https://arxiv.org/abs/2509.16861", "authors": ["Rui Yang", "Michael Fu", "Chakkrit Tantithamthavorn", "Chetan Arora", "Gunel Gulmammadova", "Joey Chua"], "title": "AdaptiveGuard: Towards Adaptive Runtime Safety for LLM-Powered Software", "comment": "Accepted to the ASE 2025 International Conference on Automated\n  Software Engineering, Industry Showcase Track", "summary": "Guardrails are critical for the safe deployment of Large Language Models\n(LLMs)-powered software. Unlike traditional rule-based systems with limited,\npredefined input-output spaces that inherently constrain unsafe behavior, LLMs\nenable open-ended, intelligent interactions--opening the door to jailbreak\nattacks through user inputs. Guardrails serve as a protective layer, filtering\nunsafe prompts before they reach the LLM. However, prior research shows that\njailbreak attacks can still succeed over 70% of the time, even against advanced\nmodels like GPT-4o. While guardrails such as LlamaGuard report up to 95%\naccuracy, our preliminary analysis shows their performance can drop sharply--to\nas low as 12%--when confronted with unseen attacks. This highlights a growing\nsoftware engineering challenge: how to build a post-deployment guardrail that\nadapts dynamically to emerging threats? To address this, we propose\nAdaptiveGuard, an adaptive guardrail that detects novel jailbreak attacks as\nout-of-distribution (OOD) inputs and learns to defend against them through a\ncontinual learning framework. Through empirical evaluation, AdaptiveGuard\nachieves 96% OOD detection accuracy, adapts to new attacks in just two update\nsteps, and retains over 85% F1-score on in-distribution data post-adaptation,\noutperforming other baselines. These results demonstrate that AdaptiveGuard is\na guardrail capable of evolving in response to emerging jailbreak strategies\npost deployment. We release our AdaptiveGuard and studied datasets at\nhttps://github.com/awsm-research/AdaptiveGuard to support further research."}
{"id": "2509.17126", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17126", "abs": "https://arxiv.org/abs/2509.17126", "authors": ["Stefanos Chaliasos", "Conner Swann", "Sina Pilehchiha", "Nicolas Mohnblatt", "Benjamin Livshits", "Assimakis Kattis"], "title": "Unaligned Incentives: Pricing Attacks Against Blockchain Rollups", "comment": null, "summary": "Rollups have become the de facto scalability solution for Ethereum, securing\nmore than $55B in assets. They achieve scale by executing transactions on a\nLayer 2 ledger, while periodically posting data and finalizing state on the\nLayer 1, either optimistically or via validity proofs. Their fees must\nsimultaneously reflect the pricing of three resources: L2 costs (e.g.,\nexecution), L1 DA, and underlying L1 gas costs for batch settlement and proof\nverification. In this work, we identify critical mis-pricings in existing\nrollup transaction fee mechanisms (TFMs) that allow for two powerful attacks.\nFirstly, an adversary can saturate the L2's DA batch capacity with\ncompute-light data-heavy transactions, forcing low-gas transaction batches that\nenable both L2 DoS attacks, and finality-delay attacks. Secondly, by crafting\nprover killer transactions that maximize proving cycles relative to the gas\ncharges, an adversary can effectively stall proof generation, delaying finality\nby hours and inflicting prover-side economic losses to the rollup at a minimal\ncost.\n  We analyze the above attack vectors across the major Ethereum rollups,\nquantifying adversarial costs and protocol losses. We find that the first\nattack enables periodic DoS on rollups, lasting up to 30 minutes, at a cost\nbelow 2 ETH for most rollups. Moreover, we identify three rollups that are\nexposed to indefinite DoS at a cost of approximately 0.8 to 2.7 ETH per hour.\nThe attack can be further modified to increase finalization delays by a factor\nof about 1.45x to 2.73x, compared to direct L1 blob-stuffing, depending on the\nrollup's parameters. Furthermore, we find that the prover killer attack induces\na finalization latency increase of about 94x. Finally, we propose comprehensive\nmitigations to prevent these attacks and suggest how some practical uses of\nmulti-dimensional rollup TFMs can rectify the identified mis-pricing attacks."}
{"id": "2509.17185", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17185", "abs": "https://arxiv.org/abs/2509.17185", "authors": ["Bence So√≥ki-T√≥th", "Istv√°n Andr√°s Seres", "Kamilla Kara", "√Åbel Nagy", "Bal√°zs Pej√≥", "Gergely Bicz√≥k"], "title": "Bribers, Bribers on The Chain, Is Resisting All in Vain? Trustless Consensus Manipulation Through Bribing Contracts", "comment": "pre-print", "summary": "The long-term success of cryptocurrencies largely depends on the incentive\ncompatibility provided to the validators. Bribery attacks, facilitated\ntrustlessly via smart contracts, threaten this foundation. This work\nintroduces, implements, and evaluates three novel and efficient bribery\ncontracts targeting Ethereum validators. The first bribery contract enables a\nbriber to fork the blockchain by buying votes on their proposed blocks. The\nsecond contract incentivizes validators to voluntarily exit the consensus\nprotocol, thus increasing the adversary's relative staking power. The third\ncontract builds a trustless bribery market that enables the briber to auction\noff their manipulative power over the RANDAO, Ethereum's distributed randomness\nbeacon. Finally, we provide an initial game-theoretical analysis of one of the\ndescribed bribery markets."}
{"id": "2509.17253", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17253", "abs": "https://arxiv.org/abs/2509.17253", "authors": ["Selma Yahia", "Ildi Alla", "Girija Bangalore Mohan", "Daniel Rau", "Mridula Singh", "Valeria Loscri"], "title": "Seeing is Deceiving: Mirror-Based LiDAR Spoofing for Autonomous Vehicle Deception", "comment": null, "summary": "Autonomous vehicles (AVs) rely heavily on LiDAR sensors for accurate 3D\nperception. We show a novel class of low-cost, passive LiDAR spoofing attacks\nthat exploit mirror-like surfaces to inject or remove objects from an AV's\nperception. Using planar mirrors to redirect LiDAR beams, these attacks require\nno electronics or custom fabrication and can be deployed in real settings. We\ndefine two adversarial goals: Object Addition Attacks (OAA), which create\nphantom obstacles, and Object Removal Attacks (ORA), which conceal real\nhazards. We develop geometric optics models, validate them with controlled\noutdoor experiments using a commercial LiDAR and an Autoware-equipped vehicle,\nand implement a CARLA-based simulation for scalable testing. Experiments show\nmirror attacks corrupt occupancy grids, induce false detections, and trigger\nunsafe planning and control behaviors. We discuss potential defenses (thermal\nsensing, multi-sensor fusion, light-fingerprinting) and their limitations."}
{"id": "2509.17263", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17263", "abs": "https://arxiv.org/abs/2509.17263", "authors": ["Colman McGuan", "Aadithyan V. Raghavan", "Komala M. Mandapati", "Chansu Yu", "Brian E. Ray", "Debbie K. Jackson", "Sathish Kumar"], "title": "Bridging Cybersecurity Practice and Law: a Hands-on, Scenario-Based Curriculum Using the NICE Framework to Foster Skill Development", "comment": null, "summary": "In an increasingly interconnected world, cybersecurity professionals play a\npivotal role in safeguarding organizations from cyber threats. To secure their\ncyberspace, organizations are forced to adopt a cybersecurity framework such as\nthe NIST National Initiative for Cybersecurity Education Workforce Framework\nfor Cybersecurity (NICE Framework). Although these frameworks are a good\nstarting point for businesses and offer critical information to identify,\nprevent, and respond to cyber incidents, they can be difficult to navigate and\nimplement, particularly for small-medium businesses (SMB). To help overcome\nthis issue, this paper identifies the most frequent attack vectors to SMBs\n(Objective 1) and proposes a practical model of both technical and\nnon-technical tasks, knowledge, skills, abilities (TKSA) from the NICE\nFramework for those attacks (Objective 2). The research develops a\nscenario-based curriculum. By immersing learners in realistic cyber threat\nscenarios, their practical understanding and preparedness in responding to\ncybersecurity incidents is enhanced (Objective 3). Finally, this work\nintegrates practical experience and real-life skill development into the\ncurriculum (Objective 4). SMBs can use the model as a guide to evaluate, equip\ntheir existing workforce, or assist in hiring new employees. In addition,\neducational institutions can use the model to develop scenario-based learning\nmodules to adequately equip the emerging cybersecurity workforce for SMBs.\nTrainees will have the opportunity to practice both technical and legal issues\nin a simulated environment, thereby strengthening their ability to identify,\nmitigate, and respond to cyber threats effectively."}
{"id": "2509.17266", "categories": ["cs.CR", "cs.IT", "cs.SY", "eess.SY", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.17266", "abs": "https://arxiv.org/abs/2509.17266", "authors": ["Farhad Farokhi"], "title": "Privacy-Preserving State Estimation with Crowd Sensors: An Information-Theoretic Respective", "comment": "Accepted for presentation at the 17th IEEE International Workshop on\n  Information Forensics and Security (WIFS2025)", "summary": "Privacy-preserving state estimation for linear time-invariant dynamical\nsystems with crowd sensors is considered. At any time step, the estimator has\naccess to measurements from a randomly selected sensor from a pool of sensors\nwith pre-specified models and noise profiles. A Luenberger-like observer is\nused to fuse the measurements with the underlying model of the system to\nrecursively generate the state estimates. An additive privacy-preserving noise\nis used to constrain information leakage. Information leakage is measured via\nmutual information between the identity of the sensors and the state estimate\nconditioned on the actual state of the system. This captures an omnipotent\nadversary that not only can access state estimates but can also gather direct\nhigh-quality state measurements. Any prescribed level of information leakage is\nshown to be achievable by appropriately selecting the variance of the\nprivacy-preserving noise. Therefore, privacy-utility trade-off can be\nfine-tuned."}
{"id": "2509.17302", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17302", "abs": "https://arxiv.org/abs/2509.17302", "authors": ["Duoxun Tang", "Xinhang Jiang", "Jiajun Niu"], "title": "TextCrafter: Optimization-Calibrated Noise for Defending Against Text Embedding Inversion", "comment": null, "summary": "Text embedding inversion attacks reconstruct original sentences from latent\nrepresentations, posing severe privacy threats in collaborative inference and\nedge computing. We propose TextCrafter, an optimization-based adversarial\nperturbation mechanism that combines RL learned, geometry aware noise injection\northogonal to user embeddings with cluster priors and PII signal guidance to\nsuppress inversion while preserving task utility. Unlike prior defenses either\nnon learnable or agnostic to perturbation direction, TextCrafter provides a\ndirectional protective policy that balances privacy and utility. Under strong\nprivacy setting, TextCrafter maintains 70 percentage classification accuracy on\nfour datasets and consistently outperforms Gaussian/LDP baselines across lower\nprivacy budgets, demonstrating a superior privacy utility trade off."}
{"id": "2509.17371", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17371", "abs": "https://arxiv.org/abs/2509.17371", "authors": ["Haotian Xu", "Qingsong Peng", "Jie Shi", "Huadi Zheng", "Yu Li", "Cheng Zhuo"], "title": "SilentStriker:Toward Stealthy Bit-Flip Attacks on Large Language Models", "comment": null, "summary": "The rapid adoption of large language models (LLMs) in critical domains has\nspurred extensive research into their security issues. While input manipulation\nattacks (e.g., prompt injection) have been well studied, Bit-Flip Attacks\n(BFAs) -- which exploit hardware vulnerabilities to corrupt model parameters\nand cause severe performance degradation -- have received far less attention.\nExisting BFA methods suffer from key limitations: they fail to balance\nperformance degradation and output naturalness, making them prone to discovery.\nIn this paper, we introduce SilentStriker, the first stealthy bit-flip attack\nagainst LLMs that effectively degrades task performance while maintaining\noutput naturalness. Our core contribution lies in addressing the challenge of\ndesigning effective loss functions for LLMs with variable output length and the\nvast output space. Unlike prior approaches that rely on output perplexity for\nattack loss formulation, which inevitably degrade output naturalness, we\nreformulate the attack objective by leveraging key output tokens as targets for\nsuppression, enabling effective joint optimization of attack effectiveness and\nstealthiness. Additionally, we employ an iterative, progressive search strategy\nto maximize attack efficacy. Experiments show that SilentStriker significantly\noutperforms existing baselines, achieving successful attacks without\ncompromising the naturalness of generated text."}
{"id": "2509.17409", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17409", "abs": "https://arxiv.org/abs/2509.17409", "authors": ["Yao Wu", "Ziye Jia", "Qihui Wu", "Yian Zhu"], "title": "A Lightweight Authentication and Key Agreement Protocol Design for FANET", "comment": null, "summary": "The advancement of low-altitude intelligent networks enables unmanned aerial\nvehicle (UAV) interconnection via flying ad-hoc networks (FANETs), offering\nflexibility and decentralized coordination. However, resource constraints,\ndynamic topologies, and UAV operations in open environments present significant\nsecurity and communication challenges. Existing multi-factor and public-key\ncryptography protocols are vulnerable due to their reliance on stored sensitive\ninformation, increasing the risk of exposure and compromise. This paper\nproposes a lightweight authentication and key agreement protocol for FANETs,\nintegrating physical unclonable functions with dynamic credential management\nand lightweight cryptographic primitives. The protocol reduces computational\nand communication overhead while enhancing security. Security analysis confirms\nits resilience against various attacks, and comparative evaluations demonstrate\nits superiority in security, communication efficiency, and computational cost."}
{"id": "2509.17416", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17416", "abs": "https://arxiv.org/abs/2509.17416", "authors": ["Jianbin Ji", "Dawen Xu", "Li Dong", "Lin Yang", "Songhan He"], "title": "DINVMark: A Deep Invertible Network for Video Watermarking", "comment": "Accepted by IEEE Transaction on Multimedia (2025)", "summary": "With the wide spread of video, video watermarking has become increasingly\ncrucial for copyright protection and content authentication. However, video\nwatermarking still faces numerous challenges. For example, existing methods\ntypically have shortcomings in terms of watermarking capacity and robustness,\nand there is a lack of specialized noise layer for High Efficiency Video\nCoding(HEVC) compression. To address these issues, this paper introduces a Deep\nInvertible Network for Video watermarking (DINVMark) and designs a noise layer\nto simulate HEVC compression. This approach not only in creases watermarking\ncapacity but also enhances robustness. DINVMark employs an Invertible Neural\nNetwork (INN), where the encoder and decoder share the same network structure\nfor both watermark embedding and extraction. This shared architecture ensures\nclose coupling between the encoder and decoder, thereby improving the accuracy\nof the watermark extraction process. Experimental results demonstrate that the\nproposed scheme significantly enhances watermark robustness, preserves video\nquality, and substantially increases watermark embedding capacity."}
{"id": "2509.17488", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17488", "abs": "https://arxiv.org/abs/2509.17488", "authors": ["Shouju Wang", "Fenglin Yu", "Xirui Liu", "Xiaoting Qin", "Jue Zhang", "Qingwei Lin", "Dongmei Zhang", "Saravan Rajmohan"], "title": "Privacy in Action: Towards Realistic Privacy Mitigation and Evaluation for LLM-Powered Agents", "comment": "To appear at EMNLP 2025 (Findings)", "summary": "The increasing autonomy of LLM agents in handling sensitive communications,\naccelerated by Model Context Protocol (MCP) and Agent-to-Agent (A2A)\nframeworks, creates urgent privacy challenges. While recent work reveals\nsignificant gaps between LLMs' privacy Q&A performance and their agent\nbehavior, existing benchmarks remain limited to static, simplified scenarios.\nWe present PrivacyChecker, a model-agnostic, contextual integrity based\nmitigation approach that effectively reduces privacy leakage from 36.08% to\n7.30% on DeepSeek-R1 and from 33.06% to 8.32% on GPT-4o, all while preserving\ntask helpfulness. We also introduce PrivacyLens-Live, transforming static\nbenchmarks into dynamic MCP and A2A environments that reveal substantially\nhigher privacy risks in practical. Our modular mitigation approach integrates\nseamlessly into agent protocols through three deployment strategies, providing\npractical privacy protection for the emerging agentic ecosystem. Our data and\ncode will be made available at https://aka.ms/privacy_in_action."}
{"id": "2509.17508", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.17508", "abs": "https://arxiv.org/abs/2509.17508", "authors": ["Eric Filiol"], "title": "Community Covert Communication - Dynamic Mass Covert Communication Through Social Media", "comment": "22 pages, 8 figures, this work has been presented at 44CON 2024 &\n  44CON 2025 in London", "summary": "Since the early 2010s, social network-based influence technologies have grown\nalmost exponentially. Initiated by the U.S. Army's early OEV system in 2011, a\nnumber of companies specializing in this field have emerged. The most\n(in)famous cases are Bell Pottinger, Cambridge Analytica, Aggregate-IQ and,\nmore recently, Team Jorge.\n  In this paper, we consider the use-case of sock puppet master activities,\nwhich consist in creating hundreds or even thousands of avatars, in organizing\nthem into communities and implement influence operations. On-purpose software\nis used to automate these operations (e.g. Ripon software, AIMS) and organize\nthese avatar populations into communities. The aim is to organize targeted and\ndirected influence communication to rather large communities (influence\ntargets).\n  The goal of the present research work is to show how these community\nmanagement techniques (social networks) can also be used to\ncommunicate/disseminate relatively large volumes (up to a few tens of Mb) of\nmulti-level encrypted information to a limited number of actors. To a certain\nextent, this can be compared to a Dark Post-type function, with a number of\nmuch more powerful potentialities. As a consequence, the concept of\ncommunication has been totally redefined and disrupted, so that eavesdropping,\ninterception and jamming operations no longer make sense."}
{"id": "2509.17595", "categories": ["cs.CR", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.17595", "abs": "https://arxiv.org/abs/2509.17595", "authors": ["Shunnosuke Ikeda", "Kazumasa Shinagawa"], "title": "Impossibility Results of Card-Based Protocols via Mathematical Optimization", "comment": null, "summary": "This paper introduces mathematical optimization as a new method for proving\nimpossibility proofs in the field of card-based cryptography. While previous\nimpossibility proofs were often limited to cases involving a small number of\ncards, this new approach establishes results that hold for a large number of\ncards. The research focuses on single-cut full-open (SCFO) protocols, which\nconsist of performing one random cut and then revealing all cards. The main\ncontribution is that for any three-variable Boolean function, no new SCFO\nprotocols exist beyond those already known, under the condition that all\nadditional cards have the same color. The significance of this work is that it\nprovides a new framework for impossibility proofs and delivers a proof that is\nvalid for any number of cards, as long as all additional cards have the same\ncolor."}
{"id": "2509.17709", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17709", "abs": "https://arxiv.org/abs/2509.17709", "authors": ["Masayuki Tezuka", "Keisuke Tanaka"], "title": "Ordered Multi-Signatures with Public-Key Aggregation from SXDH Assumption", "comment": "A preliminary version of this paper is appeared in the 20th\n  International Workshop on Security (IWSEC 2025)", "summary": "An ordered multi-signature scheme allows multiple signers to sign a common\nmessage in a sequential manner and allows anyone to verify the signing order of\nsigners with a public-key list. In this work, we propose an ordered\nmulti-signature scheme by modifying the sequential aggregate signature scheme\nby Chatterjee and Kabaleeshwaran (ACISP 2020). Our scheme offers compact public\nparameter size and the public-key aggregation property. This property allows us\nto compress a public-key list into a short aggregated key. We prove the\nsecurity of our scheme under the symmetric external Diffie-Hellman (SXDH)\nassumption without the random oracle model."}
{"id": "2509.17722", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17722", "abs": "https://arxiv.org/abs/2509.17722", "authors": ["Masayuki Tezuka", "Keisuke Tanaka"], "title": "Public Key Encryption with Equality Test from Tag-Based Encryption", "comment": "A preliminary version of this paper is appeared in the 20th\n  International Workshop on Security (IWSEC 2025)", "summary": "Public key encryption with equality test (PKEET), proposed by Yang et al.\n(CT-RSA 2010), is a variant of public key encryption that enables an equality\ntest to determine whether two ciphertexts correspond to the same plaintext.\nThis test applies not only for ciphertexts generated under the same encryption\nkey but also for those generated under different encryption keys. To date,\nseveral generic constructions of PKEET have been proposed. However, these\ngeneric constructions have the drawback of reliance on the random oracle model\nor a (hierarchical) identity-based encryption scheme. In this paper, we propose\na generic construction of a PKEET scheme based on tag-based encryption without\nthe random oracle model. Tag-based encryption is a weaker primitive than\nidentity-based encryption. Our scheme allows to derive new PKEET schemes\nwithout the random oracle model. By instantiating our construction with the\npairing-free tag-based encryption scheme by Kiltz (TCC 2006), we obtain a\npairing-free PKEET scheme without the random oracle model. Moreover, by\ninstantiating our construction with a tag-based encryption scheme based on the\nlearning parity with noise (LPN) assumption, we obtain a PKEET scheme based on\nthe LPN assumption without the random oracle model."}
{"id": "2509.17832", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17832", "abs": "https://arxiv.org/abs/2509.17832", "authors": ["Xiangmin Shen", "Wenyuan Cheng", "Yan Chen", "Zhenyuan Li", "Yuqiao Gu", "Lingzhi Wang", "Wencheng Zhao", "Dawei Sun", "Jiashui Wang"], "title": "AEAS: Actionable Exploit Assessment System", "comment": "AEAS has been implemented in the planning agent of PentestAgent, our\n  LLM-driven automated penetration testing framework. Check out our repository:\n  https://github.com/nbshenxm/pentest-agent", "summary": "Security practitioners face growing challenges in exploit assessment, as\npublic vulnerability repositories are increasingly populated with inconsistent\nand low-quality exploit artifacts. Existing scoring systems, such as CVSS and\nEPSS, offer limited support for this task. They either rely on theoretical\nmetrics or produce opaque probability estimates without assessing whether\nusable exploit code exists. In practice, security teams often resort to manual\ntriage of exploit repositories, which is time-consuming, error-prone, and\ndifficult to scale. We present AEAS, an automated system designed to assess and\nprioritize actionable exploits through static analysis. AEAS analyzes both\nexploit code and associated documentation to extract a structured set of\nfeatures reflecting exploit availability, functionality, and setup complexity.\nIt then computes an actionability score for each exploit and produces ranked\nexploit recommendations. We evaluate AEAS on a dataset of over 5,000\nvulnerabilities derived from 600+ real-world applications frequently\nencountered by red teams. Manual validation and expert review on representative\nsubsets show that AEAS achieves a 100% top-3 success rate in recommending\nfunctional exploits and shows strong alignment with expert-validated rankings.\nThese results demonstrate the effectiveness of AEAS in supporting\nexploit-driven vulnerability prioritization."}
{"id": "2509.17836", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17836", "abs": "https://arxiv.org/abs/2509.17836", "authors": ["Roberto Doriguzzi-Corin", "Petr Sabel", "Silvio Cretti", "Silvio Ranise"], "title": "Federated Learning in the Wild: A Comparative Study for Cybersecurity under Non-IID and Unbalanced Settings", "comment": null, "summary": "Machine Learning (ML) techniques have shown strong potential for network\ntraffic analysis; however, their effectiveness depends on access to\nrepresentative, up-to-date datasets, which is limited in cybersecurity due to\nprivacy and data-sharing restrictions. To address this challenge, Federated\nLearning (FL) has recently emerged as a novel paradigm that enables\ncollaborative training of ML models across multiple clients while ensuring that\nsensitive data remains local. Nevertheless, Federated Averaging (FedAvg), the\ncanonical FL algorithm, has proven poor convergence in heterogeneous\nenvironments where data distributions are non-independent and identically\ndistributed (i.i.d.) and client datasets are unbalanced, conditions frequently\nobserved in cybersecurity contexts. To overcome these challenges, several\nalternative FL strategies have been developed, yet their applicability to\nnetwork intrusion detection remains insufficiently explored. This study\nsystematically reviews and evaluates a range of FL methods in the context of\nintrusion detection for DDoS attacks. Using a dataset of network attacks within\na Kubernetes-based testbed, we assess convergence efficiency, computational\noverhead, bandwidth consumption, and model accuracy. To the best of our\nknowledge, this is the first comparative analysis of FL algorithms for\nintrusion detection under realistic non-i.i.d. and unbalanced settings,\nproviding new insights for the design of robust, privacypreserving network\nsecurity solutions."}
{"id": "2509.17871", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17871", "abs": "https://arxiv.org/abs/2509.17871", "authors": ["Samuel Breckenridge", "Dani Vilardell", "Andr√©s F√°brega", "Amy Zhao", "Patrick McCorry", "Rafael Solari", "Ari Juels"], "title": "B-Privacy: Defining and Enforcing Privacy in Weighted Voting", "comment": null, "summary": "In traditional, one-vote-per-person voting systems, privacy equates with\nballot secrecy: voting tallies are published, but individual voters' choices\nare concealed.\n  Voting systems that weight votes in proportion to token holdings, though, are\nnow prevalent in cryptocurrency and web3 systems. We show that these\nweighted-voting systems overturn existing notions of voter privacy. Our\nexperiments demonstrate that even with secret ballots, publishing raw tallies\noften reveals voters' choices.\n  Weighted voting thus requires a new framework for privacy. We introduce a\nnotion called B-privacy whose basis is bribery, a key problem in voting systems\ntoday. B-privacy captures the economic cost to an adversary of bribing voters\nbased on revealed voting tallies.\n  We propose a mechanism to boost B-privacy by noising voting tallies. We prove\nbounds on its tradeoff between B-privacy and transparency, meaning\nreported-tally accuracy. Analyzing 3,582 proposals across 30 Decentralized\nAutonomous Organizations (DAOs), we find that the prevalence of large voters\n(\"whales\") limits the effectiveness of any B-Privacy-enhancing technique.\nHowever, our mechanism proves to be effective in cases without extreme voting\nweight concentration: among proposals requiring coalitions of $\\geq5$ voters to\nflip outcomes, our mechanism raises B-privacy by a geometric mean factor of\n$4.1\\times$.\n  Our work offers the first principled guidance on transparency-privacy\ntradeoffs in weighted-voting systems, complementing existing approaches that\nfocus on ballot secrecy and revealing fundamental constraints that voting\nweight concentration imposes on privacy mechanisms."}
{"id": "2509.17962", "categories": ["cs.CR", "J.3"], "pdf": "https://arxiv.org/pdf/2509.17962", "abs": "https://arxiv.org/abs/2509.17962", "authors": ["Jon Crowcroft", "Anil Madhavapeddy", "Chris Hicks", "Richard Mortier", "Vasilios Mavroudis"], "title": "What if we could hot swap our Biometrics?", "comment": null, "summary": "What if you could really revoke your actual biometric identity, and install a\nnew one, by live rewriting your biological self? We propose some novel\nmechanisms for hot swapping identity based in novel biotechnology. We discuss\nthe potential positive use cases, and negative consequences if such technology\nwas to become available and affordable. Biometrics are selected on the basis\nthat they are supposed to be unfakeable, or at least not at reasonable cost. If\nthey become easier to fake, it may be much cheaper to fake someone else's\nbiometrics than it is for you to change your own biometrics if someone does\ncopy yours. This potentially makes biometrics a bad trade-off for the user. At\nthe time of writing, this threat is highly speculative, but we believe it is\nworth raising and considering the potential consequences."}
{"id": "2509.17969", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17969", "abs": "https://arxiv.org/abs/2509.17969", "authors": ["Gorka Guardiola M√∫zquiz", "Juan Gonz√°lez-G√≥mez", "Enrique Soriano-Salvador"], "title": "The Reverse File System: Towards open cost-effective secure WORM storage devices for logging", "comment": null, "summary": "Write Once Read Many (WORM) properties for storage devices are desirable to\nensure data immutability for applications such as secure logging, regulatory\ncompliance, archival storage, and other types of backup systems. WORM devices\nguarantee that data, once written, cannot be altered or deleted. However,\nimplementing secure and compatible WORM storage remains a challenge.\nTraditional solutions often rely on specialized hardware, which is either\ncostly, closed, or inaccessible to the general public. Distributed approaches,\nwhile promising, introduce additional risks such as denial-of-service\nvulnerabilities and operational complexity. We introduce Socarrat, a novel,\ncost-effective, and local WORM storage solution that leverages a simple\nexternal USB device (specifically, a single-board computer running Linux with\nUSB On-The-Go support). The resulting device can be connected via USB,\nappearing as an ordinary external disk formatted with an ext4 or exFAT file\nsystem, without requiring any specialized software or drivers. By isolating the\nWORM enforcement mechanism in a dedicated USB hardware module, Socarrat\nsignificantly reduces the attack surface and ensures that even privileged\nattackers cannot modify or erase stored data. In addition to the WORM capacity,\nthe system is designed to be tamper-evident, becoming resilient against\nadvanced attacks. This work describes a novel approach, the Reverse File\nSystem, based on inferring the file system operations occurring at higher\nlayers in the host computer where Socarrat is mounted. The paper also describes\nthe current Socarrat prototype, implemented in Go and available as free/libre\nsoftware. Finally, it provides a complete evaluation of the logging performance\non different single-board computers."}
{"id": "2509.18014", "categories": ["cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.18014", "abs": "https://arxiv.org/abs/2509.18014", "authors": ["Joshua Ward", "Xiaofeng Lin", "Chi-Hua Wang", "Guang Cheng"], "title": "Synth-MIA: A Testbed for Auditing Privacy Leakage in Tabular Data Synthesis", "comment": null, "summary": "Tabular Generative Models are often argued to preserve privacy by creating\nsynthetic datasets that resemble training data. However, auditing their\nempirical privacy remains challenging, as commonly used similarity metrics fail\nto effectively characterize privacy risk. Membership Inference Attacks (MIAs)\nhave recently emerged as a method for evaluating privacy leakage in synthetic\ndata, but their practical effectiveness is limited. Numerous attacks exist\nacross different threat models, each with distinct implementations targeting\nvarious sources of privacy leakage, making them difficult to apply\nconsistently. Moreover, no single attack consistently outperforms the others,\nleading to a routine underestimation of privacy risk.\n  To address these issues, we propose a unified, model-agnostic threat\nframework that deploys a collection of attacks to estimate the maximum\nempirical privacy leakage in synthetic datasets. We introduce Synth-MIA, an\nopen-source Python library that streamlines this auditing process through a\nnovel testbed that integrates seamlessly into existing synthetic data\nevaluation pipelines through a Scikit-Learn-like API. Our software implements\n13 attack methods through a Scikit-Learn-like API, designed to enable fast\nsystematic estimation of privacy leakage for practitioners as well as\nfacilitate the development of new attacks and experiments for researchers.\n  We demonstrate our framework's utility in the largest tabular synthesis\nprivacy benchmark to date, revealing that higher synthetic data quality\ncorresponds to greater privacy leakage, that similarity-based privacy metrics\nshow weak correlation with MIA results, and that the differentially private\ngenerator PATEGAN can fail to preserve privacy under such attacks. This\nunderscores the necessity of MIA-based auditing when designing and deploying\nTabular Generative Models."}
{"id": "2509.18039", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.18039", "abs": "https://arxiv.org/abs/2509.18039", "authors": ["Alessio Izzillo", "Riccardo Lazzeretti", "Emilio Coppa"], "title": "STAFF: Stateful Taint-Assisted Full-system Firmware Fuzzing", "comment": "This paper is currently under review at Computers & Security\n  (Elsevier)", "summary": "Modern embedded Linux devices, such as routers, IP cameras, and IoT gateways,\nrely on complex software stacks where numerous daemons interact to provide\nservices. Testing these devices is crucial from a security perspective since\nvendors often use custom closed- or open-source software without documenting\nreleases and patches. Recent coverage-guided fuzzing solutions primarily test\nindividual processes, ignoring deep dependencies between daemons and their\npersistent internal state. This article presents STAFF, a firmware fuzzing\nframework for discovering bugs in Linux-based firmware built around three key\nideas: (a) user-driven multi-request recording, which monitors user\ninteractions with emulated firmware to capture request sequences involving\napplication-layer protocols (e.g., HTTP); (b) intra- and inter-process\ndependency detection, which uses whole-system taint analysis to track how input\nbytes influence user-space states, including files, sockets, and memory areas;\n(c) protocol-aware taint-guided fuzzing, which applies mutations to request\nsequences based on identified dependencies, exploiting multi-staged forkservers\nto efficiently checkpoint protocol states. When evaluating STAFF on 15\nLinux-based firmware targets, it identifies 42 bugs involving multiple network\nrequests and different firmware daemons, significantly outperforming existing\nstate-of-the-art fuzzing solutions in both the number and reproducibility of\ndiscovered bugs."}
{"id": "2509.18044", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18044", "abs": "https://arxiv.org/abs/2509.18044", "authors": ["Saeid Sheikhi", "Panos Kostakos", "Lauri Loven"], "title": "Hybrid Reputation Aggregation: A Robust Defense Mechanism for Adversarial Federated Learning in 5G and Edge Network Environments", "comment": null, "summary": "Federated Learning (FL) in 5G and edge network environments face severe\nsecurity threats from adversarial clients. Malicious participants can perform\nlabel flipping, inject backdoor triggers, or launch Sybil attacks to corrupt\nthe global model. This paper introduces Hybrid Reputation Aggregation (HRA), a\nnovel robust aggregation mechanism designed to defend against diverse\nadversarial behaviors in FL without prior knowledge of the attack type. HRA\ncombines geometric anomaly detection with momentum-based reputation tracking of\nclients. In each round, it detects outlier model updates via distance-based\ngeometric analysis while continuously updating a trust score for each client\nbased on historical behavior. This hybrid approach enables adaptive filtering\nof suspicious updates and long-term penalization of unreliable clients,\ncountering attacks ranging from backdoor insertions to random noise Byzantine\nfailures. We evaluate HRA on a large-scale proprietary 5G network dataset (3M+\nrecords) and the widely used NF-CSE-CIC-IDS2018 benchmark under diverse\nadversarial attack scenarios. Experimental results reveal that HRA achieves\nrobust global model accuracy of up to 98.66% on the 5G dataset and 96.60% on\nNF-CSE-CIC-IDS2018, outperforming state-of-the-art aggregators such as Krum,\nTrimmed Mean, and Bulyan by significant margins. Our ablation studies further\ndemonstrate that the full hybrid system achieves 98.66% accuracy, while the\nanomaly-only and reputation-only variants drop to 84.77% and 78.52%,\nrespectively, validating the synergistic value of our dual-mechanism approach.\nThis demonstrates HRA's enhanced resilience and robustness in 5G/edge federated\nlearning deployments, even under significant adversarial conditions."}
{"id": "2509.16655", "categories": ["cs.SE", "cs.CR", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2509.16655", "abs": "https://arxiv.org/abs/2509.16655", "authors": ["Serena Wang", "Martino Banchio", "Krzysztof Kotowicz", "Katrina Ligett", "R. Preston McAfee", "Eduardo' Vela'' Nava"], "title": "Incentives and Outcomes in Bug Bounties", "comment": null, "summary": "Bug bounty programs have contributed significantly to security in technology\nfirms in the last decade, but little is known about the role of reward\nincentives in producing useful outcomes. We analyze incentives and outcomes in\nGoogle's Vulnerability Rewards Program (VRP), one of the world's largest bug\nbounty programs. We analyze the responsiveness of the quality and quantity of\nbugs received to changes in payments, focusing on a change in Google's reward\namounts posted in July, 2024, in which reward amounts increased by up to 200%\nfor the highest impact tier. Our empirical results show an increase in the\nvolume of high-value bugs received after the reward increase, for which we also\ncompute elasticities. We further break down the sources of this increase\nbetween veteran researchers and new researchers, showing that the reward\nincrease both redirected the attention of veteran researchers and attracted new\ntop security researchers into the program."}
{"id": "2509.16870", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16870", "abs": "https://arxiv.org/abs/2509.16870", "authors": ["Rui Yang", "Michael Fu", "Chakkrit Tantithamthavorn", "Chetan Arora", "Gunel Gulmammadova", "Joey Chua"], "title": "DecipherGuard: Understanding and Deciphering Jailbreak Prompts for a Safer Deployment of Intelligent Software Systems", "comment": "Under Review", "summary": "Intelligent software systems powered by Large Language Models (LLMs) are\nincreasingly deployed in critical sectors, raising concerns about their safety\nduring runtime. Through an industry-academic collaboration when deploying an\nLLM-powered virtual customer assistant, a critical software engineering\nchallenge emerged: how to enhance a safer deployment of LLM-powered software\nsystems at runtime? While LlamaGuard, the current state-of-the-art runtime\nguardrail, offers protection against unsafe inputs, our study reveals a Defense\nSuccess Rate (DSR) drop of 24% under obfuscation- and template-based jailbreak\nattacks. In this paper, we propose DecipherGuard, a novel framework that\nintegrates a deciphering layer to counter obfuscation-based prompts and a\nlow-rank adaptation mechanism to enhance guardrail effectiveness against\ntemplate-based attacks. Empirical evaluation on over 22,000 prompts\ndemonstrates that DecipherGuard improves DSR by 36% to 65% and Overall\nGuardrail Performance (OGP) by 20% to 50% compared to LlamaGuard and two other\nruntime guardrails. These results highlight the effectiveness of DecipherGuard\nin defending LLM-powered software systems against jailbreak attacks during\nruntime."}
{"id": "2509.16985", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16985", "abs": "https://arxiv.org/abs/2509.16985", "authors": ["James J. Cusick"], "title": "Static Security Vulnerability Scanning of Proprietary and Open-Source Software: An Adaptable Process with Variants and Results", "comment": "A total of 8 pages, 7 figures, 4 tables, and 31 references", "summary": "Software vulnerabilities remain a significant risk factor in achieving\nsecurity objectives within software development organizations. This is\nespecially true where either proprietary or open-source software (OSS) is\nincluded in the technological environment. In this paper an end-to-end process\nwith supporting methods and tools is presented. This industry proven generic\nprocess allows for the custom instantiation, configuration, and execution of\nroutinized code scanning for software vulnerabilities and their prioritized\nremediation. A select set of tools are described for this key DevSecOps\nfunction and placed into an iterative process. Examples of both industrial\nproprietary applications and open-source applications are provided including\nspecific vulnerability instances and a discussion of their treatment. The\nbenefits of each selected tool are considered, and alternative tools are also\nintroduced. Application of this method in a comprehensive SDLC model is also\nreviewed along with prospective enhancements from automation and the\napplication of advanced technologies including AI. Adoption of this method can\nbe achieved with minimal adjustments and with maximum flexibility for results\nin reducing source code vulnerabilities, reducing supply chain risk, and\nimproving the security profile of new or legacy solutions."}
