<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 6]
- [cs.SE](#cs.SE) [Total: 8]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [The WASM Cloak: Evaluating Browser Fingerprinting Defenses Under WebAssembly based Obfuscation](https://arxiv.org/abs/2508.21219)
*A H M Nazmus Sakib,Mahsin Bin Akram,Joseph Spracklen,Sahan Kalutarage,Raveen Wijewickrama,Igor Bilogrevic,Murtuza Jadliwala*

Main category: cs.CR

TL;DR: Study evaluates WASM obfuscation impact on browser fingerprinting defenses by automating JS-to-WASM script translation and testing across academic/research and commercial defense types. Finds academic methods struggle with WASM compatibility while commercial API-level tools remain robust, revealing a research-practice gap.


<details>
  <summary>Details</summary>
Motivation: WebAssembly's adoption creates a blind spot in browser fingerprinting defenses, which historically focus on JavaScript. Adversaries could exploit this by obfuscating tracking logic in WASM binary format. This work addresses the unknown impact of WASM obfuscation on defense robustness.

Method: Developed an automated pipeline to convert real-world JavaScript fingerprinting scripts into functional WASM-obfuscated variants. Tested these against two defense classes: state-of-the-art research detectors and commercial in-browser tools using API-level interception.

Result: Academic research detectors relying on feature-based source-code analysis showed moderate vulnerability to WASM obfuscation, attributed to outdated datasets and WASM compatibility issues. Commercial defenses like browser extensions and native features remained 100% effective due to their API-level interception mechanism, which is implementation-agnostic.

Conclusion: This study highlights a significant gap between academic and practical browser fingerprinting defenses when faced with WebAssembly (WASM)-based obfuscation. While research literature detectors struggle due to WASM incompatibility, API-level defenses like browser extensions remain robust. The findings emphasize the need to strengthen academic approaches against emerging obfuscation techniques and expose potential paths for more evasive future attacks.

Abstract: Browser fingerprinting defenses have historically focused on detecting
JavaScript(JS)-based tracking techniques. However, the widespread adoption of
WebAssembly (WASM) introduces a potential blind spot, as adversaries can
convert JS to WASM's low-level binary format to obfuscate malicious logic. This
paper presents the first systematic evaluation of how such WASM-based
obfuscation impacts the robustness of modern fingerprinting defenses. We
develop an automated pipeline that translates real-world JS fingerprinting
scripts into functional WASM-obfuscated variants and test them against two
classes of defenses: state-of-the-art detectors in research literature and
commercial, in-browser tools. Our findings reveal a notable divergence:
detectors proposed in the research literature that rely on feature-based
analysis of source code show moderate vulnerability, stemming from outdated
datasets or a lack of WASM compatibility. In contrast, defenses such as browser
extensions and native browser features remained completely effective, as their
API-level interception is agnostic to the script's underlying implementation.
These results highlight a gap between academic and practical defense strategies
and offer insights into strengthening detection approaches against WASM-based
obfuscation, while also revealing opportunities for more evasive techniques in
future attacks.

</details>


### [2] [Locus: Agentic Predicate Synthesis for Directed Fuzzing](https://arxiv.org/abs/2508.21302)
*Jie Zhu,Chihao Shen,Ziyang Li,Jiahao Yu,Yizheng Chen,Kexin Pei*

Main category: cs.CR

TL;DR: Locus improves directed fuzzing by auto-generating semantic predicates as milestones. This enables fuzzers to 1) reject unproductive paths, 2) guide coverage more effectively, and 3) discover bugs 41.6x faster on average. Found 8 new real-world bugs including one patched.


<details>
  <summary>Details</summary>
Motivation: Traditional directed fuzzing methods rely on imprecise branch distance metrics or manually-crafted constraints that lack generality. These approaches often fail to effectively guide fuzzers toward deeply nested target states due to insufficient semantic understanding of program progress.

Method: Locus synthesizes predicates as intermediate states through an agentic framework with program analysis tools. These predicates are automatically generated, iteratively refined, and validated via symbolic execution to ensure they safely relax target states without false rejections. The framework rejects unproductive executions and provides coverage guidance during fuzzing.

Result: Locus achieves 41.6x speedup on average across 8 state-of-the-art fuzzers. It discovers 8 new unpatched real-world vulnerabilities, including one with a draft patch implemented. The framework demonstrates effectiveness both for specific target states and broad classes of program behaviors.

Conclusion: The Locus framework outperforms existing directed fuzzing techniques by automating predicate synthesis to create semantically meaningful milestones, enabling more efficient targeting of program states through iterative refinement and symbolic execution verification.

Abstract: Directed fuzzing aims to find program inputs that lead to specified target
program states. It has broad applications, such as debugging system crashes,
confirming reported bugs, and generating exploits for potential
vulnerabilities. This task is inherently challenging because target states are
often deeply nested in the program, while the search space manifested by
numerous possible program inputs is prohibitively large. Existing approaches
rely on branch distances or manually-specified constraints to guide the search;
however, the branches alone are often insufficient to precisely characterize
progress toward reaching the target states, while the manually specified
constraints are often tailored for specific bug types and thus difficult to
generalize to diverse target states and programs.
  We present Locus, a novel framework to improve the efficiency of directed
fuzzing. Our key insight is to synthesize predicates to capture fuzzing
progress as semantically meaningful intermediate states, serving as milestones
towards reaching the target states. When used to instrument the program under
fuzzing, they can reject executions unlikely to reach the target states, while
providing additional coverage guidance. To automate this task and generalize to
diverse programs, Locus features an agentic framework with program analysis
tools to synthesize and iteratively refine the candidate predicates, while
ensuring the predicates strictly relax the target states to prevent false
rejections via symbolic execution. Our evaluation shows that Locus
substantially improves the efficiency of eight state-of-the-art fuzzers in
discovering real-world vulnerabilities, achieving an average speedup of 41.6x.
So far, Locus has found eight previously unpatched bugs, with one already
acknowledged with a draft patch.

</details>


### [3] [LLM-driven Provenance Forensics for Threat Investigation and Detection](https://arxiv.org/abs/2508.21323)
*Kunal Mukherjee,Murat Kantarcioglu*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce PROVSEEK, an LLM-powered agentic framework for automated
provenance-driven forensic analysis and threat intelligence extraction.
PROVSEEK employs specialized toolchains to dynamically retrieve relevant
context by generating precise, context-aware queries that fuse a vectorized
threat report knowledge base with data from system provenance databases. The
framework resolves provenance queries, orchestrates multiple role-specific
agents to mitigate hallucinations, and synthesizes structured, ground-truth
verifiable forensic summaries. By combining agent orchestration with
Retrieval-Augmented Generation (RAG) and chain-of-thought (CoT) reasoning,
PROVSEEK enables adaptive multi-step analysis that iteratively refines
hypotheses, verifies supporting evidence, and produces scalable, interpretable
forensic explanations of attack behaviors. By combining provenance data with
agentic reasoning, PROVSEEK establishes a new paradigm for grounded agentic
forecics to investigate APTs. We conduct a comprehensive evaluation on publicly
available DARPA datasets, demonstrating that PROVSEEK outperforms
retrieval-based methods for intelligence extraction task, achieving a 34%
improvement in contextual precision/recall; and for threat detection task,
PROVSEEK achieves 22%/29% higher precision/recall compared to both a baseline
agentic AI approach and State-Of-The-Art (SOTA) Provenance-based Intrusion
Detection System (PIDS).

</details>


### [4] [Risks and Compliance with the EU's Core Cyber Security Legislation](https://arxiv.org/abs/2508.21386)
*Jukka Ruohonen,Jesper Løffler Nielsen,Jakub Skórczynski*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The European Union (EU) has long favored a risk-based approach to regulation.
Such an approach is also used in recent cyber security legislation enacted in
the EU. Risks are also inherently related to compliance with the new
legislation. Objective: The paper investigates how risks are framed in the EU's
five core cyber security legislative acts, whether the framings indicate
convergence or divergence between the acts and their risk concepts, and what
qualifying words and terms are used when describing the legal notions of risks.
Method : The paper's methodology is based on qualitative legal interpretation
and taxonomy-building. Results: The five acts have an encompassing coverage of
different cyber security risks, including but not limited to risks related to
technical, organizational, and human security as well as those not originating
from man-made actions. Both technical aspects and assets are used to frame the
legal risk notions in many of the legislative acts. A threat-centric viewpoint
is also present in one of the acts. Notable gaps are related to acceptable
risks, non-probabilistic risks, and residual risks. Conclusion: The EU's new
cyber security legislation has significantly extended the risk-based approach
to regulations. At the same time, complexity and compliance burden have
increased. With this point in mind, the paper concludes with a few practical
takeaways about means to deal with compliance and research it.

</details>


### [5] [zkLoRA: Fine-Tuning Large Language Models with Verifiable Security via Zero-Knowledge Proofs](https://arxiv.org/abs/2508.21393)
*Guofu Liao,Taotao Wang,Shengli Zhang,Jiqun Zhang,Shi Long,Dacheng Tao*

Main category: cs.CR

TL;DR: zkLoRA enables provably secure and private large language model fine-tuning by verifying LoRA updates via zero-knowledge cryptography, enabling trustworthy deployment in sensitive environments.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs is computationally expensive and insecure in untrusted environments. Existing parameter-efficient methods like LoRA lack security and verifiability under zero-knowledge constraints.

Method: The framework combines Low-Rank Adaptation (LoRA) with zero-knowledge proofs (ZKPs) through cryptographic techniques like lookup arguments, sumcheck protocols, and polynomial commitments to verify all operations during fine-tuning while ensuring privacy.

Result: zkLoRA demonstrates practical efficiency on models up to 13 billion parameters (e.g., LLaMA) via GPU implementations, providing end-to-end verifiability for forward/backward propagation and parameter updates while protecting sensitive data.

Conclusion: zkLoRA bridges the gap between parameter-efficient fine-tuning and security in untrusted environments by integrating LoRA with zero-knowledge proofs, enabling verifiable and private large language model adaptation.

Abstract: Fine-tuning large language models (LLMs) is crucial for adapting them to
specific tasks, yet it remains computationally demanding and raises concerns
about correctness and privacy, particularly in untrusted environments. Although
parameter-efficient methods like Low-Rank Adaptation (LoRA) significantly
reduce resource requirements, ensuring the security and verifiability of
fine-tuning under zero-knowledge constraints remains an unresolved challenge.
To address this, we introduce zkLoRA, the first framework to integrate LoRA
fine-tuning with zero-knowledge proofs (ZKPs), achieving provable security and
correctness. zkLoRA employs advanced cryptographic techniques -- such as lookup
arguments, sumcheck protocols, and polynomial commitments -- to verify both
arithmetic and non-arithmetic operations in Transformer-based architectures.
The framework provides end-to-end verifiability for forward propagation,
backward propagation, and parameter updates during LoRA fine-tuning, while
safeguarding the privacy of model parameters and training data. Leveraging
GPU-based implementations, zkLoRA demonstrates practicality and efficiency
through experimental validation on open-source LLMs like LLaMA, scaling up to
13 billion parameters. By combining parameter-efficient fine-tuning with ZKPs,
zkLoRA bridges a critical gap, enabling secure and trustworthy deployment of
LLMs in sensitive or untrusted environments.

</details>


### [6] [An Empirical Study of Vulnerable Package Dependencies in LLM Repositories](https://arxiv.org/abs/2508.21417)
*Shuhan Liu,Xing Hu,Xin Xia,David Lo,Xiaohu Yang*

Main category: cs.CR

TL;DR: This study reveals severe security risks in LLM dependency chains, showing 75.8% of LLMs have vulnerable dependencies and critical vulnerabilities persist for years.


<details>
  <summary>Details</summary>
Motivation: Existing research focuses on model-level security threats, neglecting vulnerabilities in the LLM dependency supply chain despite its risks.

Method: Empirical analysis of 52 open-source LLMs, examining third-party dependencies, vulnerability management practices, and ecosystem comparisons.

Result: Half of LLM vulnerabilities remain undisclosed for over 56.2 months; 75.8% of LLMs include vulnerable dependencies in configuration files.

Conclusion: The study highlights the critical security risks in the LLM dependency supply chain, provides practitioner insights, and suggests directions for improving LLM security.

Abstract: Large language models (LLMs) have developed rapidly in recent years,
revolutionizing various fields. Despite their widespread success, LLMs heavily
rely on external code dependencies from package management systems, creating a
complex and interconnected LLM dependency supply chain. Vulnerabilities in
dependencies can expose LLMs to security risks. While existing research
predominantly focuses on model-level security threats, vulnerabilities within
the LLM dependency supply chain have been overlooked. To fill this gap, we
conducted an empirical analysis of 52 open-source LLMs, examining their
third-party dependencies and associated vulnerabilities. We then explored
activities within the LLM repositories to understand how maintainers manage
third-party vulnerabilities in practice. Finally, we compared third-party
dependency vulnerabilities in the LLM ecosystem to those in the Python
ecosystem. Our results show that half of the vulnerabilities in the LLM
ecosystem remain undisclosed for more than 56.2 months, significantly longer
than those in the Python ecosystem. Additionally, 75.8% of LLMs include
vulnerable dependencies in their configuration files. This study advances the
understanding of LLM supply chain risks, provides insights for practitioners,
and highlights potential directions for improving the security of the LLM
supply chain.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [7] [Model-Driven Quantum Code Generation Using Large Language Models and Retrieval-Augmented Generation](https://arxiv.org/abs/2508.21097)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: LLMs with RAG pipelines improve quantum code generation from models, boosting accuracy fourfold, but future work is needed to expand applications like code transpilation and model-driven RAG integration.


<details>
  <summary>Details</summary>
Motivation: Developing quantum/hybrid quantum-classical software systems faces challenges due to platform heterogeneity and developer skill gaps. Model-driven approaches aim to mitigate risks and reduce costs by enabling systematic code generation from models.

Method: The method employs Retrieval-Augmented Generation (RAG) pipelines to enhance Large Language Models (LLMs). It validates code generation from UML software system models using Qiskit, a quantum computing library, augmented with GitHub-sourced code. Optimized prompts were tested to improve output quality.

Result: Experiments demonstrated a 4× improvement in CodeBLEU scores when using well-designed prompts, resulting in more accurate and consistent quantum code. However, limitations in current implementations were noted, motivating further investigation.

Conclusion: The paper outlines a promising research direction leveraging LLMs and RAG pipelines for model-to-code transformations, particularly in quantum systems. While current results show improved CodeBLEU scores, the study emphasizes the need for further experiments to explore additional research questions like model-based RAG sources and code-to-code transpilation.

Abstract: This paper introduces a novel research direction for model-to-text/code
transformations by leveraging Large Language Models (LLMs) that can be enhanced
with Retrieval-Augmented Generation (RAG) pipelines. The focus is on quantum
and hybrid quantum-classical software systems, where model-driven approaches
can help reduce the costs and mitigate the risks associated with the
heterogeneous platform landscape and lack of developers' skills. We validate
one of the proposed ideas regarding generating code out of UML model instances
of software systems. This Python code uses a well-established library, called
Qiskit, to execute on gate-based or circuit-based quantum computers. The RAG
pipeline that we deploy incorporates sample Qiskit code from public GitHub
repositories. Experimental results show that well-engineered prompts can
improve CodeBLEU scores by up to a factor of four, yielding more accurate and
consistent quantum code. However, the proposed research direction can go beyond
this through further investigation in the future by conducting experiments to
address our other research questions and ideas proposed here, such as deploying
software system model instances as the source of information in the RAG
pipelines, or deploying LLMs for code-to-code transformations, for instance,
for transpilation use cases.

</details>


### [8] [Learning to Generate Unit Test via Adversarial Reinforcement Learning](https://arxiv.org/abs/2508.21107)
*Dongjun Lee,Changho Hwang,Kimin Lee*

Main category: cs.SE

TL;DR: Adversarial RL framework (UTRL) trains LLMs to generate better unit tests, outperforming existing methods including GPT-4.1


<details>
  <summary>Details</summary>
Motivation: Current methods for training LLMs in unit test generation lack effectiveness, as supervised approaches on human-written tests fail to achieve optimal performance, creating a need for better training paradigms.

Method: A reinforcement learning framework (UTRL) trains two adversarial LLMs: (1) a unit test generator maximizing fault-exposure rewards, and (2) a code generator maximizing pass-rate rewards. Training involves iterative adversarial optimization.

Result: Qwen3-4B trained with UTRL outperforms supervised baselines (by improving test quality) and frontier models like GPT-4.1 in test generation, producing tests that yield code evaluations closer to ground-truth test outcomes.

Conclusion: UTRL effectively trains LLMs to generate high-quality unit tests through adversarial reinforcement learning, surpassing existing methods including frontier models like GPT-4.1.

Abstract: Unit testing is a core practice in programming, enabling systematic
evaluation of programs produced by human developers or large language models
(LLMs). Given the challenges in writing comprehensive unit tests, LLMs have
been employed to automate test generation, yet methods for training LLMs to
produce high-quality tests remain underexplored. In this work, we propose UTRL,
a novel reinforcement learning framework that trains an LLM to generate
high-quality unit tests given a programming instruction. Our key idea is to
iteratively train two LLMs, the unit test generator and the code generator, in
an adversarial manner via reinforcement learning. The unit test generator is
trained to maximize a discrimination reward, which reflects its ability to
produce tests that expose faults in the code generator's solutions, and the
code generator is trained to maximize a code reward, which reflects its ability
to produce solutions that pass the unit tests generated by the test generator.
In our experiments, we demonstrate that unit tests generated by Qwen3-4B
trained via UTRL show higher quality compared to unit tests generated by the
same model trained via supervised fine-tuning on human-written ground-truth
unit tests, yielding code evaluations that more closely align with those
induced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL
outperforms frontier models such as GPT-4.1 in generating high-quality unit
tests, highlighting the effectiveness of UTRL in training LLMs for this task.

</details>


### [9] [Automated Bug Triaging using Instruction-Tuned Large Language Models](https://arxiv.org/abs/2508.21156)
*Kiana Kiashemshaki,Arsham Khosravani,Alireza Hosseinpour,Arshia Akhavan*

Main category: cs.SE

TL;DR: This paper introduces a lightweight LLM-based framework for bug triaging using LoRA and constrained decoding, achieving strong shortlist performance and demonstrating practical advantages over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Large projects often face slow and inconsistent bug triaging, requiring a lightweight and effective solution to improve assignment accuracy.

Method: A framework that combines instruction-tuned large language models (LLMs) with LoRA adapters and employs candidate-constrained decoding to ensure valid bug assignments.

Result: The model achieved a Hit at 10 score of up to 0.753 on EclipseJDT and Mozilla datasets, with improved accuracy on recent snapshots, showing effectiveness in shortlist quality and real-world potential.

Conclusion: The framework using instruction-tuned LLMs with LoRA adapters provides a practical alternative to complex feature engineering and graph-based methods for bug triaging, demonstrating strong performance in real-world scenarios.

Abstract: Bug triaging, the task of assigning new issues to developers, is often slow
and inconsistent in large projects. We present a lightweight framework that
instruction-tuned large language model (LLM) with LoRA adapters and uses
candidate-constrained decoding to ensure valid assignments. Tested on
EclipseJDT and Mozilla datasets, the model achieves strong shortlist quality
(Hit at 10 up to 0.753) despite modest exact Top-1 accuracy. On recent
snapshots, accuracy rises sharply, showing the framework's potential for
real-world, human-in-the-loop triaging. Our results suggest that
instruction-tuned LLMs offer a practical alternative to costly feature
engineering and graph-based methods.

</details>


### [10] [The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management](https://arxiv.org/abs/2508.21433)
*Tobias Lindenbauer,Igor Slinko,Ludwig Felder,Egor Bogomolov,Yaroslav Zharov*

Main category: cs.SE

TL;DR: The paper compares observation-masking and LLM-summarization for context management in SWE-agents, finding that masking is cost-effective and competitive with summarization on SWE-bench Verified.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art SE agents use LLM-summarization for context management, but its performance benefits over simpler methods (e.g., masking older observations) and cost trade-offs remain unclear.

Method: Systematic comparison of masking vs. summarization across five diverse model configurations in SWE-agent on SWE-bench Verified benchmark.

Result: Masking reduced costs by half while matching/tying summary solve rates (e.g., Qwen3-Coder 480B: 53.8% →54.8%), achieving robust performance with minimal complexity.

Conclusion: Observation-masking emerges as the most effective and efficient strategy for SWE-agents; simpler approaches can outperform complex ones in cost and task completion when properly optimized. Code and data are released for reproducibility.

Abstract: Large Language Model (LLM)-based agents solve complex tasks through iterative
reasoning, exploration, and tool-use, a process that can result in long,
expensive context histories. While state-of-the-art Software Engineering ( SE)
agents like OpenHands or Cursor use LLM-based summarization to tackle this
issue, it is unclear whether the increased complexity offers tangible
performance benefits compared to simply omitting older observations. We present
a systematic comparison of these strategies within SWE-agent on SWE-bench
Verified across five diverse model configurations. We find that a simple
observation-masking strategy halves cost relative to a raw agent while
matching, and sometimes slightly exceeding, the solve rate of LLM
summarization. For example, with Qwen3-Coder 480B, masking improves solve rate
from 53.8% (raw agent) to 54.8%, while remaining competitive with summarization
at a lower cost. These results suggest that, at least within SWE-agent on
SWE-bench Verified, the most effective and efficient context management can be
the simplest. We release code and data for reproducibility

</details>


### [11] [Enhancing Semantic Understanding in Pointer Analysis using Large Language Models](https://arxiv.org/abs/2508.21454)
*Baijun Cheng,Kailong Wang,Ling Shi,Haoyu Wang,Yao Guo,Ding Li,Xiangqun Chen*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Pointer analysis has been studied for over four decades. However, existing
frameworks continue to suffer from the propagation of incorrect facts. A major
limitation stems from their insufficient semantic understanding of code,
resulting in overly conservative treatment of user-defined functions. Recent
advances in large language models (LLMs) present new opportunities to bridge
this gap. In this paper, we propose LMPA (LLM-enhanced Pointer Analysis), a
vision that integrates LLMs into pointer analysis to enhance both precision and
scalability. LMPA identifies user-defined functions that resemble system APIs
and models them accordingly, thereby mitigating erroneous cross-calling-context
propagation. Furthermore, it enhances summary-based analysis by inferring
initial points-to sets and introducing a novel summary strategy augmented with
natural language. Finally, we discuss the key challenges involved in realizing
this vision.

</details>


### [12] [Reusable Test Suites for Reinforcement Learning](https://arxiv.org/abs/2508.21553)
*Jørn Eirik Betten,Quentin Mazouni,Dennis Gross,Pedro Lind,Helge Spieker*

Main category: cs.SE

TL;DR: MPTCS selects reusable policy-agnostic test cases for RL agents by leveraging diversity and difficulty, improving testing reliability across different policies.


<details>
  <summary>Details</summary>
Motivation: Current RL policy testing methods produce policy-specific test suites, limiting their generalization. The work addresses the need for reusable, policy-agnostic test cases to validate agent reliability for deployment.

Method: MPTCS employs a set of policies to select test cases from a candidate pool based on solvability, diversity, and difficulty scores, utilizing a discretized descriptor surface for promoting diversity inspired by quality-diversity algorithms.

Result: Results demonstrate the effectiveness of MPTCS in selecting diverse test cases, the impact of policy set size on effectiveness, and the role of diversity mechanisms in covering the state space to trigger policy faults. The difficulty score's utility and method scalability are evaluated.

Conclusion: The study introduces MPTCS, an automated method for selecting diverse, reusable policy-agnostic test cases, enhancing the identification of agent flaws and improving testing reliability across various reinforcement learning policies.

Abstract: Reinforcement learning (RL) agents show great promise in solving sequential
decision-making tasks. However, validating the reliability and performance of
the agent policies' behavior for deployment remains challenging. Most
reinforcement learning policy testing methods produce test suites tailored to
the agent policy being tested, and their relevance to other policies is
unclear. This work presents Multi-Policy Test Case Selection (MPTCS), a novel
automated test suite selection method for RL environments, designed to extract
test cases generated by any policy testing framework based on their
solvability, diversity, and general difficulty. MPTCS uses a set of policies to
select a diverse collection of reusable policy-agnostic test cases that reveal
typical flaws in the agents' behavior. The set of policies selects test cases
from a candidate pool, which can be generated by any policy testing method,
based on a difficulty score. We assess the effectiveness of the difficulty
score and how the method's effectiveness and cost depend on the number of
policies in the set. Additionally, a method for promoting diversity in the test
suite, a discretized general test case descriptor surface inspired by
quality-diversity algorithms, is examined to determine how it covers the state
space and which policies it triggers to produce faulty behaviors.

</details>


### [13] [Human-Written vs. AI-Generated Code: A Large-Scale Study of Defects, Vulnerabilities, and Complexity](https://arxiv.org/abs/2508.21634)
*Domenico Cotroneo,Cristina Improta,Pietro Liguori*

Main category: cs.SE

TL;DR: AI-generated code has unique quality tradeoffs: simpler but more vulnerable compared to human code, necessitating tailored QA strategies.


<details>
  <summary>Details</summary>
Motivation: Understanding how AI code assistants compare to human-written code in software quality is critical for ensuring developer workflows prioritize reliability, maintainability, and security.

Method: The paper evaluates 500k code samples in Python and Java from humans and three LLMs (ChatGPT, DeepSeek-Coder, Qwen-Coder), analyzing code defects through Orthogonal Defect Classification and security vulnerabilities via Common Weakness Enumeration.

Result: AI-generated code is simpler and more repetitive but contains more unused constructs, hardcoded debugging, and high-risk vulnerabilities, while human code shows greater structural complexity and maintainability challenges.

Conclusion: The study highlights the distinct defect profiles between AI-generated code and human code, emphasizing the need for specialized quality assurance practices in AI-assisted programming.

Abstract: As AI code assistants become increasingly integrated into software
development workflows, understanding how their code compares to human-written
programs is critical for ensuring reliability, maintainability, and security.
In this paper, we present a large-scale comparison of code authored by human
developers and three state-of-the-art LLMs, i.e., ChatGPT, DeepSeek-Coder, and
Qwen-Coder, on multiple dimensions of software quality: code defects, security
vulnerabilities, and structural complexity. Our evaluation spans over 500k code
samples in two widely used languages, Python and Java, classifying defects via
Orthogonal Defect Classification and security vulnerabilities using the Common
Weakness Enumeration. We find that AI-generated code is generally simpler and
more repetitive, yet more prone to unused constructs and hardcoded debugging,
while human-written code exhibits greater structural complexity and a higher
concentration of maintainability issues. Notably, AI-generated code also
contains more high-risk security vulnerabilities. These findings highlight the
distinct defect profiles of AI- and human-authored code and underscore the need
for specialized quality assurance practices in AI-assisted programming.

</details>


### [14] [The Integration of Agile Methodologies in DevOps Practices within the Information Technology Industry](https://arxiv.org/abs/2508.21811)
*Ashley Hourigan,Ridewaan Hanslo*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The demand for rapid software delivery in the Information Technology (IT)
industry has significantly intensified, emphasising the need for faster
software products and service releases with enhanced features to meet customer
expectations. Agile methodologies are replacing traditional approaches such as
Waterfall, where flexibility, iterative development and adaptation to change
are favoured over rigid planning and execution. DevOps, a subsequent evolution
from Agile, emphasises collaborative efforts in development and operations
teams, focusing on continuous integration and deployment to deliver resilient
and high-quality software products and services. This study aims to critically
assess both Agile and DevOps practices in the IT industry to identify the
feasibility and applicability of Agile methods in DevOps practices. Eleven
semi-structured interviews were conducted with Agile and DevOps practitioners
in varying capacities across several sectors within the IT industry. Through
thematic analysis, 51 unique codes were extracted and synthesised into 19
themes that reported on each phase of the DevOps lifecycle, specifically
regarding the integration and implementation of Agile methods into DevOps
practices. Based on the findings, a new understanding detailing the
interrelationship of Agile methods in DevOps practices was discussed that met
the research objectives.

</details>
