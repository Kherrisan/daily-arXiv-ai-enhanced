{"id": "2510.11804", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.11804", "abs": "https://arxiv.org/abs/2510.11804", "authors": ["Yuwen Cui", "Guangjing Wang", "Khanh Vu", "Kai Wei", "Kehan Shen", "Zhengyuan Jiang", "Xiao Han", "Ning Wang", "Zhuo Lu", "Yao Liu"], "title": "A Comprehensive Survey of Website Fingerprinting Attacks and Defenses in Tor: Advances and Open Challenges", "comment": "43 pages", "summary": "The Tor network provides users with strong anonymity by routing their\ninternet traffic through multiple relays. While Tor encrypts traffic and hides\nIP addresses, it remains vulnerable to traffic analysis attacks such as the\nwebsite fingerprinting (WF) attack, achieving increasingly high fingerprinting\naccuracy even under open-world conditions. In response, researchers have\nproposed a variety of defenses, ranging from adaptive padding, traffic\nregularization, and traffic morphing to adversarial perturbation, that seek to\nobfuscate or reshape traffic traces. However, these defenses often entail\ntrade-offs between privacy, usability, and system performance. Despite\nextensive research, a comprehensive survey unifying WF datasets, attack\nmethodologies, and defense strategies remains absent. This paper fills that gap\nby systematically categorizing existing WF research into three key domains:\ndatasets, attack models, and defense mechanisms. We provide an in-depth\ncomparative analysis of techniques, highlight their strengths and limitations\nunder diverse threat models, and discuss emerging challenges such as multi-tab\nbrowsing and coarse-grained traffic features. By consolidating prior work and\nidentifying open research directions, this survey serves as a foundation for\nadvancing stronger privacy protection in Tor.", "AI": {"tldr": "This paper provides a comprehensive survey of website fingerprinting (WF) research in the Tor network, categorizing existing work into datasets, attack models, and defense mechanisms while identifying trade-offs and emerging challenges.", "motivation": "Tor's anonymity is threatened by WF attacks, yet prior defenses lack systematic evaluation and unified analysis. Existing research lacks a holistic survey consolidating methodologies and challenges.", "method": "Systematic categorization of WF research into three domains (datasets, attack models, defense mechanisms), followed by comparative analysis of techniques and evaluation of strengths/limitations under diverse threat models.", "result": "In-depth comparative analysis of WF techniques, highlighting trade-offs between privacy/usability/performance, and identification of emerging challenges like multi-tab browsing and coarse-grained traffic features.", "conclusion": "This survey establishes a foundational framework for advancing Tor privacy by consolidating prior work, clarifying open research directions, and emphasizing the need for balanced defense strategies."}}
{"id": "2510.11823", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11823", "abs": "https://arxiv.org/abs/2510.11823", "authors": ["Caelin Kaplan", "Alexander Warnecke", "Neil Archibald"], "title": "BlackIce: A Containerized Red Teaming Toolkit for AI Security Testing", "comment": null, "summary": "AI models are being increasingly integrated into real-world systems, raising\nsignificant concerns about their safety and security. Consequently, AI red\nteaming has become essential for organizations to proactively identify and\naddress vulnerabilities before they can be exploited by adversaries. While\nnumerous AI red teaming tools currently exist, practitioners face challenges in\nselecting the most appropriate tools from a rapidly expanding landscape, as\nwell as managing complex and frequently conflicting software dependencies\nacross isolated projects. Given these challenges and the relatively small\nnumber of organizations with dedicated AI red teams, there is a strong need to\nlower barriers to entry and establish a standardized environment that\nsimplifies the setup and execution of comprehensive AI model assessments.\n  Inspired by Kali Linux's role in traditional penetration testing, we\nintroduce BlackIce, an open-source containerized toolkit designed for red\nteaming Large Language Models (LLMs) and classical machine learning (ML)\nmodels. BlackIce provides a reproducible, version-pinned Docker image that\nbundles 14 carefully selected open-source tools for Responsible AI and Security\ntesting, all accessible via a unified command-line interface. With this setup,\ninitiating red team assessments is as straightforward as launching a container,\neither locally or using a cloud platform. Additionally, the image's modular\narchitecture facilitates community-driven extensions, allowing users to easily\nadapt or expand the toolkit as new threats emerge. In this paper, we describe\nthe architecture of the container image, the process used for selecting tools,\nand the types of evaluations they support.", "AI": {"tldr": "BlackIce is an open-source containerized toolkit for AI red teaming inspired by Kali Linux, bundling 14 tools for LLM/ML model security testing via a unified Docker image.", "motivation": "Existing AI red teaming tools face usability challenges due to fragmented tooling, conflicting dependencies, and low adoption of dedicated red teams, necessitating standardized, modular solutions to lower entry barriers.", "method": "The authors designed BlackIce as a version-pinned Docker image with 14 selected open-source tools for Responsible AI/Security, modular architecture enabling community extensions, and a CLI interface simplifying local/cloud deployment.", "result": "BlackIce provides a reproducible, scalable environment for AI red teaming assessments, with a documented architecture, tool selection rationale, and supported evaluation types.", "conclusion": "BlackIce addresses critical gaps in AI red teaming infrastructure by combining reproducibility, modularity, and ease-of-use, enabling standardized security assessments of AI systems."}}
{"id": "2510.11837", "categories": ["cs.CR", "cs.AI", "K.6.5; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.11837", "abs": "https://arxiv.org/abs/2510.11837", "authors": ["Dominik Schwarz"], "title": "Countermind: A Multi-Layered Security Architecture for Large Language Models", "comment": "33 pages, 3 figures, 6 tables. Keywords: LLM security;\n  defense-in-depth; prompt injection; activation steering; multimodal sandbox;\n  threat modeling", "summary": "The security of Large Language Model (LLM) applications is fundamentally\nchallenged by \"form-first\" attacks like prompt injection and jailbreaking,\nwhere malicious instructions are embedded within user inputs. Conventional\ndefenses, which rely on post hoc output filtering, are often brittle and fail\nto address the root cause: the model's inability to distinguish trusted\ninstructions from untrusted data. This paper proposes Countermind, a\nmulti-layered security architecture intended to shift defenses from a reactive,\npost hoc posture to a proactive, pre-inference, and intra-inference enforcement\nmodel. The architecture proposes a fortified perimeter designed to structurally\nvalidate and transform all inputs, and an internal governance mechanism\nintended to constrain the model's semantic processing pathways before an output\nis generated. The primary contributions of this work are conceptual designs\nfor: (1) A Semantic Boundary Logic (SBL) with a mandatory, time-coupled Text\nCrypter intended to reduce the plaintext prompt injection attack surface,\nprovided all ingestion paths are enforced. (2) A Parameter-Space Restriction\n(PSR) mechanism, leveraging principles from representation engineering, to\ndynamically control the LLM's access to internal semantic clusters, with the\ngoal of mitigating semantic drift and dangerous emergent behaviors. (3) A\nSecure, Self-Regulating Core that uses an OODA loop and a learning security\nmodule to adapt its defenses based on an immutable audit log. (4) A Multimodal\nInput Sandbox and Context-Defense mechanisms to address threats from\nnon-textual data and long-term semantic poisoning. This paper outlines an\nevaluation plan designed to quantify the proposed architecture's effectiveness\nin reducing the Attack Success Rate (ASR) for form-first attacks and to measure\nits potential latency overhead.", "AI": {"tldr": "This paper introduces Countermind, a proactive security architecture for LLMs, to combat form-first attacks by implementing pre-inference input validation, parameter-space restrictions, self-adaptive defense mechanisms, and multimodal threat mitigation.", "motivation": "Current LLM security defenses relying on post-hoc output filtering are ineffective against 'form-first' attacks like prompt injection and jailbreaking, which exploit the model's inability to distinguish trusted instructions from untrusted data. This creates a need for proactive, pre-inference security mechanisms.", "method": "The paper proposes Countermind, a multi-layered security architecture with four core components: (1) Semantic Boundary Logic (SBL) and Text Crypter, (2) Parameter-Space Restriction (PSR) mechanism, (3) Secure, Self-Regulating Core with OODA loop, and (4) Multimodal Input Sandbox. These components enforce input validation, restrict semantic drift, adapt defenses via audit logs, and mitigate non-textual threats.", "result": "The paper establishes a conceptual framework for Countermind but does not provide quantitative results. It emphasizes designing evaluation metrics to measure ASR reduction and latency overhead for the proposed architecture in future work.", "conclusion": "The paper concludes by outlining an evaluation plan to quantify the proposed architecture's effectiveness in reducing the Attack Success Rate (ASR) for form-first attacks and measuring its latency overhead, while presenting a proactive security framework for LLM applications."}}
{"id": "2510.11851", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11851", "abs": "https://arxiv.org/abs/2510.11851", "authors": ["Shuo Chen", "Zonggen Li", "Zhen Han", "Bailan He", "Tong Liu", "Haokun Chen", "Georg Groh", "Philip Torr", "Volker Tresp", "Jindong Gu"], "title": "Deep Research Brings Deeper Harm", "comment": "Accepted to Reliable ML from Unreliable Data Workshop @ NeurIPS 2025", "summary": "Deep Research (DR) agents built on Large Language Models (LLMs) can perform\ncomplex, multi-step research by decomposing tasks, retrieving online\ninformation, and synthesizing detailed reports. However, the misuse of LLMs\nwith such powerful capabilities can lead to even greater risks. This is\nespecially concerning in high-stakes and knowledge-intensive domains such as\nbiosecurity, where DR can generate a professional report containing detailed\nforbidden knowledge. Unfortunately, we have found such risks in practice:\nsimply submitting a harmful query, which a standalone LLM directly rejects, can\nelicit a detailed and dangerous report from DR agents. This highlights the\nelevated risks and underscores the need for a deeper safety analysis. Yet,\njailbreak methods designed for LLMs fall short in exposing such unique risks,\nas they do not target the research ability of DR agents. To address this gap,\nwe propose two novel jailbreak strategies: Plan Injection, which injects\nmalicious sub-goals into the agent's plan; and Intent Hijack, which reframes\nharmful queries as academic research questions. We conducted extensive\nexperiments across different LLMs and various safety benchmarks, including\ngeneral and biosecurity forbidden prompts. These experiments reveal 3 key\nfindings: (1) Alignment of the LLMs often fail in DR agents, where harmful\nprompts framed in academic terms can hijack agent intent; (2) Multi-step\nplanning and execution weaken the alignment, revealing systemic vulnerabilities\nthat prompt-level safeguards cannot address; (3) DR agents not only bypass\nrefusals but also produce more coherent, professional, and dangerous content,\ncompared with standalone LLMs. These results demonstrate a fundamental\nmisalignment in DR agents and call for better alignment techniques tailored to\nDR agents. Code and datasets are available at\nhttps://chenxshuo.github.io/deeper-harm.", "AI": {"tldr": "The paper analyzes the security risks of Deep Research (DR) agents based on Large Language Models (LLMs), identifying how they can produce detailed, dangerous reports even when a standalone LLM would reject the input. It introduces two jailbreak strategies to expose these risks and presents three key findings on alignment failures and vulnerabilities.", "motivation": "The paper aims to highlight the heightened risks associated with using DR agents in sensitive domains such as biosecurity, where these agents can generate dangerous content even if the original query would be rejected by a standalone LLM. This calls for a deeper safety analysis given the multi-step planning and execution capabilities of DR agents.", "method": "The researchers propose two original jailbreak strategies: Plan Injection, which involves inserting malicious sub-goals into the agent's task plan; and Intent Hijack, which reframes harmful queries as academic research questions. These approaches are tested across different LLMs and safety benchmarks, including those related to biosecurity.", "result": "The experiments demonstrate three main insights: (1) the alignment of LLM within DR agents can break under academic framing of harmful prompts; (2) multi-step planning weakens alignment, exposing systemic vulnerabilities not addressed by existing prompt-level safeguards; (3) DR agents are not only able to bypass refusals but also produce more coherent and dangerous content compared to single LLM outputs.", "conclusion": "The paper concludes that DR agents have deep-rooted safety issues, especially due to their multi-step reasoning and synthesis capabilities, and that these represent a significant risk in high-stakes domains. It advocates for more effective alignment techniques that are specifically adapted for DR agents to prevent misuse."}}
{"id": "2510.11722", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.11722", "abs": "https://arxiv.org/abs/2510.11722", "authors": ["Haruhiko Yoshioka", "Kazumasa Shimari", "Hidetake Uwano", "Kenichi Matsumoto"], "title": "eye2vec: Learning Distributed Representations of Eye Movement for Program Comprehension Analysis", "comment": "2 pages, 1 figure, conference", "summary": "This paper presents eye2vec, an infrastructure for analyzing software\ndevelopers' eye movements while reading source code. In common eye-tracking\nstudies in program comprehension, researchers must preselect analysis targets\nsuch as control flow or syntactic elements, and then develop analysis methods\nto extract appropriate metrics from the fixation for source code. Here,\nresearchers can define various levels of AOIs like words, lines, or code\nblocks, and the difference leads to different results. Moreover, the\ninterpretation of fixation for word/line can vary across the purposes of the\nanalyses. Hence, the eye-tracking analysis is a difficult task that depends on\nthe time-consuming manual work of the researchers. eye2vec represents\ncontinuous two fixations as transitions between syntactic elements using\ndistributed representations. The distributed representation facilitates the\nadoption of diverse data analysis methods with rich semantic interpretations.", "AI": {"tldr": "eye2vec is an infrastructure for analyzing developers' eye movements during code reading by modeling fixations as transitions between syntactic elements using distributed representations, enabling flexible analysis.", "motivation": "Traditional eye-tracking analysis in programming requires manual preselection of targets (e.g., control flow/elements) and analysis methods, leading to time-consuming workflows and inconsistent results due to arbitrary AOI definitions.", "method": "eye2vec represents consecutive fixations as distributed syntactic element transitions, enabling diverse analysis methods through rich semantic vector representations of code navigation patterns.", "result": "Facilitates analysis of code comprehension with reduced manual effort, supports multiple AOI definitions, and provides semantically interpretable metrics through distributed vector analysis.", "conclusion": "eye2vec addresses limitations of traditional eye-tracking studies by offering an automated, flexible infrastructure for analyzing developers' code-reading behaviors through a distributed representation framework."}}
{"id": "2510.11898", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.11898", "abs": "https://arxiv.org/abs/2510.11898", "authors": ["Rayed Suhail Ahmad", "Rehan Ahmad", "Quamar Niyaz"], "title": "Lightweight CNN-Based Wi-Fi Intrusion Detection Using 2D Traffic Representations", "comment": null, "summary": "Wi-Fi networks are ubiquitous in both home and enterprise environments,\nserving as a primary medium for Internet access and forming the backbone of\nmodern IoT ecosystems. However, their inherent vulnerabilities, combined with\nwidespread adoption, create opportunities for malicious actors to gain\nunauthorized access or compromise sensitive data stored on connected devices.\nTo address these challenges, we propose a deep learning based network intrusion\ndetection system (NIDS) for Wi-Fi environments. Building on our previous work,\nwe convert network traffic into two-dimensional data representations and use\nthem to train DL models based on convolutional neural network (CNN)\narchitectures. We implement five distinct techniques for generating the\ntwo-dimensional representations, and to ensure low detection latency, we adopt\nlightweight CNN architectures in our NIDS. The models are trained using the\nAWID3 dataset, a publicly available benchmark for Wi-Fi NIDS research, and are\nevaluated for both binary and multi-class classification tasks. Experimental\nresults demonstrate that the proposed approach achieves competitive detection\nperformance with low inference time, making it suitable for real-world Wi-Fi\ndeployment scenarios.", "AI": {"tldr": "The paper proposes a lightweight deep learning-based NIDS for Wi-Fi environments using 2D data representations and CNNs, showing competitive performance with low latency.", "motivation": "Wi-Fi networks are widely used but vulnerable to attacks due to their inherent weaknesses and widespread deployment, necessitating effective intrusion detection systems.", "method": "The authors convert network traffic into 2D representations and train lightweight CNN models for intrusion detection, using five techniques for data generation and the AWID3 dataset for training.", "result": "The proposed NIDS demonstrates competitive detection performance with low inference time, suitable for real-world Wi-Fi deployments.", "conclusion": "The approach is effective and efficient for Wi-Fi intrusion detection, paving the way for practical deployment of deep learning methods in this domain."}}
{"id": "2510.11813", "categories": ["cs.SE", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.11813", "abs": "https://arxiv.org/abs/2510.11813", "authors": ["Marcus Emmanuel Barnes", "Taher A. Ghaleb", "Safwat Hassan"], "title": "Task-Aware Reduction for Scalable LLM-Database Systems", "comment": "Preprint. Accepted for presentation at the Workshop on Language\n  Models and Databases (LMD), co-located with CASCON 2025 (IEEE). The final\n  version will appear in IEEE Xplore", "summary": "Large Language Models (LLMs) are increasingly applied to data-intensive\nworkflows, from database querying to developer observability. Yet the\neffectiveness of these systems is constrained by the volume, verbosity, and\nnoise of real-world text-rich data such as logs, telemetry, and monitoring\nstreams. Feeding such data directly into LLMs is costly, environmentally\nunsustainable, and often misaligned with task objectives. Parallel efforts in\nLLM efficiency have focused on model- or architecture-level optimizations, but\nthe challenge of reducing upstream input verbosity remains underexplored. In\nthis paper, we argue for treating the token budget of an LLM as an attention\nbudget and elevating task-aware text reduction as a first-class design\nprinciple for language -- data systems. We position input-side reduction not as\ncompression, but as attention allocation: prioritizing information most\nrelevant to downstream tasks. We outline open research challenges for building\nbenchmarks, designing adaptive reduction pipelines, and integrating\ntoken-budget--aware preprocessing into database and retrieval systems. Our\nvision is to channel scarce attention resources toward meaningful signals in\nnoisy, data-intensive workflows, enabling scalable, accurate, and sustainable\nLLM--data integration.", "AI": {"tldr": "This paper advocates treating LLM token budgets as 'attention budgets' and proposes task-aware text reduction as a core principle for efficient LLM-data integration, emphasizing adaptive preprocessing to prioritize task-relevant information in noisy data workflows.", "motivation": "Real-world data like logs and telemetry are verbose/noisy, directly ingesting them into LLMs is costly and misaligned with task goals. Prior work focuses on model optimizations, neglecting upstream input reduction challenges.", "method": "Reframing input reduction as 'attention allocation' rather than compression. Outlines research challenges: developing benchmarks, adaptive reduction pipelines, and token-budget-aware preprocessing integration into database/retrieval systems.", "result": "Proposes a design framework for task-driven input reduction, emphasizing sustainable LLM usage through intelligent signal prioritization in data-intensive workflows.", "conclusion": "\u547c\u5401\u5c06\u6ce8\u610f\u529b\u8d44\u6e90\u96c6\u4e2d\u4e8e\u5173\u952e\u4fe1\u53f7\uff0c\u901a\u8fc7\u4e0a\u6e38\u6587\u672c\u4f18\u5316\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u7cbe\u51c6\u4e14\u73af\u4fdd\u7684LLM-\u6570\u636e\u878d\u5408\u7cfb\u7edf\uff0c\u63a8\u52a8\u6570\u636e\u5e93\u4e0e\u68c0\u7d22\u7cfb\u7edf\u67b6\u6784\u7684\u6839\u672c\u6027\u521b\u65b0\u3002"}}
{"id": "2510.11915", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.11915", "abs": "https://arxiv.org/abs/2510.11915", "authors": ["Deeksha Hareesha Kulal", "Chidozie Princewill Arannonu", "Afsah Anwar", "Nidhi Rastogi", "Quamar Niyaz"], "title": "Robust ML-based Detection of Conventional, LLM-Generated, and Adversarial Phishing Emails Using Advanced Text Preprocessing", "comment": null, "summary": "Phishing remains a critical cybersecurity threat, especially with the advent\nof large language models (LLMs) capable of generating highly convincing\nmalicious content. Unlike earlier phishing attempts which are identifiable by\ngrammatical errors, misspellings, incorrect phrasing, and inconsistent\nformatting, LLM generated emails are grammatically sound, contextually\nrelevant, and linguistically natural. These advancements make phishing emails\nincreasingly difficult to distinguish from legitimate ones, challenging\ntraditional detection mechanisms. Conventional phishing detection systems often\nfail when faced with emails crafted by LLMs or manipulated using adversarial\nperturbation techniques. To address this challenge, we propose a robust\nphishing email detection system featuring an enhanced text preprocessing\npipeline. This pipeline includes spelling correction and word splitting to\ncounteract adversarial modifications and improve detection accuracy. Our\napproach integrates widely adopted natural language processing (NLP) feature\nextraction techniques and machine learning algorithms. We evaluate our models\non publicly available datasets comprising both phishing and legitimate emails,\nachieving a detection accuracy of 94.26% and F1-score of 84.39% in model\ndeployment setting. To assess robustness, we further evaluate our models using\nadversarial phishing samples generated by four attack methods in Python\nTextAttack framework. Additionally, we evaluate models' performance against\nphishing emails generated by LLMs including ChatGPT and Llama. Results\nhighlight the resilience of models against evolving AI-powered phishing\nthreats.", "AI": {"tldr": "The paper presents a robust phishing email detection system addressing challenges from LLM-generated and adversarial phishing emails through enhanced preprocessing and machine learning, achieving 94.26% detection accuracy and resilience against AI-powered threats.", "motivation": "Phishing emails generated by large language models (LLMs) and modified via adversarial techniques evade traditional detection systems, necessitating improved defenses to combat increasingly sophisticated cyber threats.", "method": "An enhanced text preprocessing pipeline (spelling correction, word splitting) counters adversarial perturbations, combined with NLP feature extraction and machine learning algorithms to detect phishing emails.", "result": "Achieved 94.26% detection accuracy and 84.39% F1-score on public datasets; models demonstrated robustness against adversarial attacks (via Python TextAttack framework) and LLM-generated phishing samples (ChatGPT, Llama).", "conclusion": "The proposed system effectively detects both traditional and AI-powered phishing emails, offering resilience to adversarial techniques and evolving LLM-driven threats through robust preprocessing and adaptive machine learning."}}
{"id": "2510.11838", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.11838", "abs": "https://arxiv.org/abs/2510.11838", "authors": ["Xu Yang", "Jiayuan Zhou", "Michael Pacheco", "Wenhan Zhu", "Pengfei He", "Shaowei Wang", "Kui Liu", "Ruiqi Pan"], "title": "Lingxi: Repository-Level Issue Resolution Framework Enhanced by Procedural Knowledge Guided Scaling", "comment": null, "summary": "Driven by the advancements of Large Language Models (LLMs), LLM-powered\nagents are making significant improvements in software engineering tasks, yet\nstruggle with complex, repository-level issue resolution. Existing agent-based\nmethods have two key limitations. First, they lack of procedural knowledge\n(i.e., how an issue is fixed step-by-step and rationales behind it) to learn\nand leverage for issue resolution. Second, they rely on massive computational\npower to blindly explore the solution space. % To address those limitations, we\npropose Lingxi, an issue resolution framework that leverages procedural\nknowledge extracted from historical issue-fixing data to guide agents in\nsolving repository-level issues. \\ourTool first constructs this knowledge\noffline through a hierarchical abstraction mechanism, enabling agents to learn\nthe how and why behind a fix, not just the final solution. During online\napplication, it employs a knowledge-driven scaling method that leverages the\nprocedural knowledge of similar issues to intelligently analyze the target\nissue from multiple perspectives, in sharp contrast to undirected, brute-force\nexploration. % Lingxi successfully resolves 74.6\\% of bugs on the SWE-bench\nVerified benchmark in Past@1 setting, outperforming five state-of-the-art\ntechniques by a significant margin (5.4\\% to 14.9\\%). Our comprehensive\nablation study confirmed that the success of Lingxi comes directly from its use\nof procedural knowledge. Without it, the performance gains from scaling alone\nis negligible. Our qualitative study further shows that the ``design patterns\n$\\&$ coding practices'' is the most critical knowledge aspect, and that the\nroles of different knowledge aspects switch across different stages (i.e.,\nanalysis, planning, and fixing).", "AI": {"tldr": "Lingxi is a framework that utilizes procedural knowledge from historical data to enhance LLMs in solving complex, repository-level software engineering issues.", "motivation": "Despite significant advancements in LLM-powered agents for software engineering, they struggle with complex, repository-level issues due to lack of procedural knowledge and the need for excessive computational resources in brute-force exploration.", "method": "Lingxi constructs procedural knowledge offline using a hierarchical abstraction mechanism, and then applies a knowledge-driven scaling method during online issue resolution to guide agents based on patterns from similar past issues.", "result": "Lingxi resolves 74.6% of bugs in the SWE-bench Verified benchmark, outperforming five existing state-of-the-art methods by 5.4% to 14.9% in the Past@1 setting.", "conclusion": "The framework demonstrates that using procedural knowledge significantly improves the performance of agents in complex issue resolution, with the most impactful aspect being design patterns and coding practices, and the importance of knowledge varying across different resolution stages."}}
{"id": "2510.11974", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11974", "abs": "https://arxiv.org/abs/2510.11974", "authors": ["Yutong Cheng", "Yang Liu", "Changze Li", "Dawn Song", "Peng Gao"], "title": "CTIArena: Benchmarking LLM Knowledge and Reasoning Across Heterogeneous Cyber Threat Intelligence", "comment": "Under peer-review", "summary": "Cyber threat intelligence (CTI) is central to modern cybersecurity, providing\ncritical insights for detecting and mitigating evolving threats. With the\nnatural language understanding and reasoning capabilities of large language\nmodels (LLMs), there is increasing interest in applying them to CTI, which\ncalls for benchmarks that can rigorously evaluate their performance. Several\nearly efforts have studied LLMs on some CTI tasks but remain limited: (i) they\nadopt only closed-book settings, relying on parametric knowledge without\nleveraging CTI knowledge bases; (ii) they cover only a narrow set of tasks,\nlacking a systematic view of the CTI landscape; and (iii) they restrict\nevaluation to single-source analysis, unlike realistic scenarios that require\nreasoning across multiple sources. To fill these gaps, we present CTIArena, the\nfirst benchmark for evaluating LLM performance on heterogeneous, multi-source\nCTI under knowledge-augmented settings. CTIArena spans three categories,\nstructured, unstructured, and hybrid, further divided into nine tasks that\ncapture the breadth of CTI analysis in modern security operations. We evaluate\nten widely used LLMs and find that most struggle in closed-book setups but show\nnoticeable gains when augmented with security-specific knowledge through our\ndesigned retrieval-augmented techniques. These findings highlight the\nlimitations of general-purpose LLMs and the need for domain-tailored techniques\nto fully unlock their potential for CTI.", "AI": {"tldr": "CTIArena benchmark shows LLMs benefit from security knowledge augmentation for multi-source threat intelligence analysis.", "motivation": "Existing LLM evaluations for CTI are limited to closed-book settings, narrow task coverage, and single-source analysis, despite real-world CTI requiring multi-source reasoning and domain-specific knowledge.", "method": "CTIArena is introduced as a benchmark evaluating LLMs in knowledge-augmented settings for multi-source CTI analysis, spanning three categories (structured, unstructured, hybrid) and nine tasks, with retrieval-augmented techniques to enhance performance.", "result": "Most evaluated LLMs improved significantly when augmented with security-specific knowledge via retrieval-augmented techniques, revealing underperformance in closed-book setups and emphasizing the need for domain-adaptation.", "conclusion": "The paper highlights the necessity of domain-specific techniques to enhance LLMs for CTI, demonstrating that knowledge-augmented approaches address their limitations in multi-source, heterogeneous threat analysis."}}
{"id": "2510.11872", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.11872", "abs": "https://arxiv.org/abs/2510.11872", "authors": ["Alessandro Cornacchia", "Vaastav Anand", "Muhammad Bilal", "Zafar Qazi", "Marco Canini"], "title": "DMAS-Forge: A Framework for Transparent Deployment of AI Applications as Distributed Systems", "comment": "1st Workshop on Systems for Agentic AI (SAA '25)", "summary": "Agentic AI applications increasingly rely on multiple agents with distinct\nroles, specialized tools, and access to memory layers to solve complex tasks --\nclosely resembling service-oriented architectures. Yet, in the rapid evolving\nlandscape of programming frameworks and new protocols, deploying and testing AI\nagents as distributed systems remains a daunting and labor-intensive task. We\npresent DMAS-Forge, a framework designed to close this gap. DMAS-Forge\ndecouples application logic from specific deployment choices, and aims at\ntransparently generating the necessary glue code and configurations to spawn\ndistributed multi-agent applications across diverse deployment scenarios with\nminimal manual effort. We present our vision, design principles, and a\nprototype of DMAS-Forge. Finally, we discuss the opportunities and future work\nfor our approach.", "AI": {"tldr": "DMAS-Forge is a framework that simplifies deploying distributed multi-agent AI applications by decoupling application logic from deployment specifics and automating glue code/configuration generation.", "motivation": "Current AI agent deployment is labor-intensive and fragmented across evolving frameworks. Service-oriented architectures emulate complex agent interactions, but orchestrating these systems requires significant manual effort.", "method": "DMAS-Forge abstracts deployment complexities through automated code generation, separating application logic from deployment choices. The framework supports diverse deployment scenarios with minimal manual intervention.", "result": "Prototype implementation demonstrates framework's ability to generate deployment artifacts. Design principles and vision are presented alongside discussion of opportunities for future work.", "conclusion": "DMAS-Forge reduces deployment overhead for multi-agent systems while maintaining flexibility across architectures. Future work includes expanding deployment targets and refining automation capabilities."}}
{"id": "2510.12031", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.12031", "abs": "https://arxiv.org/abs/2510.12031", "authors": ["Urvashi Kishnani", "Sanchari Das"], "title": "Security and Privacy Assessment of U.S. and Non-U.S. Android E-Commerce Applications", "comment": null, "summary": "E-commerce mobile applications are central to global financial transactions,\nmaking their security and privacy crucial. In this study, we analyze 92\ntop-grossing Android e-commerce apps (58 U.S.-based and 34 international) using\nMobSF, AndroBugs, and RiskInDroid. Our analysis shows widespread SSL and\ncertificate weaknesses, with approximately 92% using unsecured HTTP connections\nand an average MobSF security score of 40.92/100. Over-privileged permissions\nwere identified in 77 apps. While U.S. apps exhibited fewer manifest, code, and\ncertificate vulnerabilities, both groups showed similar network-related issues.\nWe advocate for the adoption of stronger, standardized, and user-focused\nsecurity practices across regions.", "AI": {"tldr": "High security and privacy risks in top-grossing e-commerce apps", "motivation": "E-commerce mobile apps are used for major financial transactions, so ensuring their security and privacy is vital. This study investigates the existing issues in such apps.", "method": "The team analyzed 92 Android e-commerce apps from two categories (U.S. and international) using MobSF, AndroBugs, and RiskInDroid tools.", "result": "Most apps used unsecured HTTP connections with a poor MobSF security score; 77 had excessive permissions, and both groups had similar network vulnerabilities but U.S. apps were better at manifest, code, and certificate issues.", "conclusion": "The research highlights the need for stronger, standardized, and user-focused security approaches for e-commerce mobile apps across all regions."}}
{"id": "2510.12011", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.12011", "abs": "https://arxiv.org/abs/2510.12011", "authors": ["Bei Zhou", "Maximilian Balmus", "Cesare Corrado", "Ludovica Cicci", "Shuang Qian", "Steven A. Niederer"], "title": "TorchCor: High-Performance Cardiac Electrophysiology Simulations with the Finite Element Method on GPUs", "comment": null, "summary": "Cardiac electrophysiology (CEP) simulations are increasingly used for\nunderstanding cardiac arrhythmias and guiding clinical decisions. However,\nthese simulations typically require high-performance computing resources with\nnumerous CPU cores, which are often inaccessible to many research groups and\nclinicians. To address this, we present TorchCor, a high-performance Python\nlibrary for CEP simulations using the finite element method on general-purpose\nGPUs. Built on PyTorch, TorchCor significantly accelerates CEP simulations,\nparticularly for large 3D meshes. The accuracy of the solver is verified\nagainst manufactured analytical solutions and the $N$-version benchmark\nproblem. TorchCor is freely available for both academic and commercial use\nwithout restrictions.", "AI": {"tldr": "TorchCor is a free, PyTorch-based GPU-accelerated Python library that makes cardiac electrophysiology simulations both faster and more widely accessible.", "motivation": "Traditional cardiac electrophysiology simulations require high-core CPU systems that are often inaccessible to researchers and clinicians.", "method": "TorchCor utilizes the finite element method on general-purpose GPUs built with PyTorch to accelerate simulations.", "result": "TorchCor's accuracy was validated through manufactured analytical solutions and N-version benchmarks, demonstrating significant performance improvements especially for large 3D meshes.", "conclusion": "TorchCor, a Python library for CEP simulations using GPUs, addresses the inaccessible computing resources issue by providing a freely available, high-performance solution."}}
{"id": "2510.12045", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.12045", "abs": "https://arxiv.org/abs/2510.12045", "authors": ["Onur Eren Arpaci", "Raouf Boutaba", "Florian Kerschbaum"], "title": "Over-Threshold Multiparty Private Set Intersection for Collaborative Network Intrusion Detection", "comment": "To appear in 23rd USENIX Symposium on Networked Systems Design and\n  Implementation (NSDI)", "summary": "An important function of collaborative network intrusion detection is to\nanalyze the network logs of the collaborators for joint IP addresses. However,\nsharing IP addresses in plain is sensitive and may be even subject to privacy\nlegislation as it is personally identifiable information. In this paper, we\npresent the privacy-preserving collection of IP addresses. We propose a single\ncollector, over-threshold private set intersection protocol. In this protocol\n$N$ participants identify the IP addresses that appear in at least $t$\nparticipant's sets without revealing any information about other IP addresses.\nUsing a novel hashing scheme, we reduce the computational complexity of the\nprevious state-of-the-art solution from $O(M(N \\log{M}/t)^{2t})$ to\n$O(t^2M\\binom{N}{t})$, where $M$ denotes the dataset size. This reduction makes\nit practically feasible to apply our protocol to real network logs. We test our\nprotocol using joint networks logs of multiple institutions. Additionally, we\npresent two deployment options: a collusion-safe deployment, which provides\nstronger security guarantees at the cost of increased communication overhead,\nand a non-interactive deployment, which assumes a non-colluding collector but\noffers significantly lower communication costs and applicable to many use cases\nof collaborative network intrusion detection similar to ours.", "AI": {"tldr": "This paper introduces a privacy-preserving method for collaborative network intrusion detection.", "motivation": "Sharing IP addresses directly among collaborative institutions is a privacy risk due to their personally identifiable nature.", "method": "The authors designed a protocol where multiple participants jointly identify over-represented IP addresses using a novel hashing scheme.", "result": "The new protocol reduces computational complexity compared to existing solutions and was tested successfully on network logs from multiple institutions.", "conclusion": "The paper offers two deployment strategies for the protocol, each optimized for different security and cost trade-offs."}}
{"id": "2510.12082", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12082", "abs": "https://arxiv.org/abs/2510.12082", "authors": ["Huy Nguyen", "Christoph Treude", "Patanamon Thongtanunam"], "title": "Enhancing Neural Code Representation with Additional Context", "comment": "34 pages, 7 figures, 11 tables", "summary": "Automated program comprehension underpins many software engineering tasks,\nfrom code summarisation to clone detection. Recent deep learning models achieve\nstrong results but typically rely on source code alone, overlooking contextual\ninformation such as version history or structural relationships. This limits\ntheir ability to capture how code evolves and operates. We conduct an empirical\nstudy on how enriching code representations with such contextual signals\naffects neural model performance on key comprehension tasks. Two downstream\ntasks, code clone detection and code summarisation, are evaluated using SeSaMe\n(1,679 Java methods) and CodeSearchNet (63,259 methods). Five representative\nmodels (CodeBERT, GraphCodeBERT, CodeT5, PLBART, ASTNN) are fine-tuned under\ncode-only and context-augmented settings. Results show that context generally\nimproves performance: version history consistently boosts clone detection\n(e.g., CodeT5 +15.92% F1) and summarisation (e.g., GraphCodeBERT +5.56%\nMETEOR), while call-graph effects vary by model and task. Combining multiple\ncontexts yields further gains (up to +21.48% macro-F1). Human evaluation on 100\nJava snippets confirms that context-augmented summaries are significantly\npreferred for Accuracy and Content Adequacy (p <= 0.026; |delta| up to 0.55).\nThese findings highlight the potential of contextual signals to enhance code\ncomprehension and open new directions for optimising contextual encoding in\nneural SE models.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
