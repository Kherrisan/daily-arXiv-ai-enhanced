<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 22]
- [cs.SE](#cs.SE) [Total: 16]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Layered, Overlapping, and Inconsistent: A Large-Scale Analysis of the Multiple Privacy Policies and Controls of U.S. Banks](https://arxiv.org/abs/2507.05415)
*Lu Xian,Van Tran,Lauren Lee,Meera Kumar,Yichen Zhang,Florian Schaub*

Main category: cs.CR

TL;DR: Analyzes inconsistencies in U.S. banks' privacy policies under GLBA and CCPA, revealing consumer confusion and questioning the effectiveness of current regulations.


<details>
  <summary>Details</summary>
Motivation: Complex and fragmented privacy policies hinder transparency, raising concerns about compliance and consumer understanding, especially with the coexistence of federal (GLBA) and state (CCPA) laws.

Method: Collected privacy policies from 2,067 major U.S. banks (~45.3% with multiple policies), then systematically compared disclosures of third-party data sharing for marketing across GLBA notices, additional disclosures, and cookie practices.

Result: 45.3% of banks had multiple policies, but many exhibited contradictions: denying third-party sharing in GLBA notices while disclosing it elsewhere, and using marketing/advertising cookies without consent notices. Multiplicity of policies correlates with fragmented transparency.

Conclusion: Current regulatory requirements (GLBA/CCPA) fail to unify privacy disclosures effectively, risking compliance gaps and eroded consumer trust. Reforms are needed to harmonize and simplify cross-regulation privacy control frameworks.

Abstract: Privacy policies are often complex. An exception is the two-page standardized
notice that U.S. financial institutions must provide under the
Gramm-Leach-Bliley Act (GLBA). However, banks now operate websites, mobile
apps, and other services that involve complex data sharing practices that
require additional privacy notices and do-not-sell opt-outs. We conducted a
large-scale analysis of how U.S. banks implement privacy policies and controls
in response to GLBA; other federal privacy policy requirements; and the
California Consumer Privacy Act (CCPA), a key example for U.S. state privacy
laws. We focused on the disclosure and control of a set of especially
privacy-invasive practices: third-party data sharing for marketing-related
purposes. We collected privacy policies for the 2,067 largest U.S. banks,
45.3\% of which provided multiple policies. Across disclosures and controls
within the \textit{same} bank, we identified frequent, concerning
inconsistencies -- such as banks indicating in GLBA notices that they do not
share with third parties but disclosing sharing elsewhere, or using third-party
marketing/advertising cookies without disclosure. This multiplicity of
policies, with the inconsistencies it causes, may create consumer confusion and
undermine the transparency goals of the very laws that require them. Our
findings call into question whether current policy requirements, such as the
GLBA notice, are achieving their intended goals in today's online banking
landscape. We discuss potential avenues for reforming and harmonizing privacy
policies and control requirements across federal and state laws.

</details>


### [2] [FrameShift: Learning to Resize Fuzzer Inputs Without Breaking Them](https://arxiv.org/abs/2507.05421)
*Harrison Green,Claire Le Goues,Fraser Brown*

Main category: cs.CR

TL;DR: The paper introduces FrameShift, a lightweight technique to prevent frameshift mutations in fuzzers by preserving input structure through relation field detection. It shows improved fuzzer performance and coverage across multiple languages and tools.


<details>
  <summary>Details</summary>
Motivation: Fuzzers without input format knowledge generate destructive frameshift mutations, creating malformed inputs that waste time and reduce efficiency. Existing tools struggle with this issue due to format-agnostic mutation strategies.

Method: FrameShift analyzes input fields and their size relationships during mutation, automatically detecting structural constraints to prevent destructive shifts. The implementation requires no additional instrumentation beyond standard coverage feedback and integrates into existing fuzzer frameworks.

Result: FrameShift integrates into AFL++ and LibAFL with significant performance optimizations, achieving 50%+ coverage improvements in some configurations. Case studies reveal it discovers structural relationships in unknown formats while generalizing to Rust and Python targets.

Conclusion: The technique demonstrates a versatile, non-intrusive solution to structural mutation problems in fuzzing. FrameShift improves fuzzing efficiency and coverage across programming languages and fuzzer implementations by preserving input structures through relation field detection.

Abstract: Coverage-guided fuzzers are powerful automated bug-finding tools. They mutate
program inputs, observe coverage, and save any input that hits an unexplored
path for future mutation. Unfortunately, without knowledge of input
formats--for example, the relationship between formats' data fields and
sizes--fuzzers are prone to generate destructive frameshift mutations. These
time-wasting mutations yield malformed inputs that are rejected by the target
program. To avoid such breaking mutations, this paper proposes a novel,
lightweight technique that preserves the structure of inputs during mutation by
detecting and using relation fields.
  Our technique, FrameShift, is simple, fast, and does not require additional
instrumentation beyond standard coverage feedback. We implement our technique
in two state-of-the-art fuzzers, AFL++ and LibAFL, and perform a 12+ CPU-year
fuzzer evaluation, finding that FrameShift improves the performance of the
fuzzer in each configuration, sometimes increasing coverage by more than 50%.
Furthermore, through a series of case studies, we show that our technique is
versatile enough to find important structural relationships in a variety of
formats, even generalizing beyond C/C++ targets to both Rust and Python.

</details>


### [3] [A Systematization of Security Vulnerabilities in Computer Use Agents](https://arxiv.org/abs/2507.05445)
*Daniel Jones,Giorgio Severi,Martin Pouliot,Gary Lopez,Joris de Gruyter,Santiago Zanella-Beguelin,Justin Song,Blake Bullwinkel,Pamela Cortez,Amanda Minnich*

Main category: cs.CR

TL;DR: This paper identifies unique security risks and architectural flaws in Computer Use Agents (CUAs) and proposes a security evaluation framework and design principles to address them.


<details>
  <summary>Details</summary>
Motivation: CUAs are increasingly deployed in consumer and enterprise environments, but their security boundaries remain poorly understood, introducing novel attack surfaces not captured by traditional threat models.

Method: The researchers conducted a systematic threat analysis and testing of real-world CUAs under adversarial conditions, focusing on three concrete exploit scenarios and seven classes of risks.

Result: They discovered seven risk classes and demonstrated three exploit scenarios: clickjacking via visual overlays, indirect prompt injection enabling RCE through chained tools, and CoT exposure attacks manipulating interface framing. Architectural flaws include lack of input provenance tracking, weak interface-action binding, and insufficient control over memory/delegation.

Conclusion: The paper concludes by proposing a CUA-specific security evaluation framework and design principles for safe deployment in adversarial and high-stakes settings.

Abstract: Computer Use Agents (CUAs), autonomous systems that interact with software
interfaces via browsers or virtual machines, are rapidly being deployed in
consumer and enterprise environments. These agents introduce novel attack
surfaces and trust boundaries that are not captured by traditional threat
models. Despite their growing capabilities, the security boundaries of CUAs
remain poorly understood. In this paper, we conduct a systematic threat
analysis and testing of real-world CUAs under adversarial conditions. We
identify seven classes of risks unique to the CUA paradigm, and analyze three
concrete exploit scenarios in depth: (1) clickjacking via visual overlays that
mislead interface-level reasoning, (2) indirect prompt injection that enables
Remote Code Execution (RCE) through chained tool use, and (3) CoT exposure
attacks that manipulate implicit interface framing to hijack multi-step
reasoning. These case studies reveal deeper architectural flaws across current
CUA implementations. Namely, a lack of input provenance tracking, weak
interface-action binding, and insufficient control over agent memory and
delegation. We conclude by proposing a CUA-specific security evaluation
framework and design principles for safe deployment in adversarial and
high-stakes settings.

</details>


### [4] [Disappearing Ink: Obfuscation Breaks N-gram Code Watermarks in Theory and Practice](https://arxiv.org/abs/2507.05512)
*Gehao Zhang,Eugene Bagdasarian,Juan Zhai,Shiqing Ma*

Main category: cs.CR

TL;DR: This paper evaluates the robustness of N-gram-based code watermarking against sophisticated code obfuscation techniques.


<details>
  <summary>Details</summary>
Motivation: Distinguishing AI-generated code from human-written code is crucial for authorship attribution and misuse detection, but current watermarking schemes lack evaluation against advanced obfuscation attacks.

Method: The authors formally model code obfuscation, propose a distribution consistency assumption, and experiment with three watermarking schemes, two LLMs, two programming languages, four benchmarks, and four obfuscators.

Result: Watermarking detectors show poor performance on obfuscated code (AUROC ~0.5), with obfuscators achieving undetected attack rates >60% in some cases. Theoretical analysis proves increased false acceptance rates after obfuscation.

Conclusion: Existing N-gram-based watermarking is vulnerable to obfuscation attacks. The paper provides theoretical foundations and practical evidence, while proposing potential robust watermaking approaches.

Abstract: Distinguishing AI-generated code from human-written code is becoming crucial
for tasks such as authorship attribution, content tracking, and misuse
detection. Based on this, N-gram-based watermarking schemes have emerged as
prominent, which inject secret watermarks to be detected during the generation.
  However, their robustness in code content remains insufficiently evaluated.
Most claims rely solely on defenses against simple code transformations or code
optimizations as a simulation of attack, creating a questionable sense of
robustness. In contrast, more sophisticated schemes already exist in the
software engineering world, e.g., code obfuscation, which significantly alters
code while preserving functionality. Although obfuscation is commonly used to
protect intellectual property or evade software scanners, the robustness of
code watermarking techniques against such transformations remains largely
unexplored.
  In this work, we formally model the code obfuscation and prove the
impossibility of N-gram-based watermarking's robustness with only one intuitive
and experimentally verified assumption, distribution consistency, satisfied.
Given the original false positive rate of the watermarking detection, the ratio
that the detector failed on the watermarked code after obfuscation will
increase to 1 - fpr.
  The experiments have been performed on three SOTA watermarking schemes, two
LLMs, two programming languages, four code benchmarks, and four obfuscators.
Among them, all watermarking detectors show coin-flipping detection abilities
on obfuscated codes (AUROC tightly surrounds 0.5). Among all models,
watermarking schemes, and datasets, both programming languages own obfuscators
that can achieve attack effects with no detection AUROC higher than 0.6 after
the attack. Based on the theoretical and practical observations, we also
proposed a potential path of robust code watermarking.

</details>


### [5] [PROTEAN: Federated Intrusion Detection in Non-IID Environments through Prototype-Based Knowledge Sharing](https://arxiv.org/abs/2507.05524)
*Sara Chennoufi,Yufei Han,Gregory Blanc,Emiliano De Cristofaro,Christophe Kiennert*

Main category: cs.CR

TL;DR: PROTEAN is a Fed-Learning prototype framework that enables accurate intrusion detection by sharing class prototypes across data-heterogeneous distributed networks. It improves cyberattack understanding without sacrificing privacy, validated on IIoT and 5G datasets.


<details>
  <summary>Details</summary>
Motivation: Federated Learning (FL) is promising for cybersecurity due to privacy constraints, but data heterogeneity from varied organizational security policies hinders model effectiveness. Existing approaches struggle to detect diverse attack types without direct access to non-IID data.

Method: PROTEAN employs prototype learning to exchange class-specific attack prototypes between organizations. This allows collaborative model training while preserving data privacy and enabling participants to learn novel attack patterns not present in their local data.

Result: Evaluation on IIoT and 5G-connected datasets demonstrates PROTEAN's ability to improve detection accuracy in data-heterogeneous environments while maintaining privacy. Organizations gain enhanced understanding of diverse attack techniques through prototype knowledge sharing.

Conclusion: PROTEAN advances privacy-preserving federated intrusion detection by addressing data heterogeneity through systematic prototype sharing. This approach maintains security privacy while enabling cross-organizational learning of attack patterns in complex networks.

Abstract: In distributed networks, participants often face diverse and fast-evolving
cyberattacks. This makes techniques based on Federated Learning (FL) a
promising mitigation strategy. By only exchanging model updates, FL
participants can collaboratively build detection models without revealing
sensitive information, e.g., network structures or security postures. However,
the effectiveness of FL solutions is often hindered by significant data
heterogeneity, as attack patterns often differ drastically across organizations
due to varying security policies. To address these challenges, we introduce
PROTEAN, a Prototype Learning-based framework geared to facilitate
collaborative and privacy-preserving intrusion detection. PROTEAN enables
accurate detection in environments with highly non-IID attack distributions and
promotes direct knowledge sharing by exchanging class prototypes of different
attack types among participants. This allows organizations to better understand
attack techniques not present in their data collections. We instantiate PROTEAN
on two cyber intrusion datasets collected from IIoT and 5G-connected
participants and evaluate its performance in terms of utility and privacy,
demonstrating its effectiveness in addressing data heterogeneity while
improving cyber attack understanding in federated intrusion detection systems
(IDSs).

</details>


### [6] [AI Agent Smart Contract Exploit Generation](https://arxiv.org/abs/2507.05558)
*Arthur Gervais,Liyi Zhou*

Main category: cs.CR

TL;DR: A1 converts LLMs into end-to-end exploit generators with autonomous vulnerability discovery tools. It achieves a 62.96% success rate on the VERITE benchmark, identifies 9 new vulnerable contracts, and demonstrates a profitability asymmetry between attackers and defenders in on-chain scanning scenarios.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the gap in autonomous exploit generation systems, aiming to leverage LLMs for end-to-end cybersecurity attacks without hand-crafted heuristics. It also investigates the ethical implications of AI's potential asymmetry in assisting attackers versus defenders in blockchain security.

Method: A1 provides agents with six domain-specific tools to analyze smart contract behavior, generate and test exploit strategies on blockchain states, and refine methods using execution feedback. Validation eliminates false positives, and the system is evaluated across six LLMs, 432 experiments, and 19 historical attacks via Monte Carlo analysis.

Result: A1 succeeds in 62.96% of VERITE benchmarks (17/27 cases), identifies 9 new vulnerabilities (5 post-training), and generates up to $8.59M USD in exploits. Iteration gains decline (+9.7% to +2.8% for iterations 2-5), and profitability analysis shows attackers need $6k exploit value versus $60k needed by defenders at 0.1% vulnerability rates.

Conclusion: The system reveals AI-driven exploit generators can autonomously achieve significant success in real-world smart contracts. The profitability asymmetry suggests AI systems may inherently favor exploitation over defense at low vulnerability rates, raising concerns about cybersecurity ethics and the potential for AI to disproportionately empower malicious actors in blockchain ecosystems.

Abstract: We present A1, an agentic execution driven system that transforms any LLM
into an end-to-end exploit generator. A1 has no hand-crafted heuristics and
provides the agent with six domain-specific tools that enable autonomous
vulnerability discovery. The agent can flexibly leverage these tools to
understand smart contract behavior, generate exploit strategies, test them on
blockchain states, and refine approaches based on execution feedback. All
outputs are concretely validated to eliminate false positives.
  The evaluation across 36 real-world vulnerable contracts on Ethereum and
Binance Smart Chain demonstrates a 62.96% (17 out of 27) success rate on the
VERITE benchmark. Beyond the VERITE dataset, A1 identified 9 additional
vulnerable contracts, with 5 cases occurring after the strongest model's
training cutoff date. Across all 26 successful cases, A1 extracts up to 8.59
million USD per case and 9.33 million USD total. Through 432 experiments across
six LLMs, we analyze iteration-wise performance showing diminishing returns
with average marginal gains of +9.7%, +3.7%, +5.1%, and +2.8% for iterations
2-5 respectively, with per-experiment costs ranging $0.01-$3.59. A Monte Carlo
analysis of 19 historical attacks shows success probabilities of 85.9%-88.8%
without detection delays.
  We investigate whether an attacker or a defender benefits most from deploying
A1 as a continuous on-chain scanning system. Our model shows that OpenAI's
o3-pro maintains profitability up to a 30.0 days scanning delay at 0.100%
vulnerability incidence rates, while faster models require >=1.000% rates to
break-even. The findings exposes a troubling asymmetry: at 0.1% vulnerability
rates, attackers achieve an on-chain scanning profitability at a $6000 exploit
value, while defenders require $60000, raising fundamental questions about
whether AI agents inevitably favor exploitation over defense.

</details>


### [7] [iThermTroj: Exploiting Intermittent Thermal Trojans in Multi-Processor System-on-Chips](https://arxiv.org/abs/2507.05576)
*Mehdi Elahi,Mohamed R. Elshamy,Abdel-Hameed Badawy,Ahmad Patooghy*

Main category: cs.CR

TL;DR: The paper introduces Intermittent Thermal Trojans (iThermTroj) that evade threshold-based detection by sporadically manipulating SoC thermal data and proposes lightweight ML classifiers to improve detection rates by up to 29.4% with a 0.8°C precision threshold.


<details>
  <summary>Details</summary>
Motivation: Current threshold-based thermal Trojan detection solutions fail to address the challenge of evasive, sporadically activated threats, necessitating a more effective anomaly detection approach for SoC security.

Method: The authors designed iThermTroj attacks (random time-triggered, intermittent) to bypass existing methods, analyzed SoC vulnerability through attack scenarios, and developed compact ML classifiers for real-time anomaly detection.

Result: The ML classifiers outperformed existing methods, achieving 29.4%, 17.2%, and 14.3% higher detection rates for 80%, 60%, and 40% thermal data manipulation scenarios, respectively, with 100% accuracy for deviations exceeding 0.8°C.

Conclusion: The proposed ML-based framework offers robust protection against intermittent thermal Trojans, demonstrating superior detection accuracy and resolution compared to traditional threshold techniques.

Abstract: Thermal Trojan attacks present a pressing concern for the security and
reliability of System-on-Chips (SoCs), especially in mobile applications. The
situation becomes more complicated when such attacks are more evasive and
operate sporadically to stay hidden from detection mechanisms. In this paper,
we introduce Intermittent Thermal Trojans (iThermTroj) that exploit the chips'
thermal information in a random time-triggered manner. According to our
experiments, iThermTroj attack can easily bypass available threshold-based
thermal Trojan detection solutions. We investigate SoC vulnerabilities to
variations of iThermTroj through an in-depth analysis of Trojan activation and
duration scenarios. We also propose a set of tiny Machine Learning classifiers
for run-time anomaly detection to protect SoCs against such intermittent
thermal Trojan attacks. Compared to existing methods, our approach improves the
attack detection rate by 29.4\%, 17.2\%, and 14.3\% in scenarios where
iThermTroj manipulates up to 80\%, 60\%, and 40\% of SoC's thermal data,
respectively. Additionally, our method increases the full protection resolution
to 0.8 degrees Celsius, meaning that any temperature manipulations exceeding
$\pm 0.8$ degrees will be detected with 100\% accuracy.

</details>


### [8] [DATABench: Evaluating Dataset Auditing in Deep Learning from an Adversarial Perspective](https://arxiv.org/abs/2507.05622)
*Shuo Shao,Yiming Li,Mengren Zheng,Zhiyang Hu,Yukun Chen,Boheng Li,Yu He,Junfeng Guo,Tianwei Zhang,Dacheng Tao,Zhan Qin*

Main category: cs.CR

TL;DR: This paper evaluates dataset auditing methods against adversarial attacks, proposes new attack strategies, and develops a benchmark (DATABench) showing existing methods' vulnerability to evasion and forgery attacks.


<details>
  <summary>Details</summary>
Motivation: Privacy and copyright concerns arise from undeclared training data usage, but current auditing techniques lack adversarial resilience, necessitating systematic evaluation of attack strategies.

Method: Introduced a taxonomy of auditing methods (internal vs. external features), formulated evasion and forgery attacks, and developed a benchmark with 17 evasion attacks, 5 forgery attacks, and 9 auditing techniques.

Result: Extensive experiments using DATABench revealed evaluated methods fail to achieve robustness/distinctiveness under adversarial scenarios. Code repository: https://github.com/shaoshuo-ss/DATABench.

Conclusion: Existing dataset auditing methods are insufficiently secure; there is an urgent need to develop techniques that withstand sophisticated adversarial manipulations.

Abstract: The widespread application of Deep Learning across diverse domains hinges
critically on the quality and composition of training datasets. However, the
common lack of disclosure regarding their usage raises significant privacy and
copyright concerns. Dataset auditing techniques, which aim to determine if a
specific dataset was used to train a given suspicious model, provide promising
solutions to addressing these transparency gaps. While prior work has developed
various auditing methods, their resilience against dedicated adversarial
attacks remains largely unexplored. To bridge the gap, this paper initiates a
comprehensive study evaluating dataset auditing from an adversarial
perspective. We start with introducing a novel taxonomy, classifying existing
methods based on their reliance on internal features (IF) (inherent to the
data) versus external features (EF) (artificially introduced for auditing).
Subsequently, we formulate two primary attack types: evasion attacks, designed
to conceal the use of a dataset, and forgery attacks, intending to falsely
implicate an unused dataset. Building on the understanding of existing methods
and attack objectives, we further propose systematic attack strategies:
decoupling, removal, and detection for evasion; adversarial example-based
methods for forgery. These formulations and strategies lead to our new
benchmark, DATABench, comprising 17 evasion attacks, 5 forgery attacks, and 9
representative auditing methods. Extensive evaluations using DATABench reveal
that none of the evaluated auditing methods are sufficiently robust or
distinctive under adversarial settings. These findings underscore the urgent
need for developing a more secure and reliable dataset auditing method capable
of withstanding sophisticated adversarial manipulation. Code is available at
https://github.com/shaoshuo-ss/DATABench.

</details>


### [9] [How Not to Detect Prompt Injections with an LLM](https://arxiv.org/abs/2507.05630)
*Sarthak Choudhary,Divyam Anshumaan,Nils Palumbo,Somesh Jha*

Main category: cs.CR

TL;DR: The paper demonstrates a critical design flaw in KAD defenses for LLM applications, enabling the DataFlip attack to bypass them with high success rates (88%) and low detection rates (1.5%), independent of LLM optimization or access.


<details>
  <summary>Details</summary>
Motivation: Prompt injection attacks threaten LLM systems by inserting malicious instructions into inputs. While KAD-based defenses show high performance, this work reveals their structural vulnerability undermines their security claims.

Method: Formal characterization of the KAD framework to identify its core vulnerability, then developing DataFlip through systematic analysis of instruction-prompt behavior. The attack exploits this weakness without optimization/white-box access.

Result: DataFlip consistently evades KAD defenses (1.5% detection rate) while achieving 88% success in inducing malicious behavior, disproving KAD's assumed security robustness.

Conclusion: Current KAD frameworks are fundamentally insecure due to design limitations. Defenses against prompt injection need to address deeper structural weaknesses and move beyond KAD's approach.

Abstract: LLM-integrated applications and agents are vulnerable to prompt injection
attacks, in which adversaries embed malicious instructions within seemingly
benign user inputs to manipulate the LLM's intended behavior. Recent defenses
based on $\textit{known-answer detection}$ (KAD) have achieved near-perfect
performance by using an LLM to classify inputs as clean or contaminated. In
this work, we formally characterize the KAD framework and uncover a structural
vulnerability in its design that invalidates its core security premise. We
design a methodical adaptive attack, $\textit{DataFlip}$, to exploit this
fundamental weakness. It consistently evades KAD defenses with detection rates
as low as $1.5\%$ while reliably inducing malicious behavior with success rates
of up to $88\%$, without needing white-box access to the LLM or any
optimization procedures.

</details>


### [10] [DESIGN: Encrypted GNN Inference via Server-Side Input Graph Pruning](https://arxiv.org/abs/2507.05649)
*Kaixiang Zhao,Joseph Yousry Attalla,Qian Lou,Yushun Dong*

Main category: cs.CR

TL;DR: DESIGN is a novel framework for efficient encrypted GNN inference under FHE by implementing hierarchical optimization strategies like encrypted node scoring and adaptive activation schemes, significantly accelerating processing while preserving model accuracy.


<details>
  <summary>Details</summary>
Motivation: Privacy-preserving GNN inference via FHE suffers from impractical computational overhead due to redundant input data and uniform computation strategies, limiting real-time applications despite GNNs' state-of-the-art performance in graph tasks.

Method: DESIGN introduces server-side hierarchical optimizations: 1) Calculating FHE-compatible encrypted node importance scores using degree statistics from the encrypted graph. 2) Applying homomorphic partitioning to generate multi-level masks that enable input graph pruning and adaptive polynomial activation, where activation complexity varies based on node importance levels.

Result: Empirical evaluations demonstrate DESIGN accelerates FHE GNN inference substantially compared to existing methods while maintaining competitive model accuracy across graph analytics tasks.

Conclusion: DESIGN presents a robust solution for real-time encrypted GNN inference through data-driven optimizations (pruning and adaptive activation) under FHE, addressing efficiency bottlenecks in privacy-preserving graph analytics and enabling practical deployment.

Abstract: Graph Neural Networks (GNNs) have achieved state-of-the-art performance in
various graph-based learning tasks. However, enabling privacy-preserving GNNs
in encrypted domains, such as under Fully Homomorphic Encryption (FHE),
typically incurs substantial computational overhead, rendering real-time and
privacy-preserving inference impractical. In this work, we propose DESIGN
(EncrypteD GNN Inference via sErver-Side Input Graph pruNing), a novel
framework for efficient encrypted GNN inference. DESIGN tackles the critical
efficiency limitations of existing FHE GNN approaches, which often overlook
input data redundancy and apply uniform computational strategies. Our framework
achieves significant performance gains through a hierarchical optimization
strategy executed entirely on the server: first, FHE-compatible node importance
scores (based on encrypted degree statistics) are computed from the encrypted
graph. These scores then guide a homomorphic partitioning process, generating
multi-level importance masks directly under FHE. This dynamically generated
mask facilitates both input graph pruning (by logically removing unimportant
elements) and a novel adaptive polynomial activation scheme, where activation
complexity is tailored to node importance levels. Empirical evaluations
demonstrate that DESIGN substantially accelerates FHE GNN inference compared to
state-of-the-art methods while maintaining competitive model accuracy,
presenting a robust solution for secure graph analytics.

</details>


### [11] [TuneShield: Mitigating Toxicity in Conversational AI while Fine-tuning on Untrusted Data](https://arxiv.org/abs/2507.05660)
*Aravind Cheruvu,Shravya Kanchi,Sifat Muhammad Abdullah,Nicholas Kong,Daphne Yao,Murtuza Jadliwala,Bimal Viswanath*

Main category: cs.CR

TL;DR: TuneShield introduces a defense framework for mitigating toxicity during chatbot fine-tuning while preserving conversational quality through LLM-based classification and synthetic 'healing data'.


<details>
  <summary>Details</summary>
Motivation: Customizing large language models (LLMs) for conversational AI with untrusted training data presents significant challenges in preventing toxicity amplification.

Method: The framework combines LLM-based toxicity sample identification with synthetic data generation (healing data) and alignment processes to reinforce desired behaviors during fine-tuning.

Result: TuneShield successfully mitigates toxicity injection attacks with preserved conversational quality, demonstrates resilience against adaptive adversarial and jailbreak attacks, and effectively handles challenges in dialog-based learning with imperfect classifier performance.

Conclusion: TuneShield provides a robust solution for defending against various toxicity threats during chatbot training without compromising conversational quality, showing promise for practical deployment in real-world conversational AI systems.

Abstract: Recent advances in foundation models, such as LLMs, have revolutionized
conversational AI. Chatbots are increasingly being developed by customizing
LLMs on specific conversational datasets. However, mitigating toxicity during
this customization, especially when dealing with untrusted training data,
remains a significant challenge. To address this, we introduce TuneShield, a
defense framework designed to mitigate toxicity during chatbot fine-tuning
while preserving conversational quality. TuneShield leverages LLM-based
toxicity classification, utilizing the instruction-following capabilities and
safety alignment of LLMs to effectively identify toxic samples, outperforming
industry API services. TuneShield generates synthetic conversation samples,
termed 'healing data', based on the identified toxic samples, using them to
mitigate toxicity while reinforcing desirable behavior during fine-tuning. It
performs an alignment process to further nudge the chatbot towards producing
desired responses. Our findings show that TuneShield effectively mitigates
toxicity injection attacks while preserving conversational quality, even when
the toxicity classifiers are imperfect or biased. TuneShield proves to be
resilient against adaptive adversarial and jailbreak attacks. Additionally,
TuneShield demonstrates effectiveness in mitigating adaptive toxicity injection
attacks during dialog-based learning (DBL).

</details>


### [12] [Polyadic encryption](https://arxiv.org/abs/2507.05683)
*Steven Duplij,Qiang Guo*

Main category: cs.CR

TL;DR: Proposes a novel encryption/decryption method using polyadic algebraic structures and signal processing with integer amplitude signals.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop a secure and novel cryptographic procedure leveraging the mathematical properties of polyadic structures and signal processing techniques to enhance information transmission security.

Method: The method involves converting plaintext into special integers using polyadic algebraic structures, transmitting information via signals with integer amplitudes, and reconstructing plaintext at the receiver through specialized rules and systems of equations.

Result: Results demonstrate the feasibility of this approach, though specific metrics are not provided in the abstract.

Conclusion: This polyadic-signal-based encryption method presents a innovative framework for secure communication, combining algebraic systems and signal processing for potential improved security and efficiency.

Abstract: A novel original procedure of encryption/decryption based on the polyadic
algebraic structures and on signal processing methods is proposed. First, we
use signals with integer amplitudes to send information. Then we use polyadic
techniques to transfer the plaintext into series of special integers. The
receiver restores the plaintext using special rules and systems of equations.

</details>


### [13] [Asynchronous Event Error-Minimizing Noise for Safeguarding Event Dataset](https://arxiv.org/abs/2507.05728)
*Ruofei Wang,Peiqi Duan,Boxin Shi,Renjie Wan*

Main category: cs.CR

TL;DR: The paper introduces UEvs, a novel method to generate unlearnable event streams with noise to prevent unauthorized exploitation while maintaining data utility.


<details>
  <summary>Details</summary>
Motivation: As event datasets become widely available online, data owners face growing concerns about their unauthorized use. Existing 'unlearnable examples' for image data cannot address the unique asynchronous nature of event streams.

Method: The approach uses asynchronous event error-minimizing noise to perturb streams, combined with a projection strategy to maintain sparsity and create unlearnable event streams (UEvs).

Result: Experiments demonstrate that UEvs effectively prevents unauthorized model training by embedding noise while preserving usefulness for legitimate users.

Conclusion: The proposed UEvs framework advances secure event dataset sharing by combining data protection with functional usability. (Code available at https://github.com/rfww/uevs)  

Abstract: With more event datasets being released online, safeguarding the event
dataset against unauthorized usage has become a serious concern for data
owners. Unlearnable Examples are proposed to prevent the unauthorized
exploitation of image datasets. However, it's unclear how to create unlearnable
asynchronous event streams to prevent event misuse. In this work, we propose
the first unlearnable event stream generation method to prevent unauthorized
training from event datasets. A new form of asynchronous event error-minimizing
noise is proposed to perturb event streams, tricking the unauthorized model
into learning embedded noise instead of realistic features. To be compatible
with the sparse event, a projection strategy is presented to sparsify the noise
to render our unlearnable event streams (UEvs). Extensive experiments
demonstrate that our method effectively protects event data from unauthorized
exploitation, while preserving their utility for legitimate use. We hope our
UEvs contribute to the advancement of secure and trustworthy event dataset
sharing. Code is available at: https://github.com/rfww/uevs.

</details>


### [14] [Automated Reasoning for Vulnerability Management by Design](https://arxiv.org/abs/2507.05794)
*Avi Shaked,Nan Messe*

Main category: cs.CR

TL;DR: This paper proposes a formally grounded automated reasoning mechanism integrated into a security design tool to systematically manage vulnerabilities through proactive security controls.


<details>
  <summary>Details</summary>
Motivation: Current vulnerability management lacks systematic reasoning to effectively address system vulnerabilities and design appropriate security controls.

Method: The authors developed an automated reasoning mechanism based on formal methods and integrated it into an open-source tool, allowing designers to identify vulnerabilities, specify mitigation options, and declare selected controls.

Result: Demonstration of the tool's application via a real-world example, showing its ability to identify applicable vulnerabilities and manage mitigation strategies.

Conclusion: The automated mechanism provides a structured approach for managing vulnerability postures, enabling explicit control selection and systematic vulnerability mitigation in system designs.

Abstract: For securing systems, it is essential to manage their vulnerability posture
and design appropriate security controls. Vulnerability management allows to
proactively address vulnerabilities by incorporating pertinent security
controls into systems designs. Current vulnerability management approaches do
not support systematic reasoning about the vulnerability postures of systems
designs. To effectively manage vulnerabilities and design security controls, we
propose a formally grounded automated reasoning mechanism. We integrate the
mechanism into an open-source security design tool and demonstrate its
application through an illustrative example driven by real-world challenges.
The automated reasoning mechanism allows system designers to identify
vulnerabilities that are applicable to a specific system design, explicitly
specify vulnerability mitigation options, declare selected controls, and thus
systematically manage vulnerability postures.

</details>


### [15] [LDP$^3$: An Extensible and Multi-Threaded Toolkit for Local Differential Privacy Protocols and Post-Processing Methods](https://arxiv.org/abs/2507.05872)
*Berkay Kemal Balioglu,Alireza Khodaie,Mehmet Emre Gursoy*

Main category: cs.CR

TL;DR: LDP$^3$ is an open-source, extensible, and multi-threaded toolkit for optimizing local differential privacy protocols and post-processing methods, demonstrated to improve utility and efficiency in evaluations.


<details>
  <summary>Details</summary>
Motivation: Selecting optimal LDP protocol-post-processing combinations under varying privacy budgets and datasets is challenging, while the absence of a comprehensive benchmarking toolkit hinders evaluation of new methods.

Method: The paper develops LDP$^3$, a modular toolkit containing implementations of multiple LDP protocols, post-processing methods, and utility metrics, with a multi-threaded design enabling parallel execution for efficiency.

Result: Experiments show (i) protocol selection via LDP$^3$ substantially improves utility compared to suboptimal/random choices, and (ii) the multi-threaded architecture provides significant efficiency gains.

Conclusion: LDP$^3$ addresses critical gaps in LDP research by offering an extensible framework for protocol evaluation and providing efficiency through parallelization, enabling better privacy-utility tradeoffs.

Abstract: Local differential privacy (LDP) has become a prominent notion for
privacy-preserving data collection. While numerous LDP protocols and
post-processing (PP) methods have been developed, selecting an optimal
combination under different privacy budgets and datasets remains a challenge.
Moreover, the lack of a comprehensive and extensible LDP benchmarking toolkit
raises difficulties in evaluating new protocols and PP methods. To address
these concerns, this paper presents LDP$^3$ (pronounced LDP-Cube), an
open-source, extensible, and multi-threaded toolkit for LDP researchers and
practitioners. LDP$^3$ contains implementations of several LDP protocols, PP
methods, and utility metrics in a modular and extensible design. Its modular
design enables developers to conveniently integrate new protocols and PP
methods. Furthermore, its multi-threaded nature enables significant reductions
in execution times via parallelization. Experimental evaluations demonstrate
that: (i) using LDP$^3$ to select a good protocol and post-processing method
substantially improves utility compared to a bad or random choice, and (ii) the
multi-threaded design of LDP$^3$ brings substantial benefits in terms of
efficiency.

</details>


### [16] [Post-Processing in Local Differential Privacy: An Extensive Evaluation and Benchmark Platform](https://arxiv.org/abs/2507.05875)
*Alireza Khodaie,Berkay Kemal Balioglu,Mehmet Emre Gursoy*

Main category: cs.CR

TL;DR: This paper provides a comprehensive benchmark comparing post-processing methods for local differential privacy under varying privacy budgets, LDP protocols, and data characteristics, introducing LDP$^3$ as an open-source platform.


<details>
  <summary>Details</summary>
Motivation: Local differential privacy (LDP) protocols inherently reduce data utility through perturbation, and existing post-processing methods lack systematic evaluation across diverse scenarios to identify optimal approaches.

Method: We conducted empirical analysis using 6 LDP protocols, 7 post-processing methods, 4 utility metrics, and 6 datasets across varying privacy budgets and data characteristics to evaluate performance trade-offs.

Result: Post-processing significantly improves utility under strict privacy budgets (ε < 1) but provides diminishing returns as ε increases. The optimal method depends on LDP protocol choice, domain size, distribution, and selected utility metric.

Conclusion: Practitioners must consider multiple factors when selecting post-processing methods for LDP. LDP$^3$ offers a modular, extensible platform to facilitate future research and method comparisons.

Abstract: Local differential privacy (LDP) has recently gained prominence as a powerful
paradigm for collecting and analyzing sensitive data from users' devices.
However, the inherent perturbation added by LDP protocols reduces the utility
of the collected data. To mitigate this issue, several post-processing (PP)
methods have been developed. Yet, the comparative performance of PP methods
under diverse settings remains underexplored. In this paper, we present an
extensive benchmark comprising 6 popular LDP protocols, 7 PP methods, 4 utility
metrics, and 6 datasets to evaluate the behaviors and optimality of PP methods
under diverse conditions. Through extensive experiments, we show that while PP
can substantially improve utility when the privacy budget is small (i.e.,
strict privacy), its benefit diminishes as the privacy budget grows. Moreover,
our findings reveal that the optimal PP method depends on multiple factors,
including the choice of LDP protocol, privacy budget, data characteristics
(such as distribution and domain size), and the specific utility metric. To
advance research in this area and assist practitioners in identifying the most
suitable PP method for their setting, we introduce LDP$^3$, an open-source
benchmark platform. LDP$^3$ contains all methods used in our experimental
analysis, and it is designed in a modular, extensible, and multi-threaded way
for future use and development.

</details>


### [17] [The Impact of Event Data Partitioning on Privacy-aware Process Discovery](https://arxiv.org/abs/2507.06008)
*Jungeun Lim,Stephan A. Fahrenkrog-Petersen,Xixi Lu,Jan Mendling,Minseok Song*

Main category: cs.CR

TL;DR: This paper proposes a pipeline combining anonymization and event log partitioning via event abstraction to maintain privacy while reducing utility loss in process discovery.


<details>
  <summary>Details</summary>
Motivation: Anonymizing complex event logs for privacy often leads to high utility loss for process discovery, requiring a trade-off between privacy and usability.

Method: The approach partitions event logs into segments using event abstraction, allowing separate anonymization of sub-logs through a two-phase pipeline. The method is evaluated using three real-world event logs with two anonymization techniques and two process discovery frameworks.

Result: Experiments show that event partitioning improves utility in directly-follows-based anonymization techniques while preserving privacy, with performance validated across multiple real-world logs and discovery methods.

Conclusion: The pipeline effectively mitigates utility loss in anonymized event logs by leveraging abstraction-based partitioning, offering a practical solution for privacy-critical process mining scenarios.

Abstract: Information systems support the execution of business processes. The event
logs of these executions generally contain sensitive information about
customers, patients, and employees. The corresponding privacy challenges can be
addressed by anonymizing the event logs while still retaining utility for
process discovery. However, trading off utility and privacy is difficult: the
higher the complexity of event log, the higher the loss of utility by
anonymization. In this work, we propose a pipeline that combines anonymization
and event data partitioning, where event abstraction is utilized for
partitioning. By leveraging event abstraction, event logs can be segmented into
multiple parts, allowing each sub-log to be anonymized separately. This
pipeline preserves privacy while mitigating the loss of utility. To validate
our approach, we study the impact of event partitioning on two anonymization
techniques using three real-world event logs and two process discovery
techniques. Our results demonstrate that event partitioning can bring
improvements in process discovery utility for directly-follows-based
anonymization techniques.

</details>


### [18] [Enter, Exit, Page Fault, Leak: Testing Isolation Boundaries for Microarchitectural Leaks](https://arxiv.org/abs/2507.06039)
*Oleksii Oleksenko,Flavien Solt,Cédric Fournet,Jana Hofmann,Boris Köpf,Stavros Volos*

Main category: cs.CR

TL;DR: This paper introduces a tool stress-testing microarchitectural isolation boundaries in CPUs, shifting from reactive patching to proactive security validation via model-based relational testing (MRT) methodology.


<details>
  <summary>Details</summary>
Motivation: Current CPU isolation mechanisms (e.g., virtualization, privilege levels) overlook microarchitectural side channels like Meltdown/Foreshadow, forcing reactive, error-prone software patches that fail to address emerging attacks.

Method: The authors developed a tool extending MRT methodology with: (1) A test case generator and execution sandbox for multi-domain execution, (2) leakage models to encode expected leaks, and (3) analysis techniques to manage nondeterminism during stress-testing.

Result: Testing six x86-64 CPUs revealed four new microarchitectural leaks and validated numerous known leaks, demonstrating significant gaps in isolation mechanisms with only two false positives in the entire campaign.

Conclusion: This approach enables robust proactive validation of processor security, addressing flaws in isolation boundaries and reducing reliance on reactive mitigation strategies.

Abstract: CPUs provide isolation mechanisms like virtualization and privilege levels to
protect software. Yet these focus on architectural isolation while typically
overlooking microarchitectural side channels, exemplified by Meltdown and
Foreshadow. Software must therefore supplement architectural defenses with
ad-hoc microarchitectural patches, which are constantly evolving as new attacks
emerge and defenses are proposed. Such reactive approach makes ensuring
complete isolation a daunting task, and leaves room for errors and oversights.
  We address this problem by developing a tool that stress tests
microarchitectural isolation between security domains such as virtual machines,
kernel, and processes, with the goal of detecting flaws in the isolation
boundaries. The tool extends model-based relational testing (MRT) methodology
to enable detection of cross-domain information leakage. We design a new test
case generator and execution sandbox to handle multi-domain execution, new
leakage models to encode expected leaks, and new analysis techniques to manage
nondeterminism.
  We use this tool to perform an in-depth testing campaign on six x86-64 CPUs
for leakage across different isolation boundaries. The testing campaign exposed
four new leaks and corroborated numerous known ones, with only two false
positives throughout the entire campaign. These results show critical gaps in
current isolation mechanisms as well as validate a robust methodology for
detecting microarchitectural flaws. As such, this approach enables a shift from
reactive patching to proactive security validation in processor design.

</details>


### [19] [CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative Adversarial Attacks on their Internal Representations](https://arxiv.org/abs/2507.06043)
*Xiaohu Li,Yunfeng Ning,Zepeng Bao,Mayi Xu,Jianhao Chen,Tieyun Qian*

Main category: cs.CR

TL;DR: This paper proposes CAVGAN, a framework using linear separability of LLM embeddings and generative adversarial networks to enable both jailbreak attacks and defenses. It achieves 88.85% average success in attacks and 84.17% in defenses.


<details>
  <summary>Details</summary>
Motivation: Existing LLM security mechanisms are vulnerable to jailbreak attacks, and prior research has isolated attack/defense strategies. The authors aim to better understand and strengthen LLM security by integrating both perspectives through boundary analysis.

Method: Leverages GANs to learn LLM's internal security judgment boundary via linear separable properties of intermediate layer embeddings. Transforms harmful queries into safe representations while identifying vulnerabilities for attack generation.

Result: 88.85% jailbreak success on three LLMs, 84.17% defense success on state-of-the-art dataset. Datasets and code publicly available at CAVGAN GitHub repository (https://github.com/NLPGM/CAVGAN).

Conclusion: Demonstrates effectiveness of boundary analysis approach to uncover internal security mechanisms of LLMs. Provides practical method for simultaneous attack-defense strategy that offers insights for enhanced model security.

Abstract: Security alignment enables the Large Language Model (LLM) to gain the
protection against malicious queries, but various jailbreak attack methods
reveal the vulnerability of this security mechanism. Previous studies have
isolated LLM jailbreak attacks and defenses. We analyze the security protection
mechanism of the LLM, and propose a framework that combines attack and defense.
Our method is based on the linearly separable property of LLM intermediate
layer embedding, as well as the essence of jailbreak attack, which aims to
embed harmful problems and transfer them to the safe area. We utilize
generative adversarial network (GAN) to learn the security judgment boundary
inside the LLM to achieve efficient jailbreak attack and defense. The
experimental results indicate that our method achieves an average jailbreak
success rate of 88.85\% across three popular LLMs, while the defense success
rate on the state-of-the-art jailbreak dataset reaches an average of 84.17\%.
This not only validates the effectiveness of our approach but also sheds light
on the internal security mechanisms of LLMs, offering new insights for
enhancing model security The code and data are available at
https://github.com/NLPGM/CAVGAN.

</details>


### [20] [Wrapless: The trustless lending protocol on top of Bitcoin](https://arxiv.org/abs/2507.06064)
*Oleksandr Kurbatov,Kyrylo Baybula,Yaroslava Chopa,Sergey Kozlov,Oleg Komendant,Illia Dovgopoly,Dmitrii Kurbatov,Zakhar Naumets,Yulia Artikulova,Pavel Kravchenko,Volodymyr Dubinin,Lasha Antadze,Yaroslav Panasenko,Mykhailo Velykodnyi*

Main category: cs.CR

TL;DR: Wrapless is a trustless Bitcoin collateralization protocol enabling cross-chain loans without wrapping mechanisms, secured via economically rational incentives.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of collateralizing Bitcoin on cross-chain lending platforms without relying on centralized or trusted wrapping solutions.

Method: Implements 'loan channels' on the Bitcoin blockchain with game-theoretic incentives, making manipulation economically irrational for all parties.

Result: Enables secure, trustless Bitcoin collateralization for loans on any Turing-complete smart contract blockchain, with a focus on economic security.

Conclusion: Wrapless presents a promising approach to decentralized lending but requires further research to align more closely with traditional AMM financial models.

Abstract: This paper presents Wrapless -- a lending protocol that enables the
collateralization of bitcoins without requiring a trusted wrapping mechanism.
The protocol facilitates a "loan channel" on the Bitcoin blockchain, allowing
bitcoins to be locked as collateral for loans issued on any blockchain that
supports Turing-complete smart contracts. The protocol is designed in a way
that makes it economically irrational for each involved party to manipulate the
loan rules. There is still a significant research area to bring the protocol
closer to traditional AMM financial instruments.

</details>


### [21] [Taming Data Challenges in ML-based Security Tasks: Lessons from Integrating Generative AI](https://arxiv.org/abs/2507.06092)
*Shravya Kanchi,Neal Mangaokar,Aravind Cheruvu,Sifat Muhammad Abdullah,Shirin Nilizadeh,Atul Prakash,Bimal Viswanath*

Main category: cs.CR

TL;DR: This paper investigates the potential of Generative AI to enhance security classifiers by addressing data challenges, finding significant improvements with proposed synthetic data methods.


<details>
  <summary>Details</summary>
Motivation: Data challenges, such as limited samples and task-specific issues, negatively impact security classifier performance, but have received less attention compared to algorithmic advancements.

Method: The study augments training data with synthetic samples generated via six state-of-the-art Generative AI methods and introduces a novel controlled-synthesis scheme (Nimai). It evaluates these approaches across seven security tasks with severe data constraints.

Result: GenAI-based augmentation improves classifier performance by up to 32.6% with minimal training samples (~180), while adaptive methods achieve rapid updates to concept drift scenarios requiring limited labeling. However, some GenAI schemes struggle with initialization, and task characteristics like noisy labels and sparse features hinder effectiveness.

Conclusion: The work demonstrates that Generative AI can address critical data challenges in security classification but requires development to handle task-specific limitations, driving future research in this area.

Abstract: Machine learning-based supervised classifiers are widely used for security
tasks, and their improvement has been largely focused on algorithmic
advancements. We argue that data challenges that negatively impact the
performance of these classifiers have received limited attention. We address
the following research question: Can developments in Generative AI (GenAI)
address these data challenges and improve classifier performance? We propose
augmenting training datasets with synthetic data generated using GenAI
techniques to improve classifier generalization. We evaluate this approach
across 7 diverse security tasks using 6 state-of-the-art GenAI methods and
introduce a novel GenAI scheme called Nimai that enables highly controlled data
synthesis. We find that GenAI techniques can significantly improve the
performance of security classifiers, achieving improvements of up to 32.6% even
in severely data-constrained settings (only ~180 training samples).
Furthermore, we demonstrate that GenAI can facilitate rapid adaptation to
concept drift post-deployment, requiring minimal labeling in the adjustment
process. Despite successes, our study finds that some GenAI schemes struggle to
initialize (train and produce data) on certain security tasks. We also identify
characteristics of specific tasks, such as noisy labels, overlapping class
distributions, and sparse feature vectors, which hinder performance boost using
GenAI. We believe that our study will drive the development of future GenAI
tools designed for security tasks.

</details>


### [22] [Fun with flags: How Compilers Break and Fix Constant-Time Code](https://arxiv.org/abs/2507.06112)
*Antoine Geimer,Clementine Maurice*

Main category: cs.CR

TL;DR: The paper investigates how compiler optimizations break constant-time code, identifies key optimization passes in GCC and LLVM responsible, and proposes mitigating timing leaks via compiler flags without code changes or custom compilers.


<details>
  <summary>Details</summary>
Motivation: Developers struggle to prevent timing side-channel attacks due to compiler optimizations silently reintroducing leaks, while prior work lacks actionable guidance on which passes to disable.

Method: Qualitative analysis of compiler-introduced violations, constructing a dataset, analyzing GCC/LLVM internals, and characterizing pass interactions causing leakage.

Result: Disabling specific optimization passes via flags significantly reduces timing leaks with minimal performance overhead, offering an immediately deployable solution.

Conclusion: This work provides actionable insights for developers to mitigate timing leaks by configuring existing compilers with targeted flags, addressing a critical gap in compiler-secure programming.

Abstract: Developers rely on constant-time programming to prevent timing side-channel
attacks. But these efforts can be undone by compilers, whose optimizations may
silently reintroduce leaks. While recent works have measured the extent of such
leakage, they leave developers without actionable insights: which optimization
passes are responsible, and how to disable them without modifying the compiler
remains unclear.
  In this paper, we conduct a qualitative analysis of how compiler
optimizations break constant-time code. We construct a dataset of
compiler-introduced constant-time violations and analyze the internals of two
widely used compilers, GCC and LLVM, to identify the specific optimization
passes responsible. Our key insight is that a small set of passes are at the
root of most leaks. To the best of our knowledge, we are also the first to
characterize how the interactions between these passes contribute to leakage.
Based on this analysis, we propose an original and practical mitigation that
requires no source code modification or custom compiler: disabling selected
optimization passes via compiler flags. We show that this approach
significantly reduces leakage with minimal performance overhead, offering an
immediately deployable defense for developers.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [23] [CORE: Benchmarking LLMs Code Reasoning Capabilities through Static Analysis Tasks](https://arxiv.org/abs/2507.05269)
*Danning Xie,Mingwei Zheng,Xuwei Liu,Jiannan Wang,Chengpeng Wang,Lin Tan,Xiangyu Zhang*

Main category: cs.SE

TL;DR: The paper introduces CoRe, a human-verified benchmark for evaluating LLMs' abilities in fundamental static analysis tasks (dependencies, control flow, information flow) across C/C++, Java, and Python, revealing limitations in semantic reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on surface-level code patterns and end-to-end outcomes (e.g., code repair accuracy) but lack evaluation of LLMs' deeper semantic understanding required for program analysis like data/control dependencies and multi-step reasoning.

Method: Created CoRe benchmark with 12,553 instances across three languages, using a semantics-aware diverse sampling strategy that prioritizes structural coverage and dependency depth. Evaluated 10 mainstream LLMs on static analysis tasks.

Result: LLMs perform well in simple dependency identification but struggle with complex control structures (e.g., nested loops/conditionals) and backward dependencies. Qualitative analysis highlights these challenges in multi-step semantic reasoning.

Conclusion: CoRe exposes shortcomings in LLMs' program semantic reasoning capabilities, especially for structured program analysis tasks. Findings suggest pathways for improving models to better handle static analysis requirements in software engineering.

Abstract: Large language models (LLMs) have been widely adopted across diverse software
engineering domains, such as code generation, program repair, and vulnerability
detection. These applications require understanding beyond surface-level code
patterns: value propagation, control flow, and interdependence between program
elements. However, existing benchmarks primarily evaluate end-to-end outcomes,
such as whether code is correctly repaired or generated, leaving the models
ability for program semantic reasoning underexplored. This work presents CoRe,
a high-quality, human-verified benchmark designed to evaluate LLMs on
fundamental static analysis tasks. CoRe includes 12,553 task instances spanning
data dependency, control dependency, and information flow across programs
written in C/C++, Java, and Python. To ensure semantic diversity and reasoning
complexity, we propose a semantics-aware diverse sampling strategy that selects
targets and task instances based on structural coverage and dependency depth.
We evaluate 10 mainstream LLMs and show that, while they perform well at
identifying dependencies, models still struggle with tasks that require deeper
semantic understanding and multi-step reasoning. We further conduct qualitative
analyses to uncover key challenges, such as complex control structures and
backward dependency patterns, offering insights into improving LLMs code
reasoning capabilities.

</details>


### [24] [Open Source, Hidden Costs: A Systematic Literature Review on OSS License Management](https://arxiv.org/abs/2507.05270)
*Boyuan Li,Chengwei Liu,Lingling Fan,Sen Chen,Zhenlin Zhang,Zheli Liu*

Main category: cs.SE

TL;DR: The paper analyzes 80 OSS license-related studies, identifies challenges, and proposes research directions for managing software licensing risks amidst evolving open-source ecosystems and generative software engineering techniques.


<details>
  <summary>Details</summary>
Motivation: risks in software licensing pose legal/operational challenges; need to address limitations in current solutions and bridge academia-industry gaps as OSS licenses and generative software engineering (e.g., CodeLLMs) evolve

Method: systematic literature review (SLR) of 80 selected OSS license-related papers, categorized into license identification, risk assessment, and risk mitigation

Result: systematically mapped existing research, identified challenges in license identification/assessment/mitigation, and uncovered limitations in current tools and approaches

Conclusion: highlights research opportunities and practical recommendations for improved licensing governance; emphasizes need for ecosystem-wide collaboration between academia and industry

Abstract: Integrating third-party software components is a common practice in modern
software development, offering significant advantages in terms of efficiency
and innovation. However, this practice is fraught with risks related to
software licensing. A lack of understanding may lead to disputes, which can
pose serious legal and operational challenges. To these ends, both academia and
industry have conducted various investigations and proposed solutions and tools
to deal with these challenges. However, significant limitations still remain.
Moreover, the rapid evolution of open-source software (OSS) licenses, as well
as the rapidly incorporated generative software engineering techniques, such as
large language models for code (CodeLLMs), are placing greater demands on the
systematic management of software license risks. To unveil the severe
challenges and explore possible future directions, we conduct the first
systematic literature review (SLR) on 80 carefully selected OSS license-related
papers, classifying existing research into three key categories, i.e., license
identification, license risk assessment, and license risk mitigation. Based on
these, we discuss challenges in existing solutions, conclude the opportunities
to shed light on future research directions and offer practical recommendations
for practitioners. We hope this thorough review will help bridge the gaps
between academia and industry and accelerate the ecosystem-wide governance of
legitimate software risks within the software engineering community.

</details>


### [25] [FuzzFeed: An Automatic Approach to Weakest Precondition Generation using LLMs and Fuzzing](https://arxiv.org/abs/2507.05272)
*Daragh King,Vasileios Koutavas,Laura Kovacs*

Main category: cs.SE

TL;DR: The paper introduces Fuzzing Guidance (FG), which combines Large Language Models (LLMs) with fuzz testing to generate and refine correct weakest preconditions (WPs) for deterministic Java programs, enhancing their practical viability.


<details>
  <summary>Details</summary>
Motivation: Generating accurate weakest preconditions is critical for program verification and error checking. LLMs lack execution context to validate WP correctness, while fuzz testing provides runtime feedback. The paper addresses how to effectively merge these techniques.

Method: FG integrates fuzz testing as a validator for LLM-generated WPs. Fuzz testing samples execution paths to approximate the validity and weakness of candidate WPs, which are then refined alongside LLM guidance through iterative feedback loops.

Result: Experiments on a benchmark of deterministic Java array programs show: (1) LLMs generate viable WP candidates, (2) FG dramatically improves their accuracy by leveraging execution validation data via fuzzing, and (3) the combination outperforms prior approaches in correctness and comprehensiveness.

Conclusion: The FG framework demonstrates that execution-aware feedback via fuzz testing can effectively enhance LLM-based WP generation for practical verification tasks. This hybrid approach opens new opportunities for integrating language models and coverage-guided testing in program reasoning tasks.

Abstract: The weakest precondition (WP) of a program describes the largest set of
initial states from which all terminating executions of the program satisfy a
given postcondition. The generation of WPs is an important task with practical
applications in areas ranging from verification to run-time error checking.
  This paper proposes the combination of Large Language Models (LLMs) and fuzz
testing for generating WPs. In pursuit of this goal, we introduce Fuzzing
Guidance (FG); FG acts as a means of directing LLMs towards correct WPs using
program execution feedback. FG utilises fuzz testing for approximately checking
the validity and weakness of candidate WPs, this information is then fed back
to the LLM as a means of context refinement.
  We demonstrate the effectiveness of our approach on a comprehensive benchmark
set of deterministic array programs in Java. Our experiments indicate that LLMs
are capable of producing viable candidate WPs, and that this ability can be
practically enhanced through FG.

</details>


### [26] [ReservoirChat: Interactive Documentation Enhanced with LLM and Knowledge Graph for ReservoirPy](https://arxiv.org/abs/2507.05279)
*Virgile Boraud,Yannis Bendi-Ouis,Paul Bernard,Xavier Hinaut*

Main category: cs.SE

TL;DR: A tool enhances LLMs with ReservoirPy library and Reservoir Computing knowledge via RAG and knowledge graphs for reduced hallucinations.


<details>
  <summary>Details</summary>
Motivation: Address LLM hallucinations and improve factual accuracy in coding tasks and domain-specific Reservoir Computing questions.

Method: Integration of Retrieval-Augmented Generation (RAG) and knowledge graphs into LLM architecture for external knowledge incorporation and interactive coding support.

Result: Outperforms proprietary models (ChatGPT-4o, NotebookLM) in coding tasks, achieves significant improvement over base Codestral-22B.

Conclusion: Specialized LLM with RAG and knowledge graphs provides reliable domain insights while maintaining competitive general knowledge performance.

Abstract: We introduce a tool designed to improve the capabilities of Large Language
Models (LLMs) in assisting with code development using the ReservoirPy library,
as well as in answering complex questions in the field of Reservoir Computing.
By incorporating external knowledge through Retrieval-Augmented Generation
(RAG) and knowledge graphs, our approach aims to reduce hallucinations and
increase the factual accuracy of generated responses. The system provides an
interactive experience similar to ChatGPT, tailored specifically for
ReservoirPy, enabling users to write, debug, and understand Python code while
accessing reliable domain-specific insights. In our evaluation, while
proprietary models such as ChatGPT-4o and NotebookLM performed slightly better
on general knowledge questions, our model outperformed them on coding tasks and
showed a significant improvement over its base model, Codestral-22B.

</details>


### [27] [CoreCodeBench: A Configurable Multi-Scenario Repository-Level Benchmark](https://arxiv.org/abs/2507.05281)
*Lingyue Fu,Hao Guan,Bolun Zhang,Haowei Yuan,Yaoming Zhu,Jun Xu,Zongyu Wang,Lin Qiu,Xunliang Cai,Xuezhi Cao,Weiwen Liu,Weinan Zhang,Yong Yu*

Main category: cs.SE

TL;DR: CoreCodeBench and CorePipe are introduced to address limitations in existing LLM benchmarks by covering real engineering scenarios with configurable multi-scenario tests.


<details>
  <summary>Details</summary>
Motivation: Current repository-level benchmarks for LLMs on engineering code lack diversity, controllability, and reliability, focusing on single scenarios like code generation or bug fixing.

Method: CorePipe converts repositories into test cases by generating atomic and composite questions (Development, BugFix, Test-Driven Development) with adjustable difficulty via hyperparameters.

Result: Experiments with 16 LLMs across diverse scenarios revealed varying capabilities and provided multi-dimensional insights into engineering code performance.

Conclusion: CoreCodeBench offers a comprehensive benchmark to evaluate LLMs in real-world engineering projects, enabling flexible and controllable assessment of their core code processing skills.

Abstract: As Large Language Models (LLMs) demonstrate increasingly sophisticated code
processing capabilities, evaluating their performance on engineering-level code
remains challenging. Existing repository-level benchmarks primarily focus on
single scenarios, such as code generation or bug fixing, without adequately
capturing the diversity and complexity of real-world software or project
engineering workflows. Furthermore, these benchmarks suffer from limited
controllability in question positioning and reliability issues in their
generated test cases. To address these limitations, we present CorePipe, a
fully automated pipeline that converts repositories into comprehensive test
cases, and introduce CoreCodeBench, a configurable multi-scenario
repository-level benchmark. To simulate real engineering scenarios, CorePipe
generates three types of atomic questions (Development, BugFix, and Test-Driven
Development) specifically targeting core code segments. These atomic questions
are further combined into three types of composite questions, with difficulty
levels flexibly adjusted through hyperparameter tuning. CoreCodeBench provides
a comprehensive and extensive repository-level benchmark to investigate the
applicability of LLMs in real-world engineering projects. Experiments with 16
LLMs across diverse scenarios reveal varying capabilities and offer
multi-dimensional insights into LLM performance in engineering contexts. The
code for CorePipe is available at
https://github.com/AGI-Eval-Official/CoreCodeBench, and the data for
CoreCodeBench can be accessed at
https://huggingface.co/collections/tubehhh/corecodebench-68256d2faabf4b1610a08caa.

</details>


### [28] [Measuring how changes in code readability attributes affect code quality evaluation by Large Language Models](https://arxiv.org/abs/2507.05289)
*Igor Regis da Silva Simoes,Elaine Venson*

Main category: cs.SE

TL;DR: This paper evaluates using Large Language Models (LLMs) to assess code readability quality in a standardized way, showing sensitivity to interventions and agreement with reference models.


<details>
  <summary>Details</summary>
Motivation: Measuring code readability is challenging due to subjective code reviews and limited static analysis tools focusing on non-semantic attributes.

Method: Conducted a quasi-experiment with nine LLMs, applying three interventions (comment removal, identifier obfuscation, code smell refactoring) and analyzing response variability through 10 batch analyses per LLM, compared to a reference model.

Result: LLMs demonstrated sensitivity to interventions, high agreement with reference model for original/refactored code, and semantic understanding not captured by traditional metrics. Response variability ranged from 9.37% to 14.58% without compromising statistical significance.

Conclusion: LLMs show potential for evaluating semantic code quality aspects like coherence between identifiers, comments, and documentation, offering standardized readability assessments beyond traditional tools.

Abstract: Code readability is one of the main aspects of code quality, influenced by
various properties like identifier names, comments, code structure, and
adherence to standards. However, measuring this attribute poses challenges in
both industry and academia. While static analysis tools assess attributes such
as code smells and comment percentage, code reviews introduce an element of
subjectivity. This paper explores using Large Language Models (LLMs) to
evaluate code quality attributes related to its readability in a standardized,
reproducible, and consistent manner. We conducted a quasi-experiment study to
measure the effects of code changes on Large Language Model (LLM)s
interpretation regarding its readability quality attribute. Nine LLMs were
tested, undergoing three interventions: removing comments, replacing identifier
names with obscure names, and refactoring to remove code smells. Each
intervention involved 10 batch analyses per LLM, collecting data on response
variability. We compared the results with a known reference model and tool. The
results showed that all LLMs were sensitive to the interventions, with
agreement with the reference classifier being high for the original and
refactored code scenarios. The LLMs demonstrated a strong semantic sensitivity
that the reference model did not fully capture. A thematic analysis of the LLMs
reasoning confirmed their evaluations directly reflected the nature of each
intervention. The models also exhibited response variability, with 9.37% to
14.58% of executions showing a standard deviation greater than zero, indicating
response oscillation, though this did not always compromise the statistical
significance of the results. LLMs demonstrated potential for evaluating
semantic quality aspects, such as coherence between identifier names, comments,
and documentation with code purpose.

</details>


### [29] [zkSDK: Streamlining zero-knowledge proof development through automated trace-driven ZK-backend selection](https://arxiv.org/abs/2507.05294)
*William Law*

Main category: cs.SE

TL;DR: zkSDK is a modular framework with a dynamic backend selector using program profiling and user-defined criteria to simplify Zero-Knowledge (ZK) app development and unify fragmented ZK tooling ecosystems.


<details>
  <summary>Details</summary>
Motivation: Current ZK tooling lacks portability and imposes a steep learning curve by requiring developers to fixate on a single ZK backend, fragmenting the development experience.

Method: Core components: (1) Presto (a Python-like language for program profiling and workload analysis); (2) Dynamic selection algorithm combining computational intensity metrics and user-defined constraints to choose optimal ZK backend.

Result: Real-world workload evaluation demonstrates effectiveness in selecting performance-optimal backends across supported ZK ecosystems while maintaining user-friendly development workflows.

Conclusion: zkSDK provides a seamless, backend-agnostic ZK development paradigm through modular design and intelligent proving backend selection, addressing existing fragmentation and complexity barriers.

Abstract: The rapid advancement of creating Zero-Knowledge (ZK) programs has led to the
development of numerous tools designed to support developers. Popular options
include being able to write in general-purpose programming languages like Rust
from Risc Zero. Other languages exist like Circom, Lib-snark, and Cairo.
However, developers entering the ZK space are faced with many different ZK
backends to choose from, leading to a steep learning curve and a fragmented
developer experience across different platforms. As a result, many developers
tend to select a single ZK backend and remain tied to it. This thesis
introduces zkSDK, a modular framework that streamlines ZK application
development by abstracting the backend complexities. At the core of zkSDK is
Presto, a custom Python-like programming language that enables the profiling
and analysis of a program to assess its computational workload intensity.
Combined with user-defined criteria, zkSDK employs a dynamic selection
algorithm to automatically choose the optimal ZK-proving backend. Through an
in-depth analysis and evaluation of real-world workloads, we demonstrate that
zkSDK effectively selects the best-suited backend from a set of supported ZK
backends, delivering a seamless and user-friendly development experience.

</details>


### [30] [ASSURE: Metamorphic Testing for AI-powered Browser Extensions](https://arxiv.org/abs/2507.05307)
*Xuanqi Gao,Juan Zhai,Shiqing Ma,Siyi Xie,Chao Shen*

Main category: cs.SE

TL;DR: This paper introduces ASSURE, a modular automated testing framework for AI-powered browser extensions that integrates large language models, addresses testing challenges through contextual validation, and improves efficiency with a 6.4x throughput gain over manual methods.


<details>
  <summary>Details</summary>
Motivation: Current browser extension and LLM testing approaches separately fail to address the unique challenges of AI-powered extensions—including non-deterministic behavior, context-sensitivity, and environment integration—resulting in a critical evaluation gap.

Method: ASSURE employs three components: (1) a plugin-based test case generation engine for scenario extension, (2) an automated execution framework to orchestrate interactions between web content, extensions, and LLMs, and (3) a validation pipeline that assesses behavioral consistency and security invariants rather than exact output matches.

Result: ASSURE detected 531 distinct issues (security vulnerabilities, metamorphic violations, content alignment problems) in six AI extensions, achieving 6.4x faster testing throughput than manual methods with critical issues identified within 12.4 minutes on average.

Conclusion: ASSURE bridges the testing gap for AI-powered browser extensions through a modular, context-aware framework, demonstrating significant efficiency gains and robust vulnerability detection capabilities suitable for integration into development pipelines.

Abstract: The integration of Large Language Models (LLMs) into browser extensions has
revolutionized web browsing, enabling sophisticated functionalities like
content summarization, intelligent translation, and context-aware writing
assistance. However, these AI-powered extensions introduce unprecedented
challenges in testing and reliability assurance. Traditional browser extension
testing approaches fail to address the non-deterministic behavior,
context-sensitivity, and complex web environment integration inherent to
LLM-powered extensions. Similarly, existing LLM testing methodologies operate
in isolation from browser-specific contexts, creating a critical gap in
effective evaluation frameworks. To bridge this gap, we present ASSURE, a
modular automated testing framework specifically designed for AI-powered
browser extensions. ASSURE comprises three principal components: (1) a modular
test case generation engine that supports plugin-based extension of testing
scenarios, (2) an automated execution framework that orchestrates the complex
interactions between web content, extension processing, and AI model behavior,
and (3) a configurable validation pipeline that systematically evaluates
behavioral consistency and security invariants rather than relying on exact
output matching. Our evaluation across six widely-used AI browser extensions
demonstrates ASSURE's effectiveness, identifying 531 distinct issues spanning
security vulnerabilities, metamorphic relation violations, and content
alignment problems. ASSURE achieves 6.4x improved testing throughput compared
to manual approaches, detecting critical security vulnerabilities within 12.4
minutes on average. This efficiency makes ASSURE practical for integration into
development pipelines, offering a comprehensive solution to the unique
challenges of testing AI-powered browser extensions.

</details>


### [31] [OASBuilder: Generating OpenAPI Specifications from Online API Documentation with Large Language Models](https://arxiv.org/abs/2507.05316)
*Koren Lazar,Matan Vetzler,Kiran Kate,Jason Tsay,David Boaz Himanshu Gupta,Avraham Shinnar,Rohith D Vallam,David Amid Esther Goldbraich,Guy Uziel,Jim Laredo,Ateret Anaby Tavor*

Main category: cs.SE

TL;DR: OASBuilder is a framework that automates conversion of unstructured HTML API documentation into standardized machine-readable OpenAPI specifications using a pipeline combining large language models and rule-based algorithms with domain knowledge.


<details>
  <summary>Details</summary>
Motivation: API documentation is often unstructured and requires significant manual effort to convert into structured formats, hindering automation tool integration. Existing solutions lack generalization across diverse APIs.

Method: OASBuilder leverages a pipeline integrating LLMs for content understanding and rule-based algorithms guided by domain knowledge of documentation webpage structures to systematically extract and format API information.

Result: Experiments show OASBuilder achieves valid OpenAPI specification generation across hundreds of APIs, conserving 85-95% of original documentation information. Enterprise deployment saved 10,000+ hours and enabled LLM integration with 400+ complex APIs.

Conclusion: OASBuilder bridges the API usability gap through domain-informed automation, reducing manual effort requirements and enabling large-scale standardization of API documentation for enterprise AI integration.

Abstract: AI agents and business automation tools interacting with external web
services require standardized, machine-readable information about their APIs in
the form of API specifications. However, the information about APIs available
online is often presented as unstructured, free-form HTML documentation,
requiring external users to spend significant time manually converting it into
a structured format. To address this, we introduce OASBuilder, a novel
framework that transforms long and diverse API documentation pages into
consistent, machine-readable API specifications. This is achieved through a
carefully crafted pipeline that integrates large language models and rule-based
algorithms which are guided by domain knowledge of the structure of
documentation webpages. Our experiments demonstrate that OASBuilder generalizes
well across hundreds of APIs, and produces valid OpenAPI specifications that
encapsulate most of the information from the original documentation. OASBuilder
has been successfully implemented in an enterprise environment, saving
thousands of hours of manual effort and making hundreds of complex enterprise
APIs accessible as tools for LLMs.

</details>


### [32] [Exploring Empathy in Software Engineering: Insights from a Grey Literature Analysis of Practitioners' Perspectives](https://arxiv.org/abs/2507.05325)
*Lidiany Cerqueira,João Pedro Bastos,Danilo Neves,Glauco Carneiro,Rodrigo Spínola,Sávio Freire,José Amancio Macedo Santos,Manoel Mendonça*

Main category: cs.SE

TL;DR: This study explores empathy in software engineering (SE) by analyzing practitioners' perspectives through 55 web articles and expert surveys, proposing a definition, barriers, practices, and outcomes, while introducing a conceptual framework to improve team dynamics.


<details>
  <summary>Details</summary>
Motivation: Empathy, a critical social skill for effective communication and collaboration in SE, remains under-researched. The study aims to address this gap by characterizing empathy's meaning, barriers (e.g., toxic culture), and strategies to enhance team effectiveness.

Method: A qualitative content analysis was performed on web articles from DEV and Medium, followed by a survey of empathy experts to validate and refine findings.

Result: The research proposes a practitioner-centric definition of empathy in SE, identifies barriers like technical over-focus and poor work-life balance, suggests practices to foster empathy, and outlines benefits such as improved collaboration and reduced burnout. A conceptual framework integrates these insights.

Conclusion: The proposed framework is validated as clear and valuable by experts, emphasizing its potential to raise empathy awareness in SE and improve training. Future work will expand empathy's role in broader SE practices and team dynamics.

Abstract: Context. Empathy, a key social skill, is essential for communication and
collaboration in SE but remains an under-researched topic. Aims. This study
investigates empathy in SE from practitioners' perspectives, aiming to
characterize its meaning, identify barriers, discuss practices to overcome
them, and explore its effects. Method. A qualitative content analysis was
conducted on 55 web articles from DEV and Medium, two communities widely used
by practitioners. To strengthen our findings, we conducted a follow-up survey
with empathy experts. Results. The study proposes a definition of empathy in
SE, identifies barriers such as toxic culture and excessive technical focus,
practices to foster empathy in teams, and outcomes, including improved
collaboration, communication, and reduced anxiety, frustration, and stress.
These findings are synthesized into a conceptual framework. Conclusion. Survey
results indicate the framework is clear, valuable, and raises empathy
awareness, with suggestions for improvements and integration into training.
This study paves the way for improving team dynamics by addressing barriers and
offering strategies to cultivate empathy. Future work will explore empathy's
broader implications in SE practice.

</details>


### [33] [Tool for Supporting Debugging and Understanding of Normative Requirements Using LLMs](https://arxiv.org/abs/2507.05504)
*Alex Kleijwegt,Sinem Getir Yaman,Radu Calinescu*

Main category: cs.SE

TL;DR: The paper introduces SLEEC-LLM, a tool leveraging large language models to enhance the efficiency and explainability of analyzing SLEEC (social, legal, ethical, empathetic, cultural) normative requirements by translating formal verification counterexamples into natural language for non-technical stakeholders.


<details>
  <summary>Details</summary>
Motivation: Current domain-specific languages for specifying SLEEC requirements generate formal verification results that are inaccessible to non-technical stakeholders (e.g., ethicists, lawyers), leading to inefficient and error-prone requirements elicitation processes.

Method: SLEEC-LLM utilizes large language models (LLMs) to provide natural-language interpretations of counterexamples from model-checking tools, thus bridging the accessibility gap between technical formal verification outputs and non-technical stakeholders.

Result: The tool's effectiveness was demonstrated through two real-world case studies involving non-technical stakeholders, showing improvements in understanding and managing SLEEC rule inconsistencies.

Conclusion: SLEEC-LLM improves the efficiency and explainability of normative requirements elicitation and consistency analysis by enabling non-technical stakeholders to iteratively refine SLEEC norms using natural-language explanations of formal verification results.

Abstract: Normative requirements specify social, legal, ethical, empathetic, and
cultural (SLEEC) norms that must be observed by a system. To support the
identification of SLEEC requirements, numerous standards and regulations have
been developed. These requirements are typically defined by stakeholders in the
non-technical system with diverse expertise (e.g., ethicists, lawyers, social
scientists). Hence, ensuring their consistency and managing the requirement
elicitation process are complex and error-prone tasks. Recent research has
addressed this challenge using domain-specific languages to specify normative
requirements as rules, whose consistency can then be analyzed with formal
methods. Nevertheless, these approaches often present the results from formal
verification tools in a way that is inaccessible to non-technical users. This
hinders understanding and makes the iterative process of eliciting and
validating these requirements inefficient in terms of both time and effort. To
address this problem, we introduce SLEEC-LLM, a tool that uses large language
models (LLMs) to provide natural-language interpretations for model-checking
counterexamples corresponding to SLEEC rule inconsistencies. SLEEC-LLM improves
the efficiency and explainability of normative requirements elicitation and
consistency analysis. To demonstrate its effectiveness, we summarise its use in
two real-world case studies involving non-technical stakeholders.

</details>


### [34] [Search-based Selection of Metamorphic Relations for Optimized Robustness Testing of Large Language Models](https://arxiv.org/abs/2507.05565)
*Sangwon Hyun,Shaukat Ali,M. Ali Babar*

Main category: cs.SE

TL;DR: The paper proposes a search-based optimization for metamorphic relations (MRs) in Large Language Model (LLM) robustness testing, demonstrating MOEA/D as the most effective algorithm and identifying 'silver bullet' MRs that confuse LLMs across tasks.


<details>
  <summary>Details</summary>
Motivation: Current MR-based LLM robustness testing faces scalability challenges due to limited test spaces using single perturbation MRs and inefficient failure detection mechanisms, necessitating optimized MR selection strategies.

Method: The authors designed a search process with four multi-objective optimization algorithms (Single-GA, NSGA-II, SPEA2, MOEA/D) incorporating novel encoding to optimize MR groups. They extended test space coverage through combinatorial perturbations.

Result: Experiments on two major LLMs revealed MOEA/D outperforms other algorithms in robust MR selection. Silver bullet MRs consistently caused failure across diverse Text-to-Text tasks with minimal execution cost.

Conclusion: This research establishes a foundational solution for optimizing LLM robustness testing through search-based methods, highlighting combinatorial perturbations and algorithmic selection for effective, cost-efficient evaluations.

Abstract: Assessing the trustworthiness of Large Language Models (LLMs), such as
robustness, has garnered significant attention. Recently, metamorphic testing
that defines Metamorphic Relations (MRs) has been widely applied to evaluate
the robustness of LLM executions. However, the MR-based robustness testing
still requires a scalable number of MRs, thereby necessitating the optimization
of selecting MRs. Most extant LLM testing studies are limited to automatically
generating test cases (i.e., MRs) to enhance failure detection. Additionally,
most studies only considered a limited test space of single perturbation MRs in
their evaluation of LLMs. In contrast, our paper proposes a search-based
approach for optimizing the MR groups to maximize failure detection and
minimize the LLM execution cost. Moreover, our approach covers the
combinatorial perturbations in MRs, facilitating the expansion of test space in
the robustness assessment. We have developed a search process and implemented
four search algorithms: Single-GA, NSGA-II, SPEA2, and MOEA/D with novel
encoding to solve the MR selection problem in the LLM robustness testing. We
conducted comparative experiments on the four search algorithms along with a
random search, using two major LLMs with primary Text-to-Text tasks. Our
statistical and empirical investigation revealed two key findings: (1) the
MOEA/D algorithm performed the best in optimizing the MR space for LLM
robustness testing, and (2) we identified silver bullet MRs for the LLM
robustness testing, which demonstrated dominant capabilities in confusing LLMs
across different Text-to-Text tasks. In LLM robustness assessment, our research
sheds light on the fundamental problem for optimized testing and provides
insights into search-based solutions.

</details>


### [35] [TigAug: Data Augmentation for Testing Traffic Light Detection in Autonomous Driving Systems](https://arxiv.org/abs/2507.05932)
*You Lu,Dingji Wang,Kaifeng Huang,Bihuan Chen,Xin Peng*

Main category: cs.SE

TL;DR: Proposes TigAug, an automated tool for augmenting labeled traffic light images to test and improve traffic light detection models in autonomous vehicles, addressing the limitations of manual data collection.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving systems (ADSs) require reliable and robust traffic light detection. Current manual data collection is labor-intensive and lacks diversity in environmental variations, hindering effective testing.

Method: Constructs two metamorphic relation families and three transformation families based on weather environments, camera properties, and traffic light attributes. Uses these to automatically generate images for testing and retraining detection models.

Result: Experiments with four state-of-the-art models and two datasets show TigAug's effectiveness in testing (30% more error detection), efficiency in image synthesis (10x faster than manual), and acceptable naturalness of augmented images.

Conclusion: TigAug provides a scalable solution for automated testing and performance improvement of traffic light detection models in ADSs, overcoming manual data collection challenges.

Abstract: Autonomous vehicle technology has been developed in the last decades with
recent advances in sensing and computing technology. There is an urgent need to
ensure the reliability and robustness of autonomous driving systems (ADSs).
Despite the recent achievements in testing various ADS modules, little
attention has been paid on the automated testing of traffic light detection
models in ADSs. A common practice is to manually collect and label traffic
light data. However, it is labor-intensive, and even impossible to collect
diverse data under different driving environments.
  To address these problems, we propose and implement TigAug to automatically
augment labeled traffic light images for testing traffic light detection models
in ADSs. We construct two families of metamorphic relations and three families
of transformations based on a systematic understanding of weather environments,
camera properties, and traffic light properties. We use augmented images to
detect erroneous behaviors of traffic light detection models by
transformation-specific metamorphic relations, and to improve the performance
of traffic light detection models by retraining. Large-scale experiments with
four state-of-the-art traffic light detection models and two traffic light
datasets have demonstrated that i) TigAug is effective in testing traffic light
detection models, ii) TigAug is efficient in synthesizing traffic light images,
and iii) TigAug generates traffic light images with acceptable naturalness.

</details>


### [36] [Multi-Agent Debate Strategies to Enhance Requirements Engineering with Large Language Models](https://arxiv.org/abs/2507.05981)
*Marc Oriol,Quim Motger,Jordi Marco,Xavier Franch*

Main category: cs.SE

TL;DR: This paper investigates Multi-Agent Debate (MAD) strategies to enhance Requirements Engineering (RE) performance by enabling collaboration among LLM agents, addressing limitations of isolated models. A taxonomy of MAD strategies was developed, and a preliminary framework demonstrated feasibility in RE classification.


<details>
  <summary>Details</summary>
Motivation: Current methods for improving LLM accuracy in RE tasks (e.g., prompt engineering, fine-tuning) treat models as isolated systems, relying on single-pass outputs that limit robustness and adaptability. Human debates reduce bias and enhance accuracy in RE by incorporating diverse perspectives, suggesting that MAD strategies could offer similar benefits.

Method: The authors conducted a systematic review of MAD strategies across domains to identify their key characteristics. They then implemented and tested a preliminary MAD-based framework for RE classification to evaluate its applicability in this field.

Result: The study resulted in a taxonomy outlining core attributes of MAD strategies. A preliminary evaluation of the MAD-based RE classification framework confirmed the feasibility of applying these strategies to the RE domain, though results were limited due to the early stage of implementation.

Conclusion: MAD strategies represent a promising approach to improve LLM accuracy and adaptability in RE tasks. The paper provides a foundational understanding of MAD and suggests directions for future research to refine these methods in real-world RE applications.

Abstract: Context: Large Language Model (LLM) agents are becoming widely used for
various Requirements Engineering (RE) tasks. Research on improving their
accuracy mainly focuses on prompt engineering, model fine-tuning, and retrieval
augmented generation. However, these methods often treat models as isolated
black boxes - relying on single-pass outputs without iterative refinement or
collaboration, limiting robustness and adaptability. Objective: We propose
that, just as human debates enhance accuracy and reduce bias in RE tasks by
incorporating diverse perspectives, different LLM agents debating and
collaborating may achieve similar improvements. Our goal is to investigate
whether Multi-Agent Debate (MAD) strategies can enhance RE performance. Method:
We conducted a systematic study of existing MAD strategies across various
domains to identify their key characteristics. To assess their applicability in
RE, we implemented and tested a preliminary MAD-based framework for RE
classification. Results: Our study identified and categorized several MAD
strategies, leading to a taxonomy outlining their core attributes. Our
preliminary evaluation demonstrated the feasibility of applying MAD to RE
classification. Conclusions: MAD presents a promising approach for improving
LLM accuracy in RE tasks. This study provides a foundational understanding of
MAD strategies, offering insights for future research and refinements in RE
applications.

</details>


### [37] [PromiseTune: Unveiling Causally Promising and Explainable Configuration Tuning](https://arxiv.org/abs/2507.05995)
*Pengzhou Chen,Tao Chen*

Main category: cs.SE

TL;DR: PromiseTune addresses configuration tuning challenges in software systems by using causally purified rules to balance exploration-exploitation trade-offs and provide spatial explainability of system characteristics.


<details>
  <summary>Details</summary>
Motivation: Modern software systems require configuration tuning for performance optimization, but existing methods struggle due to expensive measurements, large configuration spaces, and rugged landscapes, exacerbated by weak knowledge of promising regions and poor explainability.

Method: The paper proposes PromiseTune, which learns rules representing configuration landscape regions and applies causal inference to purify them. These purified rules focus the tuner's search on promising areas while enabling landscape-level spatial explanations through measurement comparisons.

Result: PromiseTune outperformed 11 state-of-the-art tuners across 12 systems with varying budgets, achieving 42% higher rank than the second-best method while delivering richer explanatory insights about system behavior.

Conclusion: PromiseTune effectively mitigates the exploration-exploitation dilemma in configuration tuning through causal rule purification and provides explainable results that reveal hidden system characteristics during optimization.

Abstract: The high configurability of modern software systems has made configuration
tuning a crucial step for assuring system performance, e.g., latency or
throughput. However, given the expensive measurements, large configuration
space, and rugged configuration landscape, existing tuners suffer
ineffectiveness due to the difficult balance of budget utilization between
exploring uncertain regions (for escaping from local optima) and exploiting
guidance of known good configurations (for fast convergence). The root cause is
that we lack knowledge of where the promising regions lay, which also causes
challenges in the explainability of the results.
  In this paper, we propose PromiseTune that tunes configuration guided by
causally purified rules. PromiseTune is unique in the sense that we learn
rules, which reflect certain regions in the configuration landscape, and purify
them with causal inference. The remaining rules serve as approximated
reflections of the promising regions, bounding the tuning to emphasize these
places in the landscape. This, as we demonstrate, can effectively mitigate the
impact of the exploration and exploitation trade-off. Those purified regions
can then be paired with the measured configurations to provide spatial
explainability at the landscape level. Comparing with 11 state-of-the-art
tuners on 12 systems and varying budgets, we show that PromiseTune performs
significantly better than the others with $42\%$ superior rank to the overall
second best while providing richer information to explain the hidden system
characteristics.

</details>


### [38] [Model Cards Revisited: Bridging the Gap Between Theory and Practice for Ethical AI Requirements](https://arxiv.org/abs/2507.06014)
*Tim Puhlfürß,Julia Butzke,Walid Maalej*

Main category: cs.SE

TL;DR: This paper highlights gaps in ethical AI model documentation and introduces a 43-requirement taxonomy to address overlooked aspects like explainability and fairness.


<details>
  <summary>Details</summary>
Motivation: Existing model documentation frameworks (e.g., model cards) fail to comprehensively cover ethical requirements needed for compliance with laws and guidelines, creating a critical gap between AI ethics standards and practitioner documentation practices.

Method: Thematic analysis of 26 ethics/ai guidelines, 3 documentation frameworks, 3 quantitative studies, and 10 actual model cards to identify and categorize ethical requirements.

Result: 43 ethical requirements were grouped into 4 themes and 12 sub-themes. Developers primarily focus on model capabilities/reliability while neglecting ethics-related aspects such as explainability, user autonomy, and fairness.

Conclusion: The identified taxonomy provides a foundation for a revised model card framework that holistically addresses ethical AI requirements documentation.

Abstract: Model cards are the primary documentation framework for developers of
artificial intelligence (AI) models to communicate critical information to
their users. Those users are often developers themselves looking for relevant
documentation to ensure that their AI systems comply with the ethical
requirements of existing laws, guidelines, and standards. Recent studies
indicate inadequate model documentation practices, suggesting a gap between AI
requirements and current practices in model documentation. To understand this
gap and provide actionable guidance to bridge it, we conducted a thematic
analysis of 26 guidelines on ethics and AI, three AI documentation frameworks,
three quantitative studies of model cards, and ten actual model cards. We
identified a total of 43 ethical requirements relevant to model documentation
and organized them into a taxonomy featuring four themes and twelve sub-themes
representing ethical principles. Our findings indicate that model developers
predominantly emphasize model capabilities and reliability in the documentation
while overlooking other ethical aspects, such as explainability, user autonomy,
and fairness. This underscores the need for enhanced support in documenting
ethical AI considerations. Our taxonomy serves as a foundation for a revised
model card framework that holistically addresses ethical AI requirements.

</details>
