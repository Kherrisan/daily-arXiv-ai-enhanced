<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 4]
- [cs.SE](#cs.SE) [Total: 8]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Differential Robustness in Transformer Language Models: Empirical Evaluation Under Adversarial Text Attacks](https://arxiv.org/abs/2509.09706)
*Taniya Gidatkar,Oluwaseun Ajao,Matthew Shardlow*

Main category: cs.CR

TL;DR: This study evaluates LLM resilience against adversarial attacks, finding RoBERTa-Base and Flan-T5 highly robust but computationally intensive, while BERT-Base is severely vulnerable. It underscores the need for efficient defense mechanisms.


<details>
  <summary>Details</summary>
Motivation: This work addresses the critical gap in understanding LLM security by investigating how different architectures withstand adversarial attacks. The findings aim to inform the development of more secure and resource-efficient models for real-world applications.

Method: The research employs systematic adversarial testing using TextFooler and BERTAttack on three models (Flan-T5, BERT-Base, RoBERTa-Base) to measure their robustness. Attack success rates and model accuracy changes under various perturbations are quantitatively analyzed.

Result: RoBERTa-Base and Flan-T5 showed zero attack success rates, maintaining accuracy under attacks. In contrast, BERT-Base was highly vulnerable, with TextFooler reducing accuracy from 48% to 3% (93.75% attack success). The results also reveal a trade-off between robustness and computational efficiency.

Conclusion: The study concludes that while some LLMs like RoBERTa-Base and Flan-T5 exhibit strong resilience against adversarial attacks, their robustness comes at the cost of high computational resource demands. It highlights the need for developing more efficient defensive strategies to balance security and practicality.

Abstract: This study evaluates the resilience of large language models (LLMs) against
adversarial attacks, specifically focusing on Flan-T5, BERT, and RoBERTa-Base.
Using systematically designed adversarial tests through TextFooler and
BERTAttack, we found significant variations in model robustness. RoBERTa-Base
and FlanT5 demonstrated remarkable resilience, maintaining accuracy even when
subjected to sophisticated attacks, with attack success rates of 0%. In
contrast. BERT-Base showed considerable vulnerability, with TextFooler
achieving a 93.75% success rate in reducing model accuracy from 48% to just 3%.
Our research reveals that while certain LLMs have developed effective defensive
mechanisms, these safeguards often require substantial computational resources.
This study contributes to the understanding of LLM security by identifying
existing strengths and weaknesses in current safeguarding approaches and
proposes practical recommendations for developing more efficient and effective
defensive strategies.

</details>


### [2] [ZORRO: Zero-Knowledge Robustness and Privacy for Split Learning (Full Version)](https://arxiv.org/abs/2509.09787)
*Nojan Sheybani,Alessandro Pegoraro,Jonathan Knauer,Phillip Rieger,Elissa Mollakuqe,Farinaz Koushanfar,Ahmad-Reza Sadeghi*

Main category: cs.CR

TL;DR: ZORRO is a secure Split Learning defense using zero-knowledge proofs to verify client-side computations, preventing backdoor attacks with minimal overhead and robust performance across complex models.


<details>
  <summary>Details</summary>
Motivation: The motivation addresses the vulnerability of Split Learning (SL) to poisoning attacks by malicious clients who manipulate gradients to inject backdoors, as well as limitations in existing defenses that rely on server-side protection or introduce excessive server overhead.

Method: ZORRO employs interactive zero-knowledge proofs (ZKPs) to verify client-side execution of defense algorithms and leverages frequency representation of model partitions to analyze local model components in untrusted environments.

Result: Evaluation results show ZORRO reduces attack success rates to less than 6% across diverse models, attack strategies, and data scenarios, with an overhead of less than 10 seconds for models containing up to 1,000,000 client-side parameters.

Conclusion: This paper concludes that ZORRO provides an effective and efficient solution for securing Split Learning against backdoor attacks through client-side verifiable defenses, achieving significant reduction in attack success rates with minimal computational overhead.

Abstract: Split Learning (SL) is a distributed learning approach that enables
resource-constrained clients to collaboratively train deep neural networks
(DNNs) by offloading most layers to a central server while keeping in- and
output layers on the client-side. This setup enables SL to leverage server
computation capacities without sharing data, making it highly effective in
resource-constrained environments dealing with sensitive data. However, the
distributed nature enables malicious clients to manipulate the training
process. By sending poisoned intermediate gradients, they can inject backdoors
into the shared DNN. Existing defenses are limited by often focusing on
server-side protection and introducing additional overhead for the server. A
significant challenge for client-side defenses is enforcing malicious clients
to correctly execute the defense algorithm.
  We present ZORRO, a private, verifiable, and robust SL defense scheme.
Through our novel design and application of interactive zero-knowledge proofs
(ZKPs), clients prove their correct execution of a client-located defense
algorithm, resulting in proofs of computational integrity attesting to the
benign nature of locally trained DNN portions. Leveraging the frequency
representation of model partitions enables ZORRO to conduct an in-depth
inspection of the locally trained models in an untrusted environment, ensuring
that each client forwards a benign checkpoint to its succeeding client. In our
extensive evaluation, covering different model architectures as well as various
attack strategies and data scenarios, we show ZORRO's effectiveness, as it
reduces the attack success rate to less than 6\% while causing even for models
storing \numprint{1000000} parameters on the client-side an overhead of less
than 10 seconds.

</details>


### [3] [SmartCoder-R1: Towards Secure and Explainable Smart Contract Generation with Security-Aware Group Relative Policy Optimization](https://arxiv.org/abs/2509.09942)
*Lei Yu,Jingyuan Zhang,Xin Wang,Jiajia Ma,Li Yang,Fengjun Zhang*

Main category: cs.CR

TL;DR: This paper introduces SmartCoder-R1, a secure and explainable smart contract generation framework based on Qwen2.5-Coder-7B, using Continual Pre-training, Long Chain-of-Thought SFT, and Security-Aware Policy Optimization to achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Smart contracts are critical in managing high-value assets, so their security is paramount. LLMs deployed for this task and existing smart contract generation approaches lack both transparency in their reasoning (black box) and are prone to security vulnerabilities, leading to catastrophic financial loss.

Method: SmartCoder-R1 is based on Qwen2.5-Coder-7B and consists of three components: (1) Continual Pre-training (CPT) to specialize the model, (2) Long Chain-of-Thought Supervised Fine-Tuning (L-CoT SFT) on a dataset of 7,998 expert-validated reasoning-and-code samples to train the model to emulate human security analysis, and (3) Security-Aware Group Relative Policy Optimization (S-GRPO) for reinforcement learning that refines the generation policy by optimizing a weighted reward signal for compilation success, security compliance, and format correctness.

Result: SmartCoder-R1 achieves state-of-the-art performance across 5 metrics on a benchmark of 756 real-world functions: ComPass (87.70%), VulRate (8.60%), SafeAval (80.16%), FuncRate (53.84%), and FullRate (50.53%), with a 45.79% relative improvement in FullRate over DeepSeek-R1. Its generated reasoning is also rated highly in Functionality (82.7%), Security (85.3%), and Clarity (90.7%) in human assessments.

Conclusion: SmartCoder-R1 is a novel and effective approach to secure and explainable smart contract generation. The model addresses the problems of lack of transparency and security vulnerabilities in existing LLM-based solutions through a series of training and optimization techniques, demonstrating superior performance both in metrics and human evaluations.

Abstract: Smart contracts automate the management of high-value assets, where
vulnerabilities can lead to catastrophic financial losses. This challenge is
amplified in Large Language Models (LLMs) by two interconnected failures: they
operate as unauditable "black boxes" lacking a transparent reasoning process,
and consequently, generate code riddled with critical security vulnerabilities.
To address both issues, we propose SmartCoder-R1 (based on Qwen2.5-Coder-7B), a
novel framework for secure and explainable smart contract generation. It begins
with Continual Pre-training (CPT) to specialize the model. We then apply Long
Chain-of-Thought Supervised Fine-Tuning (L-CoT SFT) on 7,998 expert-validated
reasoning-and-code samples to train the model to emulate human security
analysis. Finally, to directly mitigate vulnerabilities, we employ
Security-Aware Group Relative Policy Optimization (S-GRPO), a reinforcement
learning phase that refines the generation policy by optimizing a weighted
reward signal for compilation success, security compliance, and format
correctness. Evaluated against 17 baselines on a benchmark of 756 real-world
functions, SmartCoder-R1 establishes a new state of the art, achieving top
performance across five key metrics: a ComPass of 87.70%, a VulRate of 8.60%, a
SafeAval of 80.16%, a FuncRate of 53.84%, and a FullRate of 50.53%. This
FullRate marks a 45.79% relative improvement over the strongest baseline,
DeepSeek-R1. Crucially, its generated reasoning also excels in human
evaluations, achieving high-quality ratings for Functionality (82.7%), Security
(85.3%), and Clarity (90.7%).

</details>


### [4] [Byte by Byte: Unmasking Browser Fingerprinting at the Function Level Using V8 Bytecode Transformers](https://arxiv.org/abs/2509.09950)
*Pouneh Nikkhah Bahrami,Dylan Cutler,Igor Bilogrevic*

Main category: cs.CR

TL;DR: ByteDefender detects browser fingerprinting via V8 bytecode analysis, preventing malicious tracking with minimal performance impact and robust obfuscation resistance.


<details>
  <summary>Details</summary>
Motivation: Current defenses against browser fingerprinting either fail to detect evasion techniques or cause website breakage. Precise, function-level detection is needed to preserve legitimate scripts while resisting code obfuscation and URL manipulation.

Method: ByteDefender uses V8 engine bytecode analysis with a Transformer-based classifier trained offline to identify fingerprinting functions, combined with lightweight on-device signatures applied during JavaScript compilation to block malicious functionality before execution.

Result: ByteDefender achieves high function/script-level detection accuracy on 100k+ websites with 4% average latency, outperforming AST-based methods in obfuscation robustness while maintaining low overhead for real-time prevention.

Conclusion: ByteDefender provides a practical framework for precise, robust fingerprinting mitigation by leveraging V8 bytecode analysis at the function level, overcoming limitations of existing methods through low overhead and obfuscation resilience.

Abstract: Browser fingerprinting enables persistent cross-site user tracking via subtle
techniques that often evade conventional defenses or cause website breakage
when script-level blocking countermeasures are applied. Addressing these
challenges requires detection methods offering both function-level precision to
minimize breakage and inherent robustness against code obfuscation and URL
manipulation.
  We introduce ByteDefender, the first system leveraging V8 engine bytecode to
detect fingerprinting operations specifically at the JavaScript function level.
A Transformer-based classifier, trained offline on bytecode sequences,
accurately identifies functions exhibiting fingerprinting behavior. We develop
and evaluate light-weight signatures derived from this model to enable
low-overhead, on-device matching against function bytecode during compilation
but prior to execution, which only adds a 4% (average) latency to the page load
time. This mechanism facilitates targeted, real-time prevention of
fingerprinting function execution, thereby preserving legitimate script
functionality. Operating directly on bytecode ensures inherent resilience
against common code obfuscation and URL-based evasion. Our evaluation on the
top 100k websites demonstrates high detection accuracy at both function- and
script-level, with substantial improvements over state-of-the-art AST-based
methods, particularly in robustness against obfuscation. ByteDefender offers a
practical framework for effective, precise, and robust fingerprinting
mitigation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [SWE-Effi: Re-Evaluating Software AI Agent System Effectiveness Under Resource Constraints](https://arxiv.org/abs/2509.09853)
*Zhiyu Fan,Kirill Vasilevski,Dayi Lin,Boyuan Chen,Yihao Chen,Zhiqing Zhong,Jie M. Zhang,Pinjia He,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: This paper introduces SWE-Effi, a holistic metric for AI systems in software engineering that balances accuracy and resource efficiency. Analysis on SWE-bench highlights scaffold-integration importance, 'expensive failures,' and budget trade-offs, offering practical insights for cost-effective AI deployment.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks (e.g., SWE-bench) focus solely on accuracy, neglecting resource efficiency—a universal issue in AI systems. Effective AI must balance accuracy with cost-effectiveness, especially in resource-constrained environments.

Method: The authors introduced SWE-Effi, a multi-dimensional metric framework combining outcome accuracy (e.g., issue resolution rate) and resource utilization (tokens, time). They re-ranked AI systems on a subset of SWE-bench to evaluate effectiveness under these new metrics.

Result: Systems' effectiveness depends on scaffold-integration efficiency with base models. Systematic challenges like 'token snowball' and 'expensive failures' (agents using excessive resources on unsolvable tasks) were identified. A trade-off between token and time budget effectiveness was observed, impacting scalable reinforcement learning.

Conclusion: The study emphasizes the importance of resource-efficient AI systems in software engineering and highlights the need to balance outcome accuracy with efficient resource consumption. Key findings include the impact of scaffold-integration efficiency and the criticality of addressing 'expensive failures' to reduce costs in practical deployment and reinforcement learning.

Abstract: The advancement of large language models (LLMs) and code agents has
demonstrated significant potential to assist software engineering (SWE) tasks,
such as autonomous issue resolution and feature addition. Existing AI for
software engineering leaderboards (e.g., SWE-bench) focus solely on solution
accuracy, ignoring the crucial factor of effectiveness in a
resource-constrained world. This is a universal problem that also exists beyond
software engineering tasks: any AI system should be more than correct - it must
also be cost-effective. To address this gap, we introduce SWE-Effi, a set of
new metrics to re-evaluate AI systems in terms of holistic effectiveness
scores. We define effectiveness as the balance between the accuracy of outcome
(e.g., issue resolve rate) and the resources consumed (e.g., token and time).
In this paper, we specifically focus on the software engineering scenario by
re-ranking popular AI systems for issue resolution on a subset of the SWE-bench
benchmark using our new multi-dimensional metrics. We found that AI system's
effectiveness depends not just on the scaffold itself, but on how well it
integrates with the base model, which is key to achieving strong performance in
a resource-efficient manner. We also identified systematic challenges such as
the "token snowball" effect and, more significantly, a pattern of "expensive
failures". In these cases, agents consume excessive resources while stuck on
unsolvable tasks - an issue that not only limits practical deployment but also
drives up the cost of failed rollouts during RL training. Lastly, we observed a
clear trade-off between effectiveness under the token budget and effectiveness
under the time budget, which plays a crucial role in managing project budgets
and enabling scalable reinforcement learning, where fast responses are
essential.

</details>


### [6] [From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI Ecosystem](https://arxiv.org/abs/2509.09873)
*James Jewitt,Hao Li,Bram Adams,Gopi Krishnan Rajbahadur,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: First end-to-end audit of Hugging Face's open-source AI ecosystem reveals systemic license non-compliance and introduces tools to automate conflict resolution (86.4% efficacy).


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of data-driven understanding of license conflicts in the open-source AI ecosystem, which exposes organizations to legal risks and users to undisclosed ethical risks.

Method: The authors conducted an end-to-end audit of licenses for 364,000 datasets, 1.6 million models, and 140,000 GitHub projects on Hugging Face. They developed an extensible rule engine encoding ~200 SPDX and model-specific clauses to detect license conflicts.

Result: 35.5% of model-to-application transitions eliminated restrictive licenses via relicensing under permissive terms, while the prototype engine resolved 86.4% of license conflicts. The dataset and rule engine were released for future research.

Conclusion: The study reveals that license compliance is a critical governance challenge in open-source AI and provides both a dataset of license conflicts and an extensible rule engine to enable automated, AI-aware compliance at scale.

Abstract: Hidden license conflicts in the open-source AI ecosystem pose serious legal
and ethical risks, exposing organizations to potential litigation and users to
undisclosed risk. However, the field lacks a data-driven understanding of how
frequently these conflicts occur, where they originate, and which communities
are most affected. We present the first end-to-end audit of licenses for
datasets and models on Hugging Face, as well as their downstream integration
into open-source software applications, covering 364 thousand datasets, 1.6
million models, and 140 thousand GitHub projects. Our empirical analysis
reveals systemic non-compliance in which 35.5% of model-to-application
transitions eliminate restrictive license clauses by relicensing under
permissive terms. In addition, we prototype an extensible rule engine that
encodes almost 200 SPDX and model-specific clauses for detecting license
conflicts, which can solve 86.4% of license conflicts in software applications.
To support future research, we release our dataset and the prototype engine.
Our study highlights license compliance as a critical governance challenge in
open-source AI and provides both the data and tools necessary to enable
automated, AI-aware compliance at scale.

</details>


### [7] [SLD-Spec: Enhancement LLM-assisted Specification Generation for Complex Loop Functions via Program Slicing and Logical Deletion](https://arxiv.org/abs/2509.09917)
*Zehan Chen,Long Zhang,Zhiwei Zhang,JingJing Zhang,Ruoyu Zhou,Yulong Shen,JianFeng Ma,Lin Yang*

Main category: cs.SE

TL;DR: SLD-Spec is an LLM-based method for generating formal specifications that addresses limitations in handling complex loops and verification constraints by introducing slicing and logical deletion phases. It achieves high correctness and completeness in specification generation.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based approaches struggle with programs containing complex loops and verification tool constraints, resulting in incomplete/ambiguous specifications. Verification automation requires better methods for loop-heavy code.

Method: The method introduces (1)a slicing phase to decompose functions into independent loop fragments, and (2)a logical deletion phase using LLM reasoning to filter out incorrect candidate specifications, retaining only valid ones.

Result: SLD-Spec verifies 5 more programs and reduces runtime by 23.73% on baseline datasets. On manually constructed complex loop datasets, it achieves 95.1%/90.91%
verification accuracy for assertions/programs. Ablation studies confirm both phases' effectiveness.

Conclusion: SLD-Spec improves specification completeness, correctness, and relevance for loop-intensive programs through its two-phase approach. It establishes a new benchmark for LLM-assisted formal specification generation.

Abstract: Automatically generating formal specifications from program code can greatly
enhance the efficiency of program verification and enable end-to-end automation
from requirements to reliable software. However, existing LLM-based approaches
often struggle with programs that include complex loop structures, leading to
irrelevant specifications. Moreover, the rigorous proof obligations and design
constraints imposed by verification tools can further result in incomplete and
ambiguous specifications. To address these challenges, we propose SLD-Spec, an
LLM-assisted specification generation method tailored for programs with complex
loop constructs. SLD-Spec introduces two novel phases into the traditional
specification generation framework: (1) A slicing phase, which decomposes each
function into code fragments containing independent loop structures, thereby
reducing the complexity of specification generation; and (2) A logical deletion
phase, which applies LLM-based reasoning to filter out incorrect candidate
specifications--especially those not easily identified by verification
tool--while retaining valid ones. Experimental results show that on the simple
dataset, SLD-Spec successfully verifies five more programs than the
state-of-the-art AutoSpec and reduces runtime by 23.73%. To address the
limitations of existing research, we manually construct a dataset comprising
four categories of complex loop programs. On this dataset, SLD-Spec
significantly improves the correctness, relevance, and completeness of
generated specifications compared to baseline methods, enabling 95.1% of
assertions and 90.91% of programs to pass verification. Ablation studies
further reveal that logical deletion is critical for enhancing specification
correctness and relevance, while program slicing contributes significantly to
specification completeness. Our code and data are publicly available.

</details>


### [8] [WALL: A Web Application for Automated Quality Assurance using Large Language Models](https://arxiv.org/abs/2509.09918)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As software projects become increasingly complex, the volume and variety of
issues in code files have grown substantially. Addressing this challenge
requires efficient issue detection, resolution, and evaluation tools. This
paper presents WALL, a web application that integrates SonarQube and large
language models (LLMs) such as GPT-3.5 Turbo and GPT-4o to automate these
tasks. WALL comprises three modules: an issue extraction tool, code issues
reviser, and code comparison tool. Together, they enable a seamless pipeline
for detecting software issues, generating automated code revisions, and
evaluating the accuracy of revisions. Our experiments, conducted on 563 files
with over 7,599 issues, demonstrate WALL's effectiveness in reducing human
effort while maintaining high-quality revisions. Results show that employing a
hybrid approach of cost-effective and advanced LLMs can significantly lower
costs and improve revision rates. Future work aims to enhance WALL's
capabilities by integrating open-source LLMs and eliminating human
intervention, paving the way for fully automated code quality management.

</details>


### [9] [Toward Green Code: Prompting Small Language Models for Energy-Efficient Code Generation](https://arxiv.org/abs/2509.09947)
*Humza Ashraf,Syed Muhammad Danish,Zeeshan Sattar*

Main category: cs.SE

TL;DR: This paper finds that tailored prompting strategies like CoT can improve energy efficiency in small language models for coding tasks, but their benefits vary by model architecture.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) contribute to high energy use and carbon emissions in software development. Small language models (SLMs) present a sustainable alternative, but their energy efficiency through prompting strategies remains understudied.

Method: The paper evaluates four open-source SLMs (StableCode-Instruct-3B, Qwen2.5-Coder-3B-Instruct, CodeLlama-7B-Instruct, and Phi-3-Mini-4K-Instruct) on 150 Python problems across LeetCode's difficulty categories. Four prompting strategies (role, zero-shot, few-shot, CoT) are tested, measuring runtime, memory usage, and energy consumption compared to human-written solutions.

Result: Chain-of-thought (CoT) prompting consistently reduced energy consumption for Qwen2.5-Coder and StableCode-3B across tasks, while CodeLlama-7B and Phi-3-Mini-4K underperformed all benchmarks. Prompt effectiveness is model-dependent.

Conclusion: The study highlights that prompting strategies have varying effectiveness across SLMs, with CoT prompting showing energy benefits for specific models. This underscores the need for model-specific prompt design to reduce environmental impact in software development.

Abstract: There is a growing concern about the environmental impact of large language
models (LLMs) in software development, particularly due to their high energy
use and carbon footprint. Small Language Models (SLMs) offer a more sustainable
alternative, requiring fewer computational resources while remaining effective
for fundamental programming tasks. In this study, we investigate whether prompt
engineering can improve the energy efficiency of SLMs in code generation. We
evaluate four open-source SLMs, StableCode-Instruct-3B,
Qwen2.5-Coder-3B-Instruct, CodeLlama-7B-Instruct, and Phi-3-Mini-4K-Instruct,
across 150 Python problems from LeetCode, evenly distributed into easy, medium,
and hard categories. Each model is tested under four prompting strategies: role
prompting, zero-shot, few-shot, and chain-of-thought (CoT). For every generated
solution, we measure runtime, memory usage, and energy consumption, comparing
the results with a human-written baseline. Our findings show that CoT prompting
provides consistent energy savings for Qwen2.5-Coder and StableCode-3B, while
CodeLlama-7B and Phi-3-Mini-4K fail to outperform the baseline under any
prompting strategy. These results highlight that the benefits of prompting are
model-dependent and that carefully designed prompts can guide SLMs toward
greener software development.

</details>


### [10] [Development of Automated Software Design Document Review Methods Using Large Language Models](https://arxiv.org/abs/2509.09975)
*Takasaburo Fukuda,Takao Nakagawa,Keisuke Miyazaki,Susumu Tokumoto*

Main category: cs.SE

TL;DR: This paper proposes using LLMs to automate software design document reviews by analyzing 11 perspectives, developing document-comprehension techniques, and validating effectiveness via GPT-based experiments to detect inconsistencies.


<details>
  <summary>Details</summary>
Motivation: Manual review of design documents is labor-intensive, prompting the need for automation via LLMs to improve efficiency and consistency in software development.

Method: The researchers analyzed 11 review perspectives, identified applicable areas for LLMs, and developed techniques to handle complex design documents with table data. Experiments used GPT to assess consistency in real-world business documents.

Result: Experiments confirmed that LLMs successfully detected inconsistencies in design documents, validating their utility in automating specific review tasks.

Conclusion: The study demonstrates that LLMs can effectively identify inconsistencies in software design documents, reducing reliance on human reviewers for specific tasks.

Abstract: In this study, we explored an approach to automate the review process of
software design documents by using LLM. We first analyzed the review methods of
design documents and organized 11 review perspectives. Additionally, we
analyzed the issues of utilizing LLMs for these 11 review perspectives and
determined which perspectives can be reviewed by current general-purpose LLMs
instead of humans. For the reviewable perspectives, we specifically developed
new techniques to enable LLMs to comprehend complex design documents that
include table data. For evaluation, we conducted experiments using GPT to
assess the consistency of design items and descriptions across different design
documents in the design process used in actual business operations. Our results
confirmed that LLMs can be utilized to identify inconsistencies in software
design documents during the review process.

</details>


### [11] [Sustaining Research Software: A Fitness Function Approach](https://arxiv.org/abs/2509.10085)
*Philipp Zech,Irdin Pekaric*

Main category: cs.SE

TL;DR: This paper proposes FAIR-focused fitness functions to enhance the long-term sustainability of research software through proactive architectural guidance and experimental validation.


<details>
  <summary>Details</summary>
Motivation: Research software faces sustainability challenges due to poor maintainability, adaptability issues, and rapid obsolescence, limiting its scientific impact over time.

Method: The authors design domain-specific fitness functions that enforce FAIR principles (findability, accessibility, interoperability, reusability), integrating automated architectural metrics for modular design, documentation standards, version control, and tech stack compatibility throughout development.

Result: Case studies and experiments demonstrate the approach improves long-term FAIRness metrics, bridging the gap between project-specific development and enduring scientific infrastructure.

Conclusion: The fitness function framework effectively promotes sustainable research software practices by institutionalizing proactive quality assurance, offering a scalable model for maintaining software relevance in scientific discovery.

Abstract: The long-term sustainability of research software is a critical challenge, as
it usually suffers from poor maintainability, lack of adaptability, and
eventual obsolescence. This paper proposes a novel approach to addressing this
issue by leveraging the concept of fitness functions from evolutionary
architecture. Fitness functions are automated, continuously evaluated metrics
designed to ensure that software systems meet desired non-functional,
architectural qualities over time. We define a set of fitness functions
tailored to the unique requirements of research software, focusing on
findability, accessibility, interoperability and reusability (FAIR). These
fitness functions act as proactive safeguards, promoting practices such as
modular design, comprehensive documentation, version control, and compatibility
with evolving technological ecosystems. By integrating these metrics into the
development life cycle, we aim to foster a culture of sustainability within the
research community. Case studies and experimental results demonstrate the
potential of this approach to enhance the long-term FAIR of research software,
bridging the gap between ephemeral project-based development and enduring
scientific impact.

</details>


### [12] [Generating Energy-Efficient Code via Large-Language Models -- Where are we now?](https://arxiv.org/abs/2509.10099)
*Radu Apsan,Vincenzo Stoico,Michel Albonico,Rudra Dhar,Karthik Vaidhyanathan,Ivano Malavolta*

Main category: cs.SE

TL;DR: Green software experts consistently produce more energy-efficient Python code than LLMs across hardware platforms, emphasizing the irreplaceability of human expertise in energy-conscious software development.


<details>
  <summary>Details</summary>
Motivation: The increasing adoption of Large Language Models (LLMs) in development pipelines necessitates an empirical assessment of their energy efficiency in code generation compared to human-written solutions and expert-optimized code.

Method: The study evaluated 363 Python code solutions (9 problems from EvoEval benchmark) using 6 LLMs and 4 prompting techniques across server, PC, and Raspberry Pi hardware platforms over ~881h (36.7 days), comparing energy consumption with human-developed solutions and a Green software expert.

Result: Human solutions outperformed LLMs by 16% (server) and 3% (Raspberry Pi); LLMs surpassed humans by 25% on PCs. Green software expert code was 17-30% more energy-efficient than all LLMs across all platforms. Prompting strategies showed inconsistent energy savings dependent on hardware.

Conclusion: LLM-generated code does not surpass the energy efficiency of experienced Green software developers, highlighting the necessity of human expertise in creating energy-efficient Python solutions.

Abstract: Context. The rise of Large Language Models (LLMs) has led to their widespread
adoption in development pipelines. Goal. We empirically assess the energy
efficiency of Python code generated by LLMs against human-written code and code
developed by a Green software expert. Method. We test 363 solutions to 9 coding
problems from the EvoEval benchmark using 6 widespread LLMs with 4 prompting
techniques, and comparing them to human-developed solutions. Energy consumption
is measured on three different hardware platforms: a server, a PC, and a
Raspberry Pi for a total of ~881h (36.7 days). Results. Human solutions are 16%
more energy-efficient on the server and 3% on the Raspberry Pi, while LLMs
outperform human developers by 25% on the PC. Prompting does not consistently
lead to energy savings, where the most energy-efficient prompts vary by
hardware platform. The code developed by a Green software expert is
consistently more energy-efficient by at least 17% to 30% against all LLMs on
all hardware platforms. Conclusions. Even though LLMs exhibit relatively good
code generation capabilities, no LLM-generated code was more energy-efficient
than that of an experienced Green software developer, suggesting that as of
today there is still a great need of human expertise for developing
energy-efficient Python code.

</details>
