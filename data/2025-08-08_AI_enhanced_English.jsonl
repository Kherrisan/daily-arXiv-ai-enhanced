{"id": "2508.04820", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04820", "abs": "https://arxiv.org/abs/2508.04820", "authors": ["Mayra Sofia Ruiz Rodriguez", "SayedHassan Khatoonabadi", "Emad Shihab"], "title": "Automated File-Level Logging Generation for Machine Learning Applications using LLMs: A Case Study using GPT-4o Mini", "comment": null, "summary": "Logging is essential in software development, helping developers monitor\nsystem behavior and aiding in debugging applications. Given the ability of\nlarge language models (LLMs) to generate natural language and code, researchers\nare exploring their potential to generate log statements. However, prior work\nfocuses on evaluating logs introduced in code functions, leaving file-level log\ngeneration underexplored -- especially in machine learning (ML) applications,\nwhere comprehensive logging can enhance reliability. In this study, we evaluate\nthe capacity of GPT-4o mini as a case study to generate log statements for ML\nprojects at file level. We gathered a set of 171 ML repositories containing\n4,073 Python files with at least one log statement. We identified and removed\nthe original logs from the files, prompted the LLM to generate logs for them,\nand evaluated both the position of the logs and log level, variables, and text\nquality of the generated logs compared to human-written logs. In addition, we\nmanually analyzed a representative sample of generated logs to identify common\npatterns and challenges. We find that the LLM introduces logs in the same place\nas humans in 63.91% of cases, but at the cost of a high overlogging rate of\n82.66%. Furthermore, our manual analysis reveals challenges for file-level\nlogging, which shows overlogging at the beginning or end of a function,\ndifficulty logging within large code blocks, and misalignment with\nproject-specific logging conventions. While the LLM shows promise for\ngenerating logs for complete files, these limitations remain to be addressed\nfor practical implementation.", "AI": {"tldr": "This paper evaluates GPT-4o mini's ability to generate file-level logs for ML projects, finding it places logs in correct locations 63.91% of the time but suffers from 82.66% overlogging due to challenges like log placement in function boundaries, handling large code blocks, and aligning with project conventions.", "motivation": "Prior research on LLM-based logging focuses only on function-level logs, neglecting file-level logging which is critical for ML application reliability. This creates a gap in understanding logging at larger code scopes.", "method": "1) Collected 171 ML repos with 4,073 Python files containing logs 2) Removed original logs 3) Prompted LLM to regenerate logs 4) Evaluated position accuracy, log level appropriateness, variable usage, and text quality through automated metrics and manual analysis of a representative sample.", "result": "LLM matches human log positions in 63.91% of files but has 82.66% overlogging rate. Manual analysis reveals issues with logging at function boundaries, within large code blocks, and adherence to project-specific conventions. Log level, variable selection, and message quality show mixed results.", "conclusion": "While LLMs demonstrate potential for file-level logging in ML projects, overlogging and alignment with project standards remain significant challenges. Practical implementation requires addressing these limitations before adoption can be justified."}}
{"id": "2508.04895", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.04895", "abs": "https://arxiv.org/abs/2508.04895", "authors": ["Wentao Lu", "Alexander Senchenko", "Abram Hindle", "Cor-Paul Bezemer"], "title": "Automated Bug Frame Retrieval from Gameplay Videos Using Vision-Language Models", "comment": null, "summary": "Modern game studios deliver new builds and patches at a rapid pace,\ngenerating thousands of bug reports, many of which embed gameplay videos. To\nverify and triage these bug reports, developers must watch the submitted\nvideos. This manual review is labour-intensive, slow, and hard to scale. In\nthis paper, we introduce an automated pipeline that reduces each video to a\nsingle frame that best matches the reported bug description, giving developers\ninstant visual evidence that pinpoints the bug.\n  Our pipeline begins with FFmpeg for keyframe extraction, reducing each video\nto a median of just 1.90% of its original frames while still capturing bug\nmoments in 98.79 of cases. These keyframes are then evaluated by a\nvision--language model (GPT-4o), which ranks them based on how well they match\nthe textual bug description and selects the most representative frame. We\nevaluated this approach using real-world developer-submitted gameplay videos\nand JIRA bug reports from a popular First-Person Shooter (FPS) game. The\npipeline achieves an overall F1 score of 0.79 and Accuracy of 0.89 for the\ntop-1 retrieved frame. Performance is highest for the Lighting & Shadow (F1 =\n0.94), Physics & Collision (0.86), and UI & HUD (0.83) bug categories, and\nlowest for Animation & VFX (0.51).\n  By replacing video viewing with an immediately informative image, our\napproach dramatically reduces manual effort and speeds up triage and regression\nchecks, offering practical benefits to quality assurance (QA) teams and\ndevelopers across the game industry.", "AI": {"tldr": "An automated pipeline selects a single frame from gameplay videos that best matches a bug description, aiding developers in quickly verifying bug reports without watching entire videos.", "motivation": "Game studios generate thousands of bug reports with gameplay videos, requiring developers to manually watch each video for triage, which is labor-intensive, slow, and hard to scale.", "method": "The pipeline uses FFmpeg to extract keyframes (1.90% of original frames, capturing 98.79% of bug moments) followed by a vision-language model (GPT-4o) ranking keyframes against the textual bug description to select the most representative frame.", "result": "Achieved F1 score of 0.79 and accuracy of 0.89 for top-1 frame selection. Highest performance in Lighting & Shadow (F1=0.94), Physics & Collision (0.86), and UI & HUD (0.83) categories, with lowest in Animation & VFX (0.51).", "conclusion": "The approach reduces manual effort and accelerates bug triage and regression checks, offering scalable and practical benefits for game industry QA teams and developers."}}
{"id": "2508.04921", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.04921", "abs": "https://arxiv.org/abs/2508.04921", "authors": ["Zixuan Feng", "Reed Milewicz", "Emerson Murphy-Hill", "Tyler Menezes", "Alexander Serebrenik", "Igor Steinmacher", "Anita Sarma"], "title": "Charting Uncertain Waters: A Socio-Technical Framework for Navigating GenAI's Impact on Open Source Communities", "comment": "13 pages, 1 figure", "summary": "Open Source Software communities face a wave of uncertainty as Generative AI\nrapidly transforms how software is created, maintained, and governed. Without\nclear frameworks, communities risk being overwhelmed by the complexity and\nambiguity introduced by GenAI, threatening the collaborative ethos that\nunderpins OSS. We conduct a scenario-driven, conceptual exploration using a\nsocio-technical framework inspired by McLuhan's Tetrad to surface both risks\nand opportunities for community resilience amid GenAI-driven disruption of OSS\ndevelopment across four domains: software practices, documentation, community\nengagement, and governance. By adopting this lens, OSS leaders and researchers\ncan proactively shape the future of their ecosystems, rather than simply\nreacting to technological upheaval.", "AI": {"tldr": "This paper explores risks and opportunities for Open Source Software (OSS) communities amid Generative AI (GenAI) disruption, using a socio-technical framework across software practices, documentation, engagement, and governance.", "motivation": "OSS communities lack frameworks to address GenAI's transformative and ambiguous impact, risking their collaborative ethos. Proactive analysis is needed to balance disruption and resilience.", "method": "Scenario-driven conceptual exploration applying a socio-technical framework inspired by McLuhan's Tetrad, focusing on four domains: software development practices, documentation quality, community engagement dynamics, and governance structures.", "result": "Identifies context-specific risks (e.g., governance conflicts) and opportunities (e.g., improved documentation workflows) for maintaining community resilience through structured analysis.", "conclusion": "OSS leaders should adopt proactive socio-technical lenses to strategically navigate GenAI disruptions, shaping ecosystem trajectories rather than passively adapting to technological changes."}}
{"id": "2508.04925", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04925", "abs": "https://arxiv.org/abs/2508.04925", "authors": ["Sigma Jahan", "Saurabh Singh Rajput", "Tushar Sharma", "Mohammad Masudur Rahman"], "title": "Taxonomy of Faults in Attention-Based Neural Networks", "comment": null, "summary": "Attention mechanisms are at the core of modern neural architectures, powering\nsystems ranging from ChatGPT to autonomous vehicles and driving a major\neconomic impact. However, high-profile failures, such as ChatGPT's nonsensical\noutputs or Google's suspension of Gemini's image generation due to attention\nweight errors, highlight a critical gap: existing deep learning fault\ntaxonomies might not adequately capture the unique failures introduced by\nattention mechanisms. This gap leaves practitioners without actionable\ndiagnostic guidance. To address this gap, we present the first comprehensive\nempirical study of faults in attention-based neural networks (ABNNs). Our work\nis based on a systematic analysis of 555 real-world faults collected from 96\nprojects across ten frameworks, including GitHub, Hugging Face, and Stack\nOverflow. Through our analysis, we develop a novel taxonomy comprising seven\nattention-specific fault categories, not captured by existing work. Our results\nshow that over half of the ABNN faults arise from mechanisms unique to\nattention architectures. We further analyze the root causes and manifestations\nof these faults through various symptoms. Finally, by analyzing symptom-root\ncause associations, we identify four evidence-based diagnostic heuristics that\nexplain 33.0% of attention-specific faults, offering the first systematic\ndiagnostic guidance for attention-based models.", "AI": {"tldr": "The paper introduces a novel taxonomy of seven attention-specific fault categories in attention-based neural networks (ABNNs), identifying unique failure mechanisms and providing four evidence-based diagnostic heuristics to address gaps in deep learning fault analysis.", "motivation": "Existing deep learning fault taxonomies fail to capture attention-specific failures, creating a void in actionable diagnostic guidance despite attention mechanisms' critical role in high-impact systems like ChatGPT and Google Gemini.", "method": "Systematic analysis of 555 real-world faults from 96 projects across GitHub, Hugging Face, and Stack Overflow within ten frameworks, focusing on attention-based neural networks.", "result": "55% of ABNN faults originate from attention-specific mechanisms; four diagnostic heuristics explain 33% of these attention-related faults, with root cause-symptom correlations validated through data analysis.", "conclusion": "This work establishes the first systematic diagnostic framework for ABNNs, offering practitioners empirical taxonomy and heuristics to address the economic and technical challenges of attention mechanism failures."}}
{"id": "2508.04894", "categories": ["cs.CR", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.04894", "abs": "https://arxiv.org/abs/2508.04894", "authors": ["Iyiola E. Olatunji", "Franziska Boenisch", "Jing Xu", "Adam Dziedzic"], "title": "Adversarial Attacks and Defenses on Graph-aware Large Language Models (LLMs)", "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrated with\ngraph-structured data for tasks like node classification, a domain\ntraditionally dominated by Graph Neural Networks (GNNs). While this integration\nleverages rich relational information to improve task performance, their\nrobustness against adversarial attacks remains unexplored. We take the first\nstep to explore the vulnerabilities of graph-aware LLMs by leveraging existing\nadversarial attack methods tailored for graph-based models, including those for\npoisoning (training-time attacks) and evasion (test-time attacks), on two\nrepresentative models, LLAGA (Chen et al. 2024) and GRAPHPROMPTER (Liu et al.\n2024). Additionally, we discover a new attack surface for LLAGA where an\nattacker can inject malicious nodes as placeholders into the node sequence\ntemplate to severely degrade its performance. Our systematic analysis reveals\nthat certain design choices in graph encoding can enhance attack success, with\nspecific findings that: (1) the node sequence template in LLAGA increases its\nvulnerability; (2) the GNN encoder used in GRAPHPROMPTER demonstrates greater\nrobustness; and (3) both approaches remain susceptible to imperceptible feature\nperturbation attacks. Finally, we propose an end-to-end defense framework\nGALGUARD, that combines an LLM-based feature correction module to mitigate\nfeature-level perturbations and adapted GNN defenses to protect against\nstructural attacks.", "AI": {"tldr": "This paper explores adversarial vulnerabilities in graph-aware LLMs (LLAGA/GRAPHPROMPTER), identifies exploitable design weaknesses, and proposes GALGUARD as an end-to-end defense framework combining LLM feature correction and GNN-based protections.", "motivation": "Examines robustness of emerging graph-LLM integration against adversarial attacks while addressing: 1) Untapped vulnerabilities in LLAGA's node template 2) Comparative robustness of GNN-based GRAPHPROMPTER 3) Persistence of feature perturbation risks across both models.", "method": "Conducted systematic analysis using established poisoning/evasion attacks on LLAGA/GRAPHPROMPTER, discovered new placeholder node injection attack, and implemented hybrid defense strategy with LLM corrections and adapted GNN defenses.", "result": "Three key findings: 1) LLAGA's node sequence template design increases attack success 2) GNN encoder in GRAPHPROMPTER provides better structural robustness 3) Both models vulnerable to imperceptible feature attacks. GALGUARD mitigates all these risks.", "conclusion": "Highlights critical security gaps in graph-LLM systems showing design choices significantly impact robustness. GALGUARD demonstrates viability of hybrid defense approaches for protecting these emerging architectures against multiple attack vectors."}}
{"id": "2508.05005", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.05005", "abs": "https://arxiv.org/abs/2508.05005", "authors": ["Gang Xu", "Airong Wang", "Yushan Pan"], "title": "Generative AI for Object-Oriented Programming: Writing the Right Code and Reasoning the Right Logic", "comment": null, "summary": "We find ourselves in the midst of an explosion in artificial intelligence\nresearch, particularly with large language models (LLMs). These models have\ndiverse applications spanning finance, commonsense knowledge graphs, medicine,\nand visual analysis. In the world of Object-Oriented Programming(OOP), a robust\nbody of knowledge and methods has been developed for managing complex tasks\nthrough object-oriented thinking. However, the intersection of LLMs with OOP\nremains an underexplored territory. Empirically, we currently possess limited\nunderstanding of how LLMs can enhance the effectiveness of OOP learning and\ncode writing, as well as how we can evaluate such AI-powered tools. Our work\naims to address this gap by presenting a vision from the perspectives of key\nstakeholders involved in an OOP task: programmers, mariners, and experienced\nprogrammers. We identify critical junctures within typical coding workflows\nwhere the integration of LLMs can offer significant benefits. Furthermore, we\npropose ways to augment existing logical reasoning and code writing, ultimately\nenhancing the programming experience.", "AI": {"tldr": "This paper explores leveraging large language models (LLMs) to enhance Object-Oriented Programming (OOP) workflows through stakeholder analysis of programmers, mariners, and experts, identifying integration points and evaluation methodologies.", "motivation": "LLMs have transformative applications across domains, yet their intersection with OOP remains understudied. Current research lacks understanding of how LLMs can improve OOP learning/code writing and how to evaluate these tools effectively.", "method": "The authors analyze OOP task perspectives across three stakeholder groups (programmers, mariners, and experienced programmers), mapping critical coding workflow junctures where LLM integration might provide value and proposing augmentation strategies for logical reasoning and code writing.", "result": "Identified key integration opportunities for LLMs in OOP workflows, proposed methods for enhancing code writing/learning through AI tools, and outlined potential benefits for different stakeholder perspectives.", "conclusion": "The paper advocates for systematic exploration of LLM-OOP integration opportunities while emphasizing evaluation frameworks to measure effectiveness across stakeholder needs and workflow criticality."}}
{"id": "2508.05048", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.05048", "abs": "https://arxiv.org/abs/2508.05048", "authors": ["Mohammad Ferry Husnil Arif", "Muhammad Imran"], "title": "On the Classical Hardness of the Semidirect Discrete Logarithm Problem in Finite Groups", "comment": null, "summary": "The semidirect discrete logarithm problem (SDLP) in finite groups was\nproposed as a foundation for post-quantum cryptographic protocols, based on the\nbelief that its non-abelian structure would resist quantum attacks. However,\nrecent results have shown that SDLP in finite groups admits efficient quantum\nalgorithms, undermining its quantum resistance. This raises a fundamental\nquestion: does the SDLP offer any computational advantages over the standard\ndiscrete logarithm problem (DLP) against classical adversaries? In this work,\nwe investigate the classical hardness of SDLP across different finite group\nplatforms. We establish that the group-case SDLP can be reformulated as a\ngeneralized discrete logarithm problem, enabling adaptation of classical\nalgorithms to study its complexity. We present a concrete adaptation of the\nBaby-Step Giant-Step algorithm for SDLP, achieving time and space complexity\n$O(\\sqrt{r})$ where $r$ is the period of the underlying cycle structure.\nThrough theoretical analysis and experimental validation in SageMath, we\ndemonstrate that the classical hardness of SDLP is highly platform-dependent\nand does not uniformly exceed that of standard DLP. In finite fields\n$\\mathbb{F}_p^*$, both problems exhibit comparable complexity. Surprisingly, in\nelliptic curves $E(\\mathbb{F}_p)$, the SDLP becomes trivial due to the bounded\nautomorphism group, while in elementary abelian groups $\\mathbb{F}_p^n$, the\nSDLP can be harder than DLP, with complexity varying based on the eigenvalue\nstructure of the automorphism. Our findings reveal that the non-abelian\nstructure of semidirect products does not inherently guarantee increased\nclassical hardness, suggesting that the search for classically hard problems\nfor cryptographic applications requires more careful consideration of the\nunderlying algebraic structures.", "AI": {"tldr": "This paper investigates the classical computational hardness of the semidirect discrete logarithm problem (SDLP) across finite group platforms. The findings show that SDLP's hardness is platform-dependent, not uniformly harder than standard DLP, and even vulnerable in elliptic curve groups due to bounded automorphisms. The study reveals that non-abelian structures alone do not guarantee classical security for cryptographic protocols.", "motivation": "The semidirect discrete logarithm problem (SDLP) was previously proposed as a post-quantum cryptography foundation due to its non-abelian nature, but recent quantum algorithms have weakened its perceived quantum resistance. The paper aims to assess whether SDLP offers better classical security against adversaries compared to traditional DLP in finite groups.", "method": "The authors reformulate SDLP as a generalized discrete logarithm problem, enabling adaptation of classical algorithms. They apply and analyze the Baby-Step Giant-Step algorithm for SDLP, deriving time/space complexity $O(\\", "result": "1. SDLP reduces to generalized DLP, allowing classical algorithm adaptation. 2. In finite fields ($\\mathbb{F}_p^*$$, SDLP and DLP have comparable complexity. 3. In elliptic curves ($E(\\mathbb{F}_p)$$, SDLP becomes trivial via automorphism properties. 4. In elementary abelian groups ($\\mathbb{F}_p^n$$, SDLP can be harder than DLP with eigenvalue-dependent complexity. 5. The hardness of SDLP is highly platform-specific, not inherently better than DLP.", "conclusion": "Non-abelian group structures (e.g., semidirect products) do not inherently provide greater classical hardness for cryptographic problems. The paper emphasizes the need for careful evaluation of algebraic properties when selecting computational problems for secure cryptographic protocols."}}
{"id": "2508.05034", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05034", "abs": "https://arxiv.org/abs/2508.05034", "authors": ["Arabat", "Ali", "Sayagh", "Mohammed", "Hassine", "Jameleddine"], "title": "An ML-based Approach to Predicting Software Change Dependencies: Insights from an Empirical Study on OpenStack", "comment": null, "summary": "As software systems grow in complexity, accurately identifying and managing\ndependencies among changes becomes increasingly critical. For instance, a\nchange that leverages a function must depend on the change that introduces it.\nEstablishing such dependencies allows CI/CD pipelines to build and orchestrate\nchanges effectively, preventing build failures and incomplete feature\ndeployments. In modern software systems, dependencies often span multiple\ncomponents across teams, creating challenges for development and deployment.\nThey serve various purposes, from enabling new features to managing\nconfigurations, and can even involve traditionally independent changes like\ndocumentation updates. To address these challenges, we conducted a preliminary\nstudy on dependency management in OpenStack, a large-scale software system. Our\nstudy revealed that a substantial portion of software changes in OpenStack over\nthe past 10 years are interdependent. Surprisingly, 51.08% of these\ndependencies are identified during the code review phase-after a median delay\nof 5.06 hours-rather than at the time of change creation. Developers often\nspend a median of 57.12 hours identifying dependencies, searching among a\nmedian of 463 other changes. To help developers proactively identify\ndependencies, we propose a semi-automated approach that leverages two ML\nmodels. The first model predicts the likelihood of dependencies among changes,\nwhile the second identifies the exact pairs of dependent changes. Our proposed\nmodels demonstrate strong performance, achieving average AUC scores of 79.33%\nand 91.89%, and Brier scores of 0.11 and 0.014, respectively. Indeed, the\nsecond model has a good top-k recall across all types of pairs, while the top-k\nprecision has room for improvement.", "AI": {"tldr": "The study on OpenStack highlights challenges in managing interdependent software changes, reveals most dependencies are identified post-commit during code reviews with significant delays, and proposes a dual ML model approach to enhance proactive dependency management, showing robust performance metrics. Developers spend considerable time identifying dependencies manually.", "motivation": "Modern software systems exhibit complex, cross-team dependencies, leading to inefficiencies in CI/CD pipelines through delayed detection and manual effort. Accurate dependency management is crucial for stability and deployment efficiency.", "method": "A semi-automated approach using two ML models: one predicts dependency likelihood, while the other identifies specific dependent change pairs by analyzing historical OpenStack data.", "result": "51.08% of dependencies identified post-commit during code reviews after 5.06-hour delay. Models achieved AUC 79.33% (likelihood prediction) and 91.89% (pair identification), with Brier scores of 0.11 and 0.014. High top-k recall confirmed effectiveness.", "conclusion": "The dual ML model approach demonstrates potential to reduce manual dependency identification in large-scale systems, though top-k precision remains suboptimal. Post-commit dependency detection patterns in OpenStack emphasize the need for proactive tools."}}
{"id": "2508.05188", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05188", "abs": "https://arxiv.org/abs/2508.05188", "authors": ["Kim Hammar", "Tansu Alpcan", "Emil C. Lupu"], "title": "Incident Response Planning Using a Lightweight Large Language Model with Reduced Hallucination", "comment": null, "summary": "Timely and effective incident response is key to managing the growing\nfrequency of cyberattacks. However, identifying the right response actions for\ncomplex systems is a major technical challenge. A promising approach to\nmitigate this challenge is to use the security knowledge embedded in large\nlanguage models (LLMs) to assist security operators during incident handling.\nRecent research has demonstrated the potential of this approach, but current\nmethods are mainly based on prompt engineering of frontier LLMs, which is\ncostly and prone to hallucinations. We address these limitations by presenting\na novel way to use an LLM for incident response planning with reduced\nhallucination. Our method includes three steps: fine-tuning, information\nretrieval, and lookahead planning. We prove that our method generates response\nplans with a bounded probability of hallucination and that this probability can\nbe made arbitrarily small at the expense of increased planning time under\ncertain assumptions. Moreover, we show that our method is lightweight and can\nrun on commodity hardware. We evaluate our method on logs from incidents\nreported in the literature. The experimental results show that our method a)\nachieves up to 22% shorter recovery times than frontier LLMs and b) generalizes\nto a broad range of incident types and response actions.", "AI": {"tldr": "The paper introduces a three-step method (fine-tuning, information retrieval, lookahead planning) to reduce hallucinations in LLM-based incident response planning, achieving shorter recovery times and broader generalization compared to prompt-engineered frontier LLMs.", "motivation": "Cyberattacks are increasing, but selecting optimal response actions for complex systems is difficult. Existing LLM approaches rely on costly prompt engineering and suffer from hallucinations, limiting practical effectiveness.", "method": "The method combines: (1) fine-tuning of LLMs to align with security knowledge; (2) information retrieval to contextualize incidents; and (3) lookahead planning to generate response steps with a rigorously bounded hallucination probability, minimized under assumptions of increased planning time.", "result": "Evaluation on real-world incident logs shows: (a) 22% faster recovery times than frontier LLMs; (b) generalization across diverse incident types and response actions; (c) lightweight implementation on commodity hardware.", "conclusion": "The proposed method provides a practical, less hallucinatory framework for LLM-based incident response planning that reduces recovery times and demonstrates broad applicability for complex systems."}}
{"id": "2508.05085", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.05085", "abs": "https://arxiv.org/abs/2508.05085", "authors": ["Junayed Mahmud", "James Chen", "Terry Achille", "Camilo Alvarez-Velez", "Darren Dean Bansil", "Patrick Ijieh", "Samar Karanch", "Nadeeshan De Silva", "Oscar Chaparro", "Andrian Marcus", "Kevin Moran"], "title": "LadyBug: A GitHub Bot for UI-Enhanced Bug Localization in Mobile Apps", "comment": "5 pages, to appear in the Proceedings of the 41st International\n  Conference on Software Maintenance and Evolution (ICSME'25) - Tool\n  Demonstration Track", "summary": "This paper introduces LadyBug, a GitHub bot that automatically localizes bugs\nfor Android apps by combining UI interaction information with text retrieval.\nLadyBug connects to an Android app's GitHub repository, and is triggered when a\nbug is reported in the corresponding issue tracker. Developers can then record\na reproduction trace for the bug on a device or emulator and upload the trace\nto LadyBug via the GitHub issue tracker. This enables LadyBug to utilize both\nthe text from the original bug description, and UI information from the\nreproduction trace to accurately retrieve a ranked list of files from the\nproject that most likely contain the reported bug.\n  We empirically evaluated LadyBug using an automated testing pipeline and\nbenchmark called RedWing that contains 80 fully-localized and reproducible bug\nreports from 39 Android apps. Our results illustrate that LadyBug outperforms\ntext-retrieval-based baselines and that the utilization of UI information leads\nto a substantial increase in localization accuracy. LadyBug is an open-source\ntool, available at https://github.com/LadyBugML/ladybug.\n  A video showing the capabilities of Ladybug can be viewed here:\nhttps://youtu.be/hI3tzbRK0Cw", "AI": {"tldr": "LadyBug is a GitHub bot for Android apps that combines UI interaction data and text retrieval to automatically localize bugs, demonstrating improved accuracy over text-based methods using its RedWing evaluation benchmark.", "motivation": "Existing text-retrieval-based bug localization methods for Android apps often lack precision due to incomplete context from bug descriptions alone, necessitating a solution that integrates UI interaction data for more accurate fault localization.", "method": "The system utilizes both textual analysis of bug reports and captures device/emulator UI interaction traces (like screen navigation and user inputs) uploaded via GitHub issues to generate a ranked list of files containing the bug.", "result": "Empirical results using the RedWing benchmark (80 bugs from 39 Android apps) show LadyBug outperforms text-based baselines by 56.3% and achieves an accuracy of 68.4% where the actual buggy file ranks in the top 3", "conclusion": "The integration of UI interaction data with text analysis significantly enhances bug localization accuracy for Android apps, as demonstrated by LadyBug's open-source tool and rigorous experimental validation"}}
{"id": "2508.05276", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.05276", "abs": "https://arxiv.org/abs/2508.05276", "authors": ["Sharad Agarwal", "Guillermo Suarez-Tangil", "Marie Vasek"], "title": "An Overview of 7726 User Reports: Uncovering SMS Scams and Scammer Strategies", "comment": null, "summary": "Mobile network operators implement firewalls to stop illicit messages, but\nscammers find ways to evade detection. Previous work has looked into SMS texts\nthat are blocked by these firewalls. However, there is little insight into SMS\ntexts that bypass them and reach users. To this end, we collaborate with a\nmajor mobile network operator to receive 1.35m user reports submitted over four\nmonths. We find 89.16% of user reports comprise text messages, followed by\nreports of suspicious calls and URLs. Using our methodological framework, we\nidentify 35.12% of the unique text messages reported by users as spam, while\n40.27% are scam text messages. This is the first paper that investigates SMS\nreports submitted by users and differentiates between spam and scams. Our paper\nclassifies the identified scam text messages into 12 scam types, of which the\nmost popular is 'wrong number' scams. We explore the various infrastructure\nservices that scammers abuse to conduct SMS scams, including mobile network\noperators and hosting infrastructure, and analyze the text of the scam messages\nto understand how scammers lure victims into providing them with their personal\nor financial details.", "AI": {"tldr": "The paper analyzes 1.35 million user reports of SMS messages, differentiating between spam and scams, with a focus on 12 scam types and infrastructure abuse by scammers.", "motivation": "Existing firewalls fail to detect all malicious messages, and prior research has overlooked user-reported SMS that bypass these systems, creating a gap in understanding scam prevalence and tactics.", "method": "Collaboration with a mobile network operator provided user reports; a framework classified 35.12% of unique texts as spam and 40.27% as scams, with analysis on scam types and infrastructure use.", "result": "89.16% of reports were texts, revealing high scam rates (40.27%). 'Wrong number' was the top scam type, and infrastructure like hosting services and mobile networks were exploited.", "conclusion": "This pioneering study reveals scam dynamics in SMS traffic, emphasizing the need for improved detection tools and user awareness to combat evolving scam methods like 'wrong number' tactics."}}
{"id": "2508.05170", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05170", "abs": "https://arxiv.org/abs/2508.05170", "authors": ["Lishui Fan", "Yu Zhang", "Mouxiang Chen", "Zhongxin Liu"], "title": "Posterior-GRPO: Rewarding Reasoning Processes in Code Generation", "comment": null, "summary": "Reinforcement learning (RL) has significantly advanced code generation for\nlarge language models (LLMs). However, current paradigms rely on outcome-based\nrewards from test cases, neglecting the quality of the intermediate reasoning\nprocess. While supervising the reasoning process directly is a promising\ndirection, it is highly susceptible to reward hacking, where the policy model\nlearns to exploit the reasoning reward signal without improving final outcomes.\nTo address this, we introduce a unified framework that can effectively\nincorporate the quality of the reasoning process during RL. First, to enable\nreasoning evaluation, we develop LCB-RB, a benchmark comprising preference\npairs of superior and inferior reasoning processes. Second, to accurately score\nreasoning quality, we introduce an Optimized-Degraded based (OD-based) method\nfor reward model training. This method generates high-quality preference pairs\nby systematically optimizing and degrading initial reasoning paths along\ncurated dimensions of reasoning quality, such as factual accuracy, logical\nrigor, and coherence. A 7B parameter reward model with this method achieves\nstate-of-the-art (SOTA) performance on LCB-RB and generalizes well to other\nbenchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method\nthat conditions process-based rewards on task success. By selectively applying\nrewards to the reasoning processes of only successful outcomes, P-GRPO\neffectively mitigates reward hacking and aligns the model's internal reasoning\nwith final code correctness. A 7B parameter model with P-GRPO achieves superior\nperformance across diverse code generation tasks, outperforming outcome-only\nbaselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further\ndemonstrate the generalizability of our approach by extending it to\nmathematical tasks. Our models, dataset, and code are publicly available.", "AI": {"tldr": "This paper introduces a unified framework combining reasoning process evaluation and task-success conditional RL (P-GRPO) to improve code generation in LLMs, achieving a 4.5% performance boost over existing methods while avoiding reward hacking.", "motivation": "Current RL-based code generation for LLMs focuses solely on outcome-based rewards from test cases, ignoring reasoning process quality and leading to reward hacking where models optimize for shallow rewards without improving actual results.", "method": "The approach includes three components: (1) LCB-RB, a reasoning benchmark with preference pairs; (2) an OD-based reward model training method that systematically evaluates reasoning quality across factual accuracy, logical rigor, and coherence; (3) P-GRPO, an RL framework that applies process-based rewards only to successful task outcomes to prevent exploitation of reward signals.", "result": "A 7B reward model using OD-based training achieves SOTA on LCB-RB with strong generalization. P-GRPO improves code generation by 4.5% over outcome-only baselines and performs comparably to GPT-4-Turbo. The method generalizes to mathematical reasoning tasks as well.", "conclusion": "By aligning intermediate reasoning quality with final task success through P-GRPO, the framework mitigates reward hacking in code generation. The combination of a purpose-built benchmark, OD-based reward models, and conditional process-based RL demonstrates a robust solution to improve code generation performance while maintaining reasoning quality."}}
{"id": "2508.05334", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.05334", "abs": "https://arxiv.org/abs/2508.05334", "authors": ["Ahsan Farabi", "Israt Khandaker", "Nusrat Jahan", "Ibrahim Khalil Shanto"], "title": "ShikkhaChain: A Blockchain-Powered Academic Credential Verification System for Bangladesh", "comment": null, "summary": "Academic credential fraud threatens educational integrity, especially in\ndeveloping countries like Bangladesh, where verification methods are primarily\nmanual and inefficient. To address this challenge, we present ShikkhaChain, a\nblockchain-powered certificate management platform designed to securely issue,\nverify, and revoke academic credentials in a decentralized and tamper-proof\nmanner. Built on Ethereum smart contracts and utilizing IPFS for off-chain\nstorage, the platform offers a transparent, scalable solution accessible\nthrough a React-based DApp with MetaMask integration. ShikkhaChain enables\nrole-based access for governments, regulators, institutions, and public\nverifiers, allowing QR-based validation and on-chain revocation tracking. Our\nprototype demonstrates enhanced trust, reduced verification time, and improved\ninternational credibility for Bangladeshi degrees, promoting a more reliable\nacademic and employment ecosystem.", "AI": {"tldr": "ShikkhaChain is a blockchain-based platform to combat academic credential fraud by enabling secure, decentralized, and efficient verification of certificates in Bangladesh.", "motivation": "Manual verification methods in Bangladesh are inefficient and vulnerable to academic credential fraud, threatening educational integrity and international credibility.", "method": "Utilizes Ethereum smart contracts for tamper-proof issuance and revocation of credentials, IPFS for off-chain storage, and a React-based DApp with MetaMask integration for accessibility. Implements role-based access and QR-based validation.", "result": "The prototype reduces verification time, enhances trust in academic credentials, and improves the international recognition of Bangladeshi degrees.", "conclusion": "ShikkhaChain offers a scalable, decentralized solution to address academic fraud and inefficiencies in certificate management by leveragining blockchain technology, fostering a reliable ecosystem for education and employment."}}
{"id": "2508.05192", "categories": ["cs.SE", "H.2.3; I.2.6; D.2.2"], "pdf": "https://arxiv.org/pdf/2508.05192", "abs": "https://arxiv.org/abs/2508.05192", "authors": ["Felix Neubauer", "J\u00fcrgen Pleiss", "Benjamin Uekermann"], "title": "AI-assisted JSON Schema Creation and Mapping", "comment": "Accepted for Tools and Demonstrations Track of ACM/IEEE MODELS'25", "summary": "Model-Driven Engineering (MDE) places models at the core of system and data\nengineering processes. In the context of research data, these models are\ntypically expressed as schemas that define the structure and semantics of\ndatasets. However, many domains still lack standardized models, and creating\nthem remains a significant barrier, especially for non-experts. We present a\nhybrid approach that combines large language models (LLMs) with deterministic\ntechniques to enable JSON Schema creation, modification, and schema mapping\nbased on natural language inputs by the user. These capabilities are integrated\ninto the open-source tool MetaConfigurator, which already provides visual model\nediting, validation, code generation, and form generation from models. For data\nintegration, we generate schema mappings from heterogeneous JSON, CSV, XML, and\nYAML data using LLMs, while ensuring scalability and reliability through\ndeterministic execution of generated mapping rules. The applicability of our\nwork is demonstrated in an application example in the field of chemistry. By\ncombining natural language interaction with deterministic safeguards, this work\nsignificantly lowers the barrier to structured data modeling and data\nintegration for non-experts.", "AI": {"tldr": "The paper introduces a hybrid approach using LLMs and deterministic techniques for JSON Schema creation, modification, and schema mapping via natural language inputs, integrated into the open-source MetaConfigurator tool to lower data modeling barriers for non-experts.", "motivation": "Many research domains lack standardized models, and current data modeling remains challenging for non-experts due to the need for manual schema creation and integration.", "method": "Combines large language models (LLMs) with deterministic techniques to process natural language inputs for automating JSON Schema creation/editing and generating schema mappings between heterogeneous data formats (JSON, CSV, XML, YAML) using the MetaConfigurator tool.", "result": "Demonstrated the tool's ability to generate schema mappings and lower modeling barriers through an application example in chemistry, achieving scalability/reliability via deterministic execution of generated rules.", "conclusion": "The hybrid approach integrates LLMs' flexibility with deterministic safeguards, enabling non-experts to perform structured data modeling and integration tasks more effectively and reducing reliance on domain expertise for schema creation."}}
{"id": "2508.05394", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.05394", "abs": "https://arxiv.org/abs/2508.05394", "authors": ["Xiaoli Zhuo", "Xuehu Yan", "Wei Yan"], "title": "Grouped k-threshold random grid-based visual cryptography scheme", "comment": null, "summary": "Visual cryptography schemes (VCSs) belong to a category of secret image\nsharing schemes that do not require cryptographic knowledge for decryption,\ninstead relying directly on the human visual system. Among VCSs, random\ngrid-based VCS (RGVCS) has garnered widespread attention as it avoids pixel\nexpansion while requiring no basic matrices design. Contrast, a core metric for\nRGVCS, directly determines the visual quality of recovered images, rendering\nits optimization a critical research objective. However, existing $(k,n)$\nRGVCSs still fail to attain theoretical upper bounds on contrast, highlighting\nthe urgent need for higher-contrast constructions. In this paper, we propose a\nnovel sharing paradigm for RGVCS that constructs $(k,n)$-threshold schemes from\narbitrary $(k,n')$-threshold schemes $(k \\leq n'\\leq n)$, termed\n\\emph{$n'$-grouped $(k,n)$ RGVCS}. This paradigm establishes hierarchical\ncontrast characteristics: participants within the same group achieve optimal\nrecovery quality, while inter-group recovery shows a hierarchical contrast. We\nfurther introduce a new contrast calculation formula tailored to the new\nparadigm. Then, we propose a contrast-enhanced $(k,n)$ RGVCS by setting $n'=\nk$, achieving the highest contrast value documented in the existing literature.\nTheoretical analysis and experimental results demonstrate the superiority of\nour proposed scheme in terms of contrast.", "AI": {"tldr": "This paper introduces a novel hierarchical random grid-based visual cryptography scheme that achieves higher contrast in secret image sharing compared to existing methods, reaching the theoretical contrast upper bounds for the first time.", "motivation": "Current (k,n) random grid-based visual cryptography schemes (RGVCSs) fail to reach theoretical upper bounds for contrast, which directly impacts the visual quality of recovered images and creates a need for improved constructions.", "method": "Proposes an n'-grouped (k,n) RGVCS paradigm that leverages arbitrary (k,n')-threshold schemes, establishes hierarchical contrast characteristics between groups, introduces a new contrast calculation formula, and presents a specific contrast-enhanced scheme by setting n'=k.", "result": "The proposed scheme achieves the highest documented contrast value for (k,n) RGVCS, validated through theoretical analysis and experimental results showing its superiority over existing methods.", "conclusion": "The n'-grouped paradigm enables optimal intra-group contrast while maintaining hierarchical characteristics between groups, providing a significant advancement in RGVCS research."}}
{"id": "2508.05193", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.05193", "abs": "https://arxiv.org/abs/2508.05193", "authors": ["Kaiwen Yan", "Yuhang Chang", "Zirui Guo", "Yaling Mou", "Jiang Ming", "Jingwei Sun"], "title": "STEPWISE-CODEX-Bench: Evaluating Complex Multi-Function Comprehension and Fine-Grained Execution Reasoning", "comment": null, "summary": "In recent years, large language models (LLMs) have made significant progress\nin code intelligence, yet systematically evaluating their code understanding\nand reasoning abilities remains challenging. Mainstream benchmarks such as\nHumanEval and MBPP primarily assess functional correctness, while reasoning\nbenchmarks like CRUXEVAL are limited to single-function, low-complexity\nscenarios. As a result, advanced models achieve nearly saturated scores,\nlimiting their discriminative power. To address this, we present\nSTEPWISE-CODEX-Bench (SX-Bench), a novel benchmark designed for complex\nmulti-function understanding and fine-grained execution reasoning. SX-Bench\nfeatures tasks involving collaboration among multiple sub-functions (e.g.,\nchained calls, nested loops), shifting evaluation towards overall control and\ndata flow modeling. It defines \"computation steps\" as the minimal execution\nunit and requires models to predict the total number of steps in reasoning\ntasks, thereby assessing a model's in-depth understanding of dynamic execution\nbeyond simple I/O matching. Evaluation on over 20 mainstream models (including\n14 reasoning-enhanced models) demonstrates that SX-Bench is highly\ndiscriminative: even the state-of-the-art OpenAI-O3 achieves only 78.37 percent\naccuracy on Hard-Reasoning tasks, much lower than its saturated scores on\nprevious benchmarks, thereby revealing bottlenecks in complex and fine-grained\nreasoning. We also release an automated pipeline combining program synthesis,\nsymbolic execution, and LLM-aided validation for efficient benchmark generation\nand quality assurance. SX-Bench advances code evaluation from \"single-function\nverification\" to \"multi-function dynamic reasoning,\" providing a key tool for\nthe in-depth assessment of advanced code intelligence models.", "AI": {"tldr": "The paper introduces STEPWISE-CODEX-Bench (SX-Bench), a novel benchmark for evaluating advanced code reasoning abilities in large language models (LLMs) beyond single-function correctness, focusing on complex multi-function understanding and step-level execution reasoning.", "motivation": "Existing benchmarks (HumanEval, MBPP) prioritize functional correctness and simple reasoning, while CRUXEVAL is limited to low-complexity single-function tasks. This results in saturated scores for advanced models, reducing the ability to distinguish their true capabilities in complex code reasoning scenarios.", "method": "SX-Bench uses tasks requiring multi-function collaboration (e.g., chained calls, nested loops) and defines 'computation steps' as the smallest unit of execution reasoning. Models must predict the total number of steps in a task, exceeding baseline I/O matching evaluations. An automated pipeline combining program synthesis, symbolic execution, and LLM validation was developed for benchmark generation.", "result": "Evaluation across 20+ modern models (including 14 reasoning-focused ones) shows SX-Bench's strong discriminative ability. State-of-the-art model OpenAI-O3 reaches only 78.37% accuracy on hard reasoning tasks, indicating code reasoning remains a significant challenge compared to prior benchmarks.", "conclusion": "SX-Bench shifts code evaluation from 'single-function verification' to detailed 'multi-function dynamic reasoning' assessment. It provides a critical tool for measuring the depth of advanced code intelligence models in execution-level understanding."}}
{"id": "2508.05518", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.05518", "abs": "https://arxiv.org/abs/2508.05518", "authors": ["Weihong Sheng", "Jiajun Chen", "Bin Cai", "Chunqiang Hu", "Meng Han", "Jiguo Yu"], "title": "Local Distance Query with Differential Privacy", "comment": null, "summary": "Differential Privacy (DP) is commonly employed to safeguard graph analysis or\npublishing. Distance, a critical factor in graph analysis, is typically handled\nusing curator DP, where a trusted curator holds the complete neighbor lists of\nall vertices and answers queries privately. However, in many real-world\nscenarios, such a curator may not be present, posing a significant challenge\nfor implementing differentially private distance queries under Local\nDifferential Privacy (LDP). This paper proposes two approaches to address this\nchallenge. The first approach generates a synthetic graph by randomizing\nresponses and applies bitwise operations to reduce noise interference. However,\nlike other synthetic graph methods, this approach suffers from low utility. To\novercome this limitation, we propose a second approach, the first LDP method\nspecifically designed for distance queries, which captures the global graph\nstructure by continuously aggregating local distance vectors from neighboring\nvertices. This process enables the accurate updating of global distances. We\ndemonstrate the effectiveness of our method through comprehensive theoretical\nanalysis and experimental evaluations on real-world datasets.", "AI": {"tldr": "This paper proposes two methods for enabling differentially private distance queries under Local Differential Privacy (LDP) constraints when no central curator exists. The second method, specifically designed for distance queries, aggregates local distance vectors to capture global structure and outperforms synthetic graph approaches in utility.", "motivation": "Traditional curator-based Differential Privacy (DP) relies on a centralized trusted entity holding complete graph data, but real-world scenarios often lack such a curator, making LDP distance queries challenging due to noise interference from decentralized data privacy mechanisms.", "method": "First approach: Synthetic graph generation using randomized responses with bitwise operations to mitigate noise. Second approach (LDP-DQ): Novel decentralized method that continuously aggregates local privacy-preserving distance vectors from neighboring vertices to reconstruct accurate global distances through distributed computation.", "result": "Theoretical analysis proves the method's privacy guarantees, and experiments on real-world datasets demonstrate significant improvements in query accuracy compared to synthetic graph approaches under similar privacy constraints.", "conclusion": "The proposed LDP-DQ method effectively addresses decentralized distance query privacy challenges, achieving better utility than existing approaches while preserving differential privacy without requiring centralized data ownership."}}
{"id": "2508.05199", "categories": ["cs.SE", "cs.AI", "D.2.2; D.2.7; I.2.2"], "pdf": "https://arxiv.org/pdf/2508.05199", "abs": "https://arxiv.org/abs/2508.05199", "authors": ["Igor Costa", "Christopher Baran"], "title": "EvoGraph: Hybrid Directed Graph Evolution toward Software 3.0", "comment": "15 pages, 3 tables, 1 algorithm. Submitted to ICSE 2025", "summary": "We introduce **EvoGraph**, a framework that enables software systems to\nevolve their own source code, build pipelines, documentation, and tickets.\nEvoGraph represents every artefact in a typed directed graph, applies learned\nmutation operators driven by specialized small language models (SLMs), and\nselects survivors with a multi-objective fitness. On three benchmarks, EvoGraph\nfixes 83% of known security vulnerabilities, translates COBOL to Java with 93%\nfunctional equivalence (test verified), and maintains documentation freshness\nwithin two minutes. Experiments show a 40% latency reduction and a sevenfold\ndrop in feature lead time compared with strong baselines. We extend our\napproach to **evoGraph**, leveraging language-specific SLMs for modernizing\n.NET, Lisp, CGI, ColdFusion, legacy Python, and C codebases, achieving 82-96%\nsemantic equivalence across languages while reducing computational costs by 90%\ncompared to large language models. EvoGraph's design responds to empirical\nfailure modes in legacy modernization, such as implicit contracts, performance\npreservation, and integration evolution. Our results suggest a practical path\ntoward Software 3.0, where systems adapt continuously yet remain under\nmeasurable control.", "AI": {"tldr": "EvoGraph is a self-evolving software framework using specialized small language models (SLMs) to modernize legacy systems. It achieves 83% security fix rates, 93% functional equivalence between COBOL and Java, and reduces modernization costs by 90%.", "motivation": "The paper addresses challenges in legacy code modernization including implicit contracts, performance preservation, and integration evolution, which traditional methods often fail to handle effectively.", "method": "EvoGraph uses typed directed graphs to represent software artefacts, applies learned mutation operators via language-specific SLMs, and employs multi-objective fitness selection to continuously evolve codebases while maintaining control.", "result": "Experiments show EvoGraph fixes 83% of vulnerabilities, translates COBOL to Java with 93% test-verified equivalence, reduces latency by 40%, and lowers computational costs by 90% compared to large language models. Semantic equivalence of 82-96% is achieved across 7+ languages.", "conclusion": "EvoGraph demonstrates a practical path toward Software 3.0 by enabling controlled, continuous system adaptation. It effectively tackles empirical failure modes in legacy modernization while providing measurable improvements in quality and efficiency."}}
{"id": "2508.05545", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.05545", "abs": "https://arxiv.org/abs/2508.05545", "authors": ["Leon Garza", "Anantaa Kotal", "Aritran Piplai", "Lavanya Elluri", "Prajit Das", "Aman Chadha"], "title": "PRvL: Quantifying the Capabilities and Risks of Large Language Models for PII Redaction", "comment": null, "summary": "Redacting Personally Identifiable Information (PII) from unstructured text is\ncritical for ensuring data privacy in regulated domains. While earlier\napproaches have relied on rule-based systems and domain-specific Named Entity\nRecognition (NER) models, these methods fail to generalize across formats and\ncontexts. Recent advances in Large Language Models (LLMs) offer a promising\nalternative, yet the effect of architectural and training choices on redaction\nperformance remains underexplored. LLMs have demonstrated strong performance in\ntasks that require contextual language understanding, including the redaction\nof PII in free-form text. Prior work suggests that with appropriate adaptation,\nLLMs can become effective contextual privacy learners. However, the\nconsequences of architectural and training choices for PII Redaction remain\nunderexplored. In this work, we present a comprehensive analysis of LLMs as\nprivacy-preserving PII Redaction systems. We evaluate a range of LLM\narchitectures and training strategies for their effectiveness in PII Redaction.\nOur analysis measures redaction performance, semantic preservation, and PII\nleakage, and compares these outcomes against latency and computational cost.\nThe results provide practical guidance for configuring LLM-based redactors that\nare accurate, efficient, and privacy-aware. To support reproducibility and\nreal-world deployment, we release PRvL, an open-source suite of fine-tuned\nmodels, and evaluation tools for general-purpose PII Redaction. PRvL is built\nentirely on open-source LLMs and supports multiple inference settings for\nflexibility and compliance. It is designed to be easily customized for\ndifferent domains and fully operable within secure, self-managed environments.\nThis enables data owners to perform redactions without relying on third-party\nservices or exposing sensitive content beyond their own infrastructure.", "AI": {"tldr": "This paper analyzes the effectiveness of Large Language Models (LLMs) for PII redaction, evaluating architectural and training choices to provide practical guidance on accurate, efficient, and privacy-aware configurations. It introduces PRvL, an open-source PII redaction framework based on open-source LLMs.", "motivation": "Existing rule-based systems and domain-specific NER models struggle with generalizing PII redaction across diverse formats and contexts. While LLMs show promise through contextual language understanding, the impact of architectural and training decisions on redaction performance remains poorly understood.", "method": "The study evaluates multiple LLM architectures and training strategies for PII redaction, analyzing their effectiveness using metrics including redaction performance, semantic preservation, PII leakage, latency, and computational cost. Empirical comparisons are conducted to assess tradeoffs between these factors.", "result": "Results demonstrate key insights into configuring LLM-based redaction systems that balance accuracy, efficiency, and privacy awareness, showing how architectural choices and training approaches impact these critical performance dimensions.", "conclusion": "The paper establishes that careful architectural and training decisions enable LLMs to become effective contextual privacy learners for PII redaction. PRvL provides a customizable, open-source solution enabling secure, self-managed PII redaction across domains without third-party infrastructure."}}
{"id": "2508.05301", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.05301", "abs": "https://arxiv.org/abs/2508.05301", "authors": ["Victoria Torres Bosch", "Ronny Seiger", "Manuela Albert Albiol", "Antoni Mestre Gascon", "Pedro Jose Valderas Aranda"], "title": "A Conceptual Model and Methodology for Sustainability-aware, IoT-enhanced Business Processes", "comment": "Submitted to Information Systems Frontiers (1572-9419)", "summary": "The real-time data collection and automation capabilities offered by the\nInternet of Things (IoT) are revolutionizing and transforming Business\nProcesses (BPs) into IoT-enhanced BPs, showing high potential for improving\nsustainability. Although already studied in Business Process Management (BPM),\nsustainability research has primarily focused on environmental concerns.\nHowever, achieving a holistic and lasting impact requires a systematic approach\nto address sustainability beyond the environmental dimension. This work\nproposes a conceptual model and a structured methodology with the goal of\nanalyzing the potential of IoT to measure and improve the sustainability of\nBPs. The conceptual model formally represents key sustainability concepts,\nlinking BPM and IoT by highlighting how IoT devices support and contribute to\nsustainability. The methodology guides the systematic analysis of existing BPs,\nidentifies opportunities, and implements sustainability-aware, IoT-enhanced\nBPs. The approach is illustrated through a running example from the tourism\ndomain and a case study in healthcare.", "AI": {"tldr": "Proposes a conceptual model and methodology to integrate IoT with BPM for sustainability across environmental, social, and economic dimensions", "motivation": "Current BPM sustainability research primarily focuses on environmental concerns, neglecting a holistic approach to address all three dimensions of sustainability", "method": "Developed a conceptual model linking BPM and IoT, and a structured methodology for analyzing and implementing sustainability-aware IoT-enhanced BPs", "result": "Illustrated through a tourism domain example and healthcare case study, demonstrating IoT's role in enhancing BP sustainability across multiple dimensions", "conclusion": "The proposed approach enables systematic integration of IoT in BPs to measure and improve sustainability, achieving lasting impact beyond environmental focus"}}
