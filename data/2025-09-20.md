<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 21]
- [cs.SE](#cs.SE) [Total: 21]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Early Approaches to Adversarial Fine-Tuning for Prompt Injection Defense: A 2022 Study of GPT-3 and Contemporary Models](https://arxiv.org/abs/2509.14271)
*Gustavo Sandoval,Denys Fenchenko,Junyao Chen*

Main category: cs.CR

TL;DR: A paper on Adversarial Fine-Tuning for defending against prompt injection attacks in LLMs using GPT-3 series models, showing attack success dropping from 31	emplate: %} to near zero with smaller models, prioritizing legacy systems with production constraints.


<details>
  <summary>Details</summary>
Motivation: As LLMs become prevalent, defending against prompt injections is crucial to maintain model integrity and ensure reliable user interactions.

Method: Researchers tested prompt injection and goal hijacking by crafting and executing these attacks on various LLMs and conducted Adversarial Fine-Tuning as a defense strategy which involved reinforcing the models with attack-related examples.

Result: Using Adversarial Fine-Tuning significantly lowered the success rate of attacks on GPT-3's smaller models (Ada, Babbage, Curie) to nearly 0	emplate: %, while GPT-3 Davinci showed higher vulnerability.	emplate: % The results hence demonstrated the effectiveness of the proposed defense.

Conclusion: The paper established an early foundation for robust defenses against prompt injection by showing the effectiveness of Adversarial Fine-Tuning, yet the limitations indicated that more dynamic methods like instruction hierarchy and constitutional AI which supersedes this are necessary as models evolve.

Abstract: This paper documents early research conducted in 2022 on defending against
prompt injection attacks in large language models, providing historical context
for the evolution of this critical security domain. This research focuses on
two adversarial attacks against Large Language Models (LLMs): prompt injection
and goal hijacking. We examine how to construct these attacks, test them on
various LLMs, and compare their effectiveness. We propose and evaluate a novel
defense technique called Adversarial Fine-Tuning. Our results show that,
without this defense, the attacks succeeded 31\% of the time on GPT-3 series
models. When using our Adversarial Fine-Tuning approach, attack success rates
were reduced to near zero for smaller GPT-3 variants (Ada, Babbage, Curie),
though we note that subsequent research has revealed limitations of
fine-tuning-based defenses. We also find that more flexible models exhibit
greater vulnerability to these attacks. Consequently, large models such as
GPT-3 Davinci are more vulnerable than smaller models like GPT-2. While the
specific models tested are now superseded, the core methodology and empirical
findings contributed to the foundation of modern prompt injection defense
research, including instruction hierarchy systems and constitutional AI
approaches.

</details>


### [2] [FedMentor: Domain-Aware Differential Privacy for Heterogeneous Federated LLMs in Mental Health](https://arxiv.org/abs/2509.14275)
*Nobin Sarwar,Shubhashis Roy Dipta*

Main category: cs.CR

TL;DR: FedMentor is a privacy-preserving framework for fine-tuning LLMs in sensitive domains like mental health. It uses federated learning with LoRA and dynamic domain-aware DP to achieve safer outputs (+3 accuracy), reduced toxicity, and near-baseline utility while supporting large models with efficient communication.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of adapting Large Language Models (LLMs) for sensitive domains (e.g., mental health) while maintaining strict privacy, safety, and model utility in compliance with domain-specific confidentiality requirements.

Method: FedMentor combines Low-Rank Adaptation (LoRA) with domain-aware Differential Privacy (DP) in a federated framework. Each client applies customized DP noise scaling based on data sensitivity, while the server dynamically reduces noise to maintain utility thresholds.

Result: FedMentor improves safe output rates by 3 points, reduces toxicity, and maintains BERTScore F1/ROUGE-L within 0.5% of non-private baselines. It scales to 1.7B-parameter models with <173 MB per training round and matches centralized performance bounds.

Conclusion: FedMentor demonstrates a practical approach for privacy-preserving LLM adaptation in sensitive domains, achieving safety improvements while balancing utility and scalability in healthcare applications.

Abstract: Privacy-preserving adaptation of Large Language Models (LLMs) in sensitive
domains (e.g., mental health) requires balancing strict confidentiality with
model utility and safety. We propose FedMentor, a federated fine-tuning
framework that integrates Low-Rank Adaptation (LoRA) and domain-aware
Differential Privacy (DP) to meet per-domain privacy budgets while maintaining
performance. Each client (domain) applies a custom DP noise scale proportional
to its data sensitivity, and the server adaptively reduces noise when utility
falls below a threshold. In experiments on three mental health datasets, we
show that FedMentor improves safety over standard Federated Learning without
privacy, raising safe output rates by up to three points and lowering toxicity,
while maintaining utility (BERTScore F1 and ROUGE-L) within 0.5% of the
non-private baseline and close to the centralized upper bound. The framework
scales to backbones with up to 1.7B parameters on single-GPU clients, requiring
< 173 MB of communication per round. FedMentor demonstrates a practical
approach to privately fine-tune LLMs for safer deployments in healthcare and
other sensitive fields.

</details>


### [3] [Beyond Data Privacy: New Privacy Risks for Large Language Models](https://arxiv.org/abs/2509.14278)
*Yuntao Du,Zitao Li,Ninghui Li,Bolin Ding*

Main category: cs.CR

TL;DR: This paper examines emerging privacy risks in deployed LLMs, identifies vulnerabilities from autonomous capabilities, and urges researchers to expand beyond traditional data privacy defenses to protect against sophisticated threats and societal harms.


<details>
  <summary>Details</summary>
Motivation: The integration of LLMs into everyday applications and the exploitation of their autonomous decision-making capabilities have introduced critical privacy risks, including data leakage, malicious data exfiltration, and large-scale privacy attacks that endanger individual privacy, financial security, and societal trust.

Method: The authors conducted a systematic examination of emerging privacy risks in LLMs, focusing on vulnerabilities arising during deployment, autonomous abilities exploitation, and new attack vectors.

Result: The analysis highlights new privacy vulnerabilities in LLM-powered systems, such as inadvertent data leakage and sophisticated adversarial attacks, while proposing mitigation strategies to address these escalating threats.

Conclusion: The paper urges the research community to broaden their focus beyond data privacy risks, emphasizing the development of new defenses against evolving threats posed by powerful LLMs and their systems.

Abstract: Large Language Models (LLMs) have achieved remarkable progress in natural
language understanding, reasoning, and autonomous decision-making. However,
these advancements have also come with significant privacy concerns. While
significant research has focused on mitigating the data privacy risks of LLMs
during various stages of model training, less attention has been paid to new
threats emerging from their deployment. The integration of LLMs into widely
used applications and the weaponization of their autonomous abilities have
created new privacy vulnerabilities. These vulnerabilities provide
opportunities for both inadvertent data leakage and malicious exfiltration from
LLM-powered systems. Additionally, adversaries can exploit these systems to
launch sophisticated, large-scale privacy attacks, threatening not only
individual privacy but also financial security and societal trust. In this
paper, we systematically examine these emerging privacy risks of LLMs. We also
discuss potential mitigation strategies and call for the research community to
broaden its focus beyond data privacy risks, developing new defenses to address
the evolving threats posed by increasingly powerful LLMs and LLM-powered
systems.

</details>


### [4] [Resisting Quantum Key Distribution Attacks Using Quantum Machine Learning](https://arxiv.org/abs/2509.14282)
*Ali Al-kuwari,Noureldin Mohamed,Saif Al-kuwari,Ahmed Farouk,Bikash K. Behera*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The emergence of quantum computing poses significant risks to the security of
modern communication networks as it breaks today's public-key cryptographic
algorithms. Quantum Key Distribution (QKD) offers a promising solution by
harnessing the principles of quantum mechanics to establish secure keys.
However, practical QKD implementations remain vulnerable to hardware
imperfections and advanced attacks such as Photon Number Splitting and
Trojan-Horse attacks. In this work, we investigate the potential of using
quantum machine learning (QML) to detect popular QKD attacks. In particular, we
propose a Hybrid Quantum Long Short-Term Memory (QLSTM) model to improve the
detection of common QKD attacks. By combining quantum-enhanced learning with
classical deep learning, the model captures complex temporal patterns in QKD
data, improving detection accuracy. To evaluate the proposed model, we
introduce a realistic QKD dataset simulating normal QKD operations along with
seven attack scenarios, Intercept-and-Resend, Photon-Number Splitting (PNS),
Trojan-Horse attacks Random Number Generator (RNG), Detector Blinding,
Wavelength-dependent Trojan Horse, and Combined attacks. The dataset includes
quantum security metrics such as Quantum Bit Error Rate (QBER), measurement
entropy, signal and decoy loss rates, and time-based metrics, ensuring an
accurate representation of real-world conditions. Our results demonstrate
promising performance of the quantum machine learning approach compared to
traditional classical machine learning models, highlighting the potential of
hybrid techniques to enhance the security of future quantum communication
networks. The proposed Hybrid QLSTM model achieved an accuracy of 93.7.0\%
after 50 training epochs, outperforming classical deep learning models such as
LSTM, and CNN.

</details>


### [5] [The Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration](https://arxiv.org/abs/2509.14284)
*Vaidehi Patil,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CR

TL;DR: Paper reveals 'compositional privacy leakage' during multi-agent LLM interactions and proposes mitigation strategies, with Collaborative Consensus Defense showing the best privacy-utility balance.


<details>
  <summary>Details</summary>
Motivation: Existing privacy risks (memorization, inference, single-turn attacks) are insufficient for multi-agent systems where cumulative benign interactions can leak sensitive information through context-aware composition.

Method: 1) Framework analyzing how auxiliary knowledge and interactions amplify privacy risks. 2) Two defense strategies: Theory-of-Mind (ToM) for intent-aware blocking and Collaborative Consensus Defense (CoDef) for collaborative decision-making. 3) Evaluation across sensitive/benign composition scenarios.

Result: ToM achieves 97% sensitive query blocking but reduces benign utility; CoDef attains 79.8% Balanced Outcome metric by combining reasoning and collaboration. Chain-of-thought alone blocks only 39% of leaks.

Conclusion: The study highlights a new class of privacy risks in multi-agent LLM systems through compositional leakage and demonstrates actionable mitigation strategies (ToM and CoDef) to balance privacy-utility trade-offs.

Abstract: As large language models (LLMs) become integral to multi-agent systems, new
privacy risks emerge that extend beyond memorization, direct inference, or
single-turn evaluations. In particular, seemingly innocuous responses, when
composed across interactions, can cumulatively enable adversaries to recover
sensitive information, a phenomenon we term compositional privacy leakage. We
present the first systematic study of such compositional privacy leaks and
possible mitigation methods in multi-agent LLM systems. First, we develop a
framework that models how auxiliary knowledge and agent interactions jointly
amplify privacy risks, even when each response is benign in isolation. Next, to
mitigate this, we propose and evaluate two defense strategies: (1)
Theory-of-Mind defense (ToM), where defender agents infer a questioner's intent
by anticipating how their outputs may be exploited by adversaries, and (2)
Collaborative Consensus Defense (CoDef), where responder agents collaborate
with peers who vote based on a shared aggregated state to restrict sensitive
information spread. Crucially, we balance our evaluation across compositions
that expose sensitive information and compositions that yield benign
inferences. Our experiments quantify how these defense strategies differ in
balancing the privacy-utility trade-off. We find that while chain-of-thought
alone offers limited protection to leakage (~39% sensitive blocking rate), our
ToM defense substantially improves sensitive query blocking (up to 97%) but can
reduce benign task success. CoDef achieves the best balance, yielding the
highest Balanced Outcome (79.8%), highlighting the benefit of combining
explicit reasoning with defender collaboration. Together, our results expose a
new class of risks in collaborative LLM deployments and provide actionable
insights for designing safeguards against compositional, context-driven privacy
leakage.

</details>


### [6] [A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks](https://arxiv.org/abs/2509.14285)
*S M Asif Hossain,Ruksat Khan Shayoni,Mohd Ruhul Ameen,Akif Islam,M. F. Mridha,Jungpil Shin*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Prompt injection attacks represent a major vulnerability in Large Language
Model (LLM) deployments, where malicious instructions embedded in user inputs
can override system prompts and induce unintended behaviors. This paper
presents a novel multi-agent defense framework that employs specialized LLM
agents in coordinated pipelines to detect and neutralize prompt injection
attacks in real-time. We evaluate our approach using two distinct
architectures: a sequential chain-of-agents pipeline and a hierarchical
coordinator-based system. Our comprehensive evaluation on 55 unique prompt
injection attacks, grouped into 8 categories and totaling 400 attack instances
across two LLM platforms (ChatGLM and Llama2), demonstrates significant
security improvements. Without defense mechanisms, baseline Attack Success
Rates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent
pipeline achieved 100% mitigation, reducing ASR to 0% across all tested
scenarios. The framework demonstrates robustness across multiple attack
categories including direct overrides, code execution attempts, data
exfiltration, and obfuscation techniques, while maintaining system
functionality for legitimate queries.

</details>


### [7] [A Simple and Efficient Jailbreak Method Exploiting LLMs' Helpfulness](https://arxiv.org/abs/2509.14297)
*Xuan Luo,Yue Wang,Zefeng He,Geng Tu,Jing Li,Ruifeng Xu*

Main category: cs.CR

TL;DR: This paper proposes HILL, a jailbreak method that transforms harmful requests into hypothetical learning-style questions, exposing vulnerabilities in LLM safety mechanisms with high attack success rates and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing safety alignment measures for LLMs remain vulnerable to sophisticated attacks, requiring systematic methods to uncover weaknesses and improve defenses.

Method: HILL converts imperative malicious queries into learning-style questions using minimal hypothetical indicators, supported by two new evaluation metrics for jailbreak effectiveness.

Result: HILL achieves top attack success rates across 75%+ models tested, generalizes across malicious categories, and reveals defense methods often fail or exacerbate vulnerabilities.

Conclusion: HILL demonstrates critical limitations in current safety mechanisms, highlighting the challenge of balancing helpfulness and safety alignment against learning-style elicitation attacks.

Abstract: Safety alignment aims to prevent Large Language Models (LLMs) from responding
to harmful queries. To strengthen safety protections, jailbreak methods are
developed to simulate malicious attacks and uncover vulnerabilities. In this
paper, we introduce HILL (Hiding Intention by Learning from LLMs), a novel
jailbreak approach that systematically transforms imperative harmful requests
into learning-style questions with only straightforward hypotheticality
indicators. Further, we introduce two new metrics to thoroughly evaluate the
utility of jailbreak methods. Experiments on the AdvBench dataset across a wide
range of models demonstrate HILL's strong effectiveness, generalizability, and
harmfulness. It achieves top attack success rates on the majority of models and
across malicious categories while maintaining high efficiency with concise
prompts. Results of various defense methods show the robustness of HILL, with
most defenses having mediocre effects or even increasing the attack success
rates. Moreover, the assessment on our constructed safe prompts reveals
inherent limitations of LLMs' safety mechanisms and flaws in defense methods.
This work exposes significant vulnerabilities of safety measures against
learning-style elicitation, highlighting a critical challenge of balancing
helpfulness and safety alignments.

</details>


### [8] [Beyond Classification: Evaluating LLMs for Fine-Grained Automatic Malware Behavior Auditing](https://arxiv.org/abs/2509.14335)
*Xinran Zheng,Xingzhi Qian,Yiling He,Shuo Yang,Lorenzo Cavallaro*

Main category: cs.CR

TL;DR: MalEval is an open-source framework that evaluates LLMs for Android malware auditing through task-specific metrics, revealing both promise and challenges in using LLMs for verifiable behavior analysis.


<details>
  <summary>Details</summary>
Motivation: Automated malware classification lacks verifiable explanations of malicious intent. Manual auditing is inefficient, and LLMs' auditing potential is hindered by scarce annotations, benign code noise, and unverifiable outputs. A systematic evaluation framework is urgently needed.

Method: MalEval employs expert-verified reports, static reachability analysis, updated sensitive API lists, and function-level structural representations. It defines four analyst-aligned tasks (function prioritization, evidence attribution, behavior synthesis, sample discrimination) with domain-specific metrics and a unified workload-oriented score to evaluate LLMs.

Result: A curated dataset of recent malware and misclassified benign apps was used to evaluate seven LLMs. Results revealed strengths and weaknesses across audit stages, demonstrating MalEval's effectiveness in addressing noise and ground truth scarcity while benchmarking LLM capabilities.

Conclusion: MalEval introduces a comprehensive framework to assess and enhance LLMs for malware behavior auditing, highlighting their potential and critical limitations while providing a reproducible benchmark.

Abstract: Automated malware classification has achieved strong detection performance.
Yet, malware behavior auditing seeks causal and verifiable explanations of
malicious activities -- essential not only to reveal what malware does but also
to substantiate such claims with evidence. This task is challenging, as
adversarial intent is often hidden within complex, framework-heavy
applications, making manual auditing slow and costly. Large Language Models
(LLMs) could help address this gap, but their auditing potential remains
largely unexplored due to three limitations: (1) scarce fine-grained
annotations for fair assessment; (2) abundant benign code obscuring malicious
signals; and (3) unverifiable, hallucination-prone outputs undermining
attribution credibility. To close this gap, we introduce MalEval, a
comprehensive framework for fine-grained Android malware auditing, designed to
evaluate how effectively LLMs support auditing under real-world constraints.
MalEval provides expert-verified reports and an updated sensitive API list to
mitigate ground truth scarcity and reduce noise via static reachability
analysis. Function-level structural representations serve as intermediate
attribution units for verifiable evaluation. Building on this, we define four
analyst-aligned tasks -- function prioritization, evidence attribution,
behavior synthesis, and sample discrimination -- together with domain-specific
metrics and a unified workload-oriented score. We evaluate seven widely used
LLMs on a curated dataset of recent malware and misclassified benign apps,
offering the first systematic assessment of their auditing capabilities.
MalEval reveals both promising potential and critical limitations across audit
stages, providing a reproducible benchmark and foundation for future research
on LLM-enhanced malware behavior auditing. MalEval is publicly available at
https://github.com/ZhengXR930/MalEval.git

</details>


### [9] [LLM Jailbreak Detection for (Almost) Free!](https://arxiv.org/abs/2509.14558)
*Guorui Chen,Yifan Xia,Xiaojun Jia,Zhijiang Li,Philip Torr,Jindong Gu*

Main category: cs.CR

TL;DR: The paper introduces FJD, a low-cost method for detecting jailbreak attacks in large language models by analyzing output distribution differences and enhancing first-token confidence through temperature scaling and virtual instruction learning.


<details>
  <summary>Details</summary>
Motivation: Despite the widespread use of large language models (LLMs) in enhancing security through alignment, they still face the vulnerability of jailbreak attacks which bypass safety measures. Current detection methods are computationally expensive, necessitating a more efficient solution to identify these attacks without additional resource demands during inference.

Method: The proposed FJD method leverages the distinct output distribution patterns of jailbreak versus benign prompts. It employs an affirmative instruction to guide the LLM's response and applies temperature scaling to adjust the confidence in the first token. Additionally, the method is augmented with virtual instruction learning to optimize detection further.

Result: Experimental results on various aligned LLMs demonstrate that FJD effectively identifies jailbreak prompts while maintaining computational efficiency, with minimal overhead during inference compared to existing methods.

Conclusion: FJD offers an effective and computationally inexpensive way to detect jailbreak attacks in LLMs during inference, significantly improving current capabilities and potentially setting a new standard for practical LLM safety mechanisms.

Abstract: Large language models (LLMs) enhance security through alignment when widely
used, but remain susceptible to jailbreak attacks capable of producing
inappropriate content. Jailbreak detection methods show promise in mitigating
jailbreak attacks through the assistance of other models or multiple model
inferences. However, existing methods entail significant computational costs.
In this paper, we first present a finding that the difference in output
distributions between jailbreak and benign prompts can be employed for
detecting jailbreak prompts. Based on this finding, we propose a Free Jailbreak
Detection (FJD) which prepends an affirmative instruction to the input and
scales the logits by temperature to further distinguish between jailbreak and
benign prompts through the confidence of the first token. Furthermore, we
enhance the detection performance of FJD through the integration of virtual
instruction learning. Extensive experiments on aligned LLMs show that our FJD
can effectively detect jailbreak prompts with almost no additional
computational costs during LLM inference.

</details>


### [10] [What Gets Measured Gets Managed: Mitigating Supply Chain Attacks with a Link Integrity Management System](https://arxiv.org/abs/2509.14583)
*Johnny So,Michael Ferdman,Nick Nikiforakis*

Main category: cs.CR

TL;DR: LiMS is a transparent system for verifying web resource integrity during browsing sessions with minimal overhead. It uses customizable integrity policies to enforce security guarantees, defended against supply chain attacks in 450 domain simulations, and maintains performance (initial load overhead: ~hundreds of ms, negligible on reloads).


<details>
  <summary>Details</summary>
Motivation: Current web dependency-monitoring tools lack robustness, generalizability, and performance, leaving supply chains as a critical attack vector. Existing solutions cannot scale effectively while maintaining a balance between security guarantees and accessibility for website visitors.

Method: LiMS implements a policy-based framework where administrators define (un)expected resource characteristics through customizable rules. These policies form a foundational set that can build comprehensive integrity guarantees. By analyzing dependency patterns and enforcing policies during page loads, LiMS transparently verifies resource authenticity and blocks deviations.

Result: The LiMS prototype demonstrates: defense against supply chain attacks (e.g., content substation, MITM tampering); minimal overhead (450ms average initial load cost); 96.7% of domains required under 1,000 policy rules; and policy patterns matching 83% of cross-origin dependency scenarios with low administrative effort.

Conclusion: LiMS establishes a pragmatic framework for web resource integrity that balances security, performance, and usability. The evaluation shows its feasibility against real-world attack vectors while maintaining compatibility with diverse website infrastructure. The open-sourced prototype suggests broad adoption is technically and operationally viable with negligible maintenance required.

Abstract: The web continues to grow, but dependency-monitoring tools and standards for
resource integrity lag behind. Currently, there exists no robust method to
verify the integrity of web resources, much less in a generalizable yet
performant manner, and supply chains remain one of the most targeted parts of
the attack surface of web applications.
  In this paper, we present the design of LiMS, a transparent system to
bootstrap link integrity guarantees in web browsing sessions with minimal
overhead. At its core, LiMS uses a set of customizable integrity policies to
declare the (un)expected properties of resources, verifies these policies, and
enforces them for website visitors. We discuss how basic integrity policies can
serve as building blocks for a comprehensive set of integrity policies, while
providing guarantees that would be sufficient to defend against recent supply
chain attacks detailed by security industry reports. Finally, we evaluate our
open-sourced prototype by simulating deployments on a representative sample of
450 domains that are diverse in ranking and category. We find that our proposal
offers the ability to bootstrap marked security improvements with an overall
overhead of hundreds of milliseconds on initial page loads, and negligible
overhead on reloads, regardless of network speeds. In addition, from examining
archived data for the sample sites, we find that several of the proposed policy
building blocks suit their dependency usage patterns, and would incur minimal
administrative overhead.

</details>


### [11] [ATLANTIS: AI-driven Threat Localization, Analysis, and Triage Intelligence System](https://arxiv.org/abs/2509.14589)
*Taesoo Kim,HyungSeok Han,Soyeon Park,Dae R. Jeong,Dohyeok Kim,Dongkwan Kim,Eunsoo Kim,Jiho Kim,Joshua Wang,Kangsu Kim,Sangwoo Ji,Woosun Song,Hanqing Zhao,Andrew Chin,Gyejin Lee,Kevin Stevens,Mansour Alharthi,Yizhuo Zhai,Cen Zhang,Joonun Jang,Yeongjin Jang,Ammar Askar,Dongju Kim,Fabian Fleischer,Jeongin Cho,Junsik Kim,Kyungjoon Ko,Insu Yun,Sangdon Park,Dowoo Baik,Haein Lee,Hyeon Heo,Minjae Gwon,Minjae Lee,Minwoo Baek,Seunggi Min,Wonyoung Kim,Yonghwi Jin,Younggi Park,Yunjae Choi,Jinho Jung,Gwanhyun Lee,Junyoung Jang,Kyuheon Kim,Yeonghyeon Cha,Youngjoon Kim*

Main category: cs.CR

TL;DR: ATLANTIS, a cyber reasoning system integrating LLMs with program analysis techniques (symbolic execution, fuzzing, static analysis), won DARPA's AI Cyber Challenge by addressing automated vulnerability discovery and repair across diverse codebases.


<details>
  <summary>Details</summary>
Motivation: Existing systems struggle with scalability, precision, and semantic correctness in automated vulnerability detection and repair. AIxCC demanded solutions capable of handling heterogeneous codebases (C-Java) with broad coverage and high-precision patches while preserving program intent.

Method: Combined large language models with symbolic execution, directed fuzzing, and static analysis to address codebase diversity, precision-coverage tradeoffs, and semantic correctness. Multi-institutional collaboration between Georgia Tech, Samsung, KAIST, and POSTECH enabled architectural innovations in scalability and patch validation.

Result: Won 1st place in DARPA's AIxCC final competition (DEF CON 33). Demonstrated effective integration of AI with program analysis for vulnerability discovery/patching, with published implementation details and reproducibility artifacts for subsequent work.

Conclusion: Established a new paradigm for cyber reasoning by combining AI with traditional analysis techniques. Released artifacts for reproducibility, advances automated security, and provides foundational insights for future AI-enhanced program analysis systems.

Abstract: We present ATLANTIS, the cyber reasoning system developed by Team Atlanta
that won 1st place in the Final Competition of DARPA's AI Cyber Challenge
(AIxCC) at DEF CON 33 (August 2025). AIxCC (2023-2025) challenged teams to
build autonomous cyber reasoning systems capable of discovering and patching
vulnerabilities at the speed and scale of modern software. ATLANTIS integrates
large language models (LLMs) with program analysis -- combining symbolic
execution, directed fuzzing, and static analysis -- to address limitations in
automated vulnerability discovery and program repair. Developed by researchers
at Georgia Institute of Technology, Samsung Research, KAIST, and POSTECH, the
system addresses core challenges: scaling across diverse codebases from C to
Java, achieving high precision while maintaining broad coverage, and producing
semantically correct patches that preserve intended behavior. We detail the
design philosophy, architectural decisions, and implementation strategies
behind ATLANTIS, share lessons learned from pushing the boundaries of automated
security when program analysis meets modern AI, and release artifacts to
support reproducibility and future research.

</details>


### [12] [Threats and Security Strategies for IoMT Infusion Pumps](https://arxiv.org/abs/2509.14604)
*Ramazan Yener,Muhammad Hassan,Masooda Bashir*

Main category: cs.CR

TL;DR: This study analyzes cybersecurity vulnerabilities in IoMT infusion pumps through a review of 7 recent studies (from 132 papers), identifying security gaps in device-level flaws, authentication, network weaknesses, data privacy, and operational challenges to inform proactive security strategies.


<details>
  <summary>Details</summary>
Motivation: The integration of IoMT into healthcare has increased cyber risks, particularly for critical devices like infusion pumps. This work addresses gaps in understanding their vulnerabilities to improve patient safety and healthcare IT resilience.

Method: A systematic literature review of 7 studies (selected from 132 papers over five years), synthesizing findings on cybersecurity vulnerabilities in IoMT infusion pumps through categorical analysis of security gaps and risk patterns.

Result: Identified vulnerabilities include device-level flaws, authentication/access control weaknesses, network communication risks, data privacy issues, and operational challenges. Analysis revealed recurring risk patterns across studies that expose infusion pumps to lateral attacks in healthcare networks.

Conclusion: The study establishes a structured framework of security risks for IoMT infusion pumps, emphasizing the need for targeted mitigation strategies. It provides healthcare professionals and manufacturers with actionable insights to strengthen device security and protect patient well-being.

Abstract: The integration of the Internet of Medical Things (IoMT) into healthcare
systems has transformed patient care by enabling real-time monitoring, enhanced
diagnostics, and enhanced operational efficiency. However, this increased
connectivity has also expanded the attack surface for cybercriminals, raising
significant cybersecurity and privacy concerns. This study focuses on the
cybersecurity vulnerabilities of IoMT infusion pumps, which are critical
devices in modern healthcare. Through a targeted literature review of the past
five years, we analyzed seven current studies from a pool of 132 papers to
identify security vulnerabilities. Our findings indicate that infusion pumps
face vulnerabilities such as device-level flaws, authentication and access
control issues, network and communication weaknesses, data security and privacy
risks, and operational or organizational challenges that can expose them to
lateral attacks within healthcare networks. Our analysis synthesizes findings
from seven recent studies to clarify how and why infusion pumps remain
vulnerable in each of these areas. By categorizing the security gaps, we
highlight critical risk patterns and their implications. This work underscores
the scope of the issue and provides a structured understanding that is valuable
for healthcare IT professionals and device manufacturers. Ultimately, the
findings can inform the development of targeted, proactive security strategies
to better safeguard infusion pumps and protect patient well-being.

</details>


### [13] [Enterprise AI Must Enforce Participant-Aware Access Control](https://arxiv.org/abs/2509.14608)
*Shashank Shreedhar Bhatt,Tanmay Rajore,Khushboo Aggarwal,Ganesh Ananthanarayanan,Ranveer Chandra,Nishanth Chandran,Suyash Choudhury,Divya Gupta,Emre Kiciman,Sumit Kumar Pandey,Srinath Setty,Rahul Sharma,Teijia Zhao*

Main category: cs.CR

TL;DR: This paper identifies security risks in enterprise LLMs when using fine-tuning and RAG pipelines due to weak access control. It introduces a deterministic authorization framework to prevent data leakage, validated through deployment in Microsoft's Copilot Tuning product.


<details>
  <summary>Details</summary>
Motivation: The paper motivates that enterprise LLMs face critical security risks when trained on sensitive data and combined with RAG pipelines due to inadequate access control enforcement. Existing defenses fail to prevent data exfiltration attacks where adversaries exploit architectural weaknesses to leak confidential information.

Method: The authors propose a framework requiring explicit authorization for all content used in LLM training, retrieval, or generation, ensuring strict access control enforcement for every user interaction. This contrasts with probabilistic defenses (prompt sanitization, output filtering) by demanding deterministic security guarantees.

Result: The authors achieve deployment of their framework in Microsoft Copilot Tuning, demonstrating a practical implementation that enables secure fine-tuning of LLMs with enterprise-specific data while enforcing authorization across training and inference workflows.

Conclusion: The paper concludes that deterministic enforcement of fine-grained access control during both fine-tuning and RAG-based inference is essential to prevent data leakage in multi-user LLM systems. This represents a paradigm shift from probabilistic to rigorous security measures grounded in classical access control principles.

Abstract: Large language models (LLMs) are increasingly deployed in enterprise settings
where they interact with multiple users and are trained or fine-tuned on
sensitive internal data. While fine-tuning enhances performance by
internalizing domain knowledge, it also introduces a critical security risk:
leakage of confidential training data to unauthorized users. These risks are
exacerbated when LLMs are combined with Retrieval-Augmented Generation (RAG)
pipelines that dynamically fetch contextual documents at inference time.
  We demonstrate data exfiltration attacks on AI assistants where adversaries
can exploit current fine-tuning and RAG architectures to leak sensitive
information by leveraging the lack of access control enforcement. We show that
existing defenses, including prompt sanitization, output filtering, system
isolation, and training-level privacy mechanisms, are fundamentally
probabilistic and fail to offer robust protection against such attacks.
  We take the position that only a deterministic and rigorous enforcement of
fine-grained access control during both fine-tuning and RAG-based inference can
reliably prevent the leakage of sensitive data to unauthorized recipients.
  We introduce a framework centered on the principle that any content used in
training, retrieval, or generation by an LLM is explicitly authorized for
\emph{all users involved in the interaction}. Our approach offers a simple yet
powerful paradigm shift for building secure multi-user LLM systems that are
grounded in classical access control but adapted to the unique challenges of
modern AI workflows. Our solution has been deployed in Microsoft Copilot
Tuning, a product offering that enables organizations to fine-tune models using
their own enterprise-specific data.

</details>


### [14] [Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection](https://arxiv.org/abs/2509.14622)
*Yihao Guo,Haocheng Bian,Liutong Zhou,Ze Wang,Zhaoyi Zhang,Francois Kawala,Milan Dean,Ian Fischer,Yuantao Peng,Noyan Tokgozoglu,Ivan Barrientos,Riyaaz Shaik,Rachel Li,Chandru Venkataraman,Reza Shifteh Far,Moses Pawar,Venkat Sundaranatha,Michael Xu,Frank Chu*

Main category: cs.CR

TL;DR: ADRAG is a two-stage framework for online malicious intent detection using adversarial training and knowledge distillation, achieving high accuracy with low latency.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to handle diverse, complex real-time user queries. Large model latency and poor out-of-distribution detection necessitate a robust, efficient solution.

Method: 1) Train a teacher model on adversarially perturbed, retrieval-augmented inputs. 2 Distill training into a compact student model using a dynamic online knowledge base of safety exemplars for inference.

Result: ADRAG (149M param) matches 98.5%i WildGuard-7B's performance, exceeds GPT-4 by 3.3%i and Llama-Guard-3-8B by 9.5%i on out-of-distribution detection, with 5.6× lower latency (300 QPS).

Conclusion: ADRAG provides scalable, real-time malicious intent detection through adversarial training, knowledge distillation, and dynamic retrieval, outperforming larger models in accuracy and efficiency.

Abstract: With the deployment of Large Language Models (LLMs) in interactive
applications, online malicious intent detection has become increasingly
critical. However, existing approaches fall short of handling diverse and
complex user queries in real time. To address these challenges, we introduce
ADRAG (Adversarial Distilled Retrieval-Augmented Guard), a two-stage framework
for robust and efficient online malicious intent detection. In the training
stage, a high-capacity teacher model is trained on adversarially perturbed,
retrieval-augmented inputs to learn robust decision boundaries over diverse and
complex user queries. In the inference stage, a distillation scheduler
transfers the teacher's knowledge into a compact student model, with a
continually updated knowledge base collected online. At deployment, the compact
student model leverages top-K similar safety exemplars retrieved from the
online-updated knowledge base to enable both online and real-time malicious
query detection. Evaluations across ten safety benchmarks demonstrate that
ADRAG, with a 149M-parameter model, achieves 98.5% of WildGuard-7B's
performance, surpasses GPT-4 by 3.3% and Llama-Guard-3-8B by 9.5% on
out-of-distribution detection, while simultaneously delivering up to 5.6x lower
latency at 300 queries per second (QPS) in real-time applications.

</details>


### [15] [Threat Modeling for Enhancing Security of IoT Audio Classification Devices under a Secure Protocols Framework](https://arxiv.org/abs/2509.14657)
*Sergio Benlloch-Lopez,Miquel Viel-Vazquez,Javier Naranjo-Alcazar,Jordi Grau-Haro,Pedro Zuccarello*

Main category: cs.CR

TL;DR: This paper presents a multi-layered IoT security architecture using hardware attestation, post-quantum cryptography, and 3-2-1 data storage to protect audio-sensitive devices from tampering and unauthorized access.


<details>
  <summary>Details</summary>
Motivation: The proliferation of IoT audio devices creates exposure of sensitive data under severe resource constraints while risking compromise due to tampering at any system layer. Traditional security approaches are inadequate for this scenario.

Method: The system employs a three-domain security framework (edge, network, cloud) using TPM-based remote attestation and TLS 1.3 with Kyber/Dilithium post-quantum hybridization. Secure boot measurements via TPM PCRs, LUKS disk encryption with cloud-verified unlock keys, end-to-end audio feature protection, and a 3-2-1 storage strategy (LUKS-encrypted SSD, hybrid-encrypted offline archive, cloud replica) are central to the design. STRIDE threat modeling and attack-tree analysis guide implementation.

Result: The protocol achieves boot-time integrity verification, post-quantum resilient communications, tamper-proof firmware through signed models, and comprehensive data-at-rest protections. Physical tamper-responsiveness and a detailed evaluation plan are also established.

Conclusion: The proposed defense-in-depth architecture combines hardware-based attestation, post-quantum cryptography, and robust data protection to comprehensively secure IoT audio devices against cyber and physical threats.

Abstract: The rapid proliferation of IoT nodes equipped with microphones and capable of
performing on-device audio classification exposes highly sensitive data while
operating under tight resource constraints. To protect against this, we present
a defence-in-depth architecture comprising a security protocol that treats the
edge device, cellular network and cloud backend as three separate trust
domains, linked by TPM-based remote attestation and mutually authenticated TLS
1.3. A STRIDE-driven threat model and attack-tree analysis guide the design. At
startup, each boot stage is measured into TPM PCRs. The node can only decrypt
its LUKS-sealed partitions after the cloud has verified a TPM quote and
released a one-time unlock key. This ensures that rogue or tampered devices
remain inert. Data in transit is protected by TLS 1.3 and hybridised with Kyber
and Dilithium to provide post-quantum resilience. Meanwhile, end-to-end
encryption and integrity hashes safeguard extracted audio features. Signed,
rollback-protected AI models and tamper-responsive sensors harden firmware and
hardware. Data at rest follows a 3-2-1 strategy comprising a solid-state drive
sealed with LUKS, an offline cold archive encrypted with a hybrid post-quantum
cipher and an encrypted cloud replica. Finally, we set out a plan for
evaluating the physical and logical security of the proposed protocol.

</details>


### [16] [Security Analysis of Web Applications Based on Gruyere](https://arxiv.org/abs/2509.14706)
*Yonghao Ni,Zhongwen Li,Xiaoqi Li*

Main category: cs.CR

TL;DR: The paper examines web security through the lens of the OWASP Top 10 vulnerabilities and the Gruyere platform, highlighting the enduring relevance of older vulnerabilities to modern security challenges.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the growing importance of web systems and the associated security risks, which necessitate systematic research to protect data, privacy, and ensure business continuity.

Method: The method involves reviewing the OWASP Top 10 to identify and explain vulnerabilities, followed by an in-depth analysis of the Gruyere platform to reproduce and address known flaws through case studies.

Result: The study showcases how to replicate and resolve the vulnerabilities in Gruyere, drawing connections to contemporary cases and showing their ongoing relevance.

Conclusion: The conclusion emphasizes the value of using Gruyere-based web security analysis for enhancing understanding of vulnerabilities, which supports technological innovation and contemporary security defense strategies.

Abstract: With the rapid development of Internet technologies, web systems have become
essential infrastructures for modern information exchange and business
operations. However, alongside their expansion, numerous security
vulnerabilities have emerged, making web security a critical research focus
within the broader field of cybersecurity. These issues are closely related to
data protection, privacy preservation, and business continuity, and systematic
research on web security is crucial for mitigating malicious attacks and
enhancing the reliability and robustness of network systems. This paper first
reviews the OWASP Top 10, summarizing the types, causes, and impacts of common
web vulnerabilities, and illustrates their exploitation mechanisms through
representative cases. Building upon this, the Gruyere platform is adopted as an
experimental subject for analyzing known vulnerabilities. The study presents
detailed reproduction steps for specific vulnerabilities, proposes
comprehensive remediation strategies, and further compares Gruyere's
vulnerabilities with contemporary real-world cases. The findings suggest that,
although Gruyere's vulnerabilities are relatively outdated, their underlying
principles remain highly relevant for explaining a wide range of modern
security flaws. Overall, this research demonstrates that web system security
analysis based on Gruyere not only deepens the understanding of vulnerability
mechanisms but also provides practical support for technological innovation and
security defense.

</details>


### [17] [Variables Ordering Optimization in Boolean Characteristic Set Method Using Simulated Annealing and Machine Learning-based Time Prediction](https://arxiv.org/abs/2509.14754)
*Minzhong Luo,Yudong Sun,Yin Long*

Main category: cs.CR

TL;DR: Combines ML and simulated annealing to optimize variable orderings for solving Boolean equations, significantly improving performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: The BCS method's performance is highly sensitive to variable ordering, causing unpredictable solving times. Existing approaches lack systematic methods to optimize orderings, motivating the need for a robust solution to improve efficiency in symbolic computation.

Method: The framework combines machine learning (ML)-based time prediction with simulated annealing (SA). It uses a dataset of variable frequency spectra and corresponding BCS solving times to train an ML predictor, which guides the SA algorithm to identify optimal variable orderings.

Result: Extensive experiments show the framework outperforms BCS, Gröbner basis methods, and SAT solvers, particularly for larger systems. Probabilistic time complexity bounds are derived, linking predictor accuracy to solving efficiency.

Conclusion: This paper introduces an ML-integrated optimization framework that enhances the Boolean Characteristic Set method, offering both practical efficiency gains for algebraic cryptanalysis and theoretical insights into ML-driven combinatorial optimization.

Abstract: Solving systems of Boolean equations is a fundamental task in symbolic
computation and algebraic cryptanalysis, with wide-ranging applications in
cryptography, coding theory, and formal verification. Among existing
approaches, the Boolean Characteristic Set (BCS) method[1] has emerged as one
of the most efficient algorithms for tackling such problems. However, its
performance is highly sensitive to the ordering of variables, with solving
times varying drastically under different orderings for fixed variable counts n
and equations size m. To address this challenge, this paper introduces a novel
optimization framework that synergistically integrates machine learning
(ML)-based time prediction with simulated annealing (SA) to efficiently
identify high-performance variables orderings. Weconstruct a dataset comprising
variable frequency spectrum X and corresponding BCS solving time t for
benchmark systems(e.g., n = m = 28). Utilizing this data, we train an accurate
ML predictor ft(X) to estimate solving time for any given variables ordering.
For each target system, ft serves as the cost function within an SA algorithm,
enabling rapid discovery of low-latency orderings that significantly expedite
subsequent BCS execution. Extensive experiments demonstrate that our method
substantially outperforms the standard BCS algorithm[1], Gr\"obner basis method
[2] and SAT solver[3], particularly for larger-scale systems(e.g., n = 32).
Furthermore, we derive probabilistic time complexity bounds for the overall
algorithm using stochastic process theory, establishing a quantitative
relationship between predictor accuracy and expected solving complexity. This
work provides both a practical acceleration tool for algebraic cryptanalysis
and a theoretical foundation for ML-enhanced combinatorial optimization in
symbolic computation.

</details>


### [18] [Blockchain-Enabled Explainable AI for Trusted Healthcare Systems](https://arxiv.org/abs/2509.14987)
*Md Talha Mohsin*

Main category: cs.CR

TL;DR: BXHF combines blockchain and XAI to secure healthcare data and explain AI decisions, enabling privacy-preserving collaboration and compliant clinical AI.


<details>
  <summary>Details</summary>
Motivation: Healthcare systems face challenges in safe data exchange and comprehensible AI-driven clinical decisions. The paper addresses these by combining blockchain security with explainable AI to establish trust at both data and decision levels.

Method: BXHF integrates blockchain for immutable, auditable patient records and XAI methodologies to provide transparent predictions. It uses a hybrid edge-cloud architecture for federated computation across institutions, combining security and interpretability in a unified optimization pipeline.

Result: The framework is demonstrated through use cases including cross-border clinical research, uncommon illness detection, and high-risk intervention support, showing its ability to enable collaborative analytics while protecting privacy.

Conclusion: BXHF improves the credibility, uptake, and effectiveness of AI in healthcare by ensuring transparency, auditability, and regulatory compliance, laying the groundwork for safer and more reliable clinical decision-making.

Abstract: This paper introduces a Blockchain-Integrated Explainable AI Framework (BXHF)
for healthcare systems to tackle two essential challenges confronting health
information networks: safe data exchange and comprehensible AI-driven clinical
decision-making. Our architecture incorporates blockchain, ensuring patient
records are immutable, auditable, and tamper-proof, alongside Explainable AI
(XAI) methodologies that yield transparent and clinically relevant model
predictions. By incorporating security assurances and interpretability
requirements into a unified optimization pipeline, BXHF ensures both data-level
trust (by verified and encrypted record sharing) and decision-level trust (with
auditable and clinically aligned explanations). Its hybrid edge-cloud
architecture allows for federated computation across different institutions,
enabling collaborative analytics while protecting patient privacy. We
demonstrate the framework's applicability through use cases such as
cross-border clinical research networks, uncommon illness detection and
high-risk intervention decision support. By ensuring transparency,
auditability, and regulatory compliance, BXHF improves the credibility, uptake,
and effectiveness of AI in healthcare, laying the groundwork for safer and more
reliable clinical decision-making.

</details>


### [19] [Watermarking and Anomaly Detection in Machine Learning Models for LORA RF Fingerprinting](https://arxiv.org/abs/2509.15170)
*Aarushi Mahajan,Wayne Burleson*

Main category: cs.CR

TL;DR: This paper proposes a secure RFFI system combining watermarks and anomaly detection, outperforming previous methods with 94.6% accuracy and robust authentication features.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning RFFI methods using spectrograms are vulnerable to copying, tampering, and evasion attacks, requiring stronger authentication mechanisms.

Method: Combines watermarking (simple trigger, adversarial trigger, gradient/weight signature) and anomaly detection (ResNet-34 on log-Mel spectrograms + convolutional VAE with KL warm-up and free-bits) for RFFI.

Result: Achieves 94.6% accuracy, 98% watermark success rate, and 0.94 AUROC on LoRa dataset.

Conclusion: The proposed RFFI system offers verifiable, tamper-resistant authentication with high accuracy and effective watermarking.

Abstract: Radio frequency fingerprint identification (RFFI) distinguishes wireless
devices by the small variations in their analog circuits, avoiding heavy
cryptographic authentication. While deep learning on spectrograms improves
accuracy, models remain vulnerable to copying, tampering, and evasion. We
present a stronger RFFI system combining watermarking for ownership proof and
anomaly detection for spotting suspicious inputs. Using a ResNet-34 on log-Mel
spectrograms, we embed three watermarks: a simple trigger, an adversarially
trained trigger robust to noise and filtering, and a hidden gradient/weight
signature. A convolutional Variational Autoencoders (VAE) with Kullback-Leibler
(KL) warm-up and free-bits flags off-distribution queries. On the LoRa dataset,
our system achieves 94.6% accuracy, 98% watermark success, and 0.94 AUROC,
offering verifiable, tamper-resistant authentication.

</details>


### [20] [Beyond Surface Alignment: Rebuilding LLMs Safety Mechanism via Probabilistically Ablating Refusal Direction](https://arxiv.org/abs/2509.15202)
*Yuanbo Xie,Yingjie Zhang,Tianyun Liu,Duohe Ma,Tingwen Liu*

Main category: cs.CR

TL;DR: The paper introduces DeepRefusal, a robust safety alignment framework that dynamically rebuilds refusal mechanisms from jailbreak states to defend against attacks, reducing success rates by ~95%.


<details>
  <summary>Details</summary>
Motivation: Current safety alignment methods lack depth and robust internel defense, making them vulnerable to attacks like prefilling and refusal direction manipulation.

Method: DeepRefusal dynamically rebuilds refusal mechanisms by probabilistically ablating the refusal direction during fine-tuning across layers and token depths.

Result: DeepRefusal reduced attack success rates by ~95% on four open-source LLMs and six attacks with minimal performance loss.

Conclusion: DeepRefusal provides a robust safety alignment framework that addresses previous limitations and defends against various jailbreak strategies.

Abstract: Jailbreak attacks pose persistent threats to large language models (LLMs).
Current safety alignment methods have attempted to address these issues, but
they experience two significant limitations: insufficient safety alignment
depth and unrobust internal defense mechanisms. These limitations make them
vulnerable to adversarial attacks such as prefilling and refusal direction
manipulation. We introduce DeepRefusal, a robust safety alignment framework
that overcomes these issues. DeepRefusal forces the model to dynamically
rebuild its refusal mechanisms from jailbreak states. This is achieved by
probabilistically ablating the refusal direction across layers and token depths
during fine-tuning. Our method not only defends against prefilling and refusal
direction attacks but also demonstrates strong resilience against other unseen
jailbreak strategies. Extensive evaluations on four open-source LLM families
and six representative attacks show that DeepRefusal reduces attack success
rates by approximately 95%, while maintaining model capabilities with minimal
performance degradation.

</details>


### [21] [Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems](https://arxiv.org/abs/2509.15213)
*Yicheng Zhang,Zijian Huang,Sophie Chen,Erfan Shayegani,Jiasi Chen,Nael Abu-Ghazaleh*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Extended reality (XR) applications increasingly integrate Large Language
Models (LLMs) to enhance user experience, scene understanding, and even
generate executable XR content, and are often called "AI glasses". Despite
these potential benefits, the integrated XR-LLM pipeline makes XR applications
vulnerable to new forms of attacks. In this paper, we analyze LLM-Integated XR
systems in the literature and in practice and categorize them along different
dimensions from a systems perspective. Building on this categorization, we
identify a common threat model and demonstrate a series of proof-of-concept
attacks on multiple XR platforms that employ various LLM models (Meta Quest 3,
Meta Ray-Ban, Android, and Microsoft HoloLens 2 running Llama and GPT models).
Although these platforms each implement LLM integration differently, they share
vulnerabilities where an attacker can modify the public context surrounding a
legitimate LLM query, resulting in erroneous visual or auditory feedback to
users, thus compromising their safety or privacy, sowing confusion, or other
harmful effects. To defend against these threats, we discuss mitigation
strategies and best practices for developers, including an initial defense
prototype, and call on the community to develop new protection mechanisms to
mitigate these risks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [22] [Evolution of Kernels: Automated RISC-V Kernel Optimization with Large Language Models](https://arxiv.org/abs/2509.14265)
*Siyuan Chen,Zhichao Lu,Qingfu Zhang*

Main category: cs.SE

TL;DR: Auto kernel design avoids software bottlenecks using LLMs in areas with scarce references.


<details>
  <summary>Details</summary>
Motivation: New platforms like RISC-V are having hard times with kernel design, because there aren't enough references available unlike more established ones such as CUDA. This shortage of references hampers the performance levels one can achieve.

Method: EoK, based on LLMs, uses evolutionary program search to automatically design kernels even when references are limited. It derives and formalizes reusable optimization ideas from existing kernel development histories to assist the LLM. A RAG component adds RISC-V specific context to those ideas.

Result: EoK performed between 20% and 1.27x better than the best prior approaches on all 80 tested tasks.

Conclusion: EoK shows that LLMs can do great in areas with sparse references by incorporating human insights.

Abstract: Automated kernel design is critical for overcoming software ecosystem
barriers in emerging hardware platforms like RISC-V. While large language
models (LLMs) have shown promise for automated kernel optimization,
demonstrating success in CUDA domains with comprehensive technical documents
and mature codebases, their effectiveness remains unproven for reference-scarce
domains like RISC-V. We present Evolution of Kernels (EoK), a novel LLM-based
evolutionary program search framework that automates kernel design for domains
with limited reference material. EoK mitigates reference scarcity by mining and
formalizing reusable optimization ideas (general design principles + actionable
thoughts) from established kernel libraries' development histories; it then
guides parallel LLM explorations using these ideas, enriched via
Retrieval-Augmented Generation (RAG) with RISC-V-specific context, prioritizing
historically effective techniques. Empirically, EoK achieves a median 1.27x
speedup, surpassing human experts on all 80 evaluated kernel design tasks and
improving upon prior LLM-based automated kernel design methods by 20%. These
results underscore the viability of incorporating human experience into
emerging domains and highlight the immense potential of LLM-based automated
kernel optimization.

</details>


### [23] [Automated and Context-Aware Code Documentation Leveraging Advanced LLMs](https://arxiv.org/abs/2509.14273)
*Swapnil Sharma Sarker,Tanzina Taher Ifty*

Main category: cs.SE

TL;DR: This work creates a modern Javadoc dataset and benchmarks LLMs, showing LLaMA 3.1 as a reliable tool for automated template-based documentation.


<details>
  <summary>Details</summary>
Motivation: Manual Javadoc generation is tedious, and existing datasets lack coverage of modern Java frameworks/semantics; this study addresses these gaps.

Method: The paper introduces a context-aware dataset for Javadoc generation with modern Java features and evaluates five open-source LLMs (LLaMA-3.1, Gemma-2, Phi-3, Mistral, Qwen-2.5) using zero-shot, few-shot, and fine-tuned setups.

Result: LLaMA 3.1 consistently outperformed other models in Javadoc tasks, demonstrating it as a viable open-source alternative to proprietary systems.

Conclusion: The study concludes that a tailored dataset and LLaMA 3.1 offer reliable, context-aware Javadoc generation, addressing gaps in automated template-based documentation.

Abstract: Code documentation is essential to improve software maintainability and
comprehension. The tedious nature of manual code documentation has led to much
research on automated documentation generation. Existing automated approaches
primarily focused on code summarization, leaving a gap in template-based
documentation generation (e.g., Javadoc), particularly with publicly available
Large Language Models (LLMs). Furthermore, progress in this area has been
hindered by the lack of a Javadoc-specific dataset that incorporates modern
language features, provides broad framework/library coverage, and includes
necessary contextual information. This study aims to address these gaps by
developing a tailored dataset and assessing the capabilities of publicly
available LLMs for context-aware, template-based Javadoc generation. In this
work, we present a novel, context-aware dataset for Javadoc generation that
includes critical structural and semantic information from modern Java
codebases. We evaluate five open-source LLMs (including LLaMA-3.1, Gemma-2,
Phi-3, Mistral, Qwen-2.5) using zero-shot, few-shot, and fine-tuned setups and
provide a comparative analysis of their performance. Our results demonstrate
that LLaMA 3.1 performs consistently well and is a reliable candidate for
practical, automated Javadoc generation, offering a viable alternative to
proprietary systems.

</details>


### [24] [Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization](https://arxiv.org/abs/2509.14279)
*Robert Tjarko Lange,Qi Sun,Aaditya Prasad,Maxence Faldor,Yujin Tang,David Ha*

Main category: cs.SE

TL;DR: This work advances CUDA kernel optimization for LLMs by presenting robust-kbench and an agentic pipeline that combines evolutionary optimization with LLM-based verification. The result is faster, more accurate kernel implementations verified through a diverse, loophole-free benchmark suite.


<details>
  <summary>Details</summary>
Motivation: Current LLM optimizations focus on high-level solutions, neglecting CUDA kernel-level efficiency. Existing benchmarks lack robustness and diversity, limiting meaningful generalization assessments. The work addresses these issues through targeted algorithmic and evaluative innovations.

Method: The approach combines a novel benchmark (robust-kbench) with a sequential workflow: translating PyTorch to CUDA kernels, optimizing via evolutionary meta-generation, and verifying correctness with LLM-based tools. This framework enables iterative runtime improvements and efficient hardware validation.

Result: Evaluated on robust-kbench, the framework generates CUDA kernels outperforming PyTorch implementations in runtime (forward/backward passes), supports operation fusion, and applies runtime optimizations. The verification system accurately filters invalid kernels, improving hardware testing efficiency.

Conclusion: The paper introduces robust-kbench and an automated agentic framework to enhance CUDA kernel optimization for LLMs in software engineering tasks. This addresses gaps in low-level optimization and benchmarking limitations, achieving superior performance and correctness verification compared to existing methods.

Abstract: Recent advances in large language models (LLMs) demonstrate their
effectiveness in scaling test-time compute for software engineering tasks.
However, these approaches often focus on high-level solutions, with limited
attention to optimizing low-level CUDA kernel implementations. Additionally,
existing kernel generation benchmarks suffer from exploitable loopholes and
insufficient diversity in testing conditions, hindering true generalization
assessment. To address these limitations, we introduce robust-kbench, a new
benchmark for rigorous evaluation of kernel performance and correctness across
varied scenarios. Furthermore, we present a comprehensive agentic framework
that automates CUDA kernel discovery, verification, and optimization. This
pipeline enables frontier LLMs to translate torch code to CUDA kernels and
iteratively improve their runtime within our robust evaluation setting. Our
sequential workflow first translates PyTorch code into equivalent CUDA kernels.
It then optimizes their runtime using a novel evolutionary meta-generation
procedure tailored to the CUDA ecosystem, guided by LLM-based verifiers for
correctness and efficient filtering. Evaluated on robust-kbench, our approach
produces CUDA kernels outperforming torch implementations for practical
applications, including forward and backward passes. It can fuse operations and
deploy various runtime optimization strategies. The verifier workflow
accurately classifies incorrect kernels, enhancing hardware verification
efficiency.

</details>


### [25] [SCoGen: Scenario-Centric Graph-Based Synthesis of Real-World Code Problems](https://arxiv.org/abs/2509.14281)
*Xifeng Yao,Dongyu Lang,Wu Zhang,Xintong Guo,Huarui Xie,Yinhao Ni,Ping Liu,Guang Shen,Yi Bai,Dandan Tu,Changzheng Zhang*

Main category: cs.SE

TL;DR: A novel framework synthesizes real-world code problems using domain data and graphs, outperforming existing models in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current advancements in code large language models are hindered by limited real-world coding data, motivating the need to synthesize realistic, diverse code problems for effective training and evaluation.

Method: The framework integrates domain knowledge, domain skills, and coding skills extracted from datasets like Stack Overflow and Kaggle. It constructs a scenario-centric graph to interlink these elements and employs a graph-based sampling strategy to generate diverse, complexity-controlled code problems.

Result: Experiments show the method outperforms state-of-the-art open-source models (both code-focused and general-purpose) across multiple real-world benchmarks, achieving consistent improvements in performance.

Conclusion: The proposed framework effectively addresses the scarcity of real-world coding problems by synthesizing high-quality, scenario-driven code problems, significantly advancing the development of code large language models.

Abstract: Significant advancements have been made in the capabilities of code large
language models, leading to their rapid adoption and application across a wide
range of domains. However, their further advancements are often constrained by
the scarcity of real-world coding problems. To bridge this gap, we propose a
novel framework for synthesizing code problems that emulate authentic
real-world scenarios. This framework systematically integrates domain
knowledge, domain skills, and coding skills, all of which are meticulously
extracted from real-world programming-related datasets, including Stack
Overflow and Kaggle. The extracted elements serve as the foundational building
blocks for constructing code problems. To align the generated problems with
practical applications, application scenarios are also mined from the
aforementioned datasets. These scenarios are then utilized to construct a
scenario-centric graph that interconnects domain knowledge, domain skills, and
coding skills. Based on this structured representation, a sampling strategy on
the graph is designed, which effectively controls the generation of a code
problem with complexity and diversity, reflects real-world challenges.
Experimental results demonstrate that the proposed method consistently achieves
superior performance over state-of-the-art open-source large language models of
varying sizes and functionalities, including both coders and general-purpose
models, across a diverse set of real-world benchmarks.

</details>


### [26] [Monitoring Machine Learning Systems: A Multivocal Literature Review](https://arxiv.org/abs/2509.14294)
*Hira Naveed,Scott Barnett,Chetan Arora,John Grundy,Hourieh Khalajzadeh,Omar Haggag*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Context: Dynamic production environments make it challenging to maintain
reliable machine learning (ML) systems. Runtime issues, such as changes in data
patterns or operating contexts, that degrade model performance are a common
occurrence in production settings. Monitoring enables early detection and
mitigation of these runtime issues, helping maintain users' trust and prevent
unwanted consequences for organizations. Aim: This study aims to provide a
comprehensive overview of the ML monitoring literature. Method: We conducted a
multivocal literature review (MLR) following the well established guidelines by
Garousi to investigate various aspects of ML monitoring approaches in 136
papers. Results: We analyzed selected studies based on four key areas: (1) the
motivations, goals, and context; (2) the monitored aspects, specific
techniques, metrics, and tools; (3) the contributions and benefits; and (4) the
current limitations. We also discuss several insights found in the studies,
their implications, and recommendations for future research and practice.
Conclusion: Our MLR identifies and summarizes ML monitoring practices and gaps,
emphasizing similarities and disconnects between formal and gray literature.
Our study is valuable for both academics and practitioners, as it helps select
appropriate solutions, highlights limitations in current approaches, and
provides future directions for research and tool development.

</details>


### [27] [On the Illusion of Success: An Empirical Study of Build Reruns and Silent Failures in Industrial CI](https://arxiv.org/abs/2509.14347)
*Henri Aïdasso,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: This paper pioneers the study of silent CI failures, revealing 11% rerates of seemingly successful jobs and key predictors. It identifies 11 failure categories (e.g., caching) and suggests improvements for reliable CI systems.


<details>
  <summary>Details</summary>
Motivation: Current CI practices struggle with non-deterministic issues like intermittent and silent failures, which reduce trust in build outcomes and increase costs. Previous studies focused on intermittent failures but overlooked silent failures (jobs marked as successful but incomplete/defective).

Method: The paper conducts an empirical study using data from 142,387 jobs across 81 industrial projects, analyzed with mixed-effects models (32 variables) and manual analysis of 92 public issues.

Result: 11% of successful jobs were rerun, with 35% of reruns occurring after 24 hours. Key factors included testing/static analysis tasks, Shell scripts, and developer behaviors. 11 failure categories were identified, including artifact/operation errors, caching issues, and ignored exit codes.

Conclusion: The study concludes that silent failures in CI are significant and often overlooked, leading to potential bugs in production. The research emphasizes the need for improved practices to address these failures and enhance CI reliability.

Abstract: Reliability of build outcomes is a cornerstone of effective Continuous
Integration (CI). Yet in practice, developers often struggle with
non-deterministic issues in the code or CI infrastructure, which undermine
trust in build results. When faced with such unexpected outcomes, developers
often repeatedly rerun jobs hoping for true success, but this practice is known
to increase CI costs and reduce productivity. While recent studies have focused
on intermittent job failures, no prior work has investigated silent failures,
where build jobs are marked as successful but fail to complete all or part of
their tasks. Such silent failures often go unnoticed, creating an illusion of
success with detrimental consequences such as bugs escaping into production.
This paper presents the first empirical study of silent failures through the
practice of rerunning successful jobs. An analysis of 142,387 jobs across 81
industrial projects shows that 11% of successful jobs are rerun, with 35% of
these reruns occurring after more than 24 hours. Using mixed-effects models on
32 independent variables (AUC of 85%), we identified key factors associated
with reruns of successful jobs, notably testing and static analysis tasks,
scripting languages like Shell, and developers prior rerun tendencies. A
further analysis of 92 public issues revealed 11 categories of silent failures
aligning with these factors, the most frequent being artifact operation errors,
caching errors, and ignored exit codes. Overall, our findings provide valuable
insights into the circumstances and causes of silent failures to raise
awareness among teams, and present solutions to improve CI reliability.

</details>


### [28] [CodeLSI: Leveraging Foundation Models for Automated Code Generation with Low-Rank Optimization and Domain-Specific Instruction Tuning](https://arxiv.org/abs/2509.14373)
*Huy Le,Phong Nguyen,Hao Do,Tuan Nguyen,Thien Pham,Anh Nguyen-Duc,Tho Quan*

Main category: cs.SE

TL;DR: CodeLSI introduces a framework using low-rank optimization and domain-specific instruction tuning for cost-effective, secure code generation without external APIs.


<details>
  <summary>Details</summary>
Motivation: Third-party API reliance in foundation model-based code generation poses challenges in domain specificity, cost, and security. This study aims to provide an independent, scalable solution.

Method: The framework utilizes low-rank adaptation to reduce pre-training costs and domain-specific instruction tuning to align code generation with organizational needs, validated through JavaScript tasks on internal datasets.

Result: CodeLSI outperformed baselines in relevance, accuracy, and domain fit while reducing resource requirements via low-rank optimization, enabling cost-effective, scalable infrastructure training.

Conclusion: CodeLSI demonstrates that combining low-rank techniques with domain-specific tuning enhances foundation models for secure, efficient, and domain-adapted code generation, offering a competitive alternative to commercial API solutions.

Abstract: Context: Automated code generation using Foundation Models (FMs) offers
promising solutions for enhancing software development efficiency. However,
challenges remain in ensuring domain specificity, cost-effectiveness, and
security - especially when relying on third-party APIs. This paper introduces
CodeLSI, a framework that combines low-rank optimization and domain-specific
instruction tuning to address these challenges.
  Objectives: The aim of this study is to develop and evaluate CodeLSI, a novel
approach for generating high-quality code tailored to specific domains, using
FMs fine-tuned on company infrastructure without dependence on external APIs.
  Methods: CodeLSI applies low-rank adaptation techniques to reduce the
computational cost of model pre-training and fine-tuning. Domain-specific
instruction tuning is employed to align code generation with organizational
needs. We implemented and tested the framework on real-world JavaScript coding
tasks using datasets drawn from internal software projects.
  Results: Experimental evaluations show that CodeLSI produces high-quality,
context aware code. It outperforms baseline models in terms of relevance,
accuracy, and domain fit. The use of low-rank optimization significantly
reduced resource requirements, enabling scalable training on company-owned
infrastructure.
  Conclusion: CodeLSI demonstrates that combining low-rank optimization with
domain specific tuning can enhance the practicality and performance of FMs for
automated code generation. This approach provides a secure, cost-efficient
alternative to commercial API based solutions and supports faster, more
targeted innovation in software development.

</details>


### [29] [A Taxonomy of Prompt Defects in LLM Systems](https://arxiv.org/abs/2509.14404)
*Haoye Tian,Chong Wang,BoYang Yang,Lyuye Zhang,Yang Liu*

Main category: cs.SE

TL;DR: This paper introduces a first-of-its-kind taxonomy for prompt defects in LLMs, organized into six dimensions with mitigation strategies, advocating for systematic engineering practices to build reliable AI systems.


<details>
  <summary>Details</summary>
Motivation: Prompt design for LLMs is currently empirical and error-prone, with small mistakes causing unreliable, insecure, or inefficient behavior. The paper aims to provide a structured framework to identify, categorize, and mitigate these defects systematically.

Method: The authors conduct a systematic survey and taxonomy of prompt defects, organizing them into six dimensions with detailed subtypes, examples, and root cause analyses. Mitigation strategies are distilled into a master taxonomy linking defects to remedies.

Result: The paper introduces a comprehensive taxonomy of prompt defects across six dimensions, accompanied by mitigation strategies (e.g., automated guardrails, testing frameworks). It also highlights open challenges and emphasizes engineering-oriented methodologies for dependable LLM systems.

Conclusion: The paper concludes with a call for rigorous engineering methodologies to ensure dependable LLM-driven systems by addressing prompt defects systematically.

Abstract: Large Language Models (LLMs) have become key components of modern software,
with prompts acting as their de-facto programming interface. However, prompt
design remains largely empirical and small mistakes can cascade into
unreliable, insecure, or inefficient behavior. This paper presents the first
systematic survey and taxonomy of prompt defects, recurring ways that prompts
fail to elicit their intended behavior from LLMs. We organize defects along six
dimensions: (1) Specification and Intent, (2) Input and Content, (3) Structure
and Formatting, (4) Context and Memory, (5) Performance and Efficiency, and (6)
Maintainability and Engineering. Each dimension is refined into fine-grained
subtypes, illustrated with concrete examples and root cause analysis. Grounded
in software engineering principles, we show how these defects surface in real
development workflows and examine their downstream effects. For every subtype,
we distill mitigation strategies that span emerging prompt engineering
patterns, automated guardrails, testing harnesses, and evaluation frameworks.
We then summarize these strategies in a master taxonomy that links defect,
impact, and remedy. We conclude with open research challenges and a call for
rigorous engineering-oriented methodologies to ensure that LLM-driven systems
are dependable by design.

</details>


### [30] [An LLM-based multi-agent framework for agile effort estimation](https://arxiv.org/abs/2509.14483)
*Thanh-Long Bui,Hoa Khanh Dam,Rashina Hoda*

Main category: cs.SE

TL;DR: The paper introduces an LLM-based multi-agent framework for agile software development effort estimation that outperforms existing methods and enhances collaboration with human team members.


<details>
  <summary>Details</summary>
Motivation: Current agile effort estimation practices rely on subjective assessments, leading to inaccuracies and inconsistencies. Existing machine learning-based methods provide accurate estimates but lack the ability to explain or justify their predictions and interact with human team members.

Method: The authors propose a novel Large Language Model (LLM)-based multi-agent framework designed for agile effort estimation. This framework enables agents to produce estimates, coordinate, communicate, and discuss with human developers and other agents to reach a consensus. The integration of LLMs allows for both estimation and interaction capabilities.

Result: The evaluation results on a real-life dataset demonstrate that the LLM-based approach significantly outperforms existing state-of-the-art methods on most evaluation metrics and features a favorable collaborative user experience in agile estimation.

Conclusion: The paper concludes that their LLM-based multi-agent framework effectively addresses the limitations of current agile estimation practices by offering accurate, explainable, and collaborative effort estimation.

Abstract: Effort estimation is a crucial activity in agile software development, where
teams collaboratively review, discuss, and estimate the effort required to
complete user stories in a product backlog. Current practices in agile effort
estimation heavily rely on subjective assessments, leading to inaccuracies and
inconsistencies in the estimates. While recent machine learning-based methods
show promising accuracy, they cannot explain or justify their estimates and
lack the capability to interact with human team members. Our paper fills this
significant gap by leveraging the powerful capabilities of Large Language
Models (LLMs). We propose a novel LLM-based multi-agent framework for agile
estimation that not only can produce estimates, but also can coordinate,
communicate and discuss with human developers and other agents to reach a
consensus. Evaluation results on a real-life dataset show that our approach
outperforms state-of-the-art techniques across all evaluation metrics in the
majority of the cases. Our human study with software development practitioners
also demonstrates an overwhelmingly positive experience in collaborating with
our agents in agile effort estimation.

</details>


### [31] [Automating Modelica Module Generation Using Large Language Models: A Case Study on Building Control Description Language](https://arxiv.org/abs/2509.14623)
*Hanlong Wan,Xing Lu,Yan Chen,Karthik Devaprasad,Laura Hinkle*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Dynamic energy systems and controls require advanced modeling frameworks to
design and test supervisory and fault tolerant strategies. Modelica is a widely
used equation based language, but developing control modules is labor intensive
and requires specialized expertise. This paper examines the use of large
language models (LLMs) to automate the generation of Control Description
Language modules in the Building Modelica Library as a case study. We developed
a structured workflow that combines standardized prompt scaffolds, library
aware grounding, automated compilation with OpenModelica, and human in the loop
evaluation. Experiments were carried out on four basic logic tasks (And, Or,
Not, and Switch) and five control modules (chiller enable/disable, bypass valve
control, cooling tower fan speed, plant requests, and relief damper control).
The results showed that GPT 4o failed to produce executable Modelica code in
zero shot mode, while Claude Sonnet 4 achieved up to full success for basic
logic blocks with carefully engineered prompts. For control modules, success
rates reached 83 percent, and failed outputs required medium level human repair
(estimated one to eight hours). Retrieval augmented generation often produced
mismatches in module selection (for example, And retrieved as Or), while a
deterministic hard rule search strategy avoided these errors. Human evaluation
also outperformed AI evaluation, since current LLMs cannot assess simulation
results or validate behavioral correctness. Despite these limitations, the LLM
assisted workflow reduced the average development time from 10 to 20 hours down
to 4 to 6 hours per module, corresponding to 40 to 60 percent time savings.
These results highlight both the potential and current limitations of LLM
assisted Modelica generation, and point to future research in pre simulation
validation, stronger grounding, and closed loop evaluation.

</details>


### [32] [Evaluating the Effectiveness of Coverage-Guided Fuzzing for Testing Deep Learning Library APIs](https://arxiv.org/abs/2509.14626)
*Feiran Qin,M. M. Abid Naziri,Hengyu Ai,Saikat Dutta,Marcelo d'Amorim*

Main category: cs.SE

TL;DR: This paper proposes FlashFuzz, an LLM-based coverage-guided fuzzing technique for deep learning libraries. FlashFuzz synthesizes API harnesses using templates and documentation, achieving significant improvements in code coverage, bug detection, and speed compared to existing methods, with 42 new bugs discovered in PyTorch/TensorFlow.


<details>
  <summary>Details</summary>
Motivation: Current DL library testing methods (API-level/model-level fuzzing without coverage-guidance) face limitations in effectiveness and efficiency. No prior work has successfully applied coverage-guided fuzzing (CGF), which requires test harnesses for each API, a manual and laborious process.

Method: FlashFuzz automatically generates and refines API test harnesses using Large Language Models (LLMs). It combines documentation, templates, and helper functions with a feedback-driven synthesis/repair mechanism. Applied to 1,151 PyTorch and 662 TensorFlow APIs.

Result: FlashFuzz outperformed state-of-the-art methods (ACETest/PathFinder/TitanFuzz): 101-212.88x higher coverage, 1-5.4x better validity rate, 1-1182x faster input generation. Discovered 42 new bugs, 8 of which were patched.

Conclusion: CGF is feasible and effective for DL libraries through automated harness generation. FlashFuzz establishes a strong baseline for future testing approaches and demonstrates concrete bug detection capabilities.

Abstract: Deep Learning (DL) libraries such as PyTorch provide the core components to
build major AI-enabled applications. Finding bugs in these libraries is
important and challenging. Prior approaches have tackled this by performing
either API-level fuzzing or model-level fuzzing, but they do not use coverage
guidance, which limits their effectiveness and efficiency. This raises an
intriguing question: can coverage guided fuzzing (CGF), in particular
frameworks like LibFuzzer, be effectively applied to DL libraries, and does it
offer meaningful improvements in code coverage, bug detection, and scalability
compared to prior methods?
  We present the first in-depth study to answer this question. A key challenge
in applying CGF to DL libraries is the need to create a test harness for each
API that can transform byte-level fuzzer inputs into valid API inputs. To
address this, we propose FlashFuzz, a technique that leverages Large Language
Models (LLMs) to automatically synthesize API-level harnesses by combining
templates, helper functions, and API documentation. FlashFuzz uses a feedback
driven strategy to iteratively synthesize and repair harnesses. With this
approach, FlashFuzz synthesizes harnesses for 1,151 PyTorch and 662 TensorFlow
APIs. Compared to state-of-the-art fuzzing methods (ACETest, PathFinder, and
TitanFuzz), FlashFuzz achieves up to 101.13 to 212.88 percent higher coverage
and 1.0x to 5.4x higher validity rate, while also delivering 1x to 1182x
speedups in input generation. FlashFuzz has discovered 42 previously unknown
bugs in PyTorch and TensorFlow, 8 of which are already fixed. Our study
confirms that CGF can be effectively applied to DL libraries and provides a
strong baseline for future testing approaches.

</details>


### [33] [SALT4Decompile: Inferring Source-level Abstract Logic Tree for LLM-Based Binary Decompilation](https://arxiv.org/abs/2509.14646)
*Yongpan Wang,Xin Xu,Xiaojie Zhu,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: SaltM is a binary decompilation framework that abstracts binary operations into a high-level logic tree to guide LLMs, achieving state-of-the-art accuracy (70.4% TCP) and robustness against obfuscation, while improving readability and analyst efficiency in reverse engineering.


<details>
  <summary>Details</summary>
Motivation: Traditional decompilation approaches using LLMs treat assembly code as linear instruction sequences, ignoring arbitrary jump patterns and isolated data segments critical for source code semantics. This limitation impairs their ability to infer accurate program logic.

Method: SaltM constructs a Source-level Abstract Logic Tree (Salt) to abstract binary operations (e.g., jumps) into high-level logic. It fine-tunes an LLM using the Salt tree for semantic recovery, followed by error correction and symbol recovery to generate decompiled code.

Result: SaltM achieves a 70.4% TCP rate on Decompile-Eval (10.6% improvement over SOTA), validates robustness against four obfuscation techniques, and outperforms baselines across three datasets (Decompile-Eval, MBPP, Exebench). Real-world evaluations and a user study confirm its practical effectiveness for human analysts.

Conclusion: SaltM significantly outperforms existing state-of-the-art decompilation methods by effectively recovering source code logic through abstracting binary operations into a high-level structure. It demonstrates robustness against obfuscation techniques and provides superior assistance to human analysts in understanding binary functions.

Abstract: Decompilation is widely used in reverse engineering to recover high-level
language code from binary executables. While recent approaches leveraging Large
Language Models (LLMs) have shown promising progress, they typically treat
assembly code as a linear sequence of instructions, overlooking arbitrary jump
patterns and isolated data segments inherent to binary files. This limitation
significantly hinders their ability to correctly infer source code semantics
from assembly code. To address this limitation, we propose \saltm, a novel
binary decompilation method that abstracts stable logical features shared
between binary and source code. The core idea of \saltm is to abstract selected
binary-level operations, such as specific jumps, into a high-level logic
framework that better guides LLMs in semantic recovery. Given a binary
function, \saltm constructs a Source-level Abstract Logic Tree (\salt) from
assembly code to approximate the logic structure of high-level language. It
then fine-tunes an LLM using the reconstructed \salt to generate decompiled
code. Finally, the output is refined through error correction and symbol
recovery to improve readability and correctness. We compare \saltm to three
categories of baselines (general-purpose LLMs, commercial decompilers, and
decompilation methods) using three well-known datasets (Decompile-Eval, MBPP,
Exebench). Our experimental results demonstrate that \saltm is highly effective
in recovering the logic of the source code, significantly outperforming
state-of-the-art methods (e.g., 70.4\% TCP rate on Decompile-Eval with a 10.6\%
improvement). The results further validate its robustness against four commonly
used obfuscation techniques. Additionally, analyses of real-world software and
a user study confirm that our decompiled output offers superior assistance to
human analysts in comprehending binary functions.

</details>


### [34] [Wireless Communication Performance Testing: From Laboratory Environment to Research Vessel](https://arxiv.org/abs/2509.14740)
*Andrei-Raoul Morariu,Andreas Strandberg,Bogdan Iancu,Jerker Bjorkqvist*

Main category: cs.SE

TL;DR: This study examines how laboratory objects and environment factors like distance/placement on a moving boat affect wireless signal transmission efficiency in shared spectrums. Key findings indicate environmental obstructions significantly impede communication. 


<details>
  <summary>Details</summary>
Motivation: The paper aims to evaluate how physical obstructions and placement variations in dynamic, complex environments impact wireless communication reliability. Understanding these factors helps design robust systems.

Method: Experiments combined lab-based line-of-sight obstruction simulations with field measurements on an electric research boat, varying Tx/Rx placement and distances.

Result: Lab obstructions caused measurable signal attenuation, while boat experiments demonstrated environment-specific transmission losses due to placement and distance.

Conclusion: The paper concludes that environmental factors critically influence wireless communication in obstructed/dynamic settings, requiring tailored system designs to mitigate these effects.

Abstract: This study investigates signal transmission within a shared spectrum,
focusing on measurements conducted both in laboratory and outdoor environments.
The objective was to demonstrate how laboratory objects obstructing the line of
sight can attenuate the signal between a transmitter (Tx) and a receiver (Rx).
Additionally, we examined the impact of distance and placement in various
locations aboard an electric research boat on signal transmission efficiency.
These findings contribute to understanding whether the environmental factors
influence wireless communication in dynamic and obstructed environments.

</details>


### [35] [On the Use of Agentic Coding Manifests: An Empirical Study of Claude Code](https://arxiv.org/abs/2509.14744)
*Worawalan Chatlatanagulchai,Kundjanasith Thonglek,Brittany Reid,Yutaro Kashiwa,Pattara Leelaprute,Arnon Rungsawang,Bundit Manaskasemsak,Hajimu Iida*

Main category: cs.SE

TL;DR: The paper analyzes 253 Claude.md agent manifests to reveal shallow hierarchical structures and content patterns dominated by operational instructions and technical notes, highlighting a need for better documentation to aid developers.


<details>
  <summary>Details</summary>
Motivation: The lack of clear documentation for creating agent manifests like Claude.md poses challenges for developers, prompting this study to identify structural and content patterns to guide their creation.

Method: Analyzed 253 Claude.md files from 242 repositories to identify common structural and content patterns through qualitative analysis.

Result: Manifests exhibit shallow hierarchies (one main heading with subsections) containing operational commands (60%), technical implementation notes (25%), and high-level architecture (15%).

Conclusion: The findings provide actionable insights for developers to structure manifests more effectively and motivate the creation of standardized documentation for agent configuration files.

Abstract: Agentic coding tools receive goals written in natural language as input,
break them down into specific tasks, and write/execute the actual code with
minimal human intervention. Key to this process are agent manifests,
configuration files (such as Claude.md) that provide agents with essential
project context, identity, and operational rules. However, the lack of
comprehensive and accessible documentation for creating these manifests
presents a significant challenge for developers. We analyzed 253 Claude.md
files from 242 repositories to identify structural patterns and common content.
Our findings show that manifests typically have shallow hierarchies with one
main heading and several subsections, with content dominated by operational
commands, technical implementation notes, and high-level architecture.

</details>


### [36] [On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub](https://arxiv.org/abs/2509.14745)
*Miku Watanabe,Hao Li,Yutaro Kashiwa,Brittany Reid,Hajimu Iida,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: Autonomous LLM-generated PRs have high acceptance rates (83.8%) but often need human refinement for complex tasks like bug fixes and style compliance, demonstrating their value is maximized with hybrid human-AI collaboration.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the gap in understanding whether autonomous LLM-generated pull requests (PRs) are practically useful and accepted in real-world software development workflows.

Method: The authors empirically analyzed 567 GitHub pull requests (PRs) generated using Claude Code across 157 open-source projects, evaluating acceptance rates and the extent of modifications required during integration.

Result: 83.8% of agent-assisted PRs were accepted, with 54.9% merged without modification. The remaining 45.1% required human revisions, especially for debugging, documentation improvements, and project-style compliance.

Conclusion: The study concludes that agent-assisted pull requests (PRs) are largely acceptable in open-source projects but still require human oversight for refinement, particularly for complex tasks like bug fixes and adherence to project standards.

Abstract: Large language models (LLMs) are increasingly being integrated into software
development processes. The ability to generate code and submit pull requests
with minimal human intervention, through the use of autonomous AI agents, is
poised to become a standard practice. However, little is known about the
practical usefulness of these pull requests and the extent to which their
contributions are accepted in real-world projects. In this paper, we
empirically study 567 GitHub pull requests (PRs) generated using Claude Code,
an agentic coding tool, across 157 diverse open-source projects. Our analysis
reveals that developers tend to rely on agents for tasks such as refactoring,
documentation, and testing. The results indicate that 83.8% of these
agent-assisted PRs are eventually accepted and merged by project maintainers,
with 54.9% of the merged PRs are integrated without further modification. The
remaining 45.1% require additional changes benefit from human revisions,
especially for bug fixes, documentation, and adherence to project-specific
standards. These findings suggest that while agent-assisted PRs are largely
acceptable, they still benefit from human oversight and refinement.

</details>


### [37] [RulER: Automated Rule-Based Semantic Error Localization and Repair for Code Translation](https://arxiv.org/abs/2509.14829)
*Shuo Jin,Songqiang Chen,Xiaoyuan Xie,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: RulER is a rule-based debugging method for code translation that uses automatically derived translation rules from LLMs to improve error localization and repair, outperforming existing methods by 20-272%.


<details>
  <summary>Details</summary>
Motivation: Existing code translation debugging methods struggle with unreliable code alignments and repair templates, leading to low localization accuracy and ineffective repairs due to imperfect translation models.

Method: RulER derives translation rules from LLM-generated correct translations, dynamically combines rules on expandable nodes (expressions/tokens) for adaptive alignment, and uses these rules as reliable references for precise error localization and repair.

Result: RulER outperformed baseline methods (BatFix, TransMap, direct LLM prompting) by 20-272% in Java-to-C++ and Python-to-C++ translation repair, achieving higher error localization and success rates.

Conclusion: RulER demonstrates that leveraging rule-based translation knowledge from LLMs provides a superior methodology for reliable code translation debugging, offering clear structural correspondences and reusable repair strategies.

Abstract: Automated code translation aims to convert programs between different
programming languages while maintaining their functionality. Due to the
imperfections of code translation models, the generated translations may
contain errors that compromise their reliability. Existing automated debugging
methods for code translation rely on code alignments and repair patch templates
to locate and fix erroneous translations. However, existing methods lack
reliable references to construct code alignments and design repair patch
templates, which significantly impacts their localization accuracy and repair
effectiveness. To address these limitations, we reintroduce code translation
rules and propose a rule-based debugging method for code translation, called
RulER. RulER automatically derives code translation rules from correct
translations generated by LLMs, enabling the efficient collection of diverse
translation rules. In addition, RulER dynamically combines the existing rules
on expandable nodes like expressions and tokens to further adaptively align
more statements. These rules capture clear and detailed structural
correspondences between source and target programming languages. Therefore,
they can serve as reliable and reusable references for code alignment and
repair template design, enabling RulER to locate and fix translation errors
effectively. Our evaluation of RulER on Java-to-C++ and Python-to-C++
translations produced by four code translation models demonstrates that RulER
outperforms state-of-the-art methods, BatFix and TransMap. Our experimental
results show that RulER outperformed the best baseline by 20% and 272% in terms
of error localization rates and repair success rates, respectively. RulER
exhibits superior repair performance compared to directly prompting LLMs for
patch generation, demonstrating a promising methodology for extracting and
leveraging coding knowledge from LLMs.

</details>


### [38] [CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects](https://arxiv.org/abs/2509.14856)
*Hanyang Guo,Xunjin Zheng,Zihan Liao,Hang Yu,Peng DI,Ziyin Zhang,Hong-Ning Dai*

Main category: cs.SE

TL;DR: This paper introduces CodeFuse-CR-Bench, a repository-level benchmark for holistic code review evaluation of LLMs, addressing existing benchmarks’ lack of real-world context. It proposes a mixed evaluation framework combining rule-based checks with model-based judgments, revealing Gemini 2.5 Pro’s leading performance and diverse LLM robustness to contextual noise.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for LLM-based code review (CR) use isolated tasks and simplified data, failing to capture the complex, context-rich nature of real-world CR. This limits their utility for practical LLM development.

Method: The authors created CodeFuse-CR-Bench (601 PRs across 70 projects, 9 domains), providing multi-faceted context (issue descriptions, PR details, repo states). They designed an evaluation framework with rule-based (location/syntax validation) and model-based (quality scoring) metrics, testing state-of-the-art LLMs.

Result: Key findings: (1）No LLM dominates all CR aspects; (2）Gemini 2.5 Pro achieves highest comprehensive performance; (3）LLMs show varied robustness to redundant context. Quantitative results establish baselines for future work.

Conclusion: The study underscores the necessity of holistic, multi-dimensional evaluation frameworks for CR systems. CodeFuse-CR-Bench and its methodology provide actionable insights for developing practical, context-aware CR assistants while exposing gaps in current LLM capabilities.

Abstract: Automated code review (CR) is a key application for Large Language Models
(LLMs), but progress is hampered by a "reality gap": existing benchmarks
evaluate models on isolated sub-tasks using simplified, context-poor data. This
fails to reflect the holistic context-rich nature of real-world CR. To bridge
this gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware
benchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601
high-quality instances from 70 Python projects covering nine Pull-Request (PR)
problem domains, where each instance provides rich, multi-faceted context
including the associated issue, PR details, and repository state, enabling
end-to-end evaluation. Beyond superficial metrics, we also propose a novel
evaluation framework that combines rule-based checks for location and syntax
with model-based judgments of review quality. We present the first large-scale
assessment of state-of-the-art LLMs on this comprehensive CR task. Our results
establish crucial baselines and reveal that (1) no single LLM dominates all
aspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive
performance; and (3) different LLMs exhibit varying robustness to redundant
context. These findings highlight the necessity of holistic, multi-dimensional
evaluation and provide actionable insights for advancing truly intelligent yet
practical CR assistants.

</details>


### [39] [CARGO: A Framework for Confidence-Aware Routing of Large Language Models](https://arxiv.org/abs/2509.14899)
*Amine Barrak,Yosr Fourati,Michael Olchawa,Emna Ksontini,Khalil Zoghlami*

Main category: cs.SE

TL;DR: The paper introduces CARGO, a lightweight, confidence-aware framework for dynamically selecting the most appropriate large language model (LLM) based on user prompts. It uses a two-stage system involving a regressor and an optional classifier to optimize performance and cost, achieving strong results across different LLMs.


<details>
  <summary>Details</summary>
Motivation: With the increasing diversity of large language models in terms of scale, specialization, and latency, there is a critical need for effective routing of user prompts to the optimal model to balance performance and cost without relying on human-annotated data.

Method: CARGO uses a single embedding-based regressor trained on LLM-judged pairwise comparisons to predict model performance. It optionally uses a binary classifier for uncertain predictions. Additionally, it supports category-specific regressors across five task groups (mathematics, coding, reasoning, summarization, creative writing) for domain-specific behavior.

Result: CARGO achieves a top-1 routing accuracy of 76.4% and win rates between 72% and 89% against individual experts when evaluated on four competitive LLMs (GPT-4o, Claude 3.5 Sonnet, DeepSeek V3, and Perplexity Sonar).

Conclusion: Confidence-guided, lightweight routing frameworks like CARGO can match expert-level performance with minimal overhead, making them a practical solution for real-world multi-model LLM deployments.

Abstract: As large language models (LLMs) proliferate in scale, specialization, and
latency profiles, the challenge of routing user prompts to the most appropriate
model has become increasingly critical for balancing performance and cost. We
introduce CARGO (Category-Aware Routing with Gap-based Optimization), a
lightweight, confidence-aware framework for dynamic LLM selection. CARGO
employs a single embedding-based regressor trained on LLM-judged pairwise
comparisons to predict model performance, with an optional binary classifier
invoked when predictions are uncertain. This two-stage design enables precise,
cost-aware routing without the need for human-annotated supervision. To capture
domain-specific behavior, CARGO also supports category-specific regressors
trained across five task groups: mathematics, coding, reasoning, summarization,
and creative writing. Evaluated on four competitive LLMs (GPT-4o, Claude 3.5
Sonnet, DeepSeek V3, and Perplexity Sonar), CARGO achieves a top-1 routing
accuracy of 76.4% and win rates ranging from 72% to 89% against individual
experts. These results demonstrate that confidence-guided, lightweight routing
can achieve expert-level performance with minimal overhead, offering a
practical solution for real-world, multi-model LLM deployments.

</details>


### [40] [Orion: Fuzzing Workflow Automation](https://arxiv.org/abs/2509.15195)
*Max Bazalii,Marius Fleischer*

Main category: cs.SE

TL;DR: Orion is a framework that integrates LLM reasoning with traditional tools to automate manual bottlenecks in fuzz testing, reducing human effort by 46-204x and enabling scalable vulnerability discovery.


<details>
  <summary>Details</summary>
Motivation: Modern fuzzers require substantial manual effort for code analysis, harness configuration, and result triage, with prior work addresses single stages but leaving integration to researchers.

Method: Orion combines LLMs for code reasoning/semantic guidance with deterministic tools for verification and precision tasks, creating an integrated automated fuzzing workflow.

Result: Orion achieves 46-204x human effort reduction across workflow stages and discovers two novel vulnerabilities in the clib open-source library.

Conclusion: By merging LLM capabilities with traditional tools, Orion effectively automates fuzzing campaigns at scale where human effort alone was impractical, demonstrating practical vulnerability discovery potential.

Abstract: Fuzz testing is one of the most effective techniques for finding software
vulnerabilities. While modern fuzzers can generate inputs and monitor
executions automatically, the overall workflow, from analyzing a codebase, to
configuring harnesses, to triaging results, still requires substantial manual
effort. Prior attempts focused on single stages such as harness synthesis or
input minimization, leaving researchers to manually connect the pieces into a
complete fuzzing campaign.
  We introduce Orion, a framework that automates the the manual bottlenecks of
fuzzing by integrating LLM reasoning with traditional tools, allowing campaigns
to scale to settings where human effort alone was impractical. Orion uses LLMs
for code reasoning and semantic guidance, while relying on deterministic tools
for verification, iterative refinement, and tasks that require precision.
Across our benchmark suite, Orion reduces human effort by 46-204x depending on
the workflow stage, and we demonstrate its effectiveness through the discovery
of two previously unknown vulnerabilities in the widely used open-source clib
library.

</details>


### [41] ["Let it be Chaos in the Plumbing!" Usage and Efficacy of Chaos Engineering in DevOps Pipelines](https://arxiv.org/abs/2509.14931)
*Stefano Fossati,Damian Andrew Tamburri,Massimiliano Di Penta,Marco Tonnarelli*

Main category: cs.SE

TL;DR: The paper explores how chaos engineering (CE) is being adapted by industry practitioners in DevOps environments through a systematic review of 50 industry sources from 2019 to 2024. It extends CE principles into 10 distinct concepts and highlights the growing importance of controlled experimentation, risk mitigation, and automation in improving system resilience.


<details>
  <summary>Details</summary>
Motivation: Chaos engineering has become a widely adopted practice for testing and building resilience in distributed systems, but there is a need to better understand its emerging adaptations and applications in real-world DevOps environments. This lack of understanding creates a knowledge gap about the evolving nature of CE practices and their practical implementation needs.

Method: A systematic gray literature review methodology was employed, analyzing 50 relevant sources published between 2019 and early 2024. The study used inductive qualitative analysis to develop a classification framework that extends the foundational CE principles into ten distinct conceptual categories.

Result: The analysis identified a shift toward controlled experimentation approaches using structural mitigation strategies - 39% of sources emphasized operational risk mitigation. Significant findings showed 61% of practitioners implemented observability platforms while 75% automated chaos testing processes to match agile DevOps practices. The study produced the first classification framework capturing CE's practical evolution into testing, automation, risk management, and measurement domains.

Conclusion: This paper's classification framework captures both the consistent value of CE principles and their adaptable implementation in dynamic DevOps environments. The key conclusions emphasize the transition from purely experimental CE to a matured discipline focusing on risk mitigation (83% of sources had risk evaluation models) and automation (persistent 75% implementation rate) in production systems. These findings provide practical guidance for implementing CE in continuously evolving software environments.

Abstract: Chaos Engineering (CE) has emerged as a proactive method to improve the
resilience of modern distributed systems, particularly within DevOps
environments. Originally pioneered by Netflix, CE simulates real-world failures
to expose weaknesses before they impact production. In this paper, we present a
systematic gray literature review that investigates how industry practitioners
have adopted and adapted CE principles over recent years. Analyzing 50 sources
published between 2019 and early 2024, we developed a comprehensive
classification framework that extends the foundational CE principles into ten
distinct concepts. Our study reveals that while the core tenets of CE remain
influential, practitioners increasingly emphasize controlled experimentation,
automation, and risk mitigation strategies to align with the demands of agile
and continuously evolving DevOps pipelines. Our results enhance the
understanding of how CE is intended and implemented in practice, and offer
guidance for future research and industrial applications aimed at improving
system robustness in dynamic production environments.

</details>


### [42] [Code Less to Code More: Streamlining Language Server Protocol and Type System Development for Language Families](https://arxiv.org/abs/2509.15150)
*Federico Bruzzone,Walter Cazzola,Luca Favalli*

Main category: cs.SE

TL;DR: This paper proposes Typelang and a modular framework to reduce the complexity of implementing language servers for multiple programming languages and editors by enabling reusable type systems, automated LSP plugin generation, and variant-oriented programming, achieving significant automation and effort reduction.


<details>
  <summary>Details</summary>
Motivation: Developing editing support for multiple language-editor combinations is complex and inefficient despite the Language Server Protocol (LSP). Existing tools lack modularity, reusability, and systematic generation of type-driven language servers, increasing maintenance costs.

Method: 1. Typelang: A domain-specific language family for modular type system implementation. 2. Modular language server generation workflow leveraging component reuse. 3. Variant-oriented programming and cross-artifact coordination for managing interdependent software variants. 4. An LSP plugin generator automating support for multiple editors.

Result: 93.48\% reduction in type system implementation characters and 100\% automation of LSP plugin generation. Demonstrated in Neverlang with three editors, reducing combinations from L×E to N×1 through artifact reuse.

Conclusion: The approach significantly lowers editing support development effort for language families by enabling modular, reusable type systems and automating LSP integration, particularly beneficial when reusing language artifacts across projects.

Abstract: Developing editing support for $L$ languages in $E$ editors is complex and
time-consuming. Some languages do not provide dedicated editors, while others
offer a single native editor. The $\textit{language server protocol}$ (LSP)
reduces the language-editor combinations $L \times E$ to $L + E$, where a
single language server communicates with editors via LSP plugins. However,
overlapping implementations of linguistic components remain an issue. Existing
language workbenches struggle with modularity, reusability, and leveraging type
systems for language server generation. In this work, we propose: (i) Typelang,
a family of domain-specific languages for modular, composable, and reusable
type system implementation, (ii) a modular language server generation process,
producing servers for languages built in a modular workbench, (iii) the
variant-oriented programming paradigm and a cross-artifact coordination layer
to manage interdependent software variants, and (iv) an LSP plugin generator,
reducing $E$ to $1$ by automating plugin creation for multiple editors. To
simplify editing support for language families, each language artifact
integrates its own Typelang variant, used to generate language servers. This
reduces combinations to $T \times 1$, where $T = L$ represents the number of
type systems. Further reuse of language artifacts across languages lowers this
to $N \times 1$, where $N << T$, representing unique type systems. We implement
Typelang in Neverlang, generating language servers for each artifact and LSP
plugins for three editors. Empirical evaluation shows a 93.48% reduction in
characters needed for type system implementation and 100% automation of LSP
plugin generation, significantly lowering effort for editing support in
language families, especially when artifacts are reused.

</details>
