<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 4]
- [cs.SE](#cs.SE) [Total: 12]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Fine-Grained Privacy Extraction from Retrieval-Augmented Generation Systems via Knowledge Asymmetry Exploitation](https://arxiv.org/abs/2507.23229)
*Yufei Chen,Yao Wang,Haibin Zhang,Tao Gu*

Main category: cs.CR

TL;DR: This paper introduces a novel black-box attack framework for RAG systems to extract private information with high accuracy (91% single-domain, 83% multi-domain) while generalizing across domains without prior knowledge.


<details>
  <summary>Details</summary>
Motivation: Current RAG privacy attacks struggle with isolating knowledge-base content in mixed responses and lack cross-domain robustness, creating gaps in understanding and mitigating these privacy risks.

Method: The framework combines adversarial query decomposition to maximize information disparity, semantic relationship scoring to resolve ambiguities, and a neural network trained on these features to identify sensitive sentences. It leverages knowledge asymmetry between RAG and standard LLMs through iterative domain-agnostic refinement.

Result: Achieved 91% privacy extraction rate in single-domain scenarios, 83% in multi-domain, and reduced sensitive information exposure by >65% in case studies while generalizing to unseen domains.

Conclusion: The work establishes a foundational approach for both attacking and defending RAG systems by enabling precise privacy extraction and adaptive mitigation, demonstrating robustness against knowledge heterogeneity.

Abstract: Retrieval-augmented generation (RAG) systems enhance large language models
(LLMs) by integrating external knowledge bases, but this advancement introduces
significant privacy risks. Existing privacy attacks on RAG systems can trigger
data leakage but often fail to accurately isolate knowledge-base-derived
sentences within mixed responses. They also lack robustness when applied across
multiple domains. This paper addresses these challenges by presenting a novel
black-box attack framework that exploits knowledge asymmetry between RAG and
standard LLMs to achieve fine-grained privacy extraction across heterogeneous
knowledge landscapes. We propose a chain-of-thought reasoning strategy that
creates adaptive prompts to steer RAG systems away from sensitive content.
Specifically, we first decompose adversarial queries to maximize information
disparity and then apply a semantic relationship scoring to resolve lexical and
syntactic ambiguities. We finally train a neural network on these feature
scores to precisely identify sentences containing private information. Unlike
prior work, our framework generalizes to unseen domains through iterative
refinement without pre-defined knowledge. Experimental results show that we
achieve over 91% privacy extraction rate in single-domain and 83% in
multi-domain scenarios, reducing sensitive sentence exposure by over 65% in
case studies. This work bridges the gap between attack and defense in RAG
systems, enabling precise extraction of private information while providing a
foundation for adaptive mitigation.

</details>


### [2] [Counterfactual Evaluation for Blind Attack Detection in LLM-based Evaluation Systems](https://arxiv.org/abs/2507.23453)
*Lijia Liu,Takumi Kondo,Kyohei Atarashi,Koh Takeuchi,Jiyi Li,Shigeru Saito,Hisashi Kashima*

Main category: cs.CR

TL;DR: This paper introduces a framework to defend LLM evaluation systems from 'blind attacks' by combining standard evaluation with counterfactual re-evaluation using false ground truths, improving security with minimal performance impact.


<details>
  <summary>Details</summary>
Motivation: LLM evaluation systems are vulnerable to prompt injection attacks, specifically 'blind attacks' where candidate answers are crafted to deceive evaluators without reference to the true answer, threatening evaluation integrity.

Method: The authors formalize blind attacks and propose a framework that augments Standard Evaluation (SE) with Counterfactual Evaluation (CFE), which re-evaluates submissions against deliberately false ground truths, detecting attacks when answers pass both evaluations.

Result: Experiments demonstrated that standard evaluation is highly vulnerable to blind attacks, while the SE+CFE framework significantly enhances security against these attacks with minimal performance trade-offs.

Conclusion: The SE+CFE framework effectively mitigates blind attacks on LLM evaluation systems by introducing counterfactual re-evaluations, offering a robust and practical defense with negligible efficiency costs.

Abstract: This paper investigates defenses for LLM-based evaluation systems against
prompt injection. We formalize a class of threats called blind attacks, where a
candidate answer is crafted independently of the true answer to deceive the
evaluator. To counter such attacks, we propose a framework that augments
Standard Evaluation (SE) with Counterfactual Evaluation (CFE), which
re-evaluates the submission against a deliberately false ground-truth answer.
An attack is detected if the system validates an answer under both standard and
counterfactual conditions. Experiments show that while standard evaluation is
highly vulnerable, our SE+CFE framework significantly improves security by
boosting attack detection with minimal performance trade-offs.

</details>


### [3] [LLM-Based Identification of Infostealer Infection Vectors from Screenshots: The Case of Aurora](https://arxiv.org/abs/2507.23611)
*Estelle Ruellan,Eric Clay,Nicholas Ascoli*

Main category: cs.CR

TL;DR: This paper proposes using LLMs (specifically gpt-4o-mini) to analyze infection screenshots for IoCs, infection vectors, and campaign tracking, addressing scalability challenges in malware log analysis.


<details>
  <summary>Details</summary>
Motivation: Manual analysis of over 29 million stealer logs is unfeasible, and reactive methods leveraging infection artifacts like screenshots remain underexplored in existing research.

Method: A novel LLM-driven approach uses gpt-4o-mini to process infection screenshots, extracting IoCs, mapping vectors, and correlating filenames/URLs/themes to identify campaigns.

Result: The method extracted 337 actionable URLs and 246 files from 1,000 screenshots, revealing malware distribution methods, social engineering tactics, and three distinct campaigns by correlating artifacts.

Conclusion: LLM-based artifact analysis offers a scalable solution for identifying infection vectors through reactive methods, enabling earlier threat detection and improved threat intelligence.

Abstract: Infostealers exfiltrate credentials, session cookies, and sensitive data from
infected systems. With over 29 million stealer logs reported in 2024, manual
analysis and mitigation at scale are virtually unfeasible/unpractical. While
most research focuses on proactive malware detection, a significant gap remains
in leveraging reactive analysis of stealer logs and their associated artifacts.
Specifically, infection artifacts such as screenshots, image captured at the
point of compromise, are largely overlooked by the current literature. This
paper introduces a novel approach leveraging Large Language Models (LLMs), more
specifically gpt-4o-mini, to analyze infection screenshots to extract potential
Indicators of Compromise (IoCs), map infection vectors, and track campaigns.
Focusing on the Aurora infostealer, we demonstrate how LLMs can process
screenshots to identify infection vectors, such as malicious URLs, installer
files, and exploited software themes. Our method extracted 337 actionable URLs
and 246 relevant files from 1000 screenshots, revealing key malware
distribution methods and social engineering tactics. By correlating extracted
filenames, URLs, and infection themes, we identified three distinct malware
campaigns, demonstrating the potential of LLM-driven analysis for uncovering
infection workflows and enhancing threat intelligence. By shifting malware
analysis from traditional log-based detection methods to a reactive,
artifact-driven approach that leverages infection screenshots, this research
presents a scalable method for identifying infection vectors and enabling early
intervention.

</details>


### [4] [Polynomial Lattices for the BIKE Cryptosystem](https://arxiv.org/abs/2507.23641)
*Michael Schaller*

Main category: cs.CR

TL;DR: This paper constructs a rank-2 lattice over a polynomial ring from the BIKE cryptosystem's public key, generalizes weak key recovery techniques, and shows that previous approaches implicitly solved shortest vector (SVP) problems in this lattice to enable more extensive weak key detection via reduced basis computation.


<details>
  <summary>Details</summary>
Motivation: Analyzing the security of the BIKE cryptosystem by modeling its secret key as a sparse vector in a lattice structure, building upon prior work to improve weak key detection capabilities.

Method: 1. Construct a rank-2 polynomial ring lattice from BIKE public keys. 2. Generalize the weak key recovery approach from Bardet et al. (2016) by solving the shortest vector problem (SVP). 3. Compute reduced lattice bases instead of just finding shortest vectors.

Result: Demonstration that previous weak key recovery implicitly involved SVP resolution in the constructed lattice, with improved results through reduced basis computation that reveals more potential weak keys.

Conclusion: The ability to compute reduced bases in these lattices enhances weak key detection for BIKE cryptosystems, suggesting the need for new countermeasures against SVP-based lattice attacks beyond existing sparse key protections.

Abstract: In this paper we introduce a rank $2$ lattice over a polynomial ring arising
from the public key of the BIKE cryptosystem \cite{aragon2022bike}. The secret
key is a sparse vector in this lattice. We study properties of this lattice and
generalize the recovery of weak keys from \cite{BardetDLO16}. In particular, we
show that they implicitly solved a shortest vector problem in the lattice we
constructed. Rather than finding only a shortest vector, we obtain a reduced
basis of the lattice which makes it possible to check for more weak keys.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [On LLM-Assisted Generation of Smart Contracts from Business Processes](https://arxiv.org/abs/2507.23087)
*Fabian Stiehle,Hans Weytjens,Ingo Weber*

Main category: cs.SE

TL;DR: This paper explores using LLMs for smart contract code generation from business process descriptions, introduces an automated evaluation framework, and shows LLMs currently lack the reliability required. It suggests integrating LLMs responsibly into existing tools for more dependable outputs.


<details>
  <summary>Details</summary>
Motivation: Existing code generation approaches for smart contracts (rule-based and manually Inspected LLM-based methods) are limited in reliability and scalability. There's a need for robust evaluation tools to ensure process execution correctness in smart contracts.

Method: The authors developed an automated evaluation framework that tests LLM-generated code against process execution properties (flow, resource allocation, data conditions) using large datasets of process models to assess LLMs of varying types and sizes.

Result: Empirical data reveals LLM performance fails to meet the reliability standards necessary for smart contract development, despite their wide adoption in code generation. The framework provides scalable, systematic evaluation capabilities that were previously lacking.

Conclusion: LLMs require integration with domain-specific constraints and validation tools to achieve reliability in smart contract code generation. The proposed benchmarking framework enables future research in this area and establishes evaluation criteria for responsible LLM implementation.

Abstract: Large language models (LLMs) have changed the reality of how software is
produced. Within the wider software engineering community, among many other
purposes, they are explored for code generation use cases from different types
of input. In this work, we present an exploratory study to investigate the use
of LLMs for generating smart contract code from business process descriptions,
an idea that has emerged in recent literature to overcome the limitations of
traditional rule-based code generation approaches. However, current LLM-based
work evaluates generated code on small samples, relying on manual inspection,
or testing whether code compiles but ignoring correct execution. With this
work, we introduce an automated evaluation framework and provide empirical data
from larger data sets of process models. We test LLMs of different types and
sizes in their capabilities of achieving important properties of process
execution, including enforcing process flow, resource allocation, and
data-based conditions. Our results show that LLM performance falls short of the
perfect reliability required for smart contract development. We suggest future
work to explore responsible LLM integrations in existing tools for code
generation to ensure more reliable output. Our benchmarking framework can serve
as a foundation for developing and evaluating such integrations.

</details>


### [6] [FlowETL: An Autonomous Example-Driven Pipeline for Data Engineering](https://arxiv.org/abs/2507.23118)
*Mattia Di Profio,Mingjun Zhong,Yaji Sripada,Marcel Jaspars*

Main category: cs.SE

TL;DR: FlowETL is an autonomous ETL pipeline architecture that automatically standardizes datasets using example-based plans, reducing human-in-the-loop requirements.


<details>
  <summary>Details</summary>
Motivation: Modern ETL workflows require significant manual effort for context-specific transformations, and existing automation lacks the ability to design and apply these transformations effectively.

Method: FlowETL employs a Planning Engine to generate transformation plans from paired input-output dataset samples, an ETL worker to execute the plan, and monitoring components for observability.

Result: Demonstrates promising generalization across 14 datasets with diverse domains (1.5MB-50GB), file formats, and transformation complexity.

Conclusion: FlowETL advances ETL automation by providing a scalable, example-based framework capable of handling diverse data standardization tasks with minimal human intervention.

Abstract: The Extract, Transform, Load (ETL) workflow is fundamental for populating and
maintaining data warehouses and other data stores accessed by analysts for
downstream tasks. A major shortcoming of modern ETL solutions is the extensive
need for a human-in-the-loop, required to design and implement
context-specific, and often non-generalisable transformations. While related
work in the field of ETL automation shows promising progress, there is a lack
of solutions capable of automatically designing and applying these
transformations. We present FlowETL, a novel example-based autonomous ETL
pipeline architecture designed to automatically standardise and prepare input
datasets according to a concise, user-defined target dataset. FlowETL is an
ecosystem of components which interact together to achieve the desired outcome.
A Planning Engine uses a paired input-output datasets sample to construct a
transformation plan, which is then applied by an ETL worker to the source
dataset. Monitoring and logging provide observability throughout the entire
pipeline. The results show promising generalisation capabilities across 14
datasets of various domains, file structures, and file sizes.

</details>


### [7] [Vibe Modeling: Challenges and Opportunities](https://arxiv.org/abs/2507.23120)
*Jordi Cabot*

Main category: cs.SE

TL;DR: This paper proposes 'vibe modeling' as a hybrid approach combining AI-driven methods and model-driven engineering (MDE) to address complexity in modern software systems, balancing speed with reliability.


<details>
  <summary>Details</summary>
Motivation: Software systems face challenges from complex requirements (new UIs, intelligent components, sustainability) while traditional MDE struggles with model complexity and LLM-based 'vibe coding' has shortcomings in code quality and maintainability.

Method: The authors formalize 'vibe modeling' as a framework integrating large language models into the model-driven engineering workflow, leveraging natural language descriptions for model generation while retaining MDE's rigor.

Result: Preliminary identification of core concepts, opportunities for AI-enhanced modeling, and critical challenges in ensuring system reliability, scalability, and maintainability.

Conclusion: Vibe modeling represents a dual-opportunity space: AI can streamline model creation while MDE ensures structure, but overcoming technical and methodological barriers will determine its success in complex system development.

Abstract: There is a pressing need for better development methods and tools to keep up
with the growing demand and increasing complexity of new software systems. New
types of user interfaces, the need for intelligent components, sustainability
concerns, ... bring new challenges that we need to handle. In the last years,
model-driven engineering (MDE) has been key to improving the quality and
productivity of software development, but models themselves are becoming
increasingly complex to specify and manage. At the same time, we are witnessing
the growing popularity of vibe coding approaches that rely on Large Language
Models (LLMs) to transform natural language descriptions into running code at
the expenses of code vulnerabilities, scalability issues and maintainability
concerns. In this paper, we introduce the concept of \textit{vibe modeling} as
a novel approach to integrate the best of both worlds (AI and MDE) to speed up
the development of reliable complex systems. We outline the key concepts of
vibe modeling and highlight the opportunities and open challenges it presents
for the future of modeling.

</details>


### [8] [Extension Decisions in Open Source Software Ecosystem](https://arxiv.org/abs/2507.23168)
*Elmira Onagh,Maleknaz Nayebi*

Main category: cs.SE

TL;DR: The paper analyzes redundancy in GitHub Marketplace's CI tools, finding 65% of new Actions replicate existing capabilities within six months. It proposes methods to track functionality evolution and eliminate redundancies.


<details>
  <summary>Details</summary>
Motivation: GitHub Marketplace's rapid growth (41% annual expansion) creates redundancy with many tools replicating existing CI functionalities, necessitating better understanding and management of this phenomenon.

Method: The authors link 6,983 CI Actions to 3,869 providers, mine version histories, and create a graph model to timestamp functionality debut, track adoption trends, and cluster redundant tools.

Result: 65% of new CI Actions replicate existing capabilities within six months, first-mover tools dominate forks/extensions, and the analysis reveals patterns of replication and competition in the CI ecosystem.

Conclusion: The findings suggest strategies for developers to time launches and target unmet needs, while enabling maintainers to deprioritize redundant tools. The dataset is published for further research on software ecosystem dynamics.

Abstract: GitHub Marketplace is expanding by approximately 41% annually, with new
tools; however, many additions replicate existing functionality. We study this
phenomenon in the platform's largest segment, Continuous Integration (CI), by
linking 6,983 CI Actions to 3,869 providers and mining their version histories.
Our graph model timestamps every functionality's debut, tracks its adoption,
and clusters redundant tools. We find that approximately 65% of new CI Actions
replicate existing capabilities, typically within six months, and that a small
set of first-mover Actions accounts for most subsequent forks and extensions.
These insights enable developers to choose the optimal moment to launch, target
unmet functionality, and help maintainers eliminate redundant tools. We publish
the complete graph and dataset to encourage longitudinal research on innovation
and competition in software ecosystems, and to provide practitioners with a
data-driven roadmap for identifying emerging trends and guiding product
strategy.

</details>


### [9] [AutoBridge: Automating Smart Device Integration with Centralized Platform](https://arxiv.org/abs/2507.23178)
*Siyuan Liu,Zhice Yang,Huangxun Chen*

Main category: cs.SE

TL;DR: AutoBridge automates IoT integration code generation using a divide-and-conquer strategy and multi-stage debugging pipeline, achieving 93.87% success rate and 94.87% function coverage with minimal human feedback.


<details>
  <summary>Details</summary>
Motivation: Manual programming of complex IoT integration code for centralized platforms requires significant human expertise and effort, motivating the need for automation.

Method: AutoBridge employs a divide-and-conquer approach by 1) generating device control logic through progressive retrieval of device-specific knowledge and 2) synthesizing platform-compliant code using platform-specific knowledge. It includes two-stage debugging: automated virtual IoT device testing and hardware-in-the-loop debugging with binary user feedback.

Result: Evaluates on 34 IoT devices across two platforms: 93.87% average success rate / 94.87% function coverage with no human input, reaching 100% coverage with minimal binary feedback. User study shows outperforms expert programmers by 50-80% accuracy.

Conclusion: AutoBridge enables significantly more accurate automated IoT integration code generation compared to human experts using commercial code LLMs, demonstrating potential as a new standard in multimodal IoT development.

Abstract: Multimodal IoT systems coordinate diverse IoT devices to deliver
human-centered services. The ability to incorporate new IoT devices under the
management of a centralized platform is an essential requirement. However, it
requires significant human expertise and effort to program the complex IoT
integration code that enables the platform to understand and control the device
functions. Therefore, we propose AutoBridge to automate IoT integration code
generation. Specifically, AutoBridge adopts a divide-and-conquer strategy: it
first generates device control logic by progressively retrieving
device-specific knowledge, then synthesizes platformcompliant integration code
using platform-specific knowledge. To ensure correctness, AutoBridge features a
multi-stage debugging pipeline, including an automated debugger for virtual IoT
device testing and an interactive hardware-in-the-loop debugger that requires
only binary user feedback (yes and no) for real-device verification. We
evaluate AutoBridge on a benchmark of 34 IoT devices across two open-source IoT
platforms. The results demonstrate that AutoBridge can achieves an average
success rate of 93.87% and an average function coverage of 94.87%, without any
human involvement. With minimal binary yes and no feedback from users, the code
is then revised to reach 100% function coverage. A user study with 15
participants further shows that AutoBridge outperforms expert programmers by
50% to 80% in code accuracy, even when the programmers are allowed to use
commercial code LLMs.

</details>


### [10] [XABPs: Towards eXplainable Autonomous Business Processes](https://arxiv.org/abs/2507.23269)
*Peter Fettke,Fabiana Fournier,Lior Limonad,Andreas Metzger,Stefanie Rinderle-Ma,Barbara Weber*

Main category: cs.SE

TL;DR: The paper introduces eXplainable Autonomous Business Processes (XABPs) to address concerns like trust, accountability, and regulatory compliance in AI/ML-driven workflows by enabling systematic articulation of rationale.


<details>
  <summary>Details</summary>
Motivation: Autonomous Business Processes (ABPs) offer operational efficiency and error reduction but face challenges in stakeholder trust, debugging, accountability, bias, and compliance. The motivation is to address these limitations through explainability while maintaining autonomy.

Method: The authors present a systematic approach to XABPs, including (1) characterizing forms of XABPs, (2) structuring explainability components, and (3) identifying key BPM research challenges for implementing XABPs.

Result: The paper proposes a structured framework for XABPs, categorizes different explainability forms, defines structural requirements for rationale articulation, and highlights foundational BPM research questions in this domain.

Conclusion: XABPs provide a critical pathway to balance autonomy with transparency in business workflows. The conclusion emphasizes the need for targeted research to formalize BPM methodologies addressing explainability, accountability, and regulatory demands in AI-driven processes.

Abstract: Autonomous business processes (ABPs), i.e., self-executing workflows
leveraging AI/ML, have the potential to improve operational efficiency, reduce
errors, lower costs, improve response times, and free human workers for more
strategic and creative work. However, ABPs may raise specific concerns
including decreased stakeholder trust, difficulties in debugging, hindered
accountability, risk of bias, and issues with regulatory compliance. We argue
for eXplainable ABPs (XABPs) to address these concerns by enabling systems to
articulate their rationale. The paper outlines a systematic approach to XABPs,
characterizing their forms, structuring explainability, and identifying key BPM
research challenges towards XABPs.

</details>


### [11] [SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution](https://arxiv.org/abs/2507.23348)
*Han Li,Yuling Shi,Shaoxin Lin,Xiaodong Gu,Heng Lian,Xin Wang,Yantao Jia,Tao Huang,Qianxiang Wang*

Main category: cs.SE

TL;DR: SWE-Debate is a competitive multi-agent debate framework for issue resolution in software engineering, improving over SWE-agent by using code dependency graphs and structured debates among specialized agents to avoid local solutions.


<details>
  <summary>Details</summary>
Motivation: Current agent-based issue resolution approaches rely on independent exploration, leading to local solutions and failure to identify issue patterns across different codebase parts.

Method: SWE-Debate creates multiple fault propagation traces via code dependency graph traversal, organizes three-round debates among agents with distinct reasoning perspectives along these traces, and integrates the consensus into an MCTS-based code modification agent for patch generation.

Result: Experiments on the SWE-bench benchmark show SWE-Debate achieves state-of-the-art results in open-source agent frameworks and outperforms baselines by a large margin.

Conclusion: The structured competition and diverse reasoning paths in SWE-Debate enable more consolidated issue localization and effective resolution for complex software engineering tasks.

Abstract: Issue resolution has made remarkable progress thanks to the advanced
reasoning capabilities of large language models (LLMs). Recently, agent-based
frameworks such as SWE-agent have further advanced this progress by enabling
autonomous, tool-using agents to tackle complex software engineering tasks.
While existing agent-based issue resolution approaches are primarily based on
agents' independent explorations, they often get stuck in local solutions and
fail to identify issue patterns that span across different parts of the
codebase. To address this limitation, we propose SWE-Debate, a competitive
multi-agent debate framework that encourages diverse reasoning paths and
achieves more consolidated issue localization. SWE-Debate first creates
multiple fault propagation traces as localization proposals by traversing a
code dependency graph. Then, it organizes a three-round debate among
specialized agents, each embodying distinct reasoning perspectives along the
fault propagation trace. This structured competition enables agents to
collaboratively converge on a consolidated fix plan. Finally, this consolidated
fix plan is integrated into an MCTS-based code modification agent for patch
generation. Experiments on the SWE-bench benchmark show that SWE-Debate
achieves new state-of-the-art results in open-source agent frameworks and
outperforms baselines by a large margin.

</details>


### [12] [Quality Evaluation of COBOL to Java Code Transformation](https://arxiv.org/abs/2507.23356)
*Shmulik Froimovich,Raviv Gal,Wesam Ibraheem,Avi Ziv*

Main category: cs.SE

TL;DR: An automated evaluation system for COBOL-to-Java translation in IBM's watsonx Code Assistant for Z combined analytic checkers and LLM-as-a-judge techniques to address translation quality assessment challenges.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in evaluating LLM-based translators, particularly COBOL-to-Java translation, which faces model opacity and complex quality assessment requirements due to legacy system modernization needs.

Method: The approach integrates analytic checkers (for syntactic/structural validation) with LLM-as-a-judge (LaaJ) techniques (for semantic evaluation) within continuous integration workflows. This combines automated benchmarking with scalable evaluation metrics.

Result: The resulting system provides multi-faceted evaluations, actionable insights for developers and project managers, and significantly reduces manual review requirements while enabling large-scale translation validation.

Conclusion: This automated evaluation framework offers a scalable solution for evaluating code translators, demonstrating the effectiveness of combining domain-specific analytic tools with LLM-driven evaluation to modernize legacy mainframe systems.

Abstract: We present an automated evaluation system for assessing COBOL-to-Java code
translation within IBM's watsonx Code Assistant for Z (WCA4Z). The system
addresses key challenges in evaluating LLM-based translators, including model
opacity and the complexity of translation quality assessment. Our approach
combines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver
scalable, multi-faceted evaluations. The system supports continuous integration
workflows, enables large-scale benchmarking, and reduces reliance on manual
review. We describe the system architecture, evaluation strategies, and
reporting mechanisms that provide actionable insights for developers and
project managers, facilitating the evolution of high-quality, modernized
codebases.

</details>


### [13] [SWE-Exp: Experience-Driven Software Issue Resolution](https://arxiv.org/abs/2507.23361)
*Silin Chen,Shaoxin Lin,Xiaodong Gu,Yuling Shi,Heng Lian,Longfei Yun,Dong Chen,Weiguo Sun,Lin Cao,Qianxiang Wang*

Main category: cs.SE

TL;DR: Introduces SWE-Exp, an experience-enhanced LLM agent framework for software issue resolution that systematically captures and reuses prior repair knowledge through distilled experience banks, achieving state-of-the-art 41.6% Pass@1 rate on SWE-bench-Verified.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents for software engineering operate as memoryless systems, repeatedly performing redundant exploration of failed solutions and failing to repurpose successful repair patterns for similar problems, leading to inefficiencies in issue resolution.

Method: SWE-Exp implements a multi-faceted experience bank distilling reusable knowledge from both successful and failed agent trajectories across hierarchical abstraction levels. This includes high-level problem understanding patterns and concrete code modification strategies through a continuous learning mechanism.

Result: Achieves 41.6% Pass@1 resolution rate on SWE-bench-Verified benchmark, outperforming prior memoryless approaches by systematically leveraging distilled experience rather than relying solely on brute-force exploration.

Conclusion: Establishes a fundamental shift in automated software engineering by enabling agents to strategically accumulate and apply repair expertise through experience banks, transitioning from trial-and-error exploration to knowledge-guided issue resolution.

Abstract: Recent advances in large language model (LLM) agents have shown remarkable
progress in software issue resolution, leveraging advanced techniques such as
multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current
agents act as memoryless explorers - treating each problem separately without
retaining or reusing knowledge from previous repair experiences. This leads to
redundant exploration of failed trajectories and missed chances to adapt
successful issue resolution methods to similar problems. To address this
problem, we introduce SWE-Exp, an experience - enhanced approach that distills
concise and actionable experience from prior agent trajectories, enabling
continuous learning across issues. Our method introduces a multi-faceted
experience bank that captures both successful and failed repair attempts.
Specifically, it extracts reusable issue resolution knowledge at different
levels - from high-level problem comprehension to specific code changes.
Experiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%
Pass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach
establishes a new paradigm in which automated software engineering agents
systematically accumulate and leverage repair expertise, fundamentally shifting
from trial-and-error exploration to strategic, experience-driven issue
resolution.

</details>


### [14] [Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling](https://arxiv.org/abs/2507.23370)
*Trae Research Team,Pengfei Gao,Zhao Tian,Xiangxin Meng,Xinchen Wang,Ruida Hu,Yuanan Xiao,Yizhou Liu,Zhao Zhang,Junjie Chen,Cuiyun Gao,Yun Lin,Yingfei Xiong,Chao Peng,Xia Liu*

Main category: cs.SE

TL;DR: Trae Agent introduces an agent-based ensemble reasoning approach for repository-level software issue resolution, achieving first-place results on SWE-bench with 75.20% Pass@1 and outperforming existing methods by 10.22%.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based issue resolution methods struggle with exploring large ensemble spaces and lack repository-level understanding, limiting their effectiveness.

Method: Trae Agent formulates issue resolution as an optimal solution search problem using modular agents for generation, pruning, and selection, enabling efficient exploration of ensemble spaces and repository-level analysis.

Result: Outperforms four baselines with 10.22% average Pass@1 improvement on SWE-bench, achieving a 75.20% Pass@1 score with three leading LLMs.

Conclusion: Trae Agent demonstrates significant advances in repository-level issue resolution through its agent-based ensemble framework and is released as open-source to support further research.

Abstract: Software issue resolution is a critical challenge in software engineering and
has garnered increasing attention in recent years. With the rapid advancement
of large language models (LLMs), substantial progress has been made in
addressing real-world software engineering tasks. Recent studies have
introduced ensemble reasoning techniques to enhance the performance of
LLM-based issue resolution. However, existing prompting-based methods still
face limitations in effectively exploring large ensemble spaces and lack the
capacity for repository-level understanding, both of which constrain their
overall effectiveness. In this paper, we propose Trae Agent, the first
agent-based ensemble reasoning approach for repository-level issue resolution.
Trae Agent formulates our goal as an optimal solution search problem and
addresses two key challenges, i.e., large ensemble spaces and repository-level
understanding, through modular agents for generation, pruning, and selection.
We conduct extensive experiments using three leading LLMs on the widely-adopted
SWE-bench benchmark, comparing Trae Agent against four state-of-the-art
ensemble reasoning techniques. Experimental results demonstrate that Trae Agent
consistently achieves superior performance, with an average improvement of
10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first
place on the SWE-bench Verified leaderboard, with a notable Pass@1 score of
75.20%. We are pleased to release Trae Agent as an open-source project to
support the research community, with all resources available at
https://github.com/bytedance/trae-agent.

</details>


### [15] [Dynamic and Static Analysis of Python Software with Kieker Including Reconstructed Architectures](https://arxiv.org/abs/2507.23425)
*Daphné Larrivain,Shinhyung Yang,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: This paper introduces Python support in the Kieker observability framework, combining static and dynamic analysis to provide structural insights into Python applications.


<details>
  <summary>Details</summary>
Motivation: Python's popularity has grown significantly, making structural analysis of Python applications valuable. Kieker, originally designed for Java, needed to support Python to meet this demand.

Method: The proposed Python analysis pipeline integrates static and dynamic analysis techniques to create a comprehensive system overview.

Result: The pipeline enables users to design custom observability pipelines for Python applications, offering detailed structural insights similar to Kieker's Java capabilities.

Conclusion: Supporting Python with Kieker's combined static-dynamic analysis approach is beneficial for observability, leveraging Python's widespread use while maintaining the framework's effectiveness.

Abstract: The Kieker observability framework is a tool that provides users with the
means to design a custom observability pipeline for their application.
Originally tailored for Java, supporting Python with Kieker is worthwhile.
Python's popularity has exploded over the years, thus making structural
insights of Python applications highly valuable. Our Python analysis pipeline
combines static and dynamic analysis in order to build a complete picture of a
given system.

</details>


### [16] [An Empirical Study on the Amount of Changes Required for Merge Request Acceptance](https://arxiv.org/abs/2507.23640)
*Samah Kansab,Mohammed Sayagh,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: This study defines code review effort in GitLab Merge Requests as post-submission code changes and uses ML to identify key predictors like complexity and developer experience, achieving high model accuracy (AUC 0.84-0.88).


<details>
  <summary>Details</summary>
Motivation: Existing research on code review emphasizes delays and iterations but lacks empirical analysis of actual effort from code modification volume, particularly in GitLab Merge Requests. The paper aims to address this gap for better understanding and prediction.

Method: 1. Defined CR effort as post-submission code changes using 23,600+ GitLab MRs from four projects
2. Trained an interpretable ML model with features from text, code complexity, developer experience, review history, and branching
3. Analyzed metrics across five dimensions to predict effort requirements

Result: 1. 71%% of MRs require post-submission adjustments (28%% with >200 LOC changes)
2. Machine learning model achieved 0.84-0.88 AUC for effort prediction
3. Key predictors were code complexity, developer experience, text features, and historical project patterns
4. Review effort not correlated with review duration or participant count

Conclusion: 1. CR effort can be effectively explained and predicted using interpretable ML models
2. Complexity metrics and historical patterns provide critical insights
3. Text features and developer experience show significant impact on review effort
4. Demonstrates new approaches for improving software integration processes

Abstract: Code review (CR) is essential to software development, helping ensure that
new code is properly integrated. However, the CR process often involves
significant effort, including code adjustments, responses to reviewers, and
continued implementation. While past studies have examined CR delays and
iteration counts, few have investigated the effort based on the volume of code
changes required, especially in the context of GitLab Merge Requests (MRs),
which remains underexplored. In this paper, we define and measure CR effort as
the amount of code modified after submission, using a dataset of over 23,600
MRs from four GitLab projects. We find that up to 71% of MRs require
adjustments after submission, and 28% of these involve changes to more than 200
lines of code. Surprisingly, this effort is not correlated with review time or
the number of participants. To better understand and predict CR effort, we
train an interpretable machine learning model using metrics across multiple
dimensions: text features, code complexity, developer experience, review
history, and branching. Our model achieves strong performance (AUC 0.84-0.88)
and reveals that complexity, experience, and text features are key predictors.
Historical project characteristics also influence current review effort. Our
findings highlight the feasibility of using machine learning to explain and
anticipate the effort needed to integrate code changes during review.

</details>
