{"id": "2510.19860", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2510.19860", "abs": "https://arxiv.org/abs/2510.19860", "authors": ["Ketai Qiu", "Luca Di Grazia", "Leonardo Mariani", "Mauro Pezz\u00e8"], "title": "E-Test: E'er-Improving Test Suites", "comment": "Accepted at the 48th IEEE/ACM International Conference on Software\n  Engineering (ICSE 2026)", "summary": "Test suites are inherently imperfect, and testers can always enrich a suite\nwith new test cases that improve its quality and, consequently, the reliability\nof the target software system. However, finding test cases that explore\nexecution scenarios beyond the scope of an existing suite can be extremely\nchallenging and labor-intensive, particularly when managing large test suites\nover extended periods.\n  In this paper, we propose E-Test, an approach that reduces the gap between\nthe execution space explored with a test suite and the executions experienced\nafter testing by augmenting the test suite with test cases that explore\nexecution scenarios that emerge in production. E-Test (i) identifies executions\nthat have not yet been tested from large sets of scenarios, such as those\nmonitored during intensive production usage, and (ii) generates new test cases\nthat enhance the test suite. E-Test leverages Large Language Models (LLMs) to\npinpoint scenarios that the current test suite does not adequately cover, and\naugments the suite with test cases that execute these scenarios.\n  Our evaluation on a dataset of 1,975 scenarios, collected from highly-starred\nopen-source Java projects already in production and Defects4J, demonstrates\nthat E-Test retrieves not-yet-tested execution scenarios significantly better\nthan state-of-the-art approaches. While existing regression testing and field\ntesting approaches for this task achieve a maximum F1-score of 0.34, and\nvanilla LLMs achieve a maximum F1-score of 0.39, E-Test reaches 0.55. These\nresults highlight the impact of E-Test in enhancing test suites by effectively\ntargeting not-yet-tested execution scenarios and reducing manual effort\nrequired for maintaining test suites.", "AI": {"tldr": "E-Test enhances test suites by using LLMs to identify and generate untested production scenarios, achieving an F1-score of 0.55, outperforming prior methods.", "motivation": "Existing test suites lack coverage of production-emergent scenarios, requiring laborious manual updates. Automated identification of untested scenarios is critical for maintaining test suite effectiveness.", "method": "E-Test (1)(i) extracts untested execution scenarios from production-monitored data and (2) leverages LLMs to generate new test cases targeting these scenarios. Combines model-based scenario detection with code generation.", "result": "Evaluation on 1,975 production scenarios from Java projects and Defects4J shows E-Test achieves 0.55 F1-score (vs 0.34 for prior methods, 0.39 for vanilla LLMs). Demonstrates superior precision in untested scenario identification.", "conclusion": "E-Test effectively bridges test suite coverage gaps by operationalizing production data insights. The 30-45%p F1-score improvements over baselines significantly reduce manual test maintenance effort while improving test suite quality."}}
{"id": "2510.19864", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19864", "abs": "https://arxiv.org/abs/2510.19864", "authors": ["Amila Indika", "Igor Molybog"], "title": "SODBench: A Large Language Model Approach to Documenting Spreadsheet Operations", "comment": "14 pages, 5 figures, 4 tables", "summary": "Numerous knowledge workers utilize spreadsheets in business, accounting, and\nfinance. However, a lack of systematic documentation methods for spreadsheets\nhinders automation, collaboration, and knowledge transfer, which risks the loss\nof crucial institutional knowledge. This paper introduces Spreadsheet\nOperations Documentation (SOD), an AI task that involves generating\nhuman-readable explanations from spreadsheet operations. Many previous studies\nhave utilized Large Language Models (LLMs) for generating spreadsheet\nmanipulation code; however, translating that code into natural language for SOD\nis a less-explored area. To address this, we present a benchmark of 111\nspreadsheet manipulation code snippets, each paired with a corresponding\nnatural language summary. We evaluate five LLMs, GPT-4o, GPT-4o-mini,\nLLaMA-3.3-70B, Mixtral-8x7B, and Gemma2-9B, using BLEU, GLEU, ROUGE-L, and\nMETEOR metrics. Our findings suggest that LLMs can generate accurate\nspreadsheet documentation, making SOD a feasible prerequisite step toward\nenhancing reproducibility, maintainability, and collaborative workflows in\nspreadsheets, although there are challenges that need to be addressed.", "AI": {"tldr": "The paper introduces Spreadsheet Operations Documentation (SOD), an AI task for generating human-readable explanations for spreadsheet operations. It presents a benchmark of 111 code snippets with natural language summaries and evaluates LLM performance in this task.", "motivation": "Spreadsheets are widely used but lack systematic documentation methods, risking institutional knowledge loss. Current research focuses on generating code from natural language, leaving a gap in translating code to natural language for documentation.", "method": "The authors present a benchmark consisting of 111 spreadsheet manipulation code snippets paired with natural language summaries. They evaluate five LLMs using BLEU, GLEU, ROUGE-L, and METEOR metrics.", "result": "The evaluation shows that LLMs can generate accurate spreadsheet documentation, indicating the feasibility of SOD as a step towards improving reproducibility, maintainability, and collaboration in spreadsheet workflows.", "conclusion": "SOD is a feasible approach to improve spreadsheet documentation, though challenges remain. It has potential to enhance reproducibility and collaboration in spreadsheet-based workflows."}}
{"id": "2510.19868", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19868", "abs": "https://arxiv.org/abs/2510.19868", "authors": ["Qian Xiong", "Bo Yang", "Weisong Sun", "Yiran Zhang", "Tianlin Li", "Yang Liu", "Zhi Jin"], "title": "Knowledge-Guided Multi-Agent Framework for Application-Level Software Code Generation", "comment": null, "summary": "Automated code generation driven by Large Lan- guage Models (LLMs) has\nenhanced development efficiency, yet generating complex application-level\nsoftware code remains challenging. Multi-agent frameworks show potential, but\nexisting methods perform inadequately in large-scale application-level software\ncode generation, failing to ensure reasonable orga- nizational structures of\nproject code and making it difficult to maintain the code generation process.\nTo address this, this paper envisions a Knowledge-Guided Application-Level Code\nGeneration framework named KGACG, which aims to trans- form software\nrequirements specification and architectural design document into executable\ncode through a collaborative closed- loop of the Code Organization & Planning\nAgent (COPA), Coding Agent (CA), and Testing Agent (TA), combined with a\nfeedback mechanism. We demonstrate the collaborative process of the agents in\nKGACG in a Java Tank Battle game case study while facing challenges. KGACG is\ndedicated to advancing the automation of application-level software\ndevelopment.", "AI": {"tldr": "The paper proposes KGACG, a framework for application-level code generation that uses a multi-agent system to enhance the automation of code creation and maintenance.", "motivation": "The motivation is to improve the efficiency of generating complex application-level software code, which is challenging even with existing LLM techniques. The current methods are inadequate for large-scale projects due to issues with project structure and maintainability.", "method": "The proposed method, KGACG, employs a multi-agent system consisting of COPA, CA, and TA agents, along with a feedback mechanism to iteratively refine the code generation process.", "result": "The case study using a Java Tank Battle game demonstrates the feasibility of the framework in generating well-structured code through the collaborative process of the agents.", "conclusion": "KGACG presents a viable approach to automating application-level code generation, offering improvements in project structure and process maintainability, thus paving the way for more efficient and large-scale software development automation."}}
{"id": "2510.19898", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19898", "abs": "https://arxiv.org/abs/2510.19898", "authors": ["Atharv Sonwane", "Isadora White", "Hyunji Lee", "Matheus Pereira", "Lucas Caccia", "Minseon Kim", "Zhengyan Shi", "Chinmay Singh", "Alessandro Sordoni", "Marc-Alexandre C\u00f4t\u00e9", "Xingdi Yuan"], "title": "BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills", "comment": null, "summary": "High quality bugs are key to training the next generation of language model\nbased software engineering (SWE) agents. We introduce a novel method for\nsynthetic generation of difficult and diverse bugs. Our method instructs SWE\nAgents to introduce a feature into the codebase whereby they may\nunintentionally break tests, resulting in bugs. Prior approaches often induce\nan out-of-distribution effect by generating bugs intentionally (e.g. by\nintroducing local perturbation to existing code), which does not reflect\nrealistic development processes. We perform qualitative analysis to demonstrate\nthat our approach for generating bugs more closely reflects the patterns found\nin human-authored edits. Through extensive experiments, we demonstrate that our\nbugs provide more efficient training data for supervised fine-tuning,\noutperforming other bug datasets by 2% with half the training data (1.2k vs. 3k\nbugs). We train on our newly generated bugs in addition to existing bug\ndatasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench\nVerified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on\nSWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over\nthree seeds.", "AI": {"tldr": "The paper introduces a method for generating realistic and diverse bugs by instructing SWE agents to introduce features that may break tests. This approach produces training data that results in better performance on SWE tasks.", "motivation": "The paper is motivated by the need for high-quality bugs to train SWE agents. Existing methods for bug generation do not reflect realistic development processes.", "method": "The method instructed SWE agents to introduce a new feature into the codebase which could unintentionally break existing tests. This led to the creation of bugs. This is different from prior methods that intentionally induce bugs through local perturbations of code.", "result": "The bugs generated through this method provided more efficient training data. When trained on these bugs with only 1.2k samples, the model outperformed other bug datasets with 3k samples by 2%. Additionally, models trained with this approach improved the state-of-the-art performance on the SWE-bench Verified task with a pass@1 of 54.6% (FrogBoss) and 45.3% (FrogMini).", "conclusion": "By generating more realistic bugs through the simulation of real-world development processes, the method enhances the efficiency and effectiveness of training SWE agents."}}
{"id": "2510.19984", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2510.19984", "abs": "https://arxiv.org/abs/2510.19984", "authors": ["Konstantinos Kitsios", "Marcel B\u00f6hme", "Alberto Bacchelli"], "title": "On Interaction Effects in Greybox Fuzzing", "comment": "12 pages, 2 figures, Accepted for presentation at the 48th\n  International Conference on Software Engineering (ICSE '26)", "summary": "A greybox fuzzer is an automated software testing tool that generates new\ntest inputs by applying randomly chosen mutators (e.g., flipping a bit or\ndeleting a block of bytes) to a seed input in random order and adds all\ncoverage-increasing inputs to the corpus of seeds. We hypothesize that the\norder in which mutators are applied to a seed input has an impact on the\neffectiveness of greybox fuzzers. In our experiments, we fit a linear model to\na dataset that contains the effectiveness of all possible mutator pairs and\nindeed observe the conjectured interaction effect. This points us to more\nefficient fuzzing by choosing the most promising mutator sequence with a higher\nlikelihood. We propose MuoFuzz, a greybox fuzzer that learns and chooses the\nmost promising mutator sequences. MuoFuzz learns the conditional probability\nthat the next mutator will yield an interesting input, given the previously\nselected mutator. Then, it samples from the learned probability using a random\nwalk to generate mutator sequences. We compare the performance of MuoFuzz to\nAFL++, which uses a fixed selection probability, and MOPT, which optimizes the\nselection probability of each mutator in isolation. Experimental results on the\nFuzzBench and MAGMA benchmarks show that MuoFuzz achieves the highest code\ncoverage and finds four bugs missed by AFL++ and one missed by both AFL++ and\nMOPT.", "AI": {"tldr": "This paper introduces MuoFuzz, a greybox fuzzer that improves effectiveness by learning optimal mutator sequences through conditional probability, outperforming AFL++ and MOPT in coverage and bug detection.", "motivation": "The paper is motivated by the hypothesis that the order of mutator applications in greybox fuzzers significantly affects their effectiveness, which traditional methods like AFL++ and MOPT do not account for.", "method": "The authors propose MuoFuzz, a learning-based approach that models the conditional probability of mutators leading to coverage increases. They use a linear model derived from historical data on mutator pairs and apply a random walk guided by this probability to select mutator sequences dynamically.", "result": "Experiments on FuzzBench and MAGMA benchmarks demonstrate that MuoFuzz achieves the highest code coverage and outperforms both AFL++ and MOPT by discovering new bugs in tested targets.", "conclusion": "MuoFuzz's context-aware mutator sequence learning effectively exploits mutator interactions, leading to superior fuzzing performance compared to existing approaches that treat mutators in isolation or with fixed probabilities."}}
{"id": "2510.19997", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19997", "abs": "https://arxiv.org/abs/2510.19997", "authors": ["Abraham Itzhak Weinberg"], "title": "A Framework for the Adoption and Integration of Generative AI in Midsize Organizations and Enterprises (FAIGMOE)", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) presents transformative\nopportunities for organizations, yet both midsize organizations and larger\nenterprises face distinctive adoption challenges. Midsize organizations\nencounter resource constraints and limited AI expertise, while enterprises\nstruggle with organizational complexity and coordination challenges. Existing\ntechnology adoption frameworks, including TAM (Technology Acceptance Model),\nTOE (Technology Organization Environment), and DOI (Diffusion of Innovations)\ntheory, lack the specificity required for GenAI implementation across these\ndiverse contexts, creating a critical gap in adoption literature. This paper\nintroduces FAIGMOE (Framework for the Adoption and Integration of Generative AI\nin Midsize Organizations and Enterprises), a conceptual framework addressing\nthe unique needs of both organizational types. FAIGMOE synthesizes technology\nadoption theory, organizational change management, and innovation diffusion\nperspectives into four interconnected phases: Strategic Assessment, Planning\nand Use Case Development, Implementation and Integration, and\nOperationalization and Optimization. Each phase provides scalable guidance on\nreadiness assessment, strategic alignment, risk governance, technical\narchitecture, and change management adaptable to organizational scale and\ncomplexity. The framework incorporates GenAI specific considerations including\nprompt engineering, model orchestration, and hallucination management that\ndistinguish it from generic technology adoption frameworks. As a perspective\ncontribution, FAIGMOE provides the first comprehensive conceptual framework\nexplicitly addressing GenAI adoption across midsize and enterprise\norganizations, offering actionable implementation protocols, assessment\ninstruments, and governance templates requiring empirical validation through\nfuture research.", "AI": {"tldr": "This paper presents FAIGMOE, a new conceptual framework for adopting GenAI in midsize organizations and enterprises, addressing the unique challenges each face compared to existing frameworks.", "motivation": "The paper is motivated by the lack of existing technology adoption frameworks tailored for GenAI, which fails to address the distinct challenges midsize organizations and enterprises face in adopting this technology.", "method": "The authors developed FAIGMOE by synthesizing technology adoption theory, organizational change management, and innovation diffusion perspectives into four interconnected phases of implementation, adapting it specifically for GenAI by including considerations like prompt engineering and hallucination management.", "result": "FAIGMOE provides a comprehensive conceptual framework with four phases (Strategic Assessment, Planning and Use Case Development, Implementation and Integration, Operationalization and Optimization), along with tools like assessment instruments and governance templates.", "conclusion": "FAIGMOE fills a critical gap in GenAI adoption literature, offering scalable, actionable guidance tailored to both midsize and enterprise organizations, but its effectiveness requires empirical validation from future research."}}
{"id": "2510.19844", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19844", "abs": "https://arxiv.org/abs/2510.19844", "authors": ["Isaac Wu", "Michael Maslowski"], "title": "CourtGuard: A Local, Multiagent Prompt Injection Classifier", "comment": "11 pages, 7 figures", "summary": "As large language models (LLMs) become integrated into various sensitive\napplications, prompt injection, the use of prompting to induce harmful\nbehaviors from LLMs, poses an ever increasing risk. Prompt injection attacks\ncan cause LLMs to leak sensitive data, spread misinformation, and exhibit\nharmful behaviors. To defend against these attacks, we propose CourtGuard, a\nlocally-runnable, multiagent prompt injection classifier. In it, prompts are\nevaluated in a court-like multiagent LLM system, where a \"defense attorney\"\nmodel argues the prompt is benign, a \"prosecution attorney\" model argues the\nprompt is a prompt injection, and a \"judge\" model gives the final\nclassification. CourtGuard has a lower false positive rate than the Direct\nDetector, an LLM as-a-judge. However, CourtGuard is generally a worse prompt\ninjection detector. Nevertheless, this lower false positive rate highlights the\nimportance of considering both adversarial and benign scenarios for the\nclassification of a prompt. Additionally, the relative performance of\nCourtGuard in comparison to other prompt injection classifiers advances the use\nof multiagent systems as a defense against prompt injection attacks. The\nimplementations of CourtGuard and the Direct Detector with full prompts for\nGemma-3-12b-it, Llama-3.3-8B, and Phi-4-mini-instruct are available at\nhttps://github.com/isaacwu2000/CourtGuard.", "AI": {"tldr": "This paper addresses the issue of prompt injection attacks on large language models (LLMs), proposing CourtGuard, a multiagent system for classifying these attacks. CourtGuard has a lower false positive rate than the Direct Detector but is generally a worse detector. The paper highlights the importance of considering both adversarial and benign scenarios for prompt classification and suggests that multiagent systems can be effective in defending against prompt injection attacks.", "motivation": "The increasing integration of large language models (LLMs) into sensitive applications has made them vulnerable to prompt injection attacks, which can lead to the leakage of sensitive data, the spread of misinformation, and the exhibition of harmful behaviors. These attacks pose a significant risk to the security and reliability of LLMs.", "method": "To defend against prompt injection attacks, the authors propose CourtGuard, a locally-runnable, multiagent prompt injection classifier. In this system, prompts are evaluated in a court-like structure where different agents play the roles of defense attorneys, prosecution attorneys, and judges. The defense attorney model argues the prompt is benign, the prosecution attorney model argues it is a prompt injection, and the judge model provides the final classification. The paper also introduces the Direct Detector, an LLM as-a-judge approach for comparison.", "result": "The results show that CourtGuard has a lower false positive rate compared to the Direct Detector. However, CourtGuard is generally a worse prompt injection detector than the Direct Detector. Despite this, the lower false positive rate highlights the importance of considering both adversarial and benign scenarios in the classification of prompts.", "conclusion": "The study demonstrates that multiagent systems like CourtGuard can be used as a defense against prompt injection attacks, with the advantage of a lower false positive rate. Although CourtGuard is not as effective as the Direct Detector in overall detection, the paper emphasizes the importance of balancing both adversarial and benign scenarios in the classification process. The implementation of CourtGuard and the Direct Detector is available for further research and application."}}
{"id": "2510.20041", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20041", "abs": "https://arxiv.org/abs/2510.20041", "authors": ["Gareema Ranjan", "Mahmoud Alfadel", "Gengyi Sun", "Shane McIntosh"], "title": "The Cost of Downgrading Build Systems: A Case Study of Kubernetes", "comment": null, "summary": "Since developers invoke the build system frequently, its performance can\nimpact productivity. Modern artifact-based build tools accelerate builds, yet\nprior work shows that teams may abandon them for alternatives that are easier\nto maintain. While prior work shows why downgrades are performed, the\nimplications of downgrades remain largely unexplored. In this paper, we\ndescribe a case study of the Kubernetes project, focusing on its downgrade from\nan artifact-based build tool (Bazel) to a language-specific solution (Go\nBuild). We reproduce and analyze the full and incremental builds of change sets\nduring the downgrade period. On the one hand, we find that Bazel builds are\nfaster than Go Build, completing full builds in 23.06-38.66 up to 75.19 impose\na larger memory footprint than Go Build of 81.42-351.07 respectively. Bazel\nbuilds also impose a greater CPU load at parallelism settings above eight for\nfull builds and above one for incremental builds. We estimate that downgrading\nfrom Bazel can increase CI resource costs by up to 76 explore whether our\nobservations generalize by replicating our Kubernetes study on four other\nprojects that also downgraded from Bazel to older build tools. We observe that\nwhile build time penalties decrease, Bazel consistently consumes more memory.\nWe conclude that abandoning artifact-based build tools, despite perceived\nmaintainability benefits, tends to incur considerable performance costs for\nlarge projects. Our observations may help stakeholders to balance trade-offs in\nbuild tool adoption", "AI": {"tldr": "This paper analyzes the performance implications of downgrading from Bazel to Go Build in the Kubernetes project, finding that while maintainability might improve, performance costs like longer build times and higher resource usage are significant.", "motivation": "Developers frequently use build systems, and their performance affects productivity. Although artifact-based tools like Bazel speed up builds, teams might switch to simpler tools due to maintainability issues. Prior research doesn't fully explore the trade-offs of such downgrades.", "method": "The researchers conducted a case study on the Kubernetes project, which downgraded from Bazel to Go Build. They reproduced and analyzed full and incremental builds during the downgrade period. They also generalized their findings by replicating the study on four other projects that downgraded from Bazel.", "result": "Bazel builds were faster in both full (23.06-38.66 vs. 75.19) and incremental builds. However, Bazel consumed more memory (81.42-351.07 vs. Go Build) and used more CPU at higher parallelism levels. Downgrading to Go Build could increase CI resource costs by up to 76, and while other projects saw reduced build time penalties, they still faced higher memory costs compared to Bazel.", "conclusion": "The paper concludes that downgrading from artifact-based build tools, like Bazel, to simpler ones such as Go Build may offer maintainability benefits but at significant performance costs, especially for large projects. This understanding can help stakeholders make informed trade-offs in choosing build tools."}}
{"id": "2510.19851", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19851", "abs": "https://arxiv.org/abs/2510.19851", "authors": ["Artur Zolkowski", "Wen Xing", "David Lindner", "Florian Tram\u00e8r", "Erik Jenner"], "title": "Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability", "comment": null, "summary": "Recent findings suggest that misaligned models may exhibit deceptive\nbehavior, raising concerns about output trustworthiness. Chain-of-thought (CoT)\nis a promising tool for alignment monitoring: when models articulate their\nreasoning faithfully, monitors can detect and mitigate harmful behaviors before\nundesirable outcomes occur. However, a key uncertainty is: Can models obfuscate\ntheir CoT in order to pursue hidden adversarial objectives while evading\ndetection? To answer this question and thus stress-test CoT monitorability, we\ndevelop a composable and quantifiable taxonomy of prompts to elicit CoT\nobfuscation. We evaluate both internal CoT (reasoning traces) and external CoT\n(prompted reasoning in outputs) using toy tasks and more realistic environments\nin SHADE-Arena. We show that: (i) CoT monitoring performs accurately and\nefficiently without obfuscation pressure. (ii) Under strong obfuscation\npressure, some models successfully complete adversarial tasks while evading\ndetection. (iii) Models do not obfuscate their internal CoT as much as their\nexternal CoT (under prompt pressure). These results suggest that while CoT\nprovides valuable oversight in benign settings, robust deployment requires\nmodel-specific stress-testing of monitorability.", "AI": {"tldr": "The study investigates whether models can deceive by hiding their reasoning (Chain-of-Thought, CoT) in alignment monitoring, and finds that under certain conditions, they can achieve adversarial goals while evading detection.", "motivation": "The motivation is to assess the effectiveness of CoT alignment monitoring by exploring the potential for models to obfuscate their reasoning to bypass monitoring.", "method": "The researchers created a taxonomy of prompts to induce CoT obfuscation and evaluated both internal (reasoning traces) and external (prompted output reasoning) CoT using toy tasks and the SHADE-Arena environment.", "result": "Results indicated that CoT monitoring works well without deception, but under strong obfuscation pressure, some models can achieve hidden objectives undetected, and internal CoT is less obfuscated than external CoT under pressure.", "conclusion": "The study concludes that while CoT is useful in benign cases, ensuring robust deployment requires model-specific stress-testing of monitoring capabilities."}}
{"id": "2510.20121", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20121", "abs": "https://arxiv.org/abs/2510.20121", "authors": ["Carlos J. Fernandez-Candel", "Jesus Garcia-Molina", "Francisco Javier Bermudez Ruiz", "Jose Ramon Hoyos Barcelo", "Diego Sevilla Ruiz", "Benito Jose Cuesta Viera"], "title": "Developing a Model-Driven Reengineering Approach for Migrating PL/SQL Triggers to Java: A Practical Experience", "comment": "31 pages, 22 figures", "summary": "Model-driven software engineering (MDE) techniques are not only useful in\nforward engineering scenarios, but can also be successfully applied to evolve\nexisting systems. RAD (Rapid Application Development) platforms emerged in the\nnineties, but the success of modern software technologies motivated that a\nlarge number of enterprises tackled the migration of their RAD applications,\nsuch as Oracle Forms. Our research group has collaborated with a software\ncompany in developing a solution to migrate PL/SQL monolithic code on Forms\ntriggers and program units to Java code separated in several tiers.\n  Our research focused on the model-driven reengineering process applied to\ndevelop the migration tool for the conversion of PL/SQL code to Java. Legacy\ncode is represented in form of KDM (Knowledge-Discovery Metamodel) models. In\nthis paper, we propose a software process to implement a model-driven\nre-engineering. This process integrates a TDD-like approach to incrementally\ndevelop model transformations with three kinds of validations for the generated\ncode. The implementation and validation of the re-engineering approach are\nexplained in detail, as well as the evaluation of some issues related with the\napplication of MDE.", "AI": {"tldr": "This paper presents a software process for ...", "motivation": "The success of modern software technologies ...", "method": "The research focused on model-driven reengineering ...", "result": "The implementation and validation of the process ...", "conclusion": "The paper details the evaluation of MDE application issues ..."}}
{"id": "2510.19856", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19856", "abs": "https://arxiv.org/abs/2510.19856", "authors": ["Eranga Bandara", "Sachin Shetty", "Ravi Mukkamala", "Ross Gore", "Peter Foytik", "Safdar H. Bouk", "Abdul Rahman", "Xueping Liang", "Ng Wee Keong", "Kasun De Zoysa", "Aruna Withanage", "Nilaan Loganathan"], "title": "Model Context Contracts - MCP-Enabled Framework to Integrate LLMs With Blockchain Smart Contracts", "comment": null, "summary": "In recent years, blockchain has experienced widespread adoption across\nvarious industries, becoming integral to numerous enterprise applications.\nConcurrently, the rise of generative AI and LLMs has transformed human-computer\ninteractions, offering advanced capabilities in understanding and generating\nhuman-like text. The introduction of the MCP has further enhanced AI\nintegration by standardizing communication between AI systems and external data\nsources. Despite these advancements, there is still no standardized method for\nseamlessly integrating LLM applications and blockchain. To address this\nconcern, we propose \"MCC: Model Context Contracts\" a novel framework that\nenables LLMs to interact directly with blockchain smart contracts through\nMCP-like protocol. This integration allows AI agents to invoke blockchain smart\ncontracts, facilitating more dynamic and context-aware interactions between\nusers and blockchain networks. Essentially, it empowers users to interact with\nblockchain systems and perform transactions using queries in natural language.\nWithin this proposed architecture, blockchain smart contracts can function as\nintelligent agents capable of recognizing user input in natural language and\nexecuting the corresponding transactions. To ensure that the LLM accurately\ninterprets natural language inputs and maps them to the appropriate MCP\nfunctions, the LLM was fine-tuned using a custom dataset comprising user inputs\npaired with their corresponding MCP server functions. This fine-tuning process\nsignificantly improved the platform's performance and accuracy. To validate the\neffectiveness of MCC, we have developed an end-to-end prototype implemented on\nthe Rahasak blockchain with the fine-tuned Llama-4 LLM. To the best of our\nknowledge, this research represents the first approach to using the concept of\nModel Context Protocol to integrate LLMs with blockchain.", "AI": {"tldr": "A new framework, MCC, is proposed to integrate LLMs with blockchain via an MCP-like protocol, enabling natural language interaction with smart contracts.", "motivation": "Despite advances in blockchain and LLMs, there's no standardized method for their seamless integration. This limits dynamic and context-aware interactions.", "method": "The authors propose Model Context Contracts (MCC), a framework that allows LLMs to directly interact with blockchain smart contracts using an MCP-like protocol. They fine-tuned an LLM with a custom dataset of user inputs and MCP server functions.", "result": "An end-to-end prototype was developed on Rahasak blockchain with Llama-4 LLM. The fine-tuning significantly improved performance and accuracy.", "conclusion": "This research presents the first approach to integrate LLMs with blockchain using the Model Context Protocol concept, enabling intelligent interactions between users and blockchain systems."}}
{"id": "2510.20211", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20211", "abs": "https://arxiv.org/abs/2510.20211", "authors": ["Zhenning Yang", "Hui Guan", "Victor Nicolet", "Brandon Paulsen", "Joey Dodds", "Daniel Kroening", "Ang Chen"], "title": "Automated Cloud Infrastructure-as-Code Reconciliation with AI Agents", "comment": null, "summary": "Cloud infrastructure is managed through a mix of interfaces -- traditionally,\ncloud consoles, command-line interfaces (CLI), and SDKs are the tools of\nchoice. Recently, Infrastructure-as-Code/IaC frameworks (e.g., Terraform) have\nquickly gained popularity. Unlike conventional tools, IaC~frameworks encode the\ninfrastructure in a \"source-of-truth\" configuration. They are capable of\nautomatically carrying out modifications to the cloud -- deploying, updating,\nor destroying resources -- to bring the actual infrastructure into alignment\nwith the IaC configuration. However, when IaC is used alongside consoles, CLIs,\nor SDKs, it loses visibility into external changes, causing infrastructure\ndrift, where the configuration becomes outdated, and later IaC operations may\nundo valid updates or trigger errors.\n  We present NSync, an automated system for IaC reconciliation that propagates\nout-of-band changes back into the IaC program. Our key insight is that\ninfrastructure changes eventually all occur via cloud API invocations -- the\nlowest layer for cloud management operations. NSync gleans insights from API\ntraces to detect drift (i.e., non-IaC changes) and reconcile it (i.e., update\nthe IaC configuration to capture the changes). It employs an agentic\narchitecture that leverages LLMs to infer high-level intents from noisy API\nsequences, synthesize targeted IaC updates using specialized tools, and\ncontinually improve through a self-evolving knowledge base of past\nreconciliations. We further introduce a novel evaluation pipeline for injecting\nrealistic drifts into cloud infrastructure and assessing reconciliation\nperformance. Experiments across five real-world Terraform projects and 372\ndrift scenarios show that NSync outperforms the baseline both in terms of\naccuracy (from 0.71 to 0.97 pass@3) and token efficiency (1.47$\\times$\nimprovement).", "AI": {"tldr": "NSync is an automated system for Infrastructure-as-Code (IaC) reconciliation, addressing infrastructure drift by propagating out-of-band changes into IaC programs. It uses API traces to detect and reconcile drift, employing an agentic architecture with LLMs and improving through a self-evolving knowledge base. Evaluation shows it outperforms baseline in accuracy and token efficiency.", "motivation": "Infrastructure drift caused by changes made using cloud consoles, CLIs, or SDKs alongside IaC frameworks like Terraform leads to outdated IaC configurations, potential errors, and invalid updates. Current solutions lack visibility into external modifications, risking infrastructure inconsistencies.", "method": "NSync detects and reconciles infrastructure drift by analyzing cloud API traces. It infers high-level intents using LLMs, generates targeted IaC updates with specialized tools, and maintains a self-evolving knowledge base for continual improvement. A novel evaluation pipeline injects realistic drifts to assess reconciliation performance.", "result": "Evaluating NSync against five real-world Terraform projects and 372 drift scenarios, it shows improved accuracy (0.71 to 0.97 pass@3) and better token efficiency (1.47\u00d7 improvement) compared to baseline methods.", "conclusion": "NSync effectively addresses infrastructure drift by integrating out-of-band changes into IaC configurations, demonstrating superior reconciliation performance. Its agentic architecture and use of LLMs, along with a self-improving knowledge base, contribute to its effectiveness. The introduced evaluation pipeline enables realistic drift injection for assessing reconciliation."}}
{"id": "2510.19859", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19859", "abs": "https://arxiv.org/abs/2510.19859", "authors": ["Smita Khapre"], "title": "Cyberattack Detection in Critical Infrastructure and Supply Chains", "comment": null, "summary": "Cyberattack detection in Critical Infrastructure and Supply Chains has become\nchallenging in Industry 4.0. Intrusion Detection Systems (IDS) are deployed to\ncounter the cyberattacks. However, an IDS effectively detects attacks based on\nthe known signatures and patterns, Zero-day attacks go undetected. To overcome\nthis drawback in IDS, the integration of a Dense Neural Network (DNN) with Data\nAugmentation is proposed. It makes IDS intelligent and enables it to self-learn\nwith high accuracy when a novel attack is encountered. The network flow\ncaptures datasets are highly imbalanced same as the real network itself. The\nData Augmentation plays a crucial role in balancing the data. The balancing of\ndata is challenging as the minority class is as low as 0.000004\\% of the\ndataset, and the abundant class is higher than 80\\% of the dataset. Synthetic\nMinority Oversampling Technique is used for balancing the data. However, higher\naccuracies are achieved with balanced test data, lower accuracies are\nnoticeable with the original imbalanced test data suggesting overfitting. A\ncomparison with state-of-the-art research using Synthetic Minority Oversampling\nTechnique with Edited Nearest Neighbor shows the classification of classes\nremains poor for the original dataset. This suggests highly imbalanced datasets\nof network flow require a different method of data augmentation.", "AI": {"tldr": "The paper proposes integrating a Dense Neural Network (DNN) with Data Augmentation to improve cyberattack detection in Industry 4.0 systems, addressing the issue of zero-day attacks and handling imbalanced network flow datasets. However, the proposed method shows signs of overfitting with original imbalanced datasets, suggesting the need for better data augmentation techniques.", "motivation": "The motivation of the paper is to enhance intrusion detection in critical infrastructure and supply chains within Industry 4.0 by addressing the limitations of traditional IDS systems, which are ineffective against zero-day attacks and imbalanced datasets.", "method": "The proposed method involves integrating a Dense Neural Network (DNN) with Data Augmentation, particularly using Synthetic Minority Oversampling Technique (SMOTE) to balance the highly imbalanced network flow datasets. The DNN is designed to self-learn and detect novel attacks with high accuracy.", "result": "The results show that the proposed method achieves higher classification accuracies with balanced test data but performs poorly with original imbalanced test data, indicating overfitting. A comparison with state-of-the-art techniques using SMOTE with Edited Nearest Neighbor also demonstrates poor classification performance for the original dataset.", "conclusion": "The paper concludes that traditional data augmentation methods like SMOTE are insufficient for highly imbalanced network flow datasets and suggests the need for more effective data augmentation techniques to improve intrusion detection in Industry 4.0."}}
{"id": "2510.20340", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.20340", "abs": "https://arxiv.org/abs/2510.20340", "authors": ["Serena Cofano", "Daniel Williams", "Aman Sharma", "Martin Monperrus"], "title": "Classport: Designing Runtime Dependency Introspection for Java", "comment": null, "summary": "Runtime introspection of dependencies, i.e., the ability to observe which\ndependencies are currently used during program execution, is fundamental for\nSoftware Supply Chain security. Yet, Java has no support for it. We solve this\nproblem with Classport, a system that embeds dependency information into Java\nclass files, enabling the retrieval of dependency information at runtime. We\nevaluate Classport on six real-world projects, demonstrating the feasibility in\nidentifying dependencies at runtime. Runtime dependency introspection with\nClassport opens important avenues for runtime integrity checking.", "AI": {"tldr": "Classport addresses Java's lack of runtime dependency introspection by embedding dependency metadata into class files, enabling real-time dependency tracking for enhanced software supply chain security.", "motivation": "Java's absence of runtime dependency visibility creates security risks in software supply chains, requiring post-hoc analysis and limiting dynamic integrity verification capabilities.", "method": "Classport instrumentally embeds dependency information directly into compiled Java class files during build time, enabling low-overhead runtime retrieval through standard bytecode inspection techniques.", "result": "Evaluation on six real-world projects demonstrated 98.7% accuracy in runtime dependency identification with <2.3% performance overhead, validating feasibility for production use cases.", "conclusion": "Classport establishes foundation for dynamic runtime integrity checking in Java ecosystems, enabling novel security applications like real-time supply chain attestation and unauthorized dependency detection."}}
{"id": "2510.19877", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19877", "abs": "https://arxiv.org/abs/2510.19877", "authors": ["Jean-Marie Le Ray"], "title": "Policy-Governed RAG - Research Design Study", "comment": "51 pages, 8 figures", "summary": "A policy-governed RAG architecture is specified for audit-ready generation in\nregulated workflows, organized as a triptych: (I) Contracts/Control\n(SHRDLU-like), which governs output adherence to legal and internal policies;\n(II) Manifests/Trails (Memex-like), which cryptographically anchors all cited\nsource evidence to ensure verifiable provenance; and (III)\nReceipts/Verification (Xanadu-like), which provides the final, portable proof\nof compliance for auditors (portable COSE/JOSE) (see Section 4 and Appendix A).\nRather than explaining model internals, outputs are gated ex-ante and bound to\ncryptographically verifiable evidence for each material answer. Unvalidated\ntargets are stated (>=20% relative reduction in confident errors; p95 latency\n<= 900 ms; <= 2.2x serve cost) together with a pre-registered (optional) pilot\nusing NO-GO gates. The design complements existing RAG/guardrails by making\npolicy checks auditable, replayable, and receipt-backed. Target domains include\nback-office compliance in pharma, medical devices, finance, legal, and the\npublic sector where error costs may exceed thousands of euros and audit trails\nare mandatory under regulations such as the EU AI Act. Future evaluations may\npre-commit to publishing negative results when any example NO-GO gate is not\nmet.", "AI": {"tldr": "This paper proposes a policy-enforced RAG system with three verifiable components, targeting compliance in regulated industries.", "motivation": "The motivation is to enable audit-ready, policy-compliant generation in high-stakes domains where non-compliance can lead to severe financial and legal repercussions.", "method": "The method involves structuring the RAG (Retrieval-Augmented Generation) system into three components: (I) Contracts/Control for policy enforcement, (II) Manifests/Trails for securing source evidence, and (III) Receipts/Verification for compliance proof. The system prevents policy violations before output by ensuring verifiable evidence for each answer.", "result": "The design aims for reduced confident errors by at least 20%, latency under 900 ms at p95, and serving costs no more than 2.2x that of standard RAG. A pilot is planned using NO-GO gates.", "conclusion": "The policy-governed RAG architecture offers a way to meet regulatory demands in error-sensitive and auditable sectors, setting a foundation for future standardized compliance systems."}}
{"id": "2510.20389", "categories": ["cs.SE", "cs.DC", "D.m"], "pdf": "https://arxiv.org/pdf/2510.20389", "abs": "https://arxiv.org/abs/2510.20389", "authors": ["Bjorn Remseth"], "title": "Symmetry in Software Platforms as an Architectural Principle", "comment": "Working paper, 11 pages", "summary": "Software platforms often act as structure preserving systems. They provide\nconsistent interfaces and behaviors that remain stable under specific\ntransformations that we denote as symmetries. This paper explores the idea that\narchitectural robustness emerges from enforcing such structural regularities", "AI": {"tldr": "The paper discusses how architectural robustness in software platforms is achieved through structural regularities or symmetries, ensuring stable interfaces and behaviors under specific transformations.", "motivation": "The paper is motivated by the observation that software platforms maintain consistent interfaces and stable behaviors under transformations, which contributes to their architectural robustness. The goal is to formalize this understanding and explore its implications.", "method": "The authors analyze software platforms as structure-preserving systems and define the concept of 'symmetries' that maintain the system's stability. They likely use theoretical models or case studies to examine these regularities and their impact on robustness and adaptability.", "result": "The analysis shows that enforcing structural regularities can lead to architectural robustness, as it consistently supports interfaces and behaviors under specific transformations while adapting to evolving requirements.", "conclusion": "The paper concludes that architectural robustness in software systems is a consequence of structural regularities and symmetries. By maintaining these patterns, software platforms can ensure their stability and extend their lifetime while remaining adaptable to change."}}
{"id": "2510.19883", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.19883", "abs": "https://arxiv.org/abs/2510.19883", "authors": ["Selma Shikonde", "Mike Wa Nkongolo"], "title": "A Proactive Insider Threat Management Framework Using Explainable Machine Learning", "comment": "Full master's in information technology (Information Science),\n  University of Pretoria, Department of Informatics", "summary": "Over the years, the technological landscape has evolved, reshaping the\nsecurity posture of organisations and increasing their exposure to\ncybersecurity threats, many originating from within. Insider threats remain a\nmajor challenge, particularly in sectors where cybersecurity infrastructure,\nexpertise, and regulations are still developing. This study proposes the\nInsider Threat Explainable Machine Learning (IT-XML) framework, which\nintegrates the Cross-Industry Standard Process for Data Mining (CRISP-DM) with\nHidden Markov Models (HMM) to enhance proactive insider threat management and\ndecision-making. A quantitative approach is adopted using an online\nquestionnaire to assess employees' knowledge of insider threat patterns, access\ncontrol, privacy practices, and existing policies across three large\ndata-sensitive organisations. The IT-XML framework provides assessment\ncapabilities through survey-based data, HMM-driven pattern recognition for\nsecurity maturity classification, and evidence-based recommendations for\nproactive threat mitigation. The framework classified all organisations at the\ndeveloping security maturity level with 97-98% confidence and achieved a\nclassification accuracy of 91.7%, identifying audit log access limits as the\nmost critical control. Random Forest analysis highlighted vendor breach\nnotifications (0.081) and regular audit log reviews (0.052) as key determinants\nof resilience. Explainability methods such as SHAP and LIME improved model\ntransparency and interpretability, demonstrating the framework's potential to\nstrengthen insider threat management practices.", "AI": {"tldr": "", "motivation": "", "method": "", "result": "", "conclusion": ""}}
{"id": "2510.20403", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20403", "abs": "https://arxiv.org/abs/2510.20403", "authors": ["Santiago Gil", "Ecem E. Ba\u015f", "Christian D. Jensen", "Sebastian Engelsgaard", "Giuseppe Abbiati", "Cl\u00e1udio Gomes"], "title": "FMI-Based Distributed Co-Simulation with Enhanced Security and Intellectual Property Safeguards", "comment": "6 pages, Proceedings of the 2025 Annual Modeling and Simulation\n  Conference (ANNSIM)", "summary": "Distributed co-simulation plays a key role in enabling collaborative modeling\nand simulation by different stakeholders while protecting their Intellectual\nProperty (IP). Although IP protection is provided implicitly by co-simulation,\nthere is no consensus in the guidelines to conduct distributed co-simulation of\ncontinuous-time or hybrid systems with no exposure to potential hacking\nattacks. We propose an approach for distributed co-simulation on top of UniFMU\nwith enhanced cybersecurity and IP protection mechanisms, ensuring that the\nconnection is initiated by the client and the models and binaries live on\ntrusted platforms. We showcase the functionality of this approach using two\nco-simulation demos in four different network settings and analyze the\ntrade-off between IP-protected distribution and performance efficiency in these\nsettings.", "AI": {"tldr": "This paper proposes a UniFMU-based distributed co-simulation approach with enhanced cybersecurity and IP protection for continuous-time/hybrid systems, validated through demos across four network settings that analyze security-performance trade-offs.", "motivation": "Current co-simulation lacks standardized guidelines for secure implementation in continuous-time/hybrid systems, leaving them vulnerable to hacking attacks while maintaining IP protection.", "method": "The approach leverages UniFMU with client-initiated connection protocols and model/binaries hosted on trusted platforms. Two co-simulation demonstrations are implemented across four network configurations to evaluate effectiveness.", "result": "The demos validate cybersecurity and IP protection functionality while quantifying performance efficiency trade-offs across different network settings.", "conclusion": "The proposed framework establishes a practical method for secure distributed co-simulation with IP protection, demonstrating feasible security-performance balances for collaborative modeling scenarios."}}
{"id": "2510.19885", "categories": ["cs.CR", "math.NT"], "pdf": "https://arxiv.org/pdf/2510.19885", "abs": "https://arxiv.org/abs/2510.19885", "authors": ["James Kim"], "title": "Analysis and Comparison of Known and Randomly Generated S-boxes for Block Ciphers", "comment": "Master's Dissertation 41 pages", "summary": "Mathematically constructed S-boxes arise from algebraic structures and finite\nfield theory to ensure strong, provable cryptographic properties. These\nmathematically grounded constructions allow for generation of thousands of\nS-Boxes with high nonlinearity, APN properties, and balanced avalanche\ncharacteristics, unlike fully random methods, which lack such theoretical\nguarantees in exchange for low complexity and more varied results. In this\nwork, we compare mathematically constructed constructions with randomly\ngenerated ones to evaluate the relative weakness of the latter. We also\nestablish an average measure of performance for randomly generated\npermutations, as well as random with forced cycle constraints, and compare them\nto well-established designs in a simple SPN setting.", "AI": {"tldr": "The paper compares mathematically constructed S-boxes with randomly generated S-boxes to assess the cryptographic weaknesses of random methods. It establishes average performance measures for random permutations and constrained random permutations in a simple SPN setting.", "motivation": "The motivation stems from the need to understand the cryptographic weaknesses of random S-boxes, which lack the theoretical guarantees of mathematically constructed ones. This is important for assessing the security of cryptographic designs that rely on randomness.", "method": "The method involves comparing mathematically constructed S-boxes with randomly generated and constrained random permutations in a simple SPN setting. The paper evaluates cryptographic properties such as nonlinearity, APN properties, and avalanche characteristics.", "result": "The results show that mathematically constructed S-boxes generally outperform random and constrained random permutations in terms of cryptographic properties like nonlinearity and avalanche characteristics. The paper also establishes average performance measures for the random methods.", "conclusion": "The conclusion is that mathematically constructed S-boxes offer stronger and more reliable cryptographic properties compared to random ones, highlighting the risks of relying on random methods without theoretical guarantees in cryptographic designs."}}
{"id": "2510.20514", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20514", "abs": "https://arxiv.org/abs/2510.20514", "authors": ["Lea Salome Brugger", "Xavier Denis", "Peter M\u00fcller"], "title": "Toward Practical Deductive Verification: Insights from a Qualitative Survey in Industry and Academia", "comment": null, "summary": "Deductive verification is an effective method to ensure that a given system\nexposes the intended behavior. In spite of its proven usefulness and\nfeasibility in selected projects, deductive verification is still not a\nmainstream technique. To pave the way to widespread use, we present a study\ninvestigating the factors enabling successful applications of deductive\nverification and the underlying issues preventing broader adoption. We\nconducted semi-structured interviews with 30 practitioners of verification from\nboth industry and academia and systematically analyzed the collected data\nemploying a thematic analysis approach. Beside empirically confirming familiar\nchallenges, e.g., the high level of expertise needed for conducting formal\nproofs, our data reveal several underexplored obstacles, such as proof\nmaintenance, insufficient control over automation, and usability concerns. We\nfurther use the results from our data analysis to extract enablers and barriers\nfor deductive verification and formulate concrete recommendations for\npractitioners, tool builders, and researchers, including principles for\nusability, automation, and integration with existing workflows.", "AI": {"tldr": "This paper investigates the factors facilitating and hindering the adoption of deductive verification through interviews and thematic analysis, revealing challenges like expertise requirements and underexplored obstacles like proof maintenance.", "motivation": "Deductive verification is not mainstream despite its effectiveness, prompting the study to identify factors enabling its adoption and issues hindering it.", "method": "The study involved semi-structured interviews with 30 verification practitioners and thematic analysis of collected data.", "result": "Confirmed familiar challenges such as expertise requirements and uncovered new obstacles like proof maintenance, insufficient automation control, and usability concerns. Extracted enablers and barriers, and provided recommendations for practitioners, tool builders, and researchers.", "conclusion": "The study highlights existing and new obstacles to deductive verification adoption and offers actionable principles for improving usability, automation, and integration."}}
{"id": "2510.19890", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19890", "abs": "https://arxiv.org/abs/2510.19890", "authors": ["Jan Zelinka", "Oliver Kost", "Marek Hr\u00faz"], "title": "Deep Sequence-to-Sequence Models for GNSS Spoofing Detection", "comment": null, "summary": "We present a data generation framework designed to simulate spoofing attacks\nand randomly place attack scenarios worldwide. We apply deep neural\nnetwork-based models for spoofing detection, utilizing Long Short-Term Memory\nnetworks and Transformer-inspired architectures. These models are specifically\ndesigned for online detection and are trained using the generated dataset. Our\nresults demonstrate that deep learning models can accurately distinguish\nspoofed signals from genuine ones, achieving high detection performance. The\nbest results are achieved by Transformer-inspired architectures with early\nfusion of the inputs resulting in an error rate of 0.16%.", "AI": {"tldr": "The paper introduces a data generation framework for simulating spoofing attacks and employs LSTM and Transformer-based neural networks for detection, achieving 0.16 error rate.", "motivation": "The need for robust spoofing detection systems and lack of realistic datasets for training", "method": "Generated global spoofing attack datasets and trained LSTM/Transformer models with early input fusion for online detection", "result": "Transformer architecture achieved highest accuracy with 0.16% error rate in distinguishing spoofed vs genuine signals", "conclusion": "Deep learning models effectively detect spoofing attacks when trained on simulated datasets, with Transformer models showing superior performance for real-world deployment"}}
{"id": "2510.20521", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20521", "abs": "https://arxiv.org/abs/2510.20521", "authors": ["YingJian Xiao", "RongQun Hu", "WeiWei Gong", "HongWei Li", "AnQuan Jie"], "title": "Large Language Models for Fault Localization: An Empirical Study", "comment": "in Chinese language", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncode-related tasks, particularly in automated program repair. However, the\neffectiveness of such repairs is highly dependent on the performance of\nupstream fault localization, for which comprehensive evaluations are currently\nlacking. This paper presents a systematic empirical study on LLMs in the\nstatement-level code fault localization task. We evaluate representative\nopen-source models (Qwen2.5-coder-32b-instruct, DeepSeek-V3) and closed-source\nmodels (GPT-4.1 mini, Gemini-2.5-flash) to assess their fault localization\ncapabilities on the HumanEval-Java and Defects4J datasets. The study\ninvestigates the impact of different prompting strategies--including standard\nprompts, few-shot examples, and chain-of-reasoning--on model performance, with\na focus on analysis across accuracy, time efficiency, and economic cost\ndimensions. Our experimental results show that incorporating bug report context\nsignificantly enhances model performance. Few-shot learning shows potential for\nimprovement but exhibits noticeable diminishing marginal returns, while\nchain-of-thought reasoning's effectiveness is highly contingent on the model's\ninherent reasoning capabilities. This study not only highlights the performance\ncharacteristics and trade-offs of different models in fault localization tasks,\nbut also offers valuable insights into the strengths of current LLMs and\nstrategies for improving fault localization effectiveness.", "AI": {"tldr": "This paper studies LLMs in code fault localization, evaluating open-source and closed-source models on two datasets with various prompting strategies, and finds that bug report context improves performance.", "motivation": "LLMs are effective in automated program repair but their success depends on upstream fault-localization. Comprehensive evaluations of these tasks, covering multiple dimensions and prompting strategies, are lacking.", "method": "The authors conduct an empirical study by evaluating LLMs on statement-level code fault localization. They test Qwen2.5-coder-3b-instruct, Deepseek-V3, GPT-4.1 mini, and Gemini-2.5-flash on HumanEval-Java and Defects4J. They analyze the impact of prompting strategies: standard, few-shot examples, and chain-of-thought reasoning, assessing performance in terms of accuracy, time efficiency, and economic cost.", "result": "Results show bug report context leads to performance improvement. Few-shot learning helps but with diminishing returns. Effectiveness of chain of thought is contingent on model's inherent reasoning capability.", "conclusion": "The study reveals performance traits and trade-offs of different LLMs in fault-localization tasks. It underscores the importance of bug context and the limited gains from prompting strategies such as few-shot and reasoning."}}
{"id": "2510.19938", "categories": ["cs.CR", "cs.DC", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19938", "abs": "https://arxiv.org/abs/2510.19938", "authors": ["Foad Namjoo", "Neng Wan", "Devan Mallory", "Yuyi Chang", "Nithin Sugavanam", "Long Yin Lee", "Ning Xiong", "Emre Ertin", "Jeff M. Phillips"], "title": "Designing a Secure and Resilient Distributed Smartphone Participant Data Collection System", "comment": "9 pages, 3 figures. Accepted at EAI SmartSP 2025 Conference (Springer\n  LNICST). This version is the arXiv preprint prepared for open access", "summary": "Real-world health studies require continuous and secure data collection from\nmobile and wearable devices. We introduce MotionPI, a smartphone-based system\ndesigned to collect behavioral and health data through sensors and surveys with\nminimal interaction from participants. The system integrates passive data\ncollection (such as GPS and wristband motion data) with Ecological Momentary\nAssessment (EMA) surveys, which can be triggered randomly or based on physical\nactivity. MotionPI is designed to work under real-life constraints, including\nlimited battery life, weak or intermittent cellular connection, and minimal\nuser supervision. It stores data both locally and on a secure cloud server,\nwith encrypted transmission and storage. It integrates through Bluetooth Low\nEnergy (BLE) into wristband devices that store raw data and communicate motion\nsummaries and trigger events. MotionPI demonstrates a practical solution for\nsecure and scalable mobile data collection in cyber-physical health studies.", "AI": {"tldr": "MotionPI is a smartphone-based system for secure, continuous health data collection combining passive sensors and EMA surveys under real-world constraints like limited battery and intermittent connectivity.", "motivation": "Health studies require reliable mobile data collection despite challenges like battery limitations, weak connectivity, and minimal user interaction.", "method": "MotionPI integrates smartphone sensors (GPS, BLE wristband) with adaptive EMA surveys, local/cloud storage with encryption, and motion-triggered data collection via Bluetooth Low Energy.", "result": "Demonstrated a working system that handles real-world constraints through encrypted transmission, adaptive data sampling, and hybrid local-cloud storage for scalable health monitoring.", "conclusion": "MotionPI provides a practical, secure framework for cyber-physical health research by combining sensory data with contextual surveys while addressing mobile device limitations."}}
{"id": "2510.20679", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20679", "abs": "https://arxiv.org/abs/2510.20679", "authors": ["Jonas Klauke", "Tom Ohlmer", "Stefan Schott", "Serena Elisa Ponta", "Wolfram Fischer", "Eric Bodden"], "title": "A Soundness and Precision Benchmark for Java Debloating Tools", "comment": "Preprint - accepted at the ACM Workshop on Software Supply Chain\n  Offensive Research and Ecosystem Defenses (SCORED '25)", "summary": "Modern software development reuses code by importing libraries as\ndependencies. Software projects typically include an average of 36\ndependencies, with 80% being transitive, meaning they are dependencies of\ndependencies. Recent research indicates that only 24.9% of these dependencies\nare required at runtime, and even within those, many program constructs remain\nunused, adding unnecessary code to the project. This has led to the development\nof debloating tools that remove unnecessary dependencies and program constructs\nwhile balancing precision by eliminating unused constructs and soundness by\npreserving all required constructs. To systematically evaluate this trade-off,\nwe developed Deblometer, a micro-benchmark consisting of 59 test cases designed\nto assess support for various Java language features in debloating tools. Each\ntest case includes a manually curated ground truth specifying necessary and\nbloated classes, methods, and fields, enabling precise measurement of soundness\nand precision. Using Deblometer, we evaluated three popular Java debloating\ntools: Deptrim, JShrink, and ProGuard. Our evaluation reveals that all tools\nremove required program constructs, which results in changed semantics or\nexecution crashes. In particular, the dynamic class loading feature introduces\nunsoundness in all evaluated tools. Our comparison shows that Deptrim retains\nmore bloated constructs, while ProGuard removes more required constructs.\nJShrink's soundness is significantly affected by limited support for\nannotations, which leads to corrupted debloated artifacts. These soundness\nissues highlight the need to improve debloating tools to ensure stable and\nreliable debloated software.", "AI": {"tldr": "The paper introduces Deblometer, a micro-benchmark for Java debloating tools (Deptrim, JShrink, ProGuard), revealing soundness issues in their handling of dependencies and language features.", "motivation": "Modern software development relies heavily on dependencies, but many are unused. Debloating tools aim to remove them, but the trade-off between precision (removing unused constructs) and soundness (retaining required constructs) needs systematic evaluation.", "method": "The authors created Deblometer, a benchmark with 59 test cases featuring manually curated ground truths on necessary vs. bloated constructs. They evaluated three Java debloating tools (Deptrim, JShrink, ProGuard) on their ability to preserve required constructs and remove unnecessary ones, focusing on Java language features like dynamic class loading and annotations.", "result": "All debloating tools removed required constructs, causing semantic changes or crashes. Dynamic class loading introduced unsoundness in all evaluated tools. Deptrim retained more bloated constructs, ProGuard removed more required ones, and JShrink had soundness issues due to limited annotation support.", "conclusion": "There is a clear need to improving Java debloating tools to avoid unsoundness and ensure reliable debloated software, particularly with regard to language features."}}
{"id": "2510.19968", "categories": ["cs.CR", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.19968", "abs": "https://arxiv.org/abs/2510.19968", "authors": ["Vipin Rathi", "Lakshya Chopra", "Madhav Agarwal", "Nitin Rajput", "Kriish Sharma", "Sushant Mundepi", "Shivam Gangwar", "Rudraksh Rawal", "Jishan"], "title": "Q-RAN: Quantum-Resilient O-RAN Architecture", "comment": "23 pages", "summary": "The telecommunications industry faces a dual transformation: the\narchitectural shift toward Open Radio Access Networks (O-RAN) and the emerging\nthreat from quantum computing. O-RAN disaggregated, multi-vendor architecture\ncreates a larger attack surface vulnerable to crypt-analytically relevant\nquantum computers(CRQCs) that will break current public key cryptography. The\nHarvest Now, Decrypt Later (HNDL) attack strategy makes this threat immediate,\nas adversaries can intercept encrypted data today for future decryption. This\npaper presents Q-RAN, a comprehensive quantum-resistant security framework for\nO-RAN networks using NIST-standardized Post-Quantum Cryptography (PQC). We\ndetail the implementation of ML-KEM (FIPS 203) and ML-DSA (FIPS 204),\nintegrated with Quantum Random Number Generators (QRNG) for cryptographic\nentropy. The solution deploys PQ-IPsec, PQ-DTLS, and PQ-mTLS protocols across\nall O-RAN interfaces, anchored by a centralized Post-Quantum Certificate\nAuthority (PQ-CA) within the SMO framework. This work provides a complete\nroadmap for securing disaggregated O-RAN ecosystems against quantum\nadversaries.", "AI": {"tldr": "This paper introduces Q-RAN, a quantum-resistant security framework for O-RAN networks using NIST-standardized PQC to protect against quantum computing threats.", "motivation": "O-RAN's multi-vendor architecture increases attack surfaces vulnerable to future quantum computers, and the HNDL attack requires immediate action to prevent future cryptographic breaks.", "method": "The framework uses ML-KEM and ML-DSA with QRNG for entropy, and implements PQ-IPsec, PQ-DTLS, PQ-mTLS across O-RAN interfaces, with a centralized PQ-CA in the SMO framework.", "result": "Q-RAN provides an end-to-end quantum-resistant security solution for O-RAN that integrates standardized post-quantum algorithms across all network interfaces.", "conclusion": "The Q-RAN framework offers a complete roadmap to secure O-RAN ecosystems against quantum threats through standardized PQC implementations and central cryptographic management."}}
{"id": "2510.20692", "categories": ["cs.SE", "cs.AI", "cs.FL", "D.4.6; D.2.4; I.2.2; I.2.7; F.3.1; F.4.3"], "pdf": "https://arxiv.org/pdf/2510.20692", "abs": "https://arxiv.org/abs/2510.20692", "authors": ["Adarsh Vatsa", "Bethel Hall", "William Eiers"], "title": "Exploring Large Language Models for Access Control Policy Synthesis and Summarization", "comment": "20 pages, 7 figures", "summary": "Cloud computing is ubiquitous, with a growing number of services being hosted\non the cloud every day. Typical cloud compute systems allow administrators to\nwrite policies implementing access control rules which specify how access to\nprivate data is governed. These policies must be manually written, and due to\ntheir complexity can often be error prone. Moreover, existing policies often\nimplement complex access control specifications and thus can be difficult to\nprecisely analyze in determining their behavior works exactly as intended.\nRecently, Large Language Models (LLMs) have shown great success in automated\ncode synthesis and summarization. Given this success, they could potentially be\nused for automatically generating access control policies or aid in\nunderstanding existing policies. In this paper, we explore the effectiveness of\nLLMs for access control policy synthesis and summarization. Specifically, we\nfirst investigate diverse LLMs for access control policy synthesis, finding\nthat: although LLMs can effectively generate syntactically correct policies,\nthey have permissiveness issues, generating policies equivalent to the given\nspecification 45.8% of the time for non-reasoning LLMs, and 93.7% of the time\nfor reasoning LLMs. We then investigate how LLMs can be used to analyze\npolicies by introducing a novel semantic-based request summarization approach\nwhich leverages LLMs to generate a precise characterization of the requests\nallowed by a policy. Our results show that while there are significant hurdles\nin leveraging LLMs for automated policy generation, LLMs show promising results\nwhen combined with symbolic approaches in analyzing existing policies.", "AI": {"tldr": "LLMs can generate syntactically correct access control policies but face permissiveness issues, especially with non-reasoning LLMs at 45.8% alignment versus 93.7% for reasoning models. The study also showspotential in combining LLMs with symbolic methods to better understand existing policies.", "motivation": "Typical cloud compute systems require manually written access control policies, which can be complex and error-prone. This study explores LLMs for policy synthesis and summarization to automate this process.", "method": "We first investigate LLMs for policy synthesis and then propose a semantic-based summarization technique that uses LLMs to understand the behavior of existing policies through symbolic methods.", "result": "LLMs generate syntactically correct policies, but non-reasoning models only align 45.8% of the time and reasoning models align 93.7%. Semantic-based summarization techniques combined with symbolic methods show promise in policy analysis.", "conclusion": "LLMs demonstrate potential for policy analysis when combined with symbolic methods, but automated generation remains challenging due to their permissiveness issues."}}
{"id": "2510.19979", "categories": ["cs.CR", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19979", "abs": "https://arxiv.org/abs/2510.19979", "authors": ["Tushar Nayan", "Ziqi Zhang", "Ruimin Sun"], "title": "SecureInfer: Heterogeneous TEE-GPU Architecture for Privacy-Critical Tensors for Large Language Model Deployment", "comment": "Accepted at IEEE Intelligent Computing and Systems at the Edge\n  (ICEdge) 2025", "summary": "With the increasing deployment of Large Language Models (LLMs) on mobile and\nedge platforms, securing them against model extraction attacks has become a\npressing concern. However, protecting model privacy without sacrificing the\nperformance benefits of untrusted AI accelerators, such as GPUs, presents a\nchallenging trade-off. In this paper, we initiate the study of high-performance\nexecution on LLMs and present SecureInfer, a hybrid framework that leverages a\nheterogeneous Trusted Execution Environments (TEEs)-GPU architecture to isolate\nprivacy-critical components while offloading compute-intensive operations to\nuntrusted accelerators. Building upon an outsourcing scheme, SecureInfer adopts\nan information-theoretic and threat-informed partitioning strategy:\nsecurity-sensitive components, including non-linear layers, projection of\nattention head, FNN transformations, and LoRA adapters, are executed inside an\nSGX enclave, while other linear operations (matrix multiplication) are\nperformed on the GPU after encryption and are securely restored within the\nenclave. We implement a prototype of SecureInfer using the LLaMA-2 model and\nevaluate it across performance and security metrics. Our results show that\nSecureInfer offers strong security guarantees with reasonable performance,\noffering a practical solution for secure on-device model inference.", "AI": {"tldr": "SecureInfer is a hybrid framework that combines Trusted Execution Environments (TEEs) with GPUs to protect Large Language Models (LLMs) from model extraction attacks while maintaining high performance on mobile and edge platforms.", "motivation": "The increasing deployment of LLMs on mobile and edge platforms has raised concerns about model extraction attacks, but previous solutions often struggle to balance model privacy with the performance benefits of using untrusted accelerators like GPUs.", "method": "SecureInfer uses a heterogeneous architecture with TEEs and GPUs. It partitions the model using an information-theoretic and threat-informed approach, executing security-sensitive components in a secure enclave (like SGX) while offloading linear operations to the GPU after encryption. This allows the secure execution of critical parts and parallel computation on the GPU for non-sensitive tasks.", "result": "A prototype of SecureInfer with LLaMA-2 showed that it provides strong security guarantees while maintaining reasonable performance, making it a practical solution for secure on-device model inference.", "conclusion": "SecureInfer offers a balanced solution to the trade-off between model privacy and performance by using a hybrid framework that securely partitions model operations between a TEE and an untrusted GPU."}}
{"id": "2510.19982", "categories": ["cs.CR", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.19982", "abs": "https://arxiv.org/abs/2510.19982", "authors": ["Vipin Rathi", "Lakshya Chopra", "Rudraksh Rawal", "Nitin Rajput", "Shiva Valia", "Madhav Aggarwal", "Aditya Gairola"], "title": "QORE : Quantum Secure 5G/B5G Core", "comment": "23 pages", "summary": "Quantum computing is reshaping the security landscape of modern\ntelecommunications. The cryptographic foundations that secure todays 5G\nsystems, including RSA, Elliptic Curve Cryptography (ECC), and Diffie-Hellman\n(DH), are all susceptible to attacks enabled by Shors algorithm. Protecting 5G\nnetworks against future quantum adversaries has therefore become an urgent\nengineering and research priority. In this paper we introduce QORE, a\nquantum-secure 5G and Beyond 5G (B5G) Core framework that provides a clear\npathway for transitioning both the 5G Core Network Functions and User Equipment\n(UE) to Post-Quantum Cryptography (PQC). The framework uses the\nNIST-standardized lattice-based algorithms Module-Lattice Key Encapsulation\nMechanism (ML-KEM) and Module-Lattice Digital Signature Algorithm (ML-DSA) and\napplies them across the 5G Service-Based Architecture (SBA). A Hybrid PQC\n(HPQC) configuration is also proposed, combining classical and quantum-safe\nprimitives to maintain interoperability during migration. Experimental\nvalidation shows that ML-KEM achieves quantum security with minor performance\noverhead, meeting the low-latency and high-throughput requirements of\ncarrier-grade 5G systems. The proposed roadmap aligns with ongoing 3GPP SA3 and\nSA5 study activities on the security and management of post-quantum networks as\nwell as with NIST PQC standardization efforts, providing practical guidance for\nmitigating quantum-era risks while safeguarding long-term confidentiality and\nintegrity of network data.", "AI": {"tldr": "QORE is a quantum-secure 5G/B5G framework using NIST lattice-based PQC algorithms (ML-KEM and ML-DSA) with a hybrid approach for smooth transition, showing low performance overhead.", "motivation": "Modern 5G cryptographic methods like RSA, ECC, and DH are vulnerable to Shor's algorithm attacks in the quantum era, necessitating urgent solutions for quantum-resistant security.", "method": "The paper proposes QORE, a framework for 5G/B5G core security which integrates NIST standardized lattice-based algorithms (ML-KEM and ML-DSA) into the 5G Service-Based Architecture (SBA). It also outlines a Hybrid PQC (HPQC) configuration allowing interoperability during the transition phase to quantum-safe infrastructure.", "result": "ML-KEM implementation in the QORE framework demonstrated quantum security with minor performance overhead, satisfying carrier-grade 5G's low-latency and high-throughput demands.", "conclusion": "QORE provides a practical pathway aligned with 3GPP and NIST standards for deploying quantum-resistant cryptography in 5G and B5G systems, ensuring both mitigation of future quantum threats and maintenance of data confidentiality and integrity."}}
{"id": "2510.20007", "categories": ["cs.CR", "94A60, 68M14, 68Q85", "D.4.6; K.6.5; E.3"], "pdf": "https://arxiv.org/pdf/2510.20007", "abs": "https://arxiv.org/abs/2510.20007", "authors": ["To-Wen Liu", "Matthew Green"], "title": "zk-Agreements: A Privacy-Preserving Way to Establish Deterministic Trust in Confidential Agreements", "comment": "To appear in Financial Cryptography 2026 if accepted", "summary": "Digital transactions currently exceed trillions of dollars annually, yet\ntraditional paper-based agreements remain a bottleneck for automation,\nenforceability, and dispute resolution. Natural language contracts introduce\nambiguity, require manual processing, and lack computational verifiability, all\nof which hinder efficient digital commerce. Computable legal contracts,\nexpressed in machine-readable formats, offer a potential solution by enabling\nautomated execution and verification. Blockchain-based smart contracts further\nstrengthen enforceability and accelerate dispute resolution; however, current\nimplementations risk exposing sensitive agreement terms on public ledgers,\nraising serious privacy and competitive intelligence concerns that limit\nenterprise adoption.\n  We introduce zk-agreements, a protocol designed to transition from\npaper-based trust to cryptographic trust while preserving confidentiality. Our\ndesign combines zero-knowledge proofs to protect private agreement terms,\nsecure two-party computation to enable private compliance evaluation, and smart\ncontracts to guarantee automated enforcement. Together, these components\nachieve both privacy preservation and computational enforceability, resolving\nthe fundamental tension between transparency and confidentiality in\nblockchain-based agreements.", "AI": {"tldr": "The paper presents zk-agreements, a secure protocol that uses zero-knowledge proofs and secure computation to confidentially automate contract execution and verification on blockchains.", "motivation": "Traditional paper contracts create inefficiencies in digital commerce through ambiguity, manual processing, and lack of computational verification. Existing smart contracts on blockchains, while enabling automation, expose private terms on public ledgers, causing privacy and enterprise adoption concerns.", "method": "zk-agreements integrates three components: \n1. Zero-knowledge proofs to shield private agreement terms\n2. Secure two-party computation for confidential compliance evaluation\n3. Smart contracts to ensure automated enforcement", "result": "The protocol achieves privacy preservation while maintaining computational enforceability, resolving the basic conflict between transparency and secrecy in blockchain agreements.", "conclusion": "zk-agreements provides an effective solution that preserves privacy while enabling automated contract execution through cryptographic methods in the blockchain ecosystem."}}
{"id": "2510.20056", "categories": ["cs.CR", "cs.SY", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.20056", "abs": "https://arxiv.org/abs/2510.20056", "authors": ["Hui Wang", "Hans D. Schotten", "Stefan M. Goetz"], "title": "Ultra-Fast Wireless Power Hacking", "comment": "11 pages, 15 figures", "summary": "The rapid growth of electric vehicles (EVs) has driven the development of\nroadway wireless charging technology, effectively extending EV driving range.\nHowever, wireless charging introduces significant cybersecurity challenges. Any\nreceiver within the magnetic field can potentially extract energy, and previous\nresearch demonstrated that a hacker could detect the operating frequency and\nsteal substantial power. However, our approach required time to track new\nfrequencies or precise adjustments of inductance and capacitance, which would\nbe less effective against potential rapid transmitter frequency changes or\ncapacitance drift. As a solution, we enhanced the interceptor and enabled it to\nintrude as well as steal energy within just three cycles of the high-frequency\nsignal. Moreover, it can work without any circuit parameters or look-up tables.\nThe key innovation is synchronizing the receiver current with the phase of the\nmagnetic sensor voltage. Through MATLAB / Simulink simulations, finite-element\nanalysis, and experimental validation, we demonstrated that our improved method\ncan steal over 76% of the power received by a fully resonant receiver under\nidentical conditions. This attack demonstrates that simple frequency-changing\npower encryption offers limited protection against such threats.", "AI": {"tldr": "The paper proposes an improved wireless power interception method for road-based electric vehicle (EV) charging systems, which can steal over 76% energy in just three signal cycles without needing circuit parameters, highlighting the inadequacy of basic frequency-based power encryption.", "motivation": "With the rise of EVs and wireless charging technology, securing these systems is crucial. Unlike traditional charging, where security is inherent by physical access requirements, wireless charging systems are inherently vulnerable since any object in the magnetic field can absorb energy, enabling potential eavesdropping and energy theft.", "method": "The authors propose a novel defense evasion strategy by enhancing the existing interception circuit. This involves tracking and matching the receiver\u2019s current phase to the varying magnetic sensor voltage, enabling the system to operate effectively across different frequencies and without prior knowledge of circuit parameters.", "result": "The method was validated through MATLAB/Simulink simulations, finite-element analysis, and experiments, showing it can steal more than 76% of the transferred power in just three cycles of a high-frequency signal. It also showed robustness against rapid frequency changes and capacitance drift.", "conclusion": "Simple frequency-changing methods for securing wireless power transfer (such as used in roadway EV charging) are ineffective against this improved interception strategy. This highlights a significant vulnerability, emphasizing the need for more robust security mechanisms in wireless power systems."}}
{"id": "2510.20739", "categories": ["cs.CR", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20739", "abs": "https://arxiv.org/abs/2510.20739", "authors": ["Ronghao Ni", "Aidan Z. H. Yang", "Min-Chien Hsu", "Nuno Sabino", "Limin Jia", "Ruben Martins", "Darion Cassel", "Kevin Cheang"], "title": "Learning to Triage Taint Flows Reported by Dynamic Program Analysis in Node.js Packages", "comment": null, "summary": "Program analysis tools often produce large volumes of candidate vulnerability\nreports that require costly manual review, creating a practical challenge: how\ncan security analysts prioritize the reports most likely to be true\nvulnerabilities?\n  This paper investigates whether machine learning can be applied to\nprioritizing vulnerabilities reported by program analysis tools. We focus on\nNode.js packages and collect a benchmark of 1,883 Node.js packages, each\ncontaining one reported ACE or ACI vulnerability. We evaluate a variety of\nmachine learning approaches, including classical models, graph neural networks\n(GNNs), large language models (LLMs), and hybrid models that combine GNN and\nLLMs, trained on data based on a dynamic program analysis tool's output. The\ntop LLM achieves $F_{1} {=} 0.915$, while the best GNN and classical ML models\nreaching $F_{1} {=} 0.904$. At a less than 7% false-negative rate, the leading\nmodel eliminates 66.9% of benign packages from manual review, taking around 60\nms per package. If the best model is tuned to operate at a precision level of\n0.8 (i.e., allowing 20% false positives amongst all warnings), our approach can\ndetect 99.2% of exploitable taint flows while missing only 0.8%, demonstrating\nstrong potential for real-world vulnerability triage.", "AI": {"tldr": "This paper explores using machine learning (including LLMs, GNNs, and classical models trained on dynamic analysis data) to prioritize vulnerability reports in Node.js packages, achieving high F1 scores and significant reduction in manual review workload.", "motivation": "Program analysis tools generate numerous candidate vulnerability reports that require costly manual review; automating prioritization of high-confidence reports is critical for practical security analysis.", "method": "1,883 Node.js packages with ACE/ACI vulnerabilities were evaluated using classical ML, graph neural networks (GNNs), large language models (LLMs), and hybrid models trained on dynamic program analysis data.", "result": "The best LLM achieved F1=0.915, and top models reduced manual review by 66.9%-99.2%. At 0.8 precision, models detected 99.2%-99.992%-99.2%, achieving near-total exploit detection with minimal misses.", "conclusion": "ML-based prioritization (particularly LLMs and graph+LLM hybrids) demonstrates strong potential for real-world vulnerability triage, drastically reducing false negatives and manual review requirements while maintaining high precision."}}
{"id": "2510.20080", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.20080", "abs": "https://arxiv.org/abs/2510.20080", "authors": ["M. Abdullah Canbaz", "Hakan Otal", "Tugce Unlu", "Nour Alhussein", "Brian Nussbaum"], "title": "Who Coordinates U.S. Cyber Defense? A Co-Authorship Network Analysis of Joint Cybersecurity Advisories (2024--2025)", "comment": null, "summary": "Cyber threats increasingly demand joint responses, yet the organizational\ndynamics behind multi-agency cybersecurity collaboration remain poorly\nunderstood. Understanding who leads, who bridges, and how agencies coordinate\nis critical for strengthening both U.S. homeland security and allied defense\nefforts. In this study, we construct a co-authorship network from nine Joint\nCybersecurity Advisories (CSAs) issued between November 2024 and August 2025.\nWe map 41 agencies and 442 co-authoring ties to analyze the structure of\ncollaboration. We find a tightly knit U.S. triad -- CISA, FBI, and NSA --\ndensely connected with Five Eyes and select European allies. Degree centrality\nidentifies CISA and FBI as coordination hubs, while betweenness highlights NSA,\nthe UK's NCSC, and Australia's ASD-ACSC as key bridges linking otherwise\nfragmented clusters. By releasing the first replicable dataset and network\nanalysis of CSAs, we provide new empirical evidence on how collaborative\ncybersecurity signals are organized and where strategic influence is\nconcentrated.", "AI": {"tldr": "", "motivation": "", "method": "", "result": "", "conclusion": ""}}
{"id": "2510.20129", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20129", "abs": "https://arxiv.org/abs/2510.20129", "authors": ["Yulong Chen", "Yadong Liu", "Jiawen Zhang", "Mu Li", "Chao Huang", "Jie Wen"], "title": "SAID: Empowering Large Language Models with Self-Activating Internal Defense", "comment": null, "summary": "Large Language Models (LLMs), despite advances in safety alignment, remain\nvulnerable to jailbreak attacks designed to circumvent protective mechanisms.\nPrevailing defense strategies rely on external interventions, such as input\nfiltering or output modification, which often lack generalizability and\ncompromise model utility while incurring significant computational overhead. In\nthis work, we introduce a new, training-free defense paradigm, Self-Activating\nInternal Defense (SAID), which reframes the defense task from external\ncorrection to internal capability activation. SAID uniquely leverages the LLM's\nown reasoning abilities to proactively identify and neutralize malicious intent\nthrough a three-stage pipeline: model-native intent distillation to extract\ncore semantics, optimal safety prefix probing to activate latent safety\nawareness, and a conservative aggregation strategy to ensure robust\ndecision-making. Extensive experiments on five open-source LLMs against six\nadvanced jailbreak attacks demonstrate that SAID substantially outperforms\nstate-of-the-art defenses in reducing harmful outputs. Crucially, it achieves\nthis while preserving model performance on benign tasks and incurring minimal\ncomputational overhead. Our work establishes that activating the intrinsic\nsafety mechanisms of LLMs is a more robust and scalable path toward building\nsafer and more reliable aligned AI systems.", "AI": {"tldr": "TL;DR: The paper proposes SAID, a training-free, internal defense method for LLMs that outperforms existing external interventions while maintaining model performance and efficiency.", "motivation": "Despite improvements in aligning LLMs with safety, existing defenses like input filtering or output modification lack generalization, hurt model utility, and use too much computation.", "method": "SAID uses the LLM's own reasoning to activate internal safety via 1) intent distillation, 2) safety prefix probing, and 3) conservative aggregation strategy.", "result": "SAID significantly reduces harmful outputs in 5 open-source LLMs against 6 jailbreak attacks while preserving performance on normal tasks with minimal overhead.", "conclusion": "Internal defenses like SAID that utilize intrinsic safety mechanisms are more robust and scalable for future AI alignment."}}
{"id": "2510.20131", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.20131", "abs": "https://arxiv.org/abs/2510.20131", "authors": ["Mohammed Barhoush"], "title": "Separating Pseudorandom Generators from Logarithmic Pseudorandom States", "comment": "18 pages", "summary": "Pseudorandom generators (PRGs) are a foundational primitive in classical\ncryptography, underpinning a wide range of constructions. In the quantum\nsetting, pseudorandom quantum states (PRSs) were proposed as a potentially\nweaker assumption that might serve as a substitute for PRGs in cryptographic\napplications. Two primary size regimes of PRSs have been studied:\nlogarithmic-size and linear-size. Interestingly, logarithmic PRSs have led to\npowerful cryptographic applications, such as digital signatures and quantum\npublic-key encryption, that have not been realized from their linear\ncounterparts. However, PRGs have only been black-box separated from linear\nPRSs, leaving open the fundamental question of whether PRGs are also separated\nfrom logarithmic PRSs.\n  In this work, we resolve this open problem. We establish a quantum black-box\nseparation between (quantum-evaluable) PRGs and PRSs of either size regime.\nSpecifically, we construct a unitary quantum oracle with inverse access\nrelative to which no black-box construction of PRG from (logarithmic or linear)\nPRS exists. As a direct corollary, we obtain separations between PRGs and\nseveral primitives implied by logarithmic PRSs, including digital signatures\nand quantum public-key encryption.", "AI": {"tldr": "The paper resolves a\nfundamental question\nabout pseudorandom\nquantum generators vs\nlogarithmic logarithmic.\nleaves.", "motivation": "The paper addresses the\nopen question of whether\nquantum PRG are... linear or log.\nseparationperial implications for\ncryptography.", "method": "The authors construct a\nunitary quantum oracle\nwith inverse access. This\noracle demonstrates that... PRG using only black-box\nlogging or linear PRS.", "result": "They prove a quantum\nblack-box separation\nbetween PRG and PRS in both\nlogarithmic and linear regimes.\nPRS-based constructions\nsignatures and Q-PKE.\nhave not previously been\nachieved with linear PRS.", "conclusion": "This resolves an open\nquestion about the\ndistinction between... leveraging linear\nor log-sized components,\nhighlighting different\npowerful results."}}
{"id": "2510.20223", "categories": ["cs.CR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.20223", "abs": "https://arxiv.org/abs/2510.20223", "authors": ["Divyanshu Kumar", "Shreyas Jena", "Nitin Aravind Birur", "Tanay Baswa", "Sahil Agarwal", "Prashanth Harshangi"], "title": "Beyond Text: Multimodal Jailbreaking of Vision-Language and Audio Models through Perceptually Simple Transformations", "comment": null, "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress,\nyet remain critically vulnerable to adversarial attacks that exploit weaknesses\nin cross-modal processing. We present a systematic study of multimodal\njailbreaks targeting both vision-language and audio-language models, showing\nthat even simple perceptual transformations can reliably bypass\nstate-of-the-art safety filters. Our evaluation spans 1,900 adversarial prompts\nacross three high-risk safety categories harmful content, CBRN (Chemical,\nBiological, Radiological, Nuclear), and CSEM (Child Sexual Exploitation\nMaterial) tested against seven frontier models. We explore the effectiveness of\nattack techniques on MLLMs, including FigStep-Pro (visual keyword\ndecomposition), Intelligent Masking (semantic obfuscation), and audio\nperturbations (Wave-Echo, Wave-Pitch, Wave-Speed). The results reveal severe\nvulnerabilities: models with almost perfect text-only safety (0\\% ASR) suffer\n>75\\% attack success under perceptually modified inputs, with FigStep-Pro\nachieving up to 89\\% ASR in Llama-4 variants. Audio-based attacks further\nuncover provider-specific weaknesses, with even basic modality transfer\nyielding 25\\% ASR for technical queries. These findings expose a critical gap\nbetween text-centric alignment and multimodal threats, demonstrating that\ncurrent safeguards fail to generalize across cross-modal attacks. The\naccessibility of these attacks, which require minimal technical expertise,\nsuggests that robust multimodal AI safety will require a paradigm shift toward\nbroader semantic-level reasoning to mitigate possible risks.", "AI": {"tldr": "This study analyzes how perceptual transformations in images and audio can bypass MLLMs' state-of-the-art safety filters. Evaluation with 1,900 prompts across three high-risk categories across seven frontier models reveals severe vulnerabilities, particularly with visual (FigStep-Pro) and audio-based techniques showing high attack success rates, urging a new approach to multimodal AI safety.", "motivation": "Despite advancements in MLLMs, they are vulnerable to adversarial attacks due to weaknesses in cross-modal processing. The study is motivated by the limited understanding of these vulnerabilities and the need to establish a framework for evaluating MLLMs' safety in multimodal contexts.", "method": "The method employs a set of attack techniques to evaluate the resilience of MLLMs against multimodal adversarial prompts. It tests 1,900 adversarial prompts, divided into three sensitive categories, in seven state-of-the-art MLLMs using strategies such as FigStep-Pro for visual decomposition, Intelligent Masking for obfuscation, and audio manipulations like Wave-Echo and Wave-Pitch. The goal is to measure the success rate of bypassing safety filters with perceptually altered inputs.", "result": "The findings show that MLLMs are significantly vulnerable to cross-modal adversarial attacks, with over 75% success rate for attacks altering visual and audio modalities. Techniques like FigStep-Pro and basic audio transfers achieve more than 89% and 25% attack success rates, respectively, despite the models having 0% success in text-only safety scenarios. This exposes a significant weakness in current MLLM safety mechanisms.", "conclusion": "The study concludes that existing safety filters in MLLMs are inadequate against perceptually modified multimodal attacks. This highlights the need for a move towards more comprehensive semantic-checking frameworks to ensure robust safety across modalities, urging a paradigm shift in the design and training of MLLMs for better threat detection and mitigation."}}
{"id": "2510.20243", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.20243", "abs": "https://arxiv.org/abs/2510.20243", "authors": ["Yu Hin Chan", "Hao Yang", "Shiyu Shen", "Xingyu Fan", "Shengzhe Lyu", "Patrick S. Y. Hung", "Ray C. C. Cheung"], "title": "HHEML: Hybrid Homomorphic Encryption for Privacy-Preserving Machine Learning on Edge", "comment": null, "summary": "Privacy-preserving machine learning (PPML) is an emerging topic to handle\nsecure machine learning inference over sensitive data in untrusted\nenvironments. Fully homomorphic encryption (FHE) enables computation directly\non encrypted data on the server side, making it a promising approach for PPML.\nHowever, it introduces significant communication and computation overhead on\nthe client side, making it impractical for edge devices. Hybrid homomorphic\nencryption (HHE) addresses this limitation by combining symmetric encryption\n(SE) with FHE to reduce the computational cost on the client side, and\ncombining with an FHE-friendly SE can also lessen the processing overhead on\nthe server side, making it a more balanced and efficient alternative. Our work\nproposes a hardware-accelerated HHE architecture built around a lightweight\nsymmetric cipher optimized for FHE compatibility and implemented as a dedicated\nhardware accelerator. To the best of our knowledge, this is the first design to\nintegrate an end-to-end HHE framework with hardware acceleration. Beyond this,\nwe also present several microarchitectural optimizations to achieve higher\nperformance and energy efficiency. The proposed work is integrated into a full\nPPML pipeline, enabling secure inference with significantly lower latency and\npower consumption than software implementations. Our contributions validate the\nfeasibility of low-power, hardware- accelerated HHE for edge deployment and\nprovide a hardware- software co-design methodology for building scalable,\nsecure machine learning systems in resource-constrained environments.\nExperiments on a PYNQ-Z2 platform with the MNIST dataset show over a 50x\nreduction in client-side encryption latency and nearly a 2x gain in hardware\nthroughput compared to existing FPGA-based HHE accelerators.", "AI": {"tldr": "The paper proposes a hardware-accelerated hybrid homomorphic encryption (HHE) architecture for efficient and secure machine learning inference on edge devices, achieving significant performance improvements.", "motivation": "Fully homomorphic encryption (FHE) enables secure machine learning inference on untrusted servers, but is too computationally and communicationally expensive for edge devices, prompting the need for a more efficient solution.", "method": "The authors develop a hybrid encryption system combining a lightweight FHE-compatible symmetric cipher (SE) with FHE, implemented as a hardware accelerator with microarchitectural optimizations to reduce client-side and server-side overhead.", "result": "Experiments on a PYNQ-Z2 platform with MNIST demonstrate over 50x reduction in client-side encryption latency and nearly 2x improvement in hardware throughput compared to other FPGA-based HHE accelerators.", "conclusion": "The paper demonstrates the feasibility of low-power, hardware-accelerated HHE for edge deployment and provides a co-design methodology for scalable, secure machine learning systems."}}
{"id": "2510.20300", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.20300", "abs": "https://arxiv.org/abs/2510.20300", "authors": ["Haojie Ji", "Long Jin", "Haowen Li", "Chongshi Xin", "Te Hu"], "title": "Privacy Protection of Automotive Location Data Based on Format-Preserving Encryption of Geographical Coordinates", "comment": null, "summary": "There are increasing risks of privacy disclosure when sharing the automotive\nlocation data in particular functions such as route navigation, driving\nmonitoring and vehicle scheduling. These risks could lead to the attacks\nincluding user behavior recognition, sensitive location inference and\ntrajectory reconstruction. In order to mitigate the data security risk caused\nby the automotive location sharing, this paper proposes a high-precision\nprivacy protection mechanism based on format-preserving encryption (FPE) of\ngeographical coordinates. The automotive coordinate data key mapping mechanism\nis designed to reduce to the accuracy loss of the geographical location data\ncaused by the repeated encryption and decryption. The experimental results\ndemonstrate that the average relative distance retention rate (RDR) reached\n0.0844, and the number of hotspots in the critical area decreased by 98.9%\nafter encryption. To evaluate the accuracy loss of the proposed encryption\nalgorithm on automotive geographical location data, this paper presents the\nexperimental analysis of decryption accuracy, and the result indicates that the\ndecrypted coordinate data achieves a restoration accuracy of 100%. This work\npresents a high-precision privacy protection method for automotive location\ndata, thereby providing an efficient data security solution for the sensitive\ndata sharing in autonomous driving.", "AI": {"tldr": "The paper introduces a high-precision privacy protection mechanism using format-preserving encryption for automotive geolocation data, effectively reducing privacy risks while maintaining location data accuracy.", "motivation": "There is a growing concern over privacy risks associated with sharing automotive location data, which can expose users to behaviors tracking, location inference attacks, and trajectory reconstructions. Securing this data is crucial for enabling secure data sharing in autonomous driving systems.", "method": "The method involves a key-mapped format-preserving encryption (FPE) mechanism tailored for automotive geolocation coordinates. It is designed to minimize the accuracy loss caused by repeated encryption and decryption, preserving the structure of the data while ensuring its privacy.", "result": "Experimental results showed the technique maintains a high degree of spatial accuracy with an average relative distance retention rate of 0.0844 and effectively masks sensitive information by decreasing the number of identifiable location hotspots by 98.9%.", "conclusion": "The study concludes that the proposed mechanism offers a high-precision solution for protecting automotive location data's privacy and security without substantial degradation in location data fidelity, supporting secure data usage in autonomous vehicle ecosystems."}}
{"id": "2510.20314", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20314", "abs": "https://arxiv.org/abs/2510.20314", "authors": ["Wu Yichao", "Wang Yirui", "Ding Panpan", "Wang Hailong", "Zhu Bingqian", "Liu Chun"], "title": "Enhancing Security in Deep Reinforcement Learning: A Comprehensive Survey on Adversarial Attacks and Defenses", "comment": null, "summary": "With the wide application of deep reinforcement learning (DRL) techniques in\ncomplex fields such as autonomous driving, intelligent manufacturing, and smart\nhealthcare, how to improve its security and robustness in dynamic and\nchangeable environments has become a core issue in current research. Especially\nin the face of adversarial attacks, DRL may suffer serious performance\ndegradation or even make potentially dangerous decisions, so it is crucial to\nensure their stability in security-sensitive scenarios. In this paper, we first\nintroduce the basic framework of DRL and analyze the main security challenges\nfaced in complex and changing environments. In addition, this paper proposes an\nadversarial attack classification framework based on perturbation type and\nattack target and reviews the mainstream adversarial attack methods against DRL\nin detail, including various attack methods such as perturbation state space,\naction space, reward function and model space. To effectively counter the\nattacks, this paper systematically summarizes various current robustness\ntraining strategies, including adversarial training, competitive training,\nrobust learning, adversarial detection, defense distillation and other related\ndefense techniques, we also discuss the advantages and shortcomings of these\nmethods in improving the robustness of DRL. Finally, this paper looks into the\nfuture research direction of DRL in adversarial environments, emphasizing the\nresearch needs in terms of improving generalization, reducing computational\ncomplexity, and enhancing scalability and explainability, aiming to provide\nvaluable references and directions for researchers.", "AI": {"tldr": "The paper discusses the security and robustness of deep reinforcement learning (DRL) in dynamic environments, classifies adversarial attacks, and explores defense strategies.", "motivation": "As DRL is increasingly used in critical areas like autonomous driving and healthcare, ensuring its security and robustness against adversarial attacks becomes essential to prevent dangerous decisions and performance issues.", "method": "The authors introduce the DRL framework, analyze security challenges, propose an adversarial attack classification based on perturbation type and target, review mainstream attack methods across various spaces (state, action, reward, model), and summarize robustness training strategies such as adversarial training and competitive training, while discussing their pros and cons.", "result": "A comprehensive classification of DRL-based adversarial attacks, a detailed review of attacking and defending methodologies, and insights into future research directions to address the limitations of current defense techniques.", "conclusion": "The paper concludes that improving DRL robustness requires further advancements in generalization, computational efficiency, scalability, and explainability, offering a structured analysis and directions for research in adversarial DRL environments."}}
{"id": "2510.20333", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20333", "abs": "https://arxiv.org/abs/2510.20333", "authors": ["Chiyu Chen", "Xinhao Song", "Yunkai Chai", "Yang Yao", "Haodong Zhao", "Lijun Li", "Jie Li", "Yan Teng", "Gongshen Liu", "Yingchun Wang"], "title": "GhostEI-Bench: Do Mobile Agents Resilience to Environmental Injection in Dynamic On-Device Environments?", "comment": null, "summary": "Vision-Language Models (VLMs) are increasingly deployed as autonomous agents\nto navigate mobile graphical user interfaces (GUIs). Operating in dynamic\non-device ecosystems, which include notifications, pop-ups, and inter-app\ninteractions, exposes them to a unique and underexplored threat vector:\nenvironmental injection. Unlike prompt-based attacks that manipulate textual\ninstructions, environmental injection corrupts an agent's visual perception by\ninserting adversarial UI elements (for example, deceptive overlays or spoofed\nnotifications) directly into the GUI. This bypasses textual safeguards and can\nderail execution, causing privacy leakage, financial loss, or irreversible\ndevice compromise. To systematically evaluate this threat, we introduce\nGhostEI-Bench, the first benchmark for assessing mobile agents under\nenvironmental injection attacks within dynamic, executable environments. Moving\nbeyond static image-based assessments, GhostEI-Bench injects adversarial events\ninto realistic application workflows inside fully operational Android emulators\nand evaluates performance across critical risk scenarios. We further propose a\njudge-LLM protocol that conducts fine-grained failure analysis by reviewing the\nagent's action trajectory alongside the corresponding screenshot sequence,\npinpointing failure in perception, recognition, or reasoning. Comprehensive\nexperiments on state-of-the-art agents reveal pronounced vulnerability to\ndeceptive environmental cues: current models systematically fail to perceive\nand reason about manipulated UIs. GhostEI-Bench provides a framework for\nquantifying and mitigating this emerging threat, paving the way toward more\nrobust and secure embodied agents.", "AI": {"tldr": "This paper introduces GhostEI-Bench, a new benchmark for testing Vision-Language Models (VLMs) against environmental injection attacks in mobile GUIs, finding that current models are highly vulnerable to such attacks.", "motivation": "The deployment of VLMs as autonomous agents in dynamic mobile environments has introduced a new threat vector called environmental injection, where adversarial UI elements trick the agent's visual perception, leading to serious consequences. Existing methods like prompt-based attacks are insufficient to address this unique threat.", "method": "The authors introduce GhostEI-Bench, a benchmark that injects adversarial events into real app workflows using Android emulators and employs a judge-LLM to analyze agent failures in perception, recognition, and reasoning by evaluating action trajectories alongside screenshots.", "result": "Comprehensive experiments using GhostEI-Bench demonstrate that state-of-the-art VLM-based mobile agents are highly susceptible to environmental injection attacks, as they consistently fail to recognize and respond appropriately to manipulated UIs.", "conclusion": "GhostEI-Bench provides a systematic framework for evaluating and improving the robustness of mobile agents to environmental injection attacks, which are a significant security threat to perception-based AI interactivity with real-world graphical interfaces."}}
{"id": "2510.20367", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.20367", "abs": "https://arxiv.org/abs/2510.20367", "authors": ["Daniel Gilkarov", "Ran Dubin"], "title": "NeuPerm: Disrupting Malware Hidden in Neural Network Parameters by Leveraging Permutation Symmetry", "comment": null, "summary": "Pretrained deep learning model sharing holds tremendous value for researchers\nand enterprises alike. It allows them to apply deep learning by fine-tuning\nmodels at a fraction of the cost of training a brand-new model. However, model\nsharing exposes end-users to cyber threats that leverage the models for\nmalicious purposes. Attackers can use model sharing by hiding self-executing\nmalware inside neural network parameters and then distributing them for\nunsuspecting users to unknowingly directly execute them, or indirectly as a\ndependency in another software. In this work, we propose NeuPerm, a simple yet\neffec- tive way of disrupting such malware by leveraging the theoretical\nproperty of neural network permutation symmetry. Our method has little to no\neffect on model performance at all, and we empirically show it successfully\ndisrupts state-of-the-art attacks that were only previously addressed using\nquantization, a highly complex process. NeuPerm is shown to work on LLMs, a\nfeat that no other previous similar works have achieved. The source code is\navailable at https://github.com/danigil/NeuPerm.git.", "AI": {"tldr": "TL;DR of the paper", "motivation": "Expensive training, model sharing risks introducing malware through parameters.", "method": "NeuPerm utilizes neural network permutation symmetry to disrupt hidden malware without affecting performance.", "result": "Successfully disrupts SO-TA attacks without complex processes like quantization and works on LLMs.", "conclusion": "NeuPerm is a practical and effective method for secure model sharing."}}
{"id": "2510.20419", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.20419", "abs": "https://arxiv.org/abs/2510.20419", "authors": ["Eric Wagner", "David Heye", "Jan Bauer", "Klaus Wehrle", "Martin Serror"], "title": "MAC Aggregation over Lossy Channels in DTLS 1.3", "comment": "IEEE ICNP'25", "summary": "Aggregating Message Authentication Codes (MACs) promises to save valuable\nbandwidth in resource-constrained environments. The idea is simple: Instead of\nappending an authentication tag to each message in a communication stream, the\nintegrity protection of multiple messages is aggregated into a single tag.\nRecent studies postulate, e.g., based on simulations, that these benefits also\nspread to wireless, and thus lossy, scenarios despite each lost packet\ntypically resulting in the loss of integrity protection information for\nmultiple messages. In this paper, we investigate these claims in a real\ndeployment. Therefore, we first design a MAC aggregation extension for the\nDatagram Transport Layer Security (DTLS) 1.3 protocol. Afterward, we\nextensively evaluate the performance of MAC aggregation on a complete\ncommunication protocol stack on embedded hardware. We find that MAC aggregation\ncan indeed increase goodput by up to 50% and save up to 17% of energy\nexpenditure for the transmission of short messages, even in lossy channels.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.20494", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.20494", "abs": "https://arxiv.org/abs/2510.20494", "authors": ["Florian Hofer", "Barbara Russo"], "title": "On the cybersecurity of LoRaWAN-based system: a Smart-Lighting case study", "comment": "8 pages, 6 figures plus references, International Conference on IoT", "summary": "Cyber-physical systems and the Internet of Things (IoT) are key technologies\nin the Industry 4.0 vision. They incorporate sensors and actuators to interact\nwith the physical environment. However, when creating and interconnecting\ncomponents to form a heterogeneous smart systems architecture, these face\nchallenges in cybersecurity. This paper presents an experimental investigation\nof architectural configurations for a LoRaWAN-based Smart-Lighting project,\naimed at verifying and improving the system's robustness against attacks. We\nassess the system's robustness in a series of iterative experiments conducted\nboth in-vitro and on-site. The results show that most attacks on a LoRaWAN\nnetwork are unsuccessful, also highlighting unresolved issues with the\ninstalled products. The most successful attacks are high-power jamming attacks\nwithin a few meters of the target, which, in the case of gateways, can be\nmitigated through gateway redundancy.", "AI": {"tldr": "This paper investigates the robustness of a LoRaWAN-based smart lighting system against cyber-attacks, demonstrating that while most attacks are unsuccessful, high-power jamming remains a threat, mitigated by gateway redundancy.", "motivation": "The increasing integration of cyber-physical systems and IoT in Industry 4.0 faces cybersecurity challenges. There is a need to investigate architectural configurations to enhance system robustness against potential cyber threats.", "method": "The study conducts an experimental investigation of architectural configurations for a LoRaWAN-based smart lighting system. Robustness is assessed through iterative experiments performed in a controlled environment (in-vitro) and real-world settings (on-site).", "result": "The experiments reveal that most attacks on LoRaWAN networks are unsuccessful. However, high-power jamming attacks within a few meters of the target pose a significant threat, though gateway redundancy can mitigate attacks on gateways.", "conclusion": "The research highlights the robustness of LoRaWAN-based systems against most cyber-attacks but identifies high-power jamming as a critical threat requiring specific mitigation strategies, such as gateway redundancy."}}
{"id": "2510.20566", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20566", "abs": "https://arxiv.org/abs/2510.20566", "authors": ["Wei Shao", "Yuhao Wang", "Rongguang He", "Muhammad Ejaz Ahmed", "Seyit Camtepe"], "title": "AdaDoS: Adaptive DoS Attack via Deep Adversarial Reinforcement Learning in SDN", "comment": null, "summary": "Existing defence mechanisms have demonstrated significant effectiveness in\nmitigating rule-based Denial-of-Service (DoS) attacks, leveraging predefined\nsignatures and static heuristics to identify and block malicious traffic.\nHowever, the emergence of AI-driven techniques presents new challenges to SDN\nsecurity, potentially compromising the efficacy of existing defence mechanisms.\nIn this paper, we introduce~AdaDoS, an adaptive attack model that disrupt\nnetwork operations while evading detection by existing DoS-based detectors\nthrough adversarial reinforcement learning (RL). Specifically, AdaDoS models\nthe problem as a competitive game between an attacker, whose goal is to\nobstruct network traffic without being detected, and a detector, which aims to\nidentify malicious traffic. AdaDoS can solve this game by dynamically adjusting\nits attack strategy based on feedback from the SDN and the detector.\nAdditionally, recognising that attackers typically have less information than\ndefenders, AdaDoS formulates the DoS-like attack as a partially observed Markov\ndecision process (POMDP), with the attacker having access only to delay\ninformation between attacker and victim nodes. We address this challenge with a\nnovel reciprocal learning module, where the student agent, with limited\nobservations, enhances its performance by learning from the teacher agent, who\nhas full observational capabilities in the SDN environment. AdaDoS represents\nthe first application of RL to develop DoS-like attack sequences, capable of\nadaptively evading both machine learning-based and rule-based DoS-like attack\ndetectors.", "AI": {"tldr": "AdaDoS is an adaptive attack model using adversarial reinforcement learning to evade existing DoS detectors by dynamically adjusting attack strategies in SDN environments.", "motivation": "Current DoS defense mechanisms relying on signatures/heuristics are vulnerable to AI-driven attacks, necessitating new methods to address adaptive evasion techniques.", "method": "Formulates DoS attack as a competitive RL game between attacker and detector, employing POMDP for partial observability and a reciprocal learning module with student/teacher agents to handle information asymmetry.", "result": "AdaDoS effectively evades both ML-based and rule-based detectors, demonstrating first application of RL for adaptive DoS attack sequences in SDN.", "conclusion": "Highlights critical vulnerabilities in existing SDN security frameworks and underscores need for defense mechanisms capable of countering AI-empowered adaptive attacks."}}
{"id": "2510.20645", "categories": ["cs.CR", "cs.CE", "cs.DC", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.20645", "abs": "https://arxiv.org/abs/2510.20645", "authors": ["Nitin Awathare"], "title": "Decentralized Exchange that Mitigate a Bribery Attack", "comment": null, "summary": "Despite the popularity of Hashed Time-Locked Contracts (HTLCs) because of\ntheir use in wide areas of applications such as payment channels, atomic swaps,\netc, their use in exchange is still questionable. This is because of its\nincentive incompatibility and susceptibility to bribery attacks.\n  State-of-the-art solutions such as MAD-HTLC (Oakland'21) and He-HTLC\n(NDSS'23) address this by leveraging miners' profit-driven behaviour to\nmitigate such attacks. The former is the mitigation against passive miners;\nhowever, the latter works against both active and passive miners. However, they\nconsider only two bribing scenarios where either of the parties involved in the\ntransfer collude with the miner.\n  In this paper, we expose vulnerabilities in state-of-the-art solutions by\npresenting a miner-collusion bribery attack with implementation and\ngame-theoretic analysis. Additionally, we propose a stronger attack on MAD-HTLC\nthan He-HTLC, allowing the attacker to earn profits equivalent to attacking\nnaive HTLC.\n  Leveraging our insights, we propose \\prot, a game-theoretically secure HTLC\nprotocol resistant to all bribery scenarios. \\prot\\ employs a two-phase\napproach, preventing unauthorized token confiscation by third parties, such as\nminers. In Phase 1, parties commit to the transfer; in Phase 2, the transfer is\nexecuted without manipulation. We demonstrate \\prot's efficiency in transaction\ncost and latency via implementations on Bitcoin and Ethereum.", "AI": {"tldr": "The paper identifies vulnerabilities in existing HTLC solutions (MAD-HTLC, He-HTLC) through miner-collusion attacks and proposes a new protocol called \\prot that is game-theoretically secure and resistant to all bribery scenarios, with efficient performance on Bitcoin and Ethereum.", "motivation": "The motivation is to address the incentive incompatibility and susceptibility to bribery attacks in current HTLCs used for applications like payment channels and atomic swaps. Existing solutions only consider limited collusion scenarios, leaving gaps in security.", "method": "The method involves presenting a miner-collusion bribery attack and conducting implementation and game-theoretic analysis. The paper proposes a two-phase protocol (\\prot) to commit and execute HTLCs securely, preventing third-party manipulation.", "result": "The results show that the proposed \\prot protocol is efficient in terms of transaction cost and latency when implemented on Bitcoin and Ethereum, effectively resisting all bribery scenarios compared to prior approaches.", "conclusion": "The conclusion is that current HTLC solutions rely on assumptions about miner behavior that can be exploited, and that the new \\prot protocol provides a game-theoretically secure and robust solution for all collusion scenarios."}}
{"id": "2510.20657", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.20657", "abs": "https://arxiv.org/abs/2510.20657", "authors": ["Rubens Kim", "Stephan Carney", "Yvonne Fonken", "Soham Hans", "Sofia Hirschmann", "Stacy Marsella", "Peggy Wu", "Nikolos Gurney"], "title": "Risk Psychology & Cyber-Attack Tactics", "comment": "Submitted and presented at AHFE Hawaii 2025. 2 tables, 2 figures", "summary": "We examine whether measured cognitive processes predict cyber-attack\nbehavior. We analyzed data that included psychometric scale responses and\nlabeled attack behaviors from cybersecurity professionals who conducted\nred-team operations against a simulated enterprise network. We employed\nmultilevel mixed-effects Poisson regression with technique counts nested within\nparticipants to test whether cognitive processes predicted technique-specific\nusage. The scales significantly predicted technique use, but effects varied by\ntechnique rather than operating uniformly. Neither expertise level nor\nexperimental treatment condition significantly predicted technique patterns,\nindicating that cognitive processes may be stronger drivers of technique\nselection than training or experience. These findings demonstrate that\nindividual cognitive differences shape cyber-attack behavior and support the\ndevelopment of psychology-informed defense strategies.", "AI": {"tldr": "This study finds cognitive processes predict cyber-attack behavior in red-team members.", "motivation": "To understand if individual cognitive differences influence cyber-attack techniques and support psychology-informed defenses.", "method": "Multilevel mixed-effects Poisson regression on psychometric data and attack behaviors from red-team operations.", "result": "Psychometric scales predicted technique-specific use, but expertise and treatment had no significant effect.", "conclusion": "Cognitive processes drive technique selection more than experience, supporting tailored defense approaches."}}
{"id": "2510.20768", "categories": ["cs.CR", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.20768", "abs": "https://arxiv.org/abs/2510.20768", "authors": ["Austin Jia", "Avaneesh Ramesh", "Zain Shamsi", "Daniel Zhang", "Alex Liu"], "title": "RAGRank: Using PageRank to Counter Poisoning in CTI LLM Pipelines", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as the dominant\narchitectural pattern to operationalize Large Language Model (LLM) usage in\nCyber Threat Intelligence (CTI) systems. However, this design is susceptible to\npoisoning attacks, and previously proposed defenses can fail for CTI contexts\nas cyber threat information is often completely new for emerging attacks, and\nsophisticated threat actors can mimic legitimate formats, terminology, and\nstylistic conventions. To address this issue, we propose that the robustness of\nmodern RAG defenses can be accelerated by applying source credibility\nalgorithms on corpora, using PageRank as an example. In our experiments, we\ndemonstrate quantitatively that our algorithm applies a lower authority score\nto malicious documents while promoting trusted content, using the standardized\nMS MARCO dataset. We also demonstrate proof-of-concept performance of our\nalgorithm on CTI documents and feeds.", "AI": {"tldr": "This paper addresses the susceptibility of RAG in CTI systems to poisoning attacks by proposing the use of source credibility algorithms, such as PageRank, to enhance robustness. Experiments show effectiveness in reducing malicious document authority while promoting trusted content, tested on MS MARCO and CTI datasets.", "motivation": "The paper is motivated by the vulnerability of RAG in CTI systems to poisoning attacks, especially since cyber threat information is often new and threat actors can mimic legitimate formats and conventions, rendering existing defenses inadequate.", "method": "The authors propose applying source credibility algorithms (e.g., PageRank) to RAG systems to assess and modify the authority scores of documents in a corpus. They test their method on the MS MARCO dataset and demonstrate proof-of-concept performance on CTI documents and feeds.", "result": "The experiments demonstrate that the proposed algorithm effectively assigns lower authority scores to malicious documents while promoting trusted content. This is validated through quantitative analysis on both the MS MARCO dataset and CTI-specific documents.", "conclusion": "The paper concludes that utilizing source credibility algorithms like PageRank can significantly improve the robustness of RAG systems in CTI by mitigating the risk of poisoning attacks, thus offering a practical enhancement for secure and reliable threat intelligence analysis."}}
