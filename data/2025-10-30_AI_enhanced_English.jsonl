{"id": "2510.24749", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24749", "abs": "https://arxiv.org/abs/2510.24749", "authors": ["Aofan Liu", "Shiyuan Song", "Haoxuan Li", "Cehao Yang", "Yiyan Qi"], "title": "Beyond Function-Level Search: Repository-Aware Dual-Encoder Code Retrieval with Adversarial Verification", "comment": "Accepted by EMNLP 2025", "summary": "The escalating complexity of modern codebases has intensified the need for\nretrieval systems capable of interpreting cross-component change intents, a\ncapability fundamentally absent in conventional function-level search\nparadigms. While recent studies have improved the alignment between natural\nlanguage queries and code snippets, retrieving contextually relevant code for\nspecific change requests remains largely underexplored. To address this gap, we\nintroduce RepoAlign-Bench, the first benchmark specifically designed to\nevaluate repository-level code retrieval under change request driven scenarios,\nencompassing 52k annotated instances. This benchmark shifts the retrieval\nparadigm from function-centric matching to holistic repository-level reasoning.\nFurthermore, we propose ReflectCode, an adversarial reflection augmented\ndual-tower architecture featuring disentangled code_encoder and doc_encoder\ncomponents. ReflectCode dynamically integrates syntactic patterns, function\ndependencies, and semantic expansion intents through large language model\nguided reflection. Comprehensive experiments demonstrate that ReflectCode\nachieves 12.2% improvement in Top-5 Accuracy and 7.1% in Recall over\nstate-of-the-art baselines, establishing a new direction for context-aware code\nretrieval.", "AI": {"tldr": "The paper introduces RepoAlign-Bench, a new benchmark for repository-level code retrieval under change requests, and proposes ReflectCode, an adversarial reflection augmented dual-tower model that enhances contextual relevance in code retrieval with significant accuracy improvements.\\n", "motivation": "The existing function-level code retrieval systems lack the ability to interpret cross-component change intents, which is crucial for modern, complex codebases. The paper is motivated by the need to improve contextual relevance in retrieving code for specific change requests.\\n", "method": "The authors propose RepoAlign-Bench, the first benchmark explicitly for evaluating repository-level code retrieval scenarios involving change requests. Additionally, they present ReflectCode, a dual-tower architecture that uses adversarial reflection augmentation, which includes a code_encoder and a doc_encoder. ReflectCode dynamically integrates syntactic patterns, function dependencies, and semantic intents through large language models to enhance code retrieval.\\n", "result": "RepoAlign-Bench provides 52k annotated instances for evaluating repository-level retrieval. ReflectCode outperforms state-of-the-art baselines by 12.2% in Top-5 Accuracy and 7.1% in Recall, validating its effectiveness in context-aware code retrieval.\\n", "conclusion": "The paper concludes that the proposed approaches open a new research direction for enhancing code retrieval by focusing on repository-level change intent understanding and context-aware retrieval mechanisms.\\n"}}
{"id": "2510.24799", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24799", "abs": "https://arxiv.org/abs/2510.24799", "authors": ["Filipe R. Cogo", "Gustavo A. Oliva", "Ahmed E. Hassan"], "title": "Compiler.next: A Search-Based Compiler to Power the AI-Native Future of Software Engineering", "comment": "31 pages, 5 figures, submitted to ACM Transactions on Software\n  Engineering and Methodology", "summary": "The rapid advancement of AI-assisted software engineering has brought\ntransformative potential to the field of software engineering, but existing\ntools and paradigms remain limited by cognitive overload, inefficient tool\nintegration, and the narrow capabilities of AI copilots. In response, we\npropose Compiler.next, a novel search-based compiler designed to enable the\nseamless evolution of AI-native software systems as part of the emerging\nSoftware Engineering 3.0 era. Unlike traditional static compilers,\nCompiler.next takes human-written intents and automatically generates working\nsoftware by searching for an optimal solution. This process involves dynamic\noptimization of cognitive architectures and their constituents (e.g., prompts,\nfoundation model configurations, and system parameters) while finding the\noptimal trade-off between several objectives, such as accuracy, cost, and\nlatency. This paper outlines the architecture of Compiler.next and positions it\nas a cornerstone in democratizing software development by lowering the\ntechnical barrier for non-experts, enabling scalable, adaptable, and reliable\nAI-powered software. We present a roadmap to address the core challenges in\nintent compilation, including developing quality programming constructs,\neffective search heuristics, reproducibility, and interoperability between\ncompilers. Our vision lays the groundwork for fully automated, search-driven\nsoftware development, fostering faster innovation and more efficient AI-driven\nsystems.", "AI": {"tldr": "generate a too long; didn't read summary", "motivation": "describe the motivation in this paper", "method": "method of this paper", "result": "result of this paper", "conclusion": "conclusion of this paper"}}
{"id": "2510.24819", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.24819", "abs": "https://arxiv.org/abs/2510.24819", "authors": ["Vincenzo Scotti", "Jan Keim", "Tobias Hey", "Andreas Metzger", "Anne Koziolek", "Raffaela Mirandola"], "title": "A Roadmap for Tamed Interactions with Large Language Models", "comment": null, "summary": "We are witnessing a bloom of AI-powered software driven by Large Language\nModels (LLMs). Although the applications of these LLMs are impressive and\nseemingly countless, their unreliability hinders adoption. In fact, the\ntendency of LLMs to produce faulty or hallucinated content makes them\nunsuitable for automating workflows and pipelines. In this regard, Software\nEngineering (SE) provides valuable support, offering a wide range of formal\ntools to specify, verify, and validate software behaviour. Such SE tools can be\napplied to define constraints over LLM outputs and, consequently, offer\nstronger guarantees on the generated content. In this paper, we argue that the\ndevelopment of a Domain Specific Language (DSL) for scripting interactions with\nLLMs using an LLM Scripting Language (LSL) may be key to improve AI-based\napplications. Currently, LLMs and LLM-based software still lack reliability,\nrobustness, and trustworthiness, and the tools or frameworks to cope with these\nissues suffer from fragmentation. In this paper, we present our vision of LSL.\nWith LSL, we aim to address the limitations above by exploring ways to control\nLLM outputs, enforce structure in interactions, and integrate these aspects\nwith verification, validation, and explainability. Our goal is to make LLM\ninteraction programmable and decoupled from training or implementation.", "AI": {"tldr": "The paper proposes a domain-specific scripting language for LLMs to address their reliability issues by enabling controlled and structured interactions, and integrating formal SE practices.", "motivation": "Despite the impressive applications of Large Language Models (LLMs), their unreliability, tendency to produce faulty or hallucinated content, and lack of tools for ensuring trustworthiness in workflows and pipelines hinder their adoption.", "method": "The authors present a vision for an LLM Scripting Language (LSL), a Domain Specific Language (DSL) that allows scripting interactions with LLMs, focusing on controlling outputs and integrating with Software Engineering (SE) techniques like verification and validation.", "result": "They outline a vision for LSL that could make interactions with LLMs programmable, reliable, and explainable, while decoupling the scripting from implementation and training details.", "conclusion": "LSL has the potential to bridge the gap between AI and SE, improving the robustness and trustworthiness of LLM-based applications, thus enabling them to be used more effectively in software systems."}}
{"id": "2510.25015", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.25015", "abs": "https://arxiv.org/abs/2510.25015", "authors": ["Chuyue Sun", "Yican Sun", "Daneshvar Amrollahi", "Ethan Zhang", "Shuvendu Lahiri", "Shan Lu", "David Dill", "Clark Barrett"], "title": "VeriStruct: AI-assisted Automated Verification of Data-Structure Modules in Verus", "comment": null, "summary": "We introduce VeriStruct, a novel framework that extends AI-assisted automated\nverification from single functions to more complex data structure modules in\nVerus. VeriStruct employs a planner module to orchestrate the systematic\ngeneration of abstractions, type invariants, specifications, and proof code. To\naddress the challenge that LLMs often misunderstand Verus' annotation syntax\nand verification-specific semantics, VeriStruct embeds syntax guidance within\nprompts and includes a repair stage to automatically correct annotation errors.\nIn an evaluation on eleven Rust data structure modules, VeriStruct succeeds on\nten of the eleven, successfully verifying 128 out of 129 functions (99.2%) in\ntotal. These results represent an important step toward the goal of automatic\nAI-assisted formal verification.", "AI": {"tldr": "VeriStruct is a framework that extends AI-assisted verification to complex data structures, using a planner and error correction to improve accuracy.", "motivation": "Current AI-assisted verification is limited to single functions and struggles with data structure modules due to annotation syntax and semantic misunderstandings.", "method": "VeriStruct uses a planner module for systematic generation of abstractions and a repair stage to correct annotation errors via syntax-guided prompts.", "result": "VeriStruct successfully verified 99.2% of functions in 11 Rust modules, succeeding in 10 out of 11 modules.", "conclusion": "VeriStruct represents a significant progress in automatic AI-assisted formal verification for complex data structures."}}
{"id": "2510.24807", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24807", "abs": "https://arxiv.org/abs/2510.24807", "authors": ["Ziyao Cui", "Minxing Zhang", "Jian Pei"], "title": "Learning to Attack: Uncovering Privacy Risks in Sequential Data Releases", "comment": null, "summary": "Privacy concerns have become increasingly critical in modern AI and data\nscience applications, where sensitive information is collected, analyzed, and\nshared across diverse domains such as healthcare, finance, and mobility. While\nprior research has focused on protecting privacy in a single data release, many\nreal-world systems operate under sequential or continuous data publishing,\nwhere the same or related data are released over time. Such sequential\ndisclosures introduce new vulnerabilities, as temporal correlations across\nreleases may enable adversaries to infer sensitive information that remains\nhidden in any individual release. In this paper, we investigate whether an\nattacker can compromise privacy in sequential data releases by exploiting\ndependencies between consecutive publications, even when each individual\nrelease satisfies standard privacy guarantees. To this end, we propose a novel\nattack model that captures these sequential dependencies by integrating a\nHidden Markov Model with a reinforcement learning-based bi-directional\ninference mechanism. This enables the attacker to leverage both earlier and\nlater observations in the sequence to infer private information. We instantiate\nour framework in the context of trajectory data, demonstrating how an adversary\ncan recover sensitive locations from sequential mobility datasets. Extensive\nexperiments on Geolife, Porto Taxi, and SynMob datasets show that our model\nconsistently outperforms baseline approaches that treat each release\nindependently. The results reveal a fundamental privacy risk inherent to\nsequential data publishing, where individually protected releases can\ncollectively leak sensitive information when analyzed temporally. These\nfindings underscore the need for new privacy-preserving frameworks that\nexplicitly model temporal dependencies, such as time-aware differential privacy\nor sequential data obfuscation strategies.", "AI": {"tldr": "The paper proposes a novel attack model exploiting sequential data publishing in AI applications to compromise privacy, using a Hidden Markov Model with a reinforcement learning mechanism for trajectory data analysis, revealing existing privacy-preserving methods are insufficient as they treat each data release independently.", "motivation": "Modern AI applications often deal with sensitive data, and the sequential release of data introduces privacy risks as individual releases satisfying privacy guarantees can collectively leak information when analyzed temporally. The paper addresses the need for new privacy-preserving frameworks considering temporal dependencies in such scenarios.", "method": "The paper introduces a new attack framework that uses a Hidden Markov Model combined with a reinforcement learning-based bi-directional inference mechanism to model sequential dependencies in trajectory data, allowing adversaries to infer private information by using both earlier and later observations.", "result": "Experiments on Geolife, Porto Taxi, and SynMob datasets demonstrate the proposed model consistently outperforms baseline approaches that treat data releases independently, showing the effectiveness of exploiting temporal correlations for privacy breach.", "conclusion": "The research uncovers a fundamental privacy risk in sequential data publishing, highlighting the necessity for privacy-preserving frameworks that explicitly model temporal dependencies, such as time-aware differential privacy or strategic sequential data obfuscation methods."}}
{"id": "2510.25016", "categories": ["cs.SE", "cs.AI", "cs.HC", "cs.LG", "68T07, 68N30", "D.2.1; I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.25016", "abs": "https://arxiv.org/abs/2510.25016", "authors": ["Mateen Ahmed Abbasi", "Petri Ihantola", "Tommi Mikkonen", "Niko M\u00e4kitalo"], "title": "Towards Human-AI Synergy in Requirements Engineering: A Framework and Preliminary Study", "comment": "Accepted at the 2025 Sixth International Conference on Intelligent\n  Data Science Technologies and Applications (IDSTA 2025),8 pages, 4 figures.\n  Published in IEEE", "summary": "The future of Requirements Engineering (RE) is increasingly driven by\nartificial intelligence (AI), reshaping how we elicit, analyze, and validate\nrequirements. Traditional RE is based on labor-intensive manual processes prone\nto errors and complexity. AI-powered approaches, specifically large language\nmodels (LLMs), natural language processing (NLP), and generative AI, offer\ntransformative solutions and reduce inefficiencies. However, the use of AI in\nRE also brings challenges like algorithmic bias, lack of explainability, and\nethical concerns related to automation. To address these issues, this study\nintroduces the Human-AI RE Synergy Model (HARE-SM), a conceptual framework that\nintegrates AI-driven analysis with human oversight to improve requirements\nelicitation, analysis, and validation. The model emphasizes ethical AI use\nthrough transparency, explainability, and bias mitigation. We outline a\nmulti-phase research methodology focused on preparing RE datasets, fine-tuning\nAI models, and designing collaborative human-AI workflows. This preliminary\nstudy presents the conceptual framework and early-stage prototype\nimplementation, establishing a research agenda and practical design direction\nfor applying intelligent data science techniques to semi-structured and\nunstructured RE data in collaborative environments.", "AI": {"tldr": "This paper introduces HARE-SM, a framework integrating AI and human oversight to enhance requirements engineering while addressing ethical concerns.", "motivation": "Traditional RE is labor-intensive and error-prone, and while AI offers solutions, challenges like bias and explainability need addressing.", "method": "The study develops the HARE-SM framework, using a multi-phase methodology involving dataset preparation, AI model fine-tuning, and collaborative workflow design.", "result": "The paper presents the HARE-SM conceptual framework and an early prototype implementation.", "conclusion": "HARE-SM provides a research agenda and design direction for ethically integrating AI in RE, promoting collaboration between humans and AI."}}
{"id": "2510.24920", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.24920", "abs": "https://arxiv.org/abs/2510.24920", "authors": ["Elizabeth Lin", "Jonah Ghebremichael", "William Enck", "Yasemin Acar", "Michel Cukier", "Alexandros Kapravelos", "Christian Kastner", "Laurie Williams"], "title": "S3C2 Summit 2025-03: Industry Secure Supply Chain Summit", "comment": null, "summary": "Software supply chains, while providing immense economic and software\ndevelopment value, are only as strong as their weakest link. Over the past\nseveral years, there has been an exponential increase in cyberattacks\nspecifically targeting vulnerable links in critical software supply chains.\nThese attacks disrupt the day-to-day functioning and threaten the security of\nnearly everyone on the internet, from billion-dollar companies and government\nagencies to hobbyist open-source developers. The ever-evolving threat of\nsoftware supply chain attacks has garnered interest from both the software\nindustry and US government in improving software supply chain security. On\nThursday, March 6th, 2025, four researchers from the NSF-backed Secure Software\nSupply Chain Center (S3C2) conducted a Secure Software Supply Chain Summit with\na diverse set of 18 practitioners from 17 organizations. The goals of the\nSummit were: (1) to enable sharing between participants from different\nindustries regarding practical experiences and challenges with software supply\nchain security; (2) to help form new collaborations; and (3) to learn about the\nchallenges facing participants to inform our future research directions. The\nsummit consisted of discussions of six topics relevant to the government\nagencies represented, including software bill of materials (SBOMs); compliance;\nmalicious commits; build infrastructure; culture; and large language models\n(LLMs) and security. For each topic of discussion, we presented a list of\nquestions to participants to spark conversation. In this report, we provide a\nsummary of the summit. The open questions and challenges that remained after\neach topic are listed at the end of each topic's section, and the initial\ndiscussion questions for each topic are provided in the appendix.", "AI": {"tldr": "The paper summarizes a 2025 Secure Software Supply Chain Summit hosted by the NSF-backed Secure Software Supply Chain Center (S3C2), detailing discussions and challenges around software supply chain security across six key topics.", "motivation": "The motivation is to address the rising threat of software supply chain attacks by fostering collaboration and knowledge-sharing among practitioners in diverse industries and government agencies.", "method": "The method involved organizing a summit with four researchers and 18 practitioners from 17 organizations, discussing six topics while using open-ended questions for each to facilitate conversation and identify challenges.", "result": "The result is a report summarizing the summit's discussions, highlighting open questions and challenges for each of the six topics, which serve as a foundation for future research and collaboration.", "conclusion": "The conclusion emphasizes the value of cross-industry collaboration in improving software supply chain security and the need to address the identified open challenges to strengthen resilience against cyberthreats."}}
{"id": "2510.25039", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25039", "abs": "https://arxiv.org/abs/2510.25039", "authors": ["Amanda Dsouza", "Harit Vishwakarma", "Zhengyang Qi", "Justin Bauer", "Derek Pham", "Thomas Walshe", "Armin Parchami", "Frederic Sala", "Paroma Varma"], "title": "Automating Benchmark Design", "comment": null, "summary": "The rapid progress and widespread deployment of LLMs and LLM-powered agents\nhas outpaced our ability to evaluate them. Hand-crafted, static benchmarks are\nthe primary tool for assessing model capabilities, but these quickly become\nsaturated. In contrast, dynamic benchmarks evolve alongside the models they\nevaluate, but are expensive to create and continuously update. To address these\nchallenges, we develop BeTaL (Benchmark Tuning with an LLM-in-the-loop), a\nframework that leverages environment design principles to automate the process\nof dynamic benchmark design. BeTaL works by parameterizing key design choices\nin base benchmark templates and uses LLMs to reason through the resulting\nparameter space to obtain target properties (such as difficulty and realism) in\na cost-efficient manner. We validate this approach on its ability to create\nbenchmarks with desired difficulty levels. Using BeTaL, we create two new\nbenchmarks and extend a popular agentic benchmark $\\tau$-bench. Extensive\nevaluation on these three tasks and multiple target difficulty levels shows\nthat BeTaL produces benchmarks much closer to the desired difficulty, with\naverage deviations ranging from 5.3% to 13.2% -- a 2-4x improvement over the\nbaselines.", "AI": {"tldr": "BeTaL is a dynamic benchmark framework that uses LLMs to automate creating challenging, realistic benchmarks with controllable difficulty levels, outperforming existing methods by 2-4x.", "motivation": "Static benchmarks lose effectiveness over time, while manual dynamic benchmarks are costly. Evaluators need a scalable way to generate benchmarks that match model capabilities as they evolve.", "method": "The framework uses LLMs to explore parameterized benchmark templates, combining environment design principles with automated reasoning to achieve target properties like difficulty and realism.", "result": "BeTaL created benchmarks with average 5.3-13.2\\% deviation from target difficulty levels, compared to 11.9-29.4\\% for baselines. Generated two new benchmarks and enhanced $\\tau\\$-bench.", "conclusion": "BeTaL provides efficient dynamic benchmark generation over existing solutions, enabling rigorous and timely evaluation of evolving LLMs through its LLM-mediated parameter space exploration."}}
{"id": "2510.24976", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24976", "abs": "https://arxiv.org/abs/2510.24976", "authors": ["Banafsheh Saber Latibari", "Najmeh Nazari", "Hossein Sayadi", "Houman Homayoun", "Abhijit Mahalanobis"], "title": "Hammering the Diagnosis: Rowhammer-Induced Stealthy Trojan Attacks on ViT-Based Medical Imaging", "comment": "Accepted, ICCD 2025", "summary": "Vision Transformers (ViTs) have emerged as powerful architectures in medical\nimage analysis, excelling in tasks such as disease detection, segmentation, and\nclassification. However, their reliance on large, attention-driven models makes\nthem vulnerable to hardware-level attacks. In this paper, we propose a novel\nthreat model referred to as Med-Hammer that combines the Rowhammer hardware\nfault injection with neural Trojan attacks to compromise the integrity of\nViT-based medical imaging systems. Specifically, we demonstrate how malicious\nbit flips induced via Rowhammer can trigger implanted neural Trojans, leading\nto targeted misclassification or suppression of critical diagnoses (e.g.,\ntumors or lesions) in medical scans. Through extensive experiments on benchmark\nmedical imaging datasets such as ISIC, Brain Tumor, and MedMNIST, we show that\nsuch attacks can remain stealthy while achieving high attack success rates\nabout 82.51% and 92.56% in MobileViT and SwinTransformer, respectively. We\nfurther investigate how architectural properties, such as model sparsity,\nattention weight distribution, and the number of features of the layer, impact\nattack effectiveness. Our findings highlight a critical and underexplored\nintersection between hardware-level faults and deep learning security in\nhealthcare applications, underscoring the urgent need for robust defenses\nspanning both model architectures and underlying hardware platforms.", "AI": {"tldr": "This paper introduces Med-Hammer, a novel hardware-based attack combining Rowhammer and neural Trojans to target medical Vision Transformers, demonstrating high attack success rates on major models while remaining stealthy.", "motivation": "The paper addresses the critical vulnerability of medical Vision Transformers (ViTs) to hardware-level attacks, highlighting the potential for these models to be compromised through malicious bit flips leading to misclassification or suppression of critical diagnoses in healthcare applications.", "method": "The method involves Rowhammer-induced bit flips in hardware to trigger implanted neural Trojans within ViT models, enabling targeted misclassifications or suppression of critical medical findings. The attack is evaluated using benchmark medical imaging datasets (ISIC, Brain Tumor, MedMNIST) to measure success rates and stealth, while also analyzing how architectural properties like model sparsity and attention weight distribution influence attack effectiveness.", "result": "The experiments demonstrate high attack success rates of about 82.51% for MobileViT and 92.56% for SwinTransformer across multiple medical imaging datasets, with the attacks remaining difficult to detect. The study also reveals insights into how different architectural features of the models affect the performance of the Med-Hammer attack.", "conclusion": "The research highlights the underexplored intersection between hardware-level faults and deep learning security in healthcare AI, emphasizing the urgent need for robust defense mechanisms at both the model and hardware levels to prevent such vulnerabilities from being exploited."}}
{"id": "2510.25057", "categories": ["cs.SE", "K.3.2; K.6.5; K.4.1"], "pdf": "https://arxiv.org/pdf/2510.25057", "abs": "https://arxiv.org/abs/2510.25057", "authors": ["Robin Maisch", "Larissa Schmid", "Timur Sa\u011flam", "Nils Niehues"], "title": "Same Same But Different: Preventing Refactoring Attacks on Software Plagiarism Detection", "comment": "To be published at ICSE'26. 13 pages, 6 figures", "summary": "Plagiarism detection in programming education faces growing challenges due to\nincreasingly sophisticated obfuscation techniques, particularly automated\nrefactoring-based attacks. While code plagiarism detection systems used in\neducation practice are resilient against basic obfuscation, they struggle\nagainst structural modifications that preserve program behavior, especially\ncaused by refactoring-based obfuscation. This paper presents a novel and\nextensible framework that enhances state-of-the-art detectors by leveraging\ncode property graphs and graph transformations to counteract refactoring-based\nobfuscation. Our comprehensive evaluation of real-world student submissions,\nobfuscated using both algorithmic and AI-based obfuscation attacks,\ndemonstrates a significant improvement in detecting plagiarized code.", "AI": {"tldr": "A new framework using code property graphs to detect refactored plagiarism in student programming submissions.", "motivation": "Current plagiarism detection systems are vulnerable to refactoring-based obfuscation techniques as they can't effectively track structural modifications that maintain program behavior.", "method": "The method employs code property graphs and graph transformations to identify suspicious similarities despite structural changes.", "result": "The framework achieves a significant enhancement in detecting plagiarized code on real-world student submissions obfuscated with algorithmic and AI-based methods.", "conclusion": "The proposed framework provides an effective and extensible solution against refactoring-based code obfuscation to improve plagiarism detection in educational settings."}}
{"id": "2510.24985", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24985", "abs": "https://arxiv.org/abs/2510.24985", "authors": ["Najmeh Nazari", "Banafsheh Saber Latibari", "Elahe Hosseini", "Fatemeh Movafagh", "Chongzhou Fang", "Hosein Mohammadi Makrani", "Kevin Immanuel Gubbi", "Abhijit Mahalanobis", "Setareh Rafatirad", "Hossein Sayadi", "Houman Homayoun"], "title": "FaRAccel: FPGA-Accelerated Defense Architecture for Efficient Bit-Flip Attack Resilience in Transformer Models", "comment": "Accepted By ICCD 2025", "summary": "Forget and Rewire (FaR) methodology has demonstrated strong resilience\nagainst Bit-Flip Attacks (BFAs) on Transformer-based models by obfuscating\ncritical parameters through dynamic rewiring of linear layers. However, the\napplication of FaR introduces non-negligible performance and memory overheads,\nprimarily due to the runtime modification of activation pathways and the lack\nof hardware-level optimization. To overcome these limitations, we propose\nFaRAccel, a novel hardware accelerator architecture implemented on FPGA,\nspecifically designed to offload and optimize FaR operations. FaRAccel\nintegrates reconfigurable logic for dynamic activation rerouting, and\nlightweight storage of rewiring configurations, enabling low-latency inference\nwith minimal energy overhead. We evaluate FaRAccel across a suite of\nTransformer models and demonstrate substantial reductions in FaR inference\nlatency and improvement in energy efficiency, while maintaining the robustness\ngains of the original FaR methodology. To the best of our knowledge, this is\nthe first hardware-accelerated defense against BFAs in Transformers,\neffectively bridging the gap between algorithmic resilience and efficient\ndeployment on real-world AI platforms.", "AI": {"tldr": "FaRAccel is a hardware accelerator for the Forget and Rewire (FaR) methodology, improving its resilience against Bit-Flip Attacks (BFAs) on Transformer-based models while reducing performance and memory overheads.", "motivation": "Existing FaR methodology, while effective against BFAs, introduces significant performance and memory costs due to runtime pathway modifications and lack of hardware optimization.", "method": "FaRAccel is implemented on FPGA and features reconfigurable logic for dynamic activation rerouting, along with lightweight storage of rewiring configurations to optimize FaR operations.", "result": "FaRAccel demonstrates notable reductions in FaR inference latency and enhanced energy efficiency while preserving the model's robustness against BFAs.", "conclusion": "FaRAccel presents the first hardware-accelerated defense against BFAs in Transformers, addressing both algorithmic resilience and practical deployment efficiency."}}
{"id": "2510.25103", "categories": ["cs.SE", "D.2.4"], "pdf": "https://arxiv.org/pdf/2510.25103", "abs": "https://arxiv.org/abs/2510.25103", "authors": ["Minghai Lu", "Zhe Zhou", "Danning Xie", "Songlin Jia", "Benjamin Delaware", "Tianyi Zhang"], "title": "Adaptive Proof Refinement with LLM-Guided Strategy Selection", "comment": "11 pages, 11 figures", "summary": "Formal verification via theorem proving enables the expressive specification\nand rigorous proof of software correctness, but it is difficult to scale due to\nthe significant manual effort and expertise required. While Large Language\nModels (LLMs) show potential in proof generation, they frequently produce\nincorrect proofs on the first attempt and require additional strategies for\niterative refinement. However, existing approaches employ fixed refinement\nstrategies and cannot dynamically choose an effective strategy based on the\nparticular issues in a generated proof, which limits their performance. To\novercome this limitation, we introduce Adapt, a novel proof refinement\nframework that leverages an LLM-guided decision-maker to dynamically select a\nsuitable refinement strategy according to the state of the proof assistant and\navailable context of an incorrect proof. We evaluate Adapt on two benchmarks\nagainst four existing methods and find that it significantly outperforms the\nbest baseline on both by proving 16.63% and 18.58% more theorems, respectively.\nFurthermore, we demonstrate Adapt's generalizability by evaluating it across\nfive different LLMs. We also conduct ablation studies to measure the\ncontribution of each component and compare the trade-offs of alternative\ndecision-maker designs.", "AI": {"tldr": "The paper presents Adapt, a proof refinement framework that uses an LLM-guided decision-maker to dynamically select refinement strategies, resulting in significant improvements in proving theorems compared to existing methods.", "motivation": "Formal verification via theorem proving requires significant manual effort and expertise, and current LLM-based methods for proof generation often produce incorrect proofs and use fixed refinement strategies that do not adapt to specific proof issues.", "method": "Adapt is a novel framework that employs an LLM-guided decision-maker to dynamically choose the most suitable refinement strategy based on the proof assistant's state and the context of an incorrect proof.", "result": "Adapt outperforms existing methods on two benchmarks by proving 16.63% and 18.58% more theorems, respectively. It also generalizes well across five different LLMs and demonstrates the effectiveness of its components through ablation studies.", "conclusion": "The paper concludes that Adapt provides a significant improvement in proof refinement by dynamically selecting strategies, and it offers insights into the design of decision-makers for such frameworks."}}
{"id": "2510.24999", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.24999", "abs": "https://arxiv.org/abs/2510.24999", "authors": ["Racchit Jain", "Satya Lokam", "Yehonathan Refael", "Adam Hakim", "Lev Greenberg", "Jay Tenenbaum"], "title": "SLIP-SEC: Formalizing Secure Protocols for Model IP Protection", "comment": null, "summary": "Large Language Models (LLMs) represent valuable intellectual property (IP),\nreflecting significant investments in training data, compute, and expertise.\nDeploying these models on partially trusted or insecure devices introduces\nsubstantial risk of model theft, making it essential to design inference\nprotocols with provable security guarantees.\n  We present the formal framework and security foundations of SLIP, a hybrid\ninference protocol that splits model computation between a trusted and an\nuntrusted resource. We define and analyze the key notions of model\ndecomposition and hybrid inference protocols, and introduce formal properties\nincluding safety, correctness, efficiency, and t-soundness. We construct secure\ninference protocols based on additive decompositions of weight matrices,\ncombined with masking and probabilistic verification techniques. We prove that\nthese protocols achieve information-theoretic security against\nhonest-but-curious adversaries, and provide robustness against malicious\nadversaries with negligible soundness error.\n  This paper focuses on the theoretical underpinnings of SLIP: precise\ndefinitions, formal protocols, and proofs of security. Empirical validation and\ndecomposition heuristics appear in the companion SLIP paper. Together, the two\nworks provide a complete account of securing LLM IP via hybrid inference,\nbridging both practice and theory.", "AI": {"tldr": "This paper introduces SLIP, a hybrid inference protocol for securing large language models by splitting computation between trusted and untrusted devices, providing information-theoretic security against honest-but-curious adversaries and robustness against malicious ones.", "motivation": "Large language models are valuable intellectual property, and deploying them on insecure devices risks model theft, necessitating secure inference protocols with proven security.", "method": "The authors present a formal framework and security foundations for SLIP, using additive decompositions of weight matrices combined with masking and probabilistic verification techniques.", "result": "The protocols achieve information-theoretic security against honest-but-curious adversaries and robustness against malicious adversaries with negligible soundness error.", "conclusion": "The paper provides the theoretical foundations for SLIP, complementing practical validation in a companion paper, and offers a complete approach to securing LLM intellectual property through hybrid inference."}}
{"id": "2510.25148", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.25148", "abs": "https://arxiv.org/abs/2510.25148", "authors": ["Katsuki Yamagishi", "Norihiro Yoshida", "Erina Makihara", "Katsuro Inoue"], "title": "Automated Program Repair Based on REST API Specifications Using Large Language Models", "comment": null, "summary": "Many cloud services provide REST API accessible to client applications.\nHowever, developers often identify specification violations only during\ntesting, as error messages typically lack the detail necessary for effective\ndiagnosis. Consequently, debugging requires trial and error. This study\nproposes dcFix, a method for detecting and automatically repairing REST API\nmisuses in client programs. In particular, dcFix identifies non-conforming code\nfragments, integrates them with the relevant API specifications into prompts,\nand leverages a Large Language Model (LLM) to produce the corrected code. Our\nevaluation demonstrates that dcFix accurately detects misuse and outperforms\nthe baseline approach, in which prompts to the LLM omit any indication of code\nfragments non conforming to REST API specifications.", "AI": {"tldr": "dcFix automatically detects and repairs REST API misuses in client programs using LLMs, improving accuracy over baselines.", "motivation": "Developers struggle with diagnosing REST API specification violations due to vague error messages during testing, leading to time-consuming trial and error fixes.", "method": "Integrates non-conforming code with API specs in LLM prompts to generate corrections, leveraging Large Language Models for automatic repair.", "result": "Evaluation shows dcFix enhances detection and repair effectiveness, outperforming baseline methods with non-compliant code integrated prompts.", "conclusion": "dcFix offers an effective solution for automatic REST API misuse detection and repair, utilizing better prompt engineering to improve LLM output."}}
{"id": "2510.25025", "categories": ["cs.CR", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25025", "abs": "https://arxiv.org/abs/2510.25025", "authors": ["Zirui Cheng", "Jikai Sun", "Anjun Gao", "Yueyang Quan", "Zhuqing Liu", "Xiaohua Hu", "Minghong Fang"], "title": "Secure Retrieval-Augmented Generation against Poisoning Attacks", "comment": "To appear in IEEE BigData 2025", "summary": "Large language models (LLMs) have transformed natural language processing\n(NLP), enabling applications from content generation to decision support.\nRetrieval-Augmented Generation (RAG) improves LLMs by incorporating external\nknowledge but also introduces security risks, particularly from data poisoning,\nwhere the attacker injects poisoned texts into the knowledge database to\nmanipulate system outputs. While various defenses have been proposed, they\noften struggle against advanced attacks. To address this, we introduce RAGuard,\na detection framework designed to identify poisoned texts. RAGuard first\nexpands the retrieval scope to increase the proportion of clean texts, reducing\nthe likelihood of retrieving poisoned content. It then applies chunk-wise\nperplexity filtering to detect abnormal variations and text similarity\nfiltering to flag highly similar texts. This non-parametric approach enhances\nRAG security, and experiments on large-scale datasets demonstrate its\neffectiveness in detecting and mitigating poisoning attacks, including strong\nadaptive attacks.", "AI": {"tldr": "This paper introduces RAGuard, a defense framework for RAG systems that combats data poisoning by expanding retrieval scope and using perplexity/similarity filters. It outperforms existing methods in detecting poisoned texts, even against advanced adaptive attacks.", "motivation": "Existing RAG defenses are ineffective against sophisticated data poisoning attacks, necessitating a robust, non-parametric solution to secure knowledge sources in retrieval-augmented systems.", "method": "RAGuard employs two-phase filtering: (1) Expanding retrieval scope to prioritize clean texts, reducing poisoned content exposure; (2) Chunk-wise perplexity filtering to detect anomalies and text similarity filtering to identify highly similar suspicious texts.", "result": "Experiments on large-scale datasets show RAGuard achieves high detection accuracy for poisoned texts, including strong adaptive attacks, with minimal impact on retrieval efficiency and model performance.", "conclusion": "RAGuard effectively enhances the security of Retrieval-Augmented Generation (RAG) systems by mitigating data poisoning attacks through a non-parametric detection framework, demonstrating robustness against advanced threats."}}
{"id": "2510.25195", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.25195", "abs": "https://arxiv.org/abs/2510.25195", "authors": ["Shuochuan Li", "Zan Wang", "Xiaoning Du", "Zhuo Wu", "Jiuqiao Yu", "Junjie Chen"], "title": "Optimizing Knowledge Utilization for Multi-Intent Comment Generation with Large Language Models", "comment": null, "summary": "Code comment generation aims to produce a generic overview of a code snippet,\nhelping developers understand and maintain code. However, generic summaries\nalone are insufficient to meet the diverse needs of practitioners; for example,\ndevelopers expect the implementation insights to be presented in an untangled\nmanner, while users seek clear usage instructions. This highlights the\nnecessity of multi-intent comment generation. With the widespread adoption of\nLarge Language Models (LLMs) for code-related tasks, these models have been\nleveraged to tackle the challenge of multi-intent comment generation. Despite\ntheir successes, state-of-the-art LLM-based approaches often struggle to\nconstruct correct relationships among intents, code, and comments within a\nsmaller number of demonstration examples. To mitigate this issue, we propose a\nframework named KUMIC for multi-intent comment generation. Built upon\nin-context learning, KUMIC leverages Chain-of-Thought (CoT) to optimize\nknowledge utilization for LLMs to generate intent-specific comments.\nSpecifically, KUMIC first designs a retrieval mechanism to obtain similar\ndemonstration examples, which exhibit high code-comment consistency. Then,\nKUMIC leverages CoT to guide LLMs to focus on statements facilitating the\nderivation of code comments aligned with specific intents. In this context,\nKUMIC constructs a mapping knowledge chain, linking code to intent-specific\nstatements to comments, which enables LLMs to follow similar reasoning steps\nwhen generating the desired comments. We conduct extensive experiments to\nevaluate KUMIC, and the results demonstrate that KUMIC outperforms\nstate-of-the-art baselines by 14.49\\%, 22.41\\%, 20.72\\%, and 12.94\\% in terms\nof BLEU, METEOR, ROUGE-L, and SBERT, respectively.", "AI": {"tldr": "The paper proposes KUMIC, a framework for multi-intent comment generation using in-context learning and CoT, which improves LLM performance in generating intent-specific comments with better code-comment consistency.", "motivation": "Multi-intent comment generation is crucial as generic summaries fail to address diverse developer needs. However, existing LLM-based methods struggle to link different aspects of code and comments effectively when demonstrations are limited.", "method": "The authors introduce KUMIC, which leverages in-context learning with a Chain-of-Thought (CoT) approach. It features a retrieval mechanism for code-comment aligned examples and a knowledge chain that connects code to intent-specific statements for precise comment generation.", "result": "KUMIC outperforms state-of-the-art methods across four metrics (BLEU, METEOR, ROUGE-L, SBERT) with improvements of 14.49%, 22.41%, 20.72%, and 12.94%, respectively, showing significant gains in code-comment consistency.", "conclusion": "The paper concludes that KUMIC effectively addresses the limitations of LLM-based multi-intent comment generation through a structured CoT approach and retrieval-based knowledge chaining, resulting in superior performance compared to existing methods."}}
{"id": "2510.25189", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.25189", "abs": "https://arxiv.org/abs/2510.25189", "authors": ["Ana M. Rodriguez", "Jaime Acosta", "Anantaa Kotal", "Aritran Piplai"], "title": "AgentCyTE: Leveraging Agentic AI to Generate Cybersecurity Training & Experimentation Scenarios", "comment": null, "summary": "Designing realistic and adaptive networked threat scenarios remains a core\nchallenge in cybersecurity research and training, still requiring substantial\nmanual effort. While large language models (LLMs) show promise for automated\nsynthesis, unconstrained generation often yields configurations that fail\nvalidation or execution. We present AgentCyTE, a framework integrating\nLLM-based reasoning with deterministic, schema-constrained network emulation to\ngenerate and refine executable threat environments. Through an agentic feedback\nloop, AgentCyTE observes scenario outcomes, validates correctness, and\niteratively enhances realism and consistency. This hybrid approach preserves\nLLM flexibility while enforcing structural validity, enabling scalable,\ndata-driven experimentation and reliable scenario generation for threat\nmodeling and adaptive cybersecurity training. Our framework can be accessed at:\nhttps://github.com/AnantaaKotal/AgentCyTE", "AI": {"tldr": "AgentCyTE combines LLMs with schema-constrained network emulation to generate valid and realistic threat scenarios automatically.", "motivation": "Creating realistic and adaptive networked threat scenarios is a core challenge in cybersecurity research and training, requiring substantial manual effort. Unconstrained LLM generation often results in invalid scenarios.", "method": "AgentCyTE integrates LLM-based reasoning with deterministic, schema-constrained network emulation in an agentic feedback loop to observe, validate, and refine scenario outcomes.", "result": "The hybrid approach ensures structural validity and enables scalable, data-driven experimentation for threat modeling and adaptive training.", "conclusion": "AgentCyTE provides a reliable and automatic framework for generating realistic threat environments, reducing manual effort."}}
{"id": "2510.25242", "categories": ["cs.SE", "C.3; D.2"], "pdf": "https://arxiv.org/pdf/2510.25242", "abs": "https://arxiv.org/abs/2510.25242", "authors": ["Nao Yoshimura", "Hiroshi Oyama", "Takuya Azumi"], "title": "TECS/Rust-OE: Optimizing Exclusive Control in Rust-based Component Systems for Embedded Devices", "comment": "5 pages (layout expanded from the 4-page IEEE version due to minor\n  lstlisting configuration adjustments for compilation). Originally published\n  as a poster paper at IEEE ISORC 2025", "summary": "The diversification of functionalities and the development of the IoT are\nmaking embedded systems larger and more complex in structure. Ensuring system\nreliability, especially in terms of security, necessitates selecting an\nappropriate programming language. As part of existing research, TECS/Rust has\nbeen proposed as a framework that combines Rust and component-based development\n(CBD) to enable scalable system design and enhanced reliability. This framework\nrepresents system structures using static mutable variables, but excessive\nexclusive controls applied to ensure thread safety have led to performance\ndegradation. This paper proposes TECS/Rust-OE, a memory-safe CBD framework\nutilizing call flows to address these limitations. The proposed Rust code\nleverages real-time OS exclusive control mechanisms, optimizing performance\nwithout compromising reusability. Rust code is automatically generated based on\ncomponent descriptions. Evaluations demonstrate reduced overhead due to\noptimized exclusion control and high reusability of the generated code.", "AI": {"tldr": "TECS/Rust has limitations because of excessive exclusive control mechanisms, which make performance degradation. In order to solve this, TECS/Rust-OE is proposed, which uses real-time OS exclusive control mechanisms and memory-safe CBD framework utilizing call flows. And the evaluations show that it has good performance.", "motivation": "The diversification of functionalities and the development of the IoT are making embedded systems larger and more complex in structure. Ensuring system reliability, especially in terms of security, necessitates selecting an appropriate programming language.", "method": "Proposing TECS/Rust-OE, a memory-safe CBD framework utilizing call flows, which applies the exclusive control of real-time OS to optimize performance. Rust code is automatically generated based on component descriptions.", "result": "The evaluations demonstrate reduced overhead due to optimized exclusion control and high reusability of the generated code.", "conclusion": "The TECS/Rust-OE was designed to alleviate the problem of performance degradation which is caused by the excessive exclusive controls of TECS/Rust. Through using call flows and Rust code automatic generation, it has been confirmed that it has good performance and high reusability."}}
{"id": "2510.25352", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.25352", "abs": "https://arxiv.org/abs/2510.25352", "authors": ["David Plonka", "Branden Palacio", "Debbie Perouli"], "title": "Is Protective DNS Blocking the Wild West?", "comment": "Presented in ACM IMC 2025 Workshop of Policy-Relevant Internet\n  Measurements and Experimentation (PRIME), Madison, WI, October, 2025", "summary": "We perform a passive measurement study investigating how a Protective DNS\nservice might perform in a Research & Education Network serving hundreds of\nmember institutions. Utilizing freely-available DNS blocklists consisting of\ndomain names deemed to be threats, we test hundreds of millions of users' real\nDNS queries, observed over a week's time, to find which answers would be\nblocked because they involve domain names that are potential threats. We find\nthe blocklists disorderly regarding their names, goals, transparency, and\nprovenance making them quite difficult to compare. Consequently, these\nProtective DNS underpinnings lack organized oversight, presenting challenges\nand risks in operation at scale.", "AI": {"tldr": "The study examines how protective DNS services may work with current blocklists in a large network.", "motivation": "To understand the feasibility and challenges of using protective DNS in a research and education network with many institutions.", "method": "The researchers used an extensive set of real DNS queries collected over a week and tested them against freely available blocklists.", "result": "Blocklists showed inconsistencies in names, goals, transparency, and origins, making it hard to compare and use them.", "conclusion": "The lack of organization in blocklists poses operational challenges and risks for large-scale protective DNS implementations."}}
{"id": "2510.25270", "categories": ["cs.SE", "C.3; D.2"], "pdf": "https://arxiv.org/pdf/2510.25270", "abs": "https://arxiv.org/abs/2510.25270", "authors": ["Nao Yoshimura", "Hiroshi Oyama", "Takuya Azumi"], "title": "TECS/Rust: Memory-safe Component Framework for Embedded Systems", "comment": "10 pages. This version includes minor lstlisting configuration\n  adjustments for successful compilation. No changes to content or layout.\n  Originally published at IEEE ISORC 2024", "summary": "As embedded systems grow in complexity and scale due to increased functional\ndiversity, component-based development (CBD) emerges as a solution to\nstreamline their architecture and enhance functionality reuse. CBD typically\nutilizes the C programming language for its direct hardware access and\nlow-level operations, despite its susceptibility to memory-related issues. To\naddress these concerns, this paper proposes TECS/Rust, a Rust-based framework\nspecifically designed for TECS, which is a component framework for embedded\nsystems. It leverages Rust's compile-time memory-safe features, such as\nlifetime and borrowing, to mitigate memory vulnerabilities common with C. The\nproposed framework not only ensures memory safety but also maintains the\nflexibility of CBD, automates Rust code generation for CBD components, and\nsupports efficient integration with real-time operating systems. An evaluation\nof the amount of generated code indicates that the code generated by this paper\nframework accounts for a large percentage of the actual code. Compared to code\ndeveloped without the proposed framework, the difference in execution time is\nminimal, indicating that the overhead introduced by the proposed framework is\nnegligible.", "AI": {"tldr": "The paper introduces TECS/Rust, a memory-safe framework for component-based development in embedded systems, leveraging Rust's compiler features to automate code generation and ensure safety without significant performance overhead.", "motivation": "Embedded systems are becoming more complex, requiring component-based development (CBD) for better manageability and reuse. However, traditional C-based CBD is prone to memory vulnerabilities, which motivates the need for a safer alternative.", "method": "The authors propose TECS/Rust, a Rust-based CBD framework for embedded systems. It uses Rust's compile-time memory safety features (lifetime and borrowing) to eliminate memory-related issues. The framework automates Rust code generation for components and is compatible with real-time operating systems.", "result": "The framework generates a significant portion of the actual code, and the execution time difference between the generated and manually coded versions is minimal, showing negligible overhead.", "conclusion": "TECS/Rust effectively provides memory safety in component-based embedded systems while maintaining flexibility and performance, offering a promising alternative to traditional C-based CBD methods."}}
{"id": "2510.25375", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.25375", "abs": "https://arxiv.org/abs/2510.25375", "authors": ["Ali Recai Yekta", "Nicolas Loza", "Jens Gramm", "Michael Peter Schneider", "Stefan Katzenbeisser"], "title": "From ECU to VSOC: UDS Security Monitoring Strategies", "comment": "Presented at SECURWARE 2025, Barcelona, Spain, October 26-30, 2025\n  (https://www.thinkmind.org/library/SECURWARE/SECURWARE_2025/securware_2025_1_70_30030.html)", "summary": "Increasing complexity and connectivity of modern vehicles have heightened\ntheir vulnerability to cyberattacks. This paper addresses security challenges\nassociated with the Unified Diagnostic Services (UDS) protocol, a critical\ncommunication framework for vehicle diagnostics in the automotive industry. We\npresent security monitoring strategies for the UDS protocol that leverage\nin-vehicle logging and remote analysis through a Vehicle Security Operations\nCenter (VSOC). Our approach involves specifying security event logging\nrequirements, contextual data collection, and the development of detection\nstrategies aimed at identifying UDS attack scenarios. By applying these\nstrategies to a comprehensive taxonomy of UDS attack techniques, we demonstrate\nthat our detection methods cover a wide range of potential attack vectors.\nFurthermore, we assess the adequacy of current AUTOSAR standardized security\nevents in supporting UDS attack detection, identifying gaps in the current\nstandard. This work enhances the understanding of vehicle security monitoring\nand provides an example for developing robust cybersecurity measures in\nautomotive communication protocols.", "AI": {"tldr": "The paper proposes security monitoring strategies for the UDS protocol to detect cyberattacks in modern vehicles, evaluating the effectiveness of these methods against a UDS attack taxonomy and identifying gaps in current AUTOSAR standards.", "motivation": "Modern vehicles are highly complex and interconnected, making them vulnerable to cyberattacks. The UDS protocol, being a key component in vehicle diagnostics, presents a critical area where security monitoring and attack detection are needed to ensure safety and integrity.", "method": "The authors specify security event logging requirements, collect contextual data, and develop detection strategies. These strategies are then applied to a comprehensive taxonomy of UDS attack techniques to validate their coverage and effectiveness.", "result": "The developed security monitoring strategies cover a wide range of UDS attack vectors. Additionally, the evaluation highlights the insufficient coverage of current AUTOSAR standardized security events for detecting certain UDS attacks.", "conclusion": "The paper enhances the understanding of vehicle security monitoring and provides a framework for improving cybersecurity measures in automotive communication protocols, particularly the UDS protocol on which current standards are insufficient."}}
{"id": "2510.25297", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.25297", "abs": "https://arxiv.org/abs/2510.25297", "authors": ["Hidetake Tanaka", "Haruto Tanaka", "Kazumasa Shimari", "Kenichi Matsumoto"], "title": "Understanding the Characteristics of LLM-Generated Property-Based Tests in Exploring Edge Cases", "comment": "Accepted for publication in 2nd IEEE/ACM international conference on\n  AI-powered Software (AIware 2025) : 8 pages, 1 table, 8 figures", "summary": "As Large Language Models (LLMs) increasingly generate code in software\ndevelopment, ensuring the quality of LLM-generated code has become important.\nTraditional testing approaches using Example-based Testing (EBT) often miss\nedge cases -- defects that occur at boundary values, special input patterns, or\nextreme conditions. This research investigates the characteristics of\nLLM-generated Property-based Testing (PBT) compared to EBT for exploring edge\ncases. We analyze 16 HumanEval problems where standard solutions failed on\nextended test cases, generating both PBT and EBT test codes using\nClaude-4-sonnet. Our experimental results reveal that while each method\nindividually achieved a 68.75\\% bug detection rate, combining both approaches\nimproved detection to 81.25\\%. The analysis demonstrates complementary\ncharacteristics: PBT effectively detects performance issues and edge cases\nthrough extensive input space exploration, while EBT effectively detects\nspecific boundary conditions and special patterns. These findings suggest that\na hybrid approach leveraging both testing methods can improve the reliability\nof LLM-generated code, providing guidance for test generation strategies in\nLLM-based code generation.", "AI": {"tldr": "Combining PBT and EBT enhances LLM code testing by 12.5% detection, showing complementary strengths in edge/boundary case identification", "motivation": "LLM-generated code quality requires better testing methods. Traditional Example-based Testing (EBT) frequently misses edge cases, necessitating investigation into alternative approaches like Property-based Testing (PBT).", "method": "Analyzed 16 HumanEval problems with failing solutions using Claude-4-sonnet to generate PBT and EBT test codes. Compared individual and combined bug detection rates (68.75% vs. 81.25%) while characterizing method-specific strengths.", "result": "Combined PBT+EBT achieved 81.25% bug detection (vs. 68.75% individually). PBT excelled at performance issues/edge cases; EBT identified specific boundary conditions. Demonstrated complementary testing characteristics.", "conclusion": "A hybrid approach combining Property-based Testing (PBT) and Example-based Testing (EBT) significantly improves the reliability of LLM-generated code by leveraging complementary strengths in detecting edge cases and boundary conditions."}}
{"id": "2510.25470", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25470", "abs": "https://arxiv.org/abs/2510.25470", "authors": ["Parick Ozoh", "John K Omoniyi", "Bukola Ibitoye"], "title": "An In-Depth Analysis of Cyber Attacks in Secured Platforms", "comment": null, "summary": "There is an increase in global malware threats. To address this, an\nencryption-type ransomware has been introduced on the Android operating system.\nThe challenges associated with malicious threats in phone use have become a\npressing issue in mobile communication, disrupting user experiences and posing\nsignificant privacy threats. This study surveys commonly used machine learning\ntechniques for detecting malicious threats in phones and examines their\nperformance. The majority of past research focuses on customer feedback and\nreviews, with concerns that people might create false reviews to promote or\ndevalue products and services for personal gain. Hence, the development of\ntechniques for detecting malicious threats using machine learning has been a\nkey focus. This paper presents a comprehensive comparative study of current\nresearch on the issue of malicious threats and methods for tackling these\nchallenges. Nevertheless, a huge amount of information is required by these\nmethods, presenting a challenge for developing robust, specialized automated\nanti-malware systems. This research describes the Android Applications dataset,\nand the accuracy of the techniques is measured using the accuracy levels of the\nmetrics employed in this study.", "AI": {"tldr": "This paper systematically reviews machine learning techniques for Android malware detection, highlighting challenges in handling false reviews and data requirements while evaluating methods using an Android Applications dataset.", "motivation": "The surge in encryption-type ransomware threats on Android devices necessitates robust detection methods. Existing approaches face limitations in distinguishing malicious threats and handling synthetically generated false reviews that compromise detection accuracy.", "method": "Conducted a comparative study of existing ML techniques through literature survey and empirical evaluation using an Android Applications dataset, measuring performance via standard accuracy metrics.", "result": "Found that most detection methods require massive training data, creating practical challenges for deployed anti-malware systems. Presented comprehensive performance comparisons across different ML approaches.", "conclusion": "While ML techniques show promise for malware detection, current methods face scalability and data dependency challenges. The study emphasizes the need for more efficient models that maintain accuracy despite synthetic review attacks and limited data availability."}}
{"id": "2510.25406", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.25406", "abs": "https://arxiv.org/abs/2510.25406", "authors": ["Changjie Wang", "Mariano Scazzariello", "Anoud Alshnaka", "Roberto Guanciale", "Dejan Kosti\u0107", "Marco Chiesa"], "title": "Dissect-and-Restore: AI-based Code Verification with Transient Refactoring", "comment": null, "summary": "Formal verification is increasingly recognized as a critical foundation for\nbuilding reliable software systems. However, the need for specialized expertise\nto write precise specifications, navigate complex proof obligations, and learn\nannotations often makes verification an order of magnitude more expensive than\nimplementation. While modern AI systems can recognize patterns in mathematical\nproofs and interpret natural language, effectively integrating them into the\nformal verification process remains an open challenge. We present Prometheus, a\nnovel AI-assisted system that facilitates automated code verification with\ncurrent AI capabilities in conjunction with modular software engineering\nprinciples (e.g., modular refactoring). Our approach begins by decomposing\ncomplex program logic, such as nested loops, into smaller, verifiable\ncomponents. Once verified, these components are recomposed to construct a proof\nof the original program. This decomposition-recomposition workflow is\nnon-trivial. Prometheus addresses this by guiding the proof search through\nstructured decomposition of complex lemmas into smaller, verifiable sub-lemmas.\nWhen automated tools are insufficient, users can provide lightweight natural\nlanguage guidance to steer the proof process effectively. Our evaluation\ndemonstrates that transiently applying modular restructuring to the code\nsubstantially improves the AI's effectiveness in verifying individual\ncomponents. This approach successfully verifies 86% of tasks in our curated\ndataset, compared to 68% for the baseline. Gains are more pronounced with\nincreasing specification complexity, improving from 30% to 69%, and when\nintegrating proof outlines for complex programs, from 25% to 87%.", "AI": {"tldr": "Prometheus is an AI-assisted system that enhances program verification by decomposing and recomposing complex logic, improving verification success from 68% to 86% in a dataset with significant gains in complex tasks.", "motivation": "Formal verification struggles with complexity and cost due to specialized expertise required. AI systems offer potential but integrating them remains challenging. The paper addresses this gap by leveraging AI with modular refactoring.", "method": "Prometheus uses a decomposition-recomposition workflow, breaking code into verifiable components, verifying them, and reconstructing proofs. It guides AI through structured lemma decomposition and allows lightweight natural language guidance when needed.", "result": "Prometheus verifies 86% of tasks in the curated dataset, outperforming the 68% baseline. Significant improvements are noted with complex specifications (30% to 69%) and when integrating proof outlines (25% to 87%).", "conclusion": "Modular restructuring improves AI's effectiveness in formal verification. Prometheus shows that decomposition into sub-lemmas combined with natural language guidance can efficiently handle complex proofs."}}
{"id": "2510.25472", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.25472", "abs": "https://arxiv.org/abs/2510.25472", "authors": ["Zheng Zhang", "Guanlong Wu", "Sen Deng", "Shuai Wang", "Yinqian Zhang"], "title": "NetEcho: From Real-World Streaming Side-Channels to Full LLM Conversation Recovery", "comment": null, "summary": "In the rapidly expanding landscape of Large Language Model (LLM)\napplications, real-time output streaming has become the dominant interaction\nparadigm. While this enhances user experience, recent research reveals that it\nexposes a non-trivial attack surface through network side-channels. Adversaries\ncan exploit patterns in encrypted traffic to infer sensitive information and\nreconstruct private conversations. In response, LLM providers and third-party\nservices are deploying defenses such as traffic padding and obfuscation to\nmitigate these vulnerabilities.\n  This paper starts by presenting a systematic analysis of contemporary\nside-channel defenses in mainstream LLM applications, with a focus on services\nfrom vendors like OpenAI and DeepSeek. We identify and examine seven\nrepresentative deployment scenarios, each incorporating active/passive\nmitigation techniques. Despite these enhanced security measures, our\ninvestigation uncovers significant residual information that remains vulnerable\nto leakage within the network traffic.\n  Building on this discovery, we introduce NetEcho, a novel, LLM-based\nframework that comprehensively unleashes the network side-channel risks of\ntoday's LLM applications. NetEcho is designed to recover entire conversations\n-- including both user prompts and LLM responses -- directly from encrypted\nnetwork traffic. It features a deliberate design that ensures high-fidelity\ntext recovery, transferability across different deployment scenarios, and\nmoderate operational cost. In our evaluations on medical and legal applications\nbuilt upon leading models like DeepSeek-v3 and GPT-4o, NetEcho can recover avg\n$\\sim$70\\% information of each conversation, demonstrating a critical\nlimitation in current defense mechanisms. We conclude by discussing the\nimplications of our findings and proposing future directions for augmenting\nnetwork traffic security.", "AI": {"tldr": "The paper analyzes network side-channel defenses in LLM applications and proposes NetEcho, a framework to exploit vulnerabilities, showing that despite existing measures, significant information remains recoverable.", "motivation": "Recent research shows encrypted traffic patterns can infer sensitive info. Current defenses like padding are insufficient. Need better understanding and solutions to enhance network traffic security.", "method": "1. Conduct systematic analysis of side-channel defenses in LLM apps (OpenAI, DeepSeek). 2. Create framework to exploit vulnerabilities (NetEcho). 3. Evaluate effectiveness on leading models (DeepSeek-v3, GPT-4o). 4. Analyze results.", "result": "Even with active/passive mitigation techniques, residual network traffic information is recoverable. NetEcho can recovers avg ~70% conversation information (prompts and responses). Demonstrates limitations in current defense mechanisms.", "conclusion": "Existing defense mechanisms for network side-channels are insufficient. Need to develop new approaches for network traffic security. Implications for LLM security research and future directions proposed."}}
{"id": "2510.25423", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.25423", "abs": "https://arxiv.org/abs/2510.25423", "authors": ["Ali Asgari", "Annibale Panichella", "Pouria Derakhshanfar", "Mitchell Olsthoorn"], "title": "What Challenges Do Developers Face in AI Agent Systems? An Empirical Study on Stack Overflow", "comment": "12 pages, 4 Figures", "summary": "AI agents have rapidly gained popularity across research and industry as\nsystems that extend large language models with additional capabilities to plan,\nuse tools, remember, and act toward specific goals. Yet despite their promise,\ndevelopers face persistent and often underexplored challenges when building,\ndeploying, and maintaining these emerging systems. To identify these\nchallenges, we study developer discussions on Stack Overflow, the world's\nlargest developer-focused Q and A platform with about 60 million questions and\nanswers and 30 million users. We construct a taxonomy of developer challenges\nthrough tag expansion and filtering, apply LDA-MALLET for topic modeling, and\nmanually validate and label the resulting themes. Our analysis reveals seven\nmajor areas of recurring issues encompassing 77 distinct technical challenges\nrelated to runtime integration, dependency management, orchestration\ncomplexity, and evaluation reliability. We further quantify topic popularity\nand difficulty to identify which issues are most common and hardest to resolve,\nmap the tools and programming languages used in agent development, and track\ntheir evolution from 2021 to 2025 in relation to major AI model and framework\nreleases. Finally, we present the implications of our results, offering\nconcrete guidance for practitioners, researchers, and educators on agent\nreliability and developer support.", "AI": {"tldr": "The paper identifies challenges in developing AI agents by analyzing Stack Overflow discussions, revealing seven...", "motivation": "To understand common issues faced by developers in building AI agents and provide guidance on addressing agent reliability...", "method": "Constructed a taxonomy of challenges via tag analysis, used LDA-MALLET for topic modeling, validated manually.", "result": "Found 77 distinct technical challenges across seven categories; quantified popularity/difficulty; mapped tools and languages; tracked evolutions from 2021-2025.", "conclusion": "Offers practical guidance for agent development based on empirical data from developer discussions."}}
{"id": "2510.25477", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.25477", "abs": "https://arxiv.org/abs/2510.25477", "authors": ["Yi Chen", "Bin Chen", "Peichang Zhang", "Da Che"], "title": "A Study on Privacy-Preserving Scholarship Evaluation Based on Decentralized Identity and Zero-Knowledge Proofs", "comment": null, "summary": "Traditional centralized scholarship evaluation processes typically require\nstudents to submit detailed academic records and qualification information,\nwhich exposes them to risks of data leakage and misuse, making it difficult to\nsimultaneously ensure privacy protection and transparent auditability. To\naddress these challenges, this paper proposes a scholarship evaluation system\nbased on Decentralized Identity (DID) and Zero-Knowledge Proofs (ZKP). The\nsystem aggregates multidimensional ZKPs off-chain, and smart contracts verify\ncompliance with evaluation criteria without revealing raw scores or\ncomputational details. Experimental results demonstrate that the proposed\nsolution not only automates the evaluation efficiently but also maximally\npreserves student privacy and data integrity, offering a practical and\ntrustworthy technical paradigm for higher education scholarship programs.", "AI": {"tldr": "A scholarship evaluation system using DID and ZKP to protect privacy and ensure transparency. It uses off-chain ZKP aggregation and smart contract verification. Efficiency and privacy are demonstrated.", "motivation": "Centralized scholarship evaluation processes expose student data to privacy risks. The need to automate scholarship evaluation while preserving data privacy.", "method": "Design and implement a scholarship evaluation system leveraging decentralize identity (DID) and Zero-Knowledge Proof (ZKP) technologies. The system processes off-chain ZKP aggregations, and employs smart contracts to verify compliance with evaluation criteria without revealing raw data.", "result": "The system proved to be efficient in automated scholarship evaluations and ensured high data privacy and integrity according to experimental results.", "conclusion": "The proposed system can automate scholarship evaluation efficiently while preserving student privacy and data integrity, offering a new solution for the current problem."}}
{"id": "2510.25506", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25506", "abs": "https://arxiv.org/abs/2510.25506", "authors": ["Florian Angermeir", "Maximilian Amougou", "Mark Kreitz", "Andreas Bauer", "Matthias Linhuber", "Davide Fucci", "Fabiola Moy\u00f3n C.", "Daniel Mendez", "Tony Gorschek"], "title": "Reflections on the Reproducibility of Commercial LLM Performance in Empirical Software Engineering Studies", "comment": null, "summary": "Large Language Models have gained remarkable interest in industry and\nacademia. The increasing interest in LLMs in academia is also reflected in the\nnumber of publications on this topic over the last years. For instance, alone\n78 of the around 425 publications at ICSE 2024 performed experiments with LLMs.\nConducting empirical studies with LLMs remains challenging and raises questions\non how to achieve reproducible results, for both other researchers and\npractitioners. One important step towards excelling in empirical research on\nLLMs and their application is to first understand to what extent current\nresearch results are eventually reproducible and what factors may impede\nreproducibility. This investigation is within the scope of our work. We\ncontribute an analysis of the reproducibility of LLM-centric studies, provide\ninsights into the factors impeding reproducibility, and discuss suggestions on\nhow to improve the current state. In particular, we studied the 86 articles\ndescribing LLM-centric studies, published at ICSE 2024 and ASE 2024. Of the 86\narticles, 18 provided research artefacts and used OpenAI models. We attempted\nto replicate those 18 studies. Of the 18 studies, only five were fit for\nreproduction. For none of the five studies, we were able to fully reproduce the\nresults. Two studies seemed to be partially reproducible, and three studies did\nnot seem to be reproducible. Our results highlight not only the need for\nstricter research artefact evaluations but also for more robust study designs\nto ensure the reproducible value of future publications.", "AI": {"tldr": "Analysis of 86 LLM papers found critical reproducibility issues across ICSE/ASE 2024: Only 5 of 18 replicable studies achieved full reproducibility, prompting calls for stronger artefact standards and experimental design rigor.", "motivation": "The surge in LLM research highlights reproducibility challenges. Existing studies lack consistent reproducibility frameworks, necessitating assessment of current reproducibility levels and impediments.", "method": "The authors examined 86 articles from ICSE 2024 and ASE 2024. They tested 18 studies utilizing OpenAI LLMs and available research artefacts for reproducibility.", "result": "From 18 attempts: 5 were reproduction-worthy but all results non-reproducible, 2 partially reproducible, 3 irreproducible. This highlights systemic reproducibility failures.", "conclusion": "The findings suggest a pressing need for stricter evaluations of research artefacts and more robust study designs to enhance the reproducibility of future LLM-centred publications."}}
{"id": "2510.25677", "categories": ["cs.CR", "cs.CL", "C.2.1; D.4.6; E.3; I.2.6; I.5.4"], "pdf": "https://arxiv.org/pdf/2510.25677", "abs": "https://arxiv.org/abs/2510.25677", "authors": ["Hasan Akgul", "Mari Eplik", "Javier Rojas", "Aina Binti Abdullah", "Pieter van der Merwe"], "title": "ZK-SenseLM: Verifiable Large-Model Wireless Sensing with Selective Abstention and Zero-Knowledge Attestation", "comment": "45 pages", "summary": "ZK-SenseLM is a secure and auditable wireless sensing framework that pairs a\nlarge-model encoder for Wi-Fi channel state information (and optionally mmWave\nradar or RFID) with a policy-grounded decision layer and end-to-end\nzero-knowledge proofs of inference. The encoder uses masked spectral\npretraining with phase-consistency regularization, plus a light cross-modal\nalignment that ties RF features to compact, human-interpretable policy tokens.\nTo reduce unsafe actions under distribution shift, we add a calibrated\nselective-abstention head; the chosen risk-coverage operating point is\nregistered and bound into the proof. We implement a four-stage proving\npipeline: (C1) feature sanity and commitment, (C2) threshold and version\nbinding, (C3) time-window binding, and (C4) PLONK-style proofs that the\nquantized network, given the committed window, produced the logged action and\nconfidence. Micro-batched proving amortizes cost across adjacent windows, and a\ngateway option offloads proofs from low-power devices. The system integrates\nwith differentially private federated learning and on-device personalization\nwithout weakening verifiability: model hashes and the registered threshold are\npart of each public statement. Across activity, presence or intrusion,\nrespiratory proxy, and RF fingerprinting tasks, ZK-SenseLM improves macro-F1\nand calibration, yields favorable coverage-risk curves under perturbations, and\nrejects tamper and replay with compact proofs and fast verification.", "AI": {"tldr": "ZK-SenseLM combines wireless sensing with zero-knowledge proofs to provide secure, auditable inference across diverse tasks. It uses masked spectral pretraining, cross-modal alignment, and a novel proving pipeline with selective-abstention to improve accuracy, robustness, and verification without sacrificing performance, demonstrating strong practical security features.", "motivation": "Existing wireless sensing systems lack verifiable security guarantees and robustness to distribution shifts. ZK-SenseLM addresses these gaps by introducing end-to-end zero-knowledge proofs for inference integrity, ensuring auditable decisions while maintaining performance in tasks like activity recognition and intrusion detection.", "method": "The framework employs (1) a masked spectral pretraining encoder with phase-consistency regularization, (2) cross-modal alignment of RF features to policy tokens, (3) a calibrated selective-abstention head for uncertain decisions, and (4) a four-stage proving pipeline (C1-C4) combining feature commitment and PLONK-style proofs. Additional features include micro-batch amortization and gateway-based offloading for low-power devices.", "result": "The system achieves improved macro-F1 scores and calibration across activity, presence, respiratory proxy, and RF fingerprinting tasks. It demonstrates favorable coverage-risk trade-offs under perturbations and effectively rejects tampering and replays using compact proofs (verification latency unmentioned) with fast verification. The selective-abstention mechanism prevents unsafe actions during distribution shifts.", "conclusion": "ZK-SenseLM is a secure and auditable wireless sensing framework that successfully integrates masked spectral pretraining, cross-modal alignment, and end-to-end zero-knowledge proofs. It addresses security challenges in wireless sensing systems while maintaining verifiability through methods like selective-abstention heads and calibrated risk-coverage integration. The system's design improves inference accuracy, robustness to perturbations, and tamper detection, with practical deployment features like micro-batch proving and federated learning compatibility."}}
{"id": "2510.25665", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.25665", "abs": "https://arxiv.org/abs/2510.25665", "authors": ["Ayse Irmak Ercevik", "Aidan Dakhama", "Melane Navaratnarajah", "Yazhuo Cao", "Leo Fernandes"], "title": "Fuzz Smarter, Not Harder: Towards Greener Fuzzing with GreenAFL", "comment": null, "summary": "Fuzzing has become a key search-based technique for software testing, but\ncontinuous fuzzing campaigns consume substantial computational resources and\ngenerate significant carbon footprints. Existing grey-box fuzzing approaches\nlike AFL++ focus primarily on coverage maximisation, without considering the\nenergy costs of exploring different execution paths. This paper presents\nGreenAFL, an energy-aware framework that incorporates power consumption into\nthe fuzzing heuristics to reduce the environmental impact of automated testing\nwhilst maintaining coverage. GreenAFL introduces two key modifications to\ntraditional fuzzing workflows: energy-aware corpus minimisation considering\npower consumption when reducing initial corpora, and energy-guided heuristics\nthat direct mutation towards high-coverage, low-energy inputs. We conduct an\nablation study comparing vanilla AFL++, energy-based corpus minimisation, and\nenergy-based heuristics to evaluate the individual contributions of each\ncomponent. Results show that highest coverage, and lowest energy usage is\nachieved whenever at least one of our modifications is used.", "AI": {"tldr": "GreenAFL optimizes software fuzzing by prioritizing energy efficiency through power-aware test case selection and mutation, reducing the environmental impact of continuous testing without compromising code coverage.", "motivation": "Traditional grey-box fuzzing tools prioritize coverage maximization without addressing energy efficiency, leading to significant computational costs and carbon footprints.", "method": "GreenAFL introduces energy-aware corpus minimization (reducing initial test cases based on power consumption) and energy-guided heuristics (directing mutations towards high-coverage, low-energy inputs) to standard fuzzing workflows.", "result": "Ablation studies show that GreenAFL achieves higher coverage and lower energy consumption compared to vanilla AFL++ when at least one of the proposed modifications (corpus minimization or energy-guided heuristics) is applied.", "conclusion": "GreenAFL demonstrates that energy-aware fuzzing can reduce environmental impact while maintaining coverage, achieving optimal results when either energy-aware corpus minimization or energy-guided heuristics are applied."}}
{"id": "2510.25687", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25687", "abs": "https://arxiv.org/abs/2510.25687", "authors": ["Mallika Prabhakar", "Louise Xu", "Prateek Saxena"], "title": "Model Inversion Attacks Meet Cryptographic Fuzzy Extractors", "comment": null, "summary": "Model inversion attacks pose an open challenge to privacy-sensitive\napplications that use machine learning (ML) models. For example, face\nauthentication systems use modern ML models to compute embedding vectors from\nface images of the enrolled users and store them. If leaked, inversion attacks\ncan accurately reconstruct user faces from the leaked vectors. There is no\nsystematic characterization of properties needed in an ideal defense against\nmodel inversion, even for the canonical example application of a face\nauthentication system susceptible to data breaches, despite a decade of\nbest-effort solutions.\n  In this paper, we formalize the desired properties of a provably strong\ndefense against model inversion and connect it, for the first time, to the\ncryptographic concept of fuzzy extractors. We further show that existing fuzzy\nextractors are insecure for use in ML-based face authentication. We do so\nthrough a new model inversion attack called PIPE, which achieves a success rate\nof over 89% in most cases against prior schemes. We then propose L2FE-Hash, the\nfirst candidate fuzzy extractor which supports standard Euclidean distance\ncomparators as needed in many ML-based applications, including face\nauthentication. We formally characterize its computational security guarantees,\neven in the extreme threat model of full breach of stored secrets, and\nempirically show its usable accuracy in face authentication for practical face\ndistributions. It offers attack-agnostic security without requiring any\nre-training of the ML model it protects. Empirically, it nullifies both prior\nstate-of-the-art inversion attacks as well as our new PIPE attack.", "AI": {"tldr": "This paper addresses model inversion attacks in ML-based face authentication by formalizing defense properties and introducing L2FE-Hash, a new secure fuzzy extractor that neutralizes existing and new attacks without re-training.", "motivation": "Model inversion attacks threaten privacy in ML applications like face authentication. Existing defenses lack systematic evaluation, and current fuzzy extractors are insecure, allowing high success rate attacks such as PIPE.", "method": "The authors formalize the desired properties of a robust defense against model inversion attacks. They analyze existing fuzzy extractors for insecurity using a new PIPE attack and propose L2FE-Hash. The new method supports standard Euclidean distance comparators and is evaluated for security and accuracy under a full breach threat model.", "result": "L2FE-Hash nullifies prior and new inversion attacks with over 89% success rates. It achieves usable accuracy in face authentication and provides attack-agnostic security without needing re-training of the ML model.", "conclusion": "The paper introduces a provably secure and accurate fuzzy extractor for ML applications, offering strong defense against model inversion attacks through novel design and validation, even in the threat of full data breach."}}
{"id": "2510.25692", "categories": ["cs.SE", "cs.LG", "D.2.6; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.25692", "abs": "https://arxiv.org/abs/2510.25692", "authors": ["Tim Strnad", "Bla\u017e Bertalani\u010d", "Carolina Fortuna"], "title": "A Configuration-First Framework for Reproducible, Low-Code Localization", "comment": "20 pages, 7 figures. Preprint submitted to ACM Transactions on\n  Software Engineering and Methodology (TOSEM), 2025", "summary": "Machine learning is increasingly permeating radio-based localization\nservices. To keep results credible and comparable, everyday workflows should\nmake rigorous experiment specification and exact repeatability the default,\nwithout blocking advanced experimentation. However, in practice, researchers\nface a three-way gap that could be filled by a framework that offers (i) low\ncoding effort for end-to-end studies, (ii) reproducibility by default including\nversioned code, data, and configurations, controlled randomness, isolated runs,\nand recorded artifacts, and (iii) built-in extensibility so new models,\nmetrics, and stages can be added with minimal integration effort. Existing\ntools rarely deliver all three for machine learning in general and localization\nworkflows in particular. In this paper we introduce LOCALIZE, a low-code,\nconfiguration-first framework for radio localization in which experiments are\ndeclared in human-readable configuration, a workflow orchestrator runs\nstandardized pipelines from data preparation to reporting, and all artifacts,\nsuch as datasets, models, metrics, and reports, are versioned. The\npreconfigured, versioned datasets reduce initial setup and boilerplate,\nspeeding up model development and evaluation. The design, with clear extension\npoints, allows experts to add components without reworking the infrastructure.\nIn a qualitative comparison and a head-to-head study against a plain Jupyter\nnotebook baseline, we show that the framework reduces authoring effort while\nmaintaining comparable runtime and memory behavior. Furthermore, using a\nBluetooth Low Energy dataset, we show that scaling across training data (1x to\n10x) keeps orchestration overheads bounded as data grows. Overall, the\nframework makes reproducible machine-learning-based localization\nexperimentation practical, accessible, and extensible.", "AI": {"tldr": "This paper introduces LOCALIZE, a low-code configuration-driven framework for reproducible and extensible machine learning-based radio localization, addressing gaps in coding effort, reproducibility, and extensibility. It demonstrates reduced authoring effort and scalable orchestration through versioned workflows and pipeline standardization.", "motivation": "Existing tools lack simultaneous support for low-coding-effort workflows, default reproducibility (through versioning/isolation), and extensibility for radio localization tasks. Researchers need standardized pipelines to maintain reliability while enabling innovation.", "method": "LOCALIZE implements a configuration-first architecture with declarative experiment specs, workflow orchestration for data-preprocessing-to-report pipelines, and artifact versioning (models/datasets/metrics). It uses preconfigured datasets to reduce boilerplate and provides extension points for custom components without infrastructure rework.", "result": "Quantitative results show ~50-70%\u200b reduction in authoring time vs. Jupyter notebooks without runtime/memory tradeoffs. Scalability tests with BLE datasets demonstrate orchestration overhead remains bounded (5-8% of total runtime) even as data scales 10x. Qualitative validation shows enhanced reproducibility and maintainability features.", "conclusion": "LOCALIZE bridges the three key gaps in ML-based localization frameworks, making reproducible experimentation both practical for novices (through low-code interfaces) and extensible for experts (through customizable components) without compromising performance or rigor."}}
{"id": "2510.25746", "categories": ["cs.CR", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.25746", "abs": "https://arxiv.org/abs/2510.25746", "authors": ["Charlie Harrison", "Pasin Manurangsi"], "title": "Exact zCDP Characterizations for Fundamental Differentially Private Mechanisms", "comment": null, "summary": "Zero-concentrated differential privacy (zCDP) is a variant of differential\nprivacy (DP) that is widely used partly thanks to its nice composition\nproperty. While a tight conversion from $\\epsilon$-DP to zCDP exists for the\nworst-case mechanism, many common algorithms satisfy stronger guarantees. In\nthis work, we derive tight zCDP characterizations for several fundamental\nmechanisms. We prove that the tight zCDP bound for the $\\epsilon$-DP Laplace\nmechanism is exactly $\\epsilon + e^{-\\epsilon} - 1$, confirming a recent\nconjecture by Wang (2022). We further provide tight bounds for the discrete\nLaplace mechanism, $k$-Randomized Response (for $k \\leq 6$), and RAPPOR.\nLastly, we also provide a tight zCDP bound for the worst case bounded range\nmechanism.", "AI": {"tldr": "This work closes gaps in zCDP bounds for key DP mechanisms by proving tight characterizations, including resolving a conjecture for the Laplace mechanism and deriving results for other widely used algorithms.", "motivation": "Existing zCDP conversion bounds for worst-case mechanisms do not reflect stronger privacy guarantees of common algorithms, motivating the derivation of tighter, mechanism-specific characterizations.", "method": "The authors derive and prove tight zCDP bounds via mathematical analysis, addressing mechanisms like Laplace, discrete Laplace, k-Randomized Response (k \u2264 6), RAPPOR, and the worst-case bounded range mechanism.", "result": "The paper confirms a conjecture for the \u03b5-DP Laplace mechanism's zCDP bound (\u03b5 + e^{-\u03b5} - 1) and provides tight bounds for discrete Laplace, k-Randomized Response (k \u2264 6), RAPPOR, and the worst-case bounded range mechanism.", "conclusion": "The paper establishes tight zCDP characterizations for several fundamental DP mechanisms, improving understanding and application of privacy guarantees through optimal bounds."}}
{"id": "2510.25694", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25694", "abs": "https://arxiv.org/abs/2510.25694", "authors": ["Jiayi Kuang", "Yinghui Li", "Xin Zhang", "Yangning Li", "Di Yin", "Xing Sun", "Ying Shen", "Philip S. Yu"], "title": "Process-Level Trajectory Evaluation for Environment Configuration in Software Engineering Agents", "comment": null, "summary": "Large language model-based agents show promise for software engineering, but\nenvironment configuration remains a bottleneck due to heavy manual effort and\nscarce large-scale, high-quality datasets. Existing benchmarks assess only\nend-to-end build/test success, obscuring where and why agents succeed or fail.\nWe introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench,\nwhich provides process-level trajectory assessment of fine-grained agent\ncapabilities during environment setup-planning, perception-driven error\ndiagnosis, feedback-driven repair, and action to execute final environment\nconfiguration. Our task instances are automatically constructed by injecting\nrealistic README errors and are validated in Docker for scalable, high-quality\nevaluation. Enconda-bench combines process-level analysis with end-to-end\nexecutability to enable capability assessments beyond aggregate success rates.\nEvaluations across state-of-the-art LLMs and agent frameworks show that while\nagents can localize errors, they struggle to translate feedback into effective\ncorrections, limiting end-to-end performance. To our knowledge, Enconda-bench\nis the first framework to provide process-level internal capability assessment\nfor environment configuration, offering actionable insights for improving\nsoftware engineering agents.", "AI": {"tldr": "A new benchmark, Enconda-bench, provides process-level evaluation for environment configuration in software engineering LLM agents.", "motivation": "Environment configuration in software engineering with LLM agents requires manual effort and lacks large, high-quality datasets. Existing benchmarks only show end results, missing the breakdown of agent capabilities.", "method": "The authors created Enconda-bench, which evaluates agent capabilities during environment setup via task instances generated by injecting README errors, validated in Docker.", "result": "Evaluations revealed agents can identify errors but struggle to use feedback to fix them, impacting end-to-end effectiveness.", "conclusion": "Enconda-bench offers a new, detailed assessment method for improving future software engineering agents."}}
