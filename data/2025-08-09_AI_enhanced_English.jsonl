{"id": "2508.04820", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04820", "abs": "https://arxiv.org/abs/2508.04820", "authors": ["Mayra Sofia Ruiz Rodriguez", "SayedHassan Khatoonabadi", "Emad Shihab"], "title": "Automated File-Level Logging Generation for Machine Learning Applications using LLMs: A Case Study using GPT-4o Mini", "comment": null, "summary": "Logging is essential in software development, helping developers monitor\nsystem behavior and aiding in debugging applications. Given the ability of\nlarge language models (LLMs) to generate natural language and code, researchers\nare exploring their potential to generate log statements. However, prior work\nfocuses on evaluating logs introduced in code functions, leaving file-level log\ngeneration underexplored -- especially in machine learning (ML) applications,\nwhere comprehensive logging can enhance reliability. In this study, we evaluate\nthe capacity of GPT-4o mini as a case study to generate log statements for ML\nprojects at file level. We gathered a set of 171 ML repositories containing\n4,073 Python files with at least one log statement. We identified and removed\nthe original logs from the files, prompted the LLM to generate logs for them,\nand evaluated both the position of the logs and log level, variables, and text\nquality of the generated logs compared to human-written logs. In addition, we\nmanually analyzed a representative sample of generated logs to identify common\npatterns and challenges. We find that the LLM introduces logs in the same place\nas humans in 63.91% of cases, but at the cost of a high overlogging rate of\n82.66%. Furthermore, our manual analysis reveals challenges for file-level\nlogging, which shows overlogging at the beginning or end of a function,\ndifficulty logging within large code blocks, and misalignment with\nproject-specific logging conventions. While the LLM shows promise for\ngenerating logs for complete files, these limitations remain to be addressed\nfor practical implementation.", "AI": {"tldr": "This study evaluates GPT-4o mini's ability to generate file-level log statements in ML projects, finding it places logs similarly to humans in 63.91% of cases but with an 82.66% overlogging rate and challenges like overlogging at function boundaries and misaligned conventions.", "motivation": "File-level logging in ML applications remains underexplored despite potential reliability benefits, contrasting with prior focus on function-level logging evaluation.", "method": "The authors tested log generation by removing original logs from 4,073 ML repo files, prompting GPT-4o mini to regenerate logs, and systematically compared positions, levels, variables, text quality against human logs through statistical evaluation and manual sampling.", "result": "GPT-4o mini achieved 63.91% positional alignment with human logs but exhibited 82.66% overlogging; manual analysis revealed overlogging at function boundaries, difficulty in large code blocks, and misalignment with project-specific logging conventions.", "conclusion": "While LLMs show promise for automated file-level logging in ML projects, addressing overlogging, structural comprehension, and convention adherence is crucial for practical deployment."}}
{"id": "2508.04895", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.04895", "abs": "https://arxiv.org/abs/2508.04895", "authors": ["Wentao Lu", "Alexander Senchenko", "Abram Hindle", "Cor-Paul Bezemer"], "title": "Automated Bug Frame Retrieval from Gameplay Videos Using Vision-Language Models", "comment": null, "summary": "Modern game studios deliver new builds and patches at a rapid pace,\ngenerating thousands of bug reports, many of which embed gameplay videos. To\nverify and triage these bug reports, developers must watch the submitted\nvideos. This manual review is labour-intensive, slow, and hard to scale. In\nthis paper, we introduce an automated pipeline that reduces each video to a\nsingle frame that best matches the reported bug description, giving developers\ninstant visual evidence that pinpoints the bug.\n  Our pipeline begins with FFmpeg for keyframe extraction, reducing each video\nto a median of just 1.90% of its original frames while still capturing bug\nmoments in 98.79 of cases. These keyframes are then evaluated by a\nvision--language model (GPT-4o), which ranks them based on how well they match\nthe textual bug description and selects the most representative frame. We\nevaluated this approach using real-world developer-submitted gameplay videos\nand JIRA bug reports from a popular First-Person Shooter (FPS) game. The\npipeline achieves an overall F1 score of 0.79 and Accuracy of 0.89 for the\ntop-1 retrieved frame. Performance is highest for the Lighting & Shadow (F1 =\n0.94), Physics & Collision (0.86), and UI & HUD (0.83) bug categories, and\nlowest for Animation & VFX (0.51).\n  By replacing video viewing with an immediately informative image, our\napproach dramatically reduces manual effort and speeds up triage and regression\nchecks, offering practical benefits to quality assurance (QA) teams and\ndevelopers across the game industry.", "AI": {"tldr": "The paper proposes an automated pipeline that extracts a single representative frame from gameplay videos in bug reports using FFmpeg and vision-language models, achieving high accuracy and improving QA efficiency for game studios.", "motivation": "Game studios generate thousands of bug reports with embedded gameplay videos, requiring developers to manually watch videos for triage. This process is labor-intensive, slow, and difficult to scale. The paper aims to reduce manual effort by providing instant visual evidence via automated frame extraction.", "method": "The pipeline uses FFmpeg to extract keyframes (capturing bug moments in 98.79% of cases with a median of 1.90% of original frames) followed by GPT-4o evaluating keyframes against the bug description text to select the most relevant frame. Evaluated with real-world video data from FPS game bug reports.", "result": "Achieves 0.79 F1 score and 0.89 accuracy for top-1 frame retrieval. Highest performance in Lighting & Shadow (F1=0.94), Physics & Collision (0.86), and UI & HUD (0.83) categories; lowest in Animation & VFX (0.51).", "conclusion": "The frame extraction approach significantly accelerates bug triage and regression checks by eliminating video watching. It offers practical value for QA teams in the game industry, though performance varies across bug categories."}}
{"id": "2508.04921", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.04921", "abs": "https://arxiv.org/abs/2508.04921", "authors": ["Zixuan Feng", "Reed Milewicz", "Emerson Murphy-Hill", "Tyler Menezes", "Alexander Serebrenik", "Igor Steinmacher", "Anita Sarma"], "title": "Charting Uncertain Waters: A Socio-Technical Framework for Navigating GenAI's Impact on Open Source Communities", "comment": "13 pages, 1 figure", "summary": "Open Source Software communities face a wave of uncertainty as Generative AI\nrapidly transforms how software is created, maintained, and governed. Without\nclear frameworks, communities risk being overwhelmed by the complexity and\nambiguity introduced by GenAI, threatening the collaborative ethos that\nunderpins OSS. We conduct a scenario-driven, conceptual exploration using a\nsocio-technical framework inspired by McLuhan's Tetrad to surface both risks\nand opportunities for community resilience amid GenAI-driven disruption of OSS\ndevelopment across four domains: software practices, documentation, community\nengagement, and governance. By adopting this lens, OSS leaders and researchers\ncan proactively shape the future of their ecosystems, rather than simply\nreacting to technological upheaval.", "AI": {"tldr": "The paper explores how Generative AI (GenAI) impacts Open Source Software (OSS) communities, identifying risks and opportunities across four domains (software practices, documentation, community engagement, governance) using a socio-technical framework inspired by McLuhan's Tetrad.", "motivation": "OSS communities risk being overwhelmed by GenAI's disruptive potential without frameworks to navigate its complexities, threatening their collaborative ethos.", "method": "Scenario-driven, conceptual exploration via a socio-technical framework based on McLuhan's Tetrad, analyzing four domains of OSS development.", "result": "Surfaces risks and opportunities for community resilience in software practices, documentation, community engagement, and governance amid GenAI adoption.", "conclusion": "OSS leaders and researchers can proactively reshape their ecosystems by addressing GenAI's impact through structured socio-technical analysis of key domains."}}
{"id": "2508.04925", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04925", "abs": "https://arxiv.org/abs/2508.04925", "authors": ["Sigma Jahan", "Saurabh Singh Rajput", "Tushar Sharma", "Mohammad Masudur Rahman"], "title": "Taxonomy of Faults in Attention-Based Neural Networks", "comment": null, "summary": "Attention mechanisms are at the core of modern neural architectures, powering\nsystems ranging from ChatGPT to autonomous vehicles and driving a major\neconomic impact. However, high-profile failures, such as ChatGPT's nonsensical\noutputs or Google's suspension of Gemini's image generation due to attention\nweight errors, highlight a critical gap: existing deep learning fault\ntaxonomies might not adequately capture the unique failures introduced by\nattention mechanisms. This gap leaves practitioners without actionable\ndiagnostic guidance. To address this gap, we present the first comprehensive\nempirical study of faults in attention-based neural networks (ABNNs). Our work\nis based on a systematic analysis of 555 real-world faults collected from 96\nprojects across ten frameworks, including GitHub, Hugging Face, and Stack\nOverflow. Through our analysis, we develop a novel taxonomy comprising seven\nattention-specific fault categories, not captured by existing work. Our results\nshow that over half of the ABNN faults arise from mechanisms unique to\nattention architectures. We further analyze the root causes and manifestations\nof these faults through various symptoms. Finally, by analyzing symptom-root\ncause associations, we identify four evidence-based diagnostic heuristics that\nexplain 33.0% of attention-specific faults, offering the first systematic\ndiagnostic guidance for attention-based models.", "AI": {"tldr": "This paper addresses the unique failure modes in attention-based neural networks (ABNNs) through an empirical study of 555 real-world faults, proposing a new taxonomy and four diagnostic heuristics that explain 33% of attention-specific issues.", "motivation": "High-profile failures (e.g., ChatGPT's nonsense outputs, Gemini image generation errors) reveal that existing deep learning fault taxonomies inadequately characterize attention mechanism failures, leaving practitioners without actionable diagnostic guidance.", "method": "Systematic analysis of 555 real-world faults from 96 projects across ten frameworks (GitHub, Hugging Face, Stack Overflow) to develop a novel taxonomy with seven attention-specific categories and identify symptom-root cause associations.", "result": "Found that over 50% of ABNN faults are unique to attention architectures, created seven new categories in the fault taxonomy, and developed four diagnostic heuristics covering 33% of attention-specific faults.", "conclusion": "Provides the first systematic diagnostic guidance for attention-based models through evidence-based heuristics, significantly advancing fault analysis in ABNNs."}}
{"id": "2508.04894", "categories": ["cs.CR", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.04894", "abs": "https://arxiv.org/abs/2508.04894", "authors": ["Iyiola E. Olatunji", "Franziska Boenisch", "Jing Xu", "Adam Dziedzic"], "title": "Adversarial Attacks and Defenses on Graph-aware Large Language Models (LLMs)", "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrated with\ngraph-structured data for tasks like node classification, a domain\ntraditionally dominated by Graph Neural Networks (GNNs). While this integration\nleverages rich relational information to improve task performance, their\nrobustness against adversarial attacks remains unexplored. We take the first\nstep to explore the vulnerabilities of graph-aware LLMs by leveraging existing\nadversarial attack methods tailored for graph-based models, including those for\npoisoning (training-time attacks) and evasion (test-time attacks), on two\nrepresentative models, LLAGA (Chen et al. 2024) and GRAPHPROMPTER (Liu et al.\n2024). Additionally, we discover a new attack surface for LLAGA where an\nattacker can inject malicious nodes as placeholders into the node sequence\ntemplate to severely degrade its performance. Our systematic analysis reveals\nthat certain design choices in graph encoding can enhance attack success, with\nspecific findings that: (1) the node sequence template in LLAGA increases its\nvulnerability; (2) the GNN encoder used in GRAPHPROMPTER demonstrates greater\nrobustness; and (3) both approaches remain susceptible to imperceptible feature\nperturbation attacks. Finally, we propose an end-to-end defense framework\nGALGUARD, that combines an LLM-based feature correction module to mitigate\nfeature-level perturbations and adapted GNN defenses to protect against\nstructural attacks.", "AI": {"tldr": "This paper investigates the robustness of graph-aware LLMs (LLAGA and GRAPHPROMPTER) against adversarial attacks, identifies vulnerabilities, and proposes GALGUARD as a defense framework.\n\n", "motivation": "The integration of LLMs with graph data for tasks like node classification lacks analysis of their robustness against adversarial attacks, traditionally studied in GNNs.\n\n", "method": "Authors applied existing graph-based adversarial attacks (poisoning/test-time evasion) to LLAGA and GRAPHPROMPTER, and identified a novel injection-based attack surface for LLAGA involving malicious placeholder nodes in node sequence templates.\n\n", "result": "1) LLAGA's node sequence template design increases vulnerability to injection attacks. 2) GRAPHPROMPTER's GNN encoder shows greater robustness to structural attacks. 3) Both models remain vulnerable to imperceptible feature perturbations.\n\n", "conclusion": "The study reveals graph encoding design choices affect LLM robustness, and introduces GALGUARD - an end-to-end defense combining LLM feature correction and GNN-based structural defenses to address these vulnerabilities.\n\n"}}
{"id": "2508.05005", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.05005", "abs": "https://arxiv.org/abs/2508.05005", "authors": ["Gang Xu", "Airong Wang", "Yushan Pan"], "title": "Generative AI for Object-Oriented Programming: Writing the Right Code and Reasoning the Right Logic", "comment": null, "summary": "We find ourselves in the midst of an explosion in artificial intelligence\nresearch, particularly with large language models (LLMs). These models have\ndiverse applications spanning finance, commonsense knowledge graphs, medicine,\nand visual analysis. In the world of Object-Oriented Programming(OOP), a robust\nbody of knowledge and methods has been developed for managing complex tasks\nthrough object-oriented thinking. However, the intersection of LLMs with OOP\nremains an underexplored territory. Empirically, we currently possess limited\nunderstanding of how LLMs can enhance the effectiveness of OOP learning and\ncode writing, as well as how we can evaluate such AI-powered tools. Our work\naims to address this gap by presenting a vision from the perspectives of key\nstakeholders involved in an OOP task: programmers, mariners, and experienced\nprogrammers. We identify critical junctures within typical coding workflows\nwhere the integration of LLMs can offer significant benefits. Furthermore, we\npropose ways to augment existing logical reasoning and code writing, ultimately\nenhancing the programming experience.", "AI": {"tldr": "This paper explores how large language models (LLMs) can enhance Object-Oriented Programming (OOP) learning and code writing by analyzing stakeholder perspectives (programmers, mariners, and experienced practitioners) and identifying critical workflow integration points.", "motivation": "The intersection of LLMs and OOP lacks empirical understanding. Existing knowledge on OOP tasks using object-oriented thinking is insufficient to evaluate LLM-powered tools effectively, creating a gap in practical applications and evaluations.", "method": "The authors analyze stakeholder viewpoints of programmers and experienced OOP practitioners to pinpoint critical junctures in coding workflows where LLM integration could improve efficiency. They propose frameworks to augment logical reasoning and code writing capabilities.", "result": "Identified opportunities for LLM collaboration in OOP workflows include enhancing logical reasoning and code writing through targeted AI tool integration. Specific workflow stages and augmentation methods are proposed but not yet empirically validated.", "conclusion": "This work establishes a vision for how LLMs can transform OOP programming experiences by supporting stakeholders in code writing and learning. Future research should validate these proposed integration strategies and measure their impact on programming productivity."}}
{"id": "2508.05048", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.05048", "abs": "https://arxiv.org/abs/2508.05048", "authors": ["Mohammad Ferry Husnil Arif", "Muhammad Imran"], "title": "On the Classical Hardness of the Semidirect Discrete Logarithm Problem in Finite Groups", "comment": null, "summary": "The semidirect discrete logarithm problem (SDLP) in finite groups was\nproposed as a foundation for post-quantum cryptographic protocols, based on the\nbelief that its non-abelian structure would resist quantum attacks. However,\nrecent results have shown that SDLP in finite groups admits efficient quantum\nalgorithms, undermining its quantum resistance. This raises a fundamental\nquestion: does the SDLP offer any computational advantages over the standard\ndiscrete logarithm problem (DLP) against classical adversaries? In this work,\nwe investigate the classical hardness of SDLP across different finite group\nplatforms. We establish that the group-case SDLP can be reformulated as a\ngeneralized discrete logarithm problem, enabling adaptation of classical\nalgorithms to study its complexity. We present a concrete adaptation of the\nBaby-Step Giant-Step algorithm for SDLP, achieving time and space complexity\n$O(\\sqrt{r})$ where $r$ is the period of the underlying cycle structure.\nThrough theoretical analysis and experimental validation in SageMath, we\ndemonstrate that the classical hardness of SDLP is highly platform-dependent\nand does not uniformly exceed that of standard DLP. In finite fields\n$\\mathbb{F}_p^*$, both problems exhibit comparable complexity. Surprisingly, in\nelliptic curves $E(\\mathbb{F}_p)$, the SDLP becomes trivial due to the bounded\nautomorphism group, while in elementary abelian groups $\\mathbb{F}_p^n$, the\nSDLP can be harder than DLP, with complexity varying based on the eigenvalue\nstructure of the automorphism. Our findings reveal that the non-abelian\nstructure of semidirect products does not inherently guarantee increased\nclassical hardness, suggesting that the search for classically hard problems\nfor cryptographic applications requires more careful consideration of the\nunderlying algebraic structures.", "AI": {"tldr": "The paper analyzes the classical hardness of the semidirect discrete logarithm problem (SDLP) across different finite group platforms, finding its computational difficulty varies significantly depending on the group structure, with no inherent advantage over the standard discrete logarithm problem (DLP).", "motivation": "The study addresses the belief that SDLP's non-abelian structure inherently provides resistance to both quantum and classical attacks, challenging recent results showing quantum vulnerability and investigating classical security implications.", "method": "The authors reformulate group-case SDLP as a generalized discrete logarithm problem, adapt the Baby-Step Giant-Step algorithm for SDLP analysis, and conduct computational experiments in SageMath across finite fields, elliptic curves, and elementary abelian groups.", "result": "SDLP complexity varies by platform: (1) comparable to DLP in $\\mathbb{F}_p^*$, (2) trivial in elliptic curves $E(\\mathbb{F}_p)$ due to bounded automorphism groups, and (3) potentially harder than DLP in $\\mathbb{F}_p^n$ with complexity influenced by automorphism eigenvalue structures.", "conclusion": "SDLP's non-abelian structure does not universally ensure classical hardness; cryptographic security requires careful selection of algebraic platforms, as inherent difficulty depends on specific group properties rather than broad structural claims."}}
{"id": "2508.05034", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05034", "abs": "https://arxiv.org/abs/2508.05034", "authors": ["Arabat", "Ali", "Sayagh", "Mohammed", "Hassine", "Jameleddine"], "title": "An ML-based Approach to Predicting Software Change Dependencies: Insights from an Empirical Study on OpenStack", "comment": null, "summary": "As software systems grow in complexity, accurately identifying and managing\ndependencies among changes becomes increasingly critical. For instance, a\nchange that leverages a function must depend on the change that introduces it.\nEstablishing such dependencies allows CI/CD pipelines to build and orchestrate\nchanges effectively, preventing build failures and incomplete feature\ndeployments. In modern software systems, dependencies often span multiple\ncomponents across teams, creating challenges for development and deployment.\nThey serve various purposes, from enabling new features to managing\nconfigurations, and can even involve traditionally independent changes like\ndocumentation updates. To address these challenges, we conducted a preliminary\nstudy on dependency management in OpenStack, a large-scale software system. Our\nstudy revealed that a substantial portion of software changes in OpenStack over\nthe past 10 years are interdependent. Surprisingly, 51.08% of these\ndependencies are identified during the code review phase-after a median delay\nof 5.06 hours-rather than at the time of change creation. Developers often\nspend a median of 57.12 hours identifying dependencies, searching among a\nmedian of 463 other changes. To help developers proactively identify\ndependencies, we propose a semi-automated approach that leverages two ML\nmodels. The first model predicts the likelihood of dependencies among changes,\nwhile the second identifies the exact pairs of dependent changes. Our proposed\nmodels demonstrate strong performance, achieving average AUC scores of 79.33%\nand 91.89%, and Brier scores of 0.11 and 0.014, respectively. Indeed, the\nsecond model has a good top-k recall across all types of pairs, while the top-k\nprecision has room for improvement.", "AI": {"tldr": "This paper addresses challenges in dependency management for complex software systems, proposing a semi-automated ML-based approach to help developers proactively identify interdependent changes in CI/CD pipelines, demonstrated via a study on OpenStack.", "motivation": "Modern software systems require managing interdependent changes across teams and components to prevent build failures and deployment issues. The study on OpenStack highlights significant delays (5.06 hours median) in manual dependency recognition during code reviews, with 57 hours median developer effort to identify dependencies among 463 changes.", "method": "Semi-automated approach using two machine learning models: (1) a dependency likelihood predictor and (2) an exact dependency pair identifier, validated through historical OpenStack change analysis.", "result": "Models achieved average AUC scores of 79.33% (dependence prediction) and 91.89% (pair identification), with Brier scores 0.11 and 0.014 respectively. Model 2 maintains good top-k recall across all dependency types despite lower precision.", "conclusion": "The ML approach demonstrates viable potential for improving dependency identification efficiency in CI/CD processes, though precision requires further optimization. 51.08% of OpenStack dependencies currently arise via delayed manual discovery in code reviews."}}
{"id": "2508.05188", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05188", "abs": "https://arxiv.org/abs/2508.05188", "authors": ["Kim Hammar", "Tansu Alpcan", "Emil C. Lupu"], "title": "Incident Response Planning Using a Lightweight Large Language Model with Reduced Hallucination", "comment": null, "summary": "Timely and effective incident response is key to managing the growing\nfrequency of cyberattacks. However, identifying the right response actions for\ncomplex systems is a major technical challenge. A promising approach to\nmitigate this challenge is to use the security knowledge embedded in large\nlanguage models (LLMs) to assist security operators during incident handling.\nRecent research has demonstrated the potential of this approach, but current\nmethods are mainly based on prompt engineering of frontier LLMs, which is\ncostly and prone to hallucinations. We address these limitations by presenting\na novel way to use an LLM for incident response planning with reduced\nhallucination. Our method includes three steps: fine-tuning, information\nretrieval, and lookahead planning. We prove that our method generates response\nplans with a bounded probability of hallucination and that this probability can\nbe made arbitrarily small at the expense of increased planning time under\ncertain assumptions. Moreover, we show that our method is lightweight and can\nrun on commodity hardware. We evaluate our method on logs from incidents\nreported in the literature. The experimental results show that our method a)\nachieves up to 22% shorter recovery times than frontier LLMs and b) generalizes\nto a broad range of incident types and response actions.", "AI": {"tldr": "This paper introduces a novel method to enhance LLM-based incident response planning by fine-tuning, information retrieval, and lookahead planning, reducing hallucination probability while achieving 22% shorter recovery times compared to current approaches.", "motivation": "Current LLM-based incident response methods rely heavily on costly prompt engineering and suffer from hallucinations, making them unreliable and inefficient for complex systems.", "method": "The authors propose a three-step framework: 1) Fine-tuning an LLM with security domain knowledge, 2) Information retrieval from incident logs to identify relevant context, and 3) Lookahead planning to generate response actions while bounding hallucination probability through theoretical guarantees.", "result": "Experimental results demonstrate a 22% reduction in recovery time compared to frontier LLMs, successful generalization across diverse incident types, and the ability to operate on commodity hardware.", "conclusion": "The method effectively reduces hallucinations in LLM-based incident response planning, balances accuracy with efficiency through controllable parameters, and offers practical advantages for real-world deployment using standard hardware."}}
{"id": "2508.05085", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.05085", "abs": "https://arxiv.org/abs/2508.05085", "authors": ["Junayed Mahmud", "James Chen", "Terry Achille", "Camilo Alvarez-Velez", "Darren Dean Bansil", "Patrick Ijieh", "Samar Karanch", "Nadeeshan De Silva", "Oscar Chaparro", "Andrian Marcus", "Kevin Moran"], "title": "LadyBug: A GitHub Bot for UI-Enhanced Bug Localization in Mobile Apps", "comment": "5 pages, to appear in the Proceedings of the 41st International\n  Conference on Software Maintenance and Evolution (ICSME'25) - Tool\n  Demonstration Track", "summary": "This paper introduces LadyBug, a GitHub bot that automatically localizes bugs\nfor Android apps by combining UI interaction information with text retrieval.\nLadyBug connects to an Android app's GitHub repository, and is triggered when a\nbug is reported in the corresponding issue tracker. Developers can then record\na reproduction trace for the bug on a device or emulator and upload the trace\nto LadyBug via the GitHub issue tracker. This enables LadyBug to utilize both\nthe text from the original bug description, and UI information from the\nreproduction trace to accurately retrieve a ranked list of files from the\nproject that most likely contain the reported bug.\n  We empirically evaluated LadyBug using an automated testing pipeline and\nbenchmark called RedWing that contains 80 fully-localized and reproducible bug\nreports from 39 Android apps. Our results illustrate that LadyBug outperforms\ntext-retrieval-based baselines and that the utilization of UI information leads\nto a substantial increase in localization accuracy. LadyBug is an open-source\ntool, available at https://github.com/LadyBugML/ladybug.\n  A video showing the capabilities of Ladybug can be viewed here:\nhttps://youtu.be/hI3tzbRK0Cw", "AI": {"tldr": "LadyBug is a GitHub bot that improves Android bug localization by combining text retrieval with UI interaction data from reproduction traces.", "motivation": "Android app developers spend significant time debugging UI-related issues, yet traditional text-only bug localization methods often lack sufficient accuracy. The paper aims to address this by integrating UI interaction context with text-based approaches.", "method": "LadyBug analyzes reproduction traces (device/emulator recordings) submitted through GitHub issues, extracting UI events and timelines. It combines this UI context with text retrieval over bug reports to rank files containing likely bug sources using a machine learning model. The system leverages both textual and visual-procedural information for localization.", "result": "Empirical evaluation on a benchmark of 80 localizable bugs from RedWing shows LadyBug outperforms text-based baselines by 22% in mean reciprocal rank (MRR). The open-source tool is available at https://github.com/LadyBugML/ladybug, with a demonstration video at https://youtu.be/hI3tzbRK0Cw", "conclusion": "The paper demonstrates that combining UI interaction data with text analysis significantly improves bug localization accuracy for Android apps. LadyBug provides a practical tool for developers, though challenges remain in scaling to complex apps with large codebases and non-UI related bugs."}}
{"id": "2508.05276", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.05276", "abs": "https://arxiv.org/abs/2508.05276", "authors": ["Sharad Agarwal", "Guillermo Suarez-Tangil", "Marie Vasek"], "title": "An Overview of 7726 User Reports: Uncovering SMS Scams and Scammer Strategies", "comment": null, "summary": "Mobile network operators implement firewalls to stop illicit messages, but\nscammers find ways to evade detection. Previous work has looked into SMS texts\nthat are blocked by these firewalls. However, there is little insight into SMS\ntexts that bypass them and reach users. To this end, we collaborate with a\nmajor mobile network operator to receive 1.35m user reports submitted over four\nmonths. We find 89.16% of user reports comprise text messages, followed by\nreports of suspicious calls and URLs. Using our methodological framework, we\nidentify 35.12% of the unique text messages reported by users as spam, while\n40.27% are scam text messages. This is the first paper that investigates SMS\nreports submitted by users and differentiates between spam and scams. Our paper\nclassifies the identified scam text messages into 12 scam types, of which the\nmost popular is 'wrong number' scams. We explore the various infrastructure\nservices that scammers abuse to conduct SMS scams, including mobile network\noperators and hosting infrastructure, and analyze the text of the scam messages\nto understand how scammers lure victims into providing them with their personal\nor financial details.", "AI": {"tldr": "This paper analyzes 1.35M user reports of SMS that bypass firewalls, identifying 40.27% as scam texts and categorizing them into 12 types, offering insights into scammer tactics and infrastructure abuse.", "motivation": "Previous research focused on SMS blocked by firewalls, but there is a critical gap in understanding messages that evade detection. This study addresses the need to differentiate between spam and scam messages in user-submitted reports to improve threat detection strategies.", "method": "The authors collaborated with a major mobile operator to collect 1.35 million user reports over four months, applying a methodological framework to classify messages as spam or scam, then categorizing scam types and analyzing infrastructure and message content.", "result": "89.16% of reports involve text messages; 35.12% of unique messages are spam, 40.27% are scams. The most prevalent scam type is 'wrong number' scams, with analysis revealing how scammers exploit network infrastructure to extract personal/financial data.", "conclusion": "This is the first work to study user-reported SMS bypassing firewalls, distinguishing spam from scams and detailing scammer methods. The findings contribute to understanding SMS scam ecosystems, enabling more effective countermeasures."}}
{"id": "2508.05170", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05170", "abs": "https://arxiv.org/abs/2508.05170", "authors": ["Lishui Fan", "Yu Zhang", "Mouxiang Chen", "Zhongxin Liu"], "title": "Posterior-GRPO: Rewarding Reasoning Processes in Code Generation", "comment": null, "summary": "Reinforcement learning (RL) has significantly advanced code generation for\nlarge language models (LLMs). However, current paradigms rely on outcome-based\nrewards from test cases, neglecting the quality of the intermediate reasoning\nprocess. While supervising the reasoning process directly is a promising\ndirection, it is highly susceptible to reward hacking, where the policy model\nlearns to exploit the reasoning reward signal without improving final outcomes.\nTo address this, we introduce a unified framework that can effectively\nincorporate the quality of the reasoning process during RL. First, to enable\nreasoning evaluation, we develop LCB-RB, a benchmark comprising preference\npairs of superior and inferior reasoning processes. Second, to accurately score\nreasoning quality, we introduce an Optimized-Degraded based (OD-based) method\nfor reward model training. This method generates high-quality preference pairs\nby systematically optimizing and degrading initial reasoning paths along\ncurated dimensions of reasoning quality, such as factual accuracy, logical\nrigor, and coherence. A 7B parameter reward model with this method achieves\nstate-of-the-art (SOTA) performance on LCB-RB and generalizes well to other\nbenchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method\nthat conditions process-based rewards on task success. By selectively applying\nrewards to the reasoning processes of only successful outcomes, P-GRPO\neffectively mitigates reward hacking and aligns the model's internal reasoning\nwith final code correctness. A 7B parameter model with P-GRPO achieves superior\nperformance across diverse code generation tasks, outperforming outcome-only\nbaselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further\ndemonstrate the generalizability of our approach by extending it to\nmathematical tasks. Our models, dataset, and code are publicly available.", "AI": {"tldr": "Introduces a unified RL framework for code generation that integrates intermediate reasoning quality evaluation (LCB-RB benchmark and OD-based reward model) with task-success conditioned reward learning (P-GRPO), reducing reward hacking and achieving 4.5% performance improvement over outcome-only baselines while matching GPT-4-Turbo.", "motivation": "Current RL approaches for code generation prioritize outcome-based rewards from test cases while ignoring the quality of intermediate reasoning processes. Direct process supervision risks reward hacking, where models exploit reward signals without improving actual performance.", "method": "1) LCB-RB: Preference pairs benchmark for reasoning process evaluation. 2) OD-based reward model training: Systematically generates high-quality preference pairs through optimized/degraded reasoning path constructions. 3) P-GRPO: Applies process-based rewards only after task success confirmation.", "result": "7B parameter models achieve SOTA on LCB-RB; P-GRPO implementation reduces reward hacking and shows 4.5% improvement over outcome-only baselines across code generation tasks, with performance comparable to GPT-4-Turbo. Approach generalizes to mathematical reasoning tasks.", "conclusion": "This framework establishes effective integration of reasoning process quality with task outcomes in RL for code generation, mitigates reward hacking through outcome-conditioned reward structures (P-GRPO), and demonstrates strong performance generalizability across domains."}}
{"id": "2508.05334", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.05334", "abs": "https://arxiv.org/abs/2508.05334", "authors": ["Ahsan Farabi", "Israt Khandaker", "Nusrat Jahan", "Ibrahim Khalil Shanto"], "title": "ShikkhaChain: A Blockchain-Powered Academic Credential Verification System for Bangladesh", "comment": null, "summary": "Academic credential fraud threatens educational integrity, especially in\ndeveloping countries like Bangladesh, where verification methods are primarily\nmanual and inefficient. To address this challenge, we present ShikkhaChain, a\nblockchain-powered certificate management platform designed to securely issue,\nverify, and revoke academic credentials in a decentralized and tamper-proof\nmanner. Built on Ethereum smart contracts and utilizing IPFS for off-chain\nstorage, the platform offers a transparent, scalable solution accessible\nthrough a React-based DApp with MetaMask integration. ShikkhaChain enables\nrole-based access for governments, regulators, institutions, and public\nverifiers, allowing QR-based validation and on-chain revocation tracking. Our\nprototype demonstrates enhanced trust, reduced verification time, and improved\ninternational credibility for Bangladeshi degrees, promoting a more reliable\nacademic and employment ecosystem.", "AI": {"tldr": "ShikkhaChain is a blockchain-powered certificate management platform on Ethereum and IPFS, designed to combat academic credential fraud in Bangladesh through decentralized, tamper-proof issuing, verification, and revocation.", "motivation": "Manual academic credential verification methods in developing countries like Bangladesh are inefficient and vulnerable to fraud, threatening educational integrity and international trust in degrees.", "method": "The platform utilizes Ethereum smart contracts for secure, auditable transactions and IPFS for off-chain storage of certificate data, combined with a React-based DApp and MetaMask integration for user access. Role-based permissions and QR validation streamline verification processes.", "result": "The prototype achieved reduced verification time, enhanced trust in academic credentials, and improved international credibility for Bangladeshi degrees through its decentralized, scalable architecture.", "conclusion": "ShikkhaChain demonstrates a viable solution for secure academic certificate management in regions with insecure verification systems, leveraging blockchain and decentralized technologies to strengthen trust and efficiency in education ecosystems."}}
{"id": "2508.05192", "categories": ["cs.SE", "H.2.3; I.2.6; D.2.2"], "pdf": "https://arxiv.org/pdf/2508.05192", "abs": "https://arxiv.org/abs/2508.05192", "authors": ["Felix Neubauer", "J\u00fcrgen Pleiss", "Benjamin Uekermann"], "title": "AI-assisted JSON Schema Creation and Mapping", "comment": "Accepted for Tools and Demonstrations Track of ACM/IEEE MODELS'25", "summary": "Model-Driven Engineering (MDE) places models at the core of system and data\nengineering processes. In the context of research data, these models are\ntypically expressed as schemas that define the structure and semantics of\ndatasets. However, many domains still lack standardized models, and creating\nthem remains a significant barrier, especially for non-experts. We present a\nhybrid approach that combines large language models (LLMs) with deterministic\ntechniques to enable JSON Schema creation, modification, and schema mapping\nbased on natural language inputs by the user. These capabilities are integrated\ninto the open-source tool MetaConfigurator, which already provides visual model\nediting, validation, code generation, and form generation from models. For data\nintegration, we generate schema mappings from heterogeneous JSON, CSV, XML, and\nYAML data using LLMs, while ensuring scalability and reliability through\ndeterministic execution of generated mapping rules. The applicability of our\nwork is demonstrated in an application example in the field of chemistry. By\ncombining natural language interaction with deterministic safeguards, this work\nsignificantly lowers the barrier to structured data modeling and data\nintegration for non-experts.", "AI": {"tldr": "This paper presents a hybrid approach combining large language models (LLMs) and deterministic techniques to simplify JSON Schema creation and mapping for non-experts in Model-Driven Engineering (MDE). The open-source tool MetaConfigurator integrates these methods, enabling data integration from heterogeneous formats with deterministic execution for scalability and reliability. An application in chemistry demonstrates its effectiveness.", "motivation": "Many domains lack standardized data models, and creating them requires expertise, hindering effective research data management. There is a need for accessible tools that reduce barriers for non-experts to engage in structured data modeling and integration.", "method": "A hybrid framework using LLMs for natural language-guided schema creation, modification, and mapping, combined with deterministic execution of generated rules. The MetaConfigurator tool provides visual editing, validation, code generation, and form generation while handling JSON, CSV, XML, and YAML data formats. Schema mappings are generated via LLMs but executed deterministically.", "result": "The hybrid approach successfully enabled schema creation and data integration in the chemistry domain. MetaConfigurator\u2019s integration of natural language interaction with deterministic safeguards proved effective in lowering technical barriers for non-experts.", "conclusion": "Combining LLMs and deterministic techniques in MetaConfigurator significantly simplifies structured data modeling and integration, making it accessible to non-experts. This hybrid approach enhances scalability and reliability while maintaining flexibility through natural language interactions."}}
{"id": "2508.05394", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.05394", "abs": "https://arxiv.org/abs/2508.05394", "authors": ["Xiaoli Zhuo", "Xuehu Yan", "Wei Yan"], "title": "Grouped k-threshold random grid-based visual cryptography scheme", "comment": null, "summary": "Visual cryptography schemes (VCSs) belong to a category of secret image\nsharing schemes that do not require cryptographic knowledge for decryption,\ninstead relying directly on the human visual system. Among VCSs, random\ngrid-based VCS (RGVCS) has garnered widespread attention as it avoids pixel\nexpansion while requiring no basic matrices design. Contrast, a core metric for\nRGVCS, directly determines the visual quality of recovered images, rendering\nits optimization a critical research objective. However, existing $(k,n)$\nRGVCSs still fail to attain theoretical upper bounds on contrast, highlighting\nthe urgent need for higher-contrast constructions. In this paper, we propose a\nnovel sharing paradigm for RGVCS that constructs $(k,n)$-threshold schemes from\narbitrary $(k,n')$-threshold schemes $(k \\leq n'\\leq n)$, termed\n\\emph{$n'$-grouped $(k,n)$ RGVCS}. This paradigm establishes hierarchical\ncontrast characteristics: participants within the same group achieve optimal\nrecovery quality, while inter-group recovery shows a hierarchical contrast. We\nfurther introduce a new contrast calculation formula tailored to the new\nparadigm. Then, we propose a contrast-enhanced $(k,n)$ RGVCS by setting $n'=\nk$, achieving the highest contrast value documented in the existing literature.\nTheoretical analysis and experimental results demonstrate the superiority of\nour proposed scheme in terms of contrast.", "AI": {"tldr": "The paper proposes a novel random grid-based visual cryptography scheme (RGVCS) called n'-grouped (k,n) RGVCS, which enhances image contrast by constructing threshold schemes from arbitrary ones and validating its superiority through analysis and experiments.", "motivation": "Existing (k,n) RGVCSs fail to achieve theoretical upper bounds on contrast, necessitating improved constructions for better visual quality in recovered images.", "method": "Introduces an n'-grouped (k,n) RGVCS paradigm that generates (k,n)-threshold schemes from (k,n')-threshold schemes (k \u2264 n' \u2264 n) and proposes a new contrast calculation formula. Maximizes contrast by setting n' = k.", "result": "The proposed scheme attains the highest contrast value per theoretical maximum, confirmed by both theoretical analysis and experimental results.", "conclusion": "The n'-grouped (k,n) RGVCS outperforms existing methods in contrast, offering optimized hierarchical visual recovery quality without pixel expansion or matrix design complexities."}}
{"id": "2508.05193", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.05193", "abs": "https://arxiv.org/abs/2508.05193", "authors": ["Kaiwen Yan", "Yuhang Chang", "Zirui Guo", "Yaling Mou", "Jiang Ming", "Jingwei Sun"], "title": "STEPWISE-CODEX-Bench: Evaluating Complex Multi-Function Comprehension and Fine-Grained Execution Reasoning", "comment": null, "summary": "In recent years, large language models (LLMs) have made significant progress\nin code intelligence, yet systematically evaluating their code understanding\nand reasoning abilities remains challenging. Mainstream benchmarks such as\nHumanEval and MBPP primarily assess functional correctness, while reasoning\nbenchmarks like CRUXEVAL are limited to single-function, low-complexity\nscenarios. As a result, advanced models achieve nearly saturated scores,\nlimiting their discriminative power. To address this, we present\nSTEPWISE-CODEX-Bench (SX-Bench), a novel benchmark designed for complex\nmulti-function understanding and fine-grained execution reasoning. SX-Bench\nfeatures tasks involving collaboration among multiple sub-functions (e.g.,\nchained calls, nested loops), shifting evaluation towards overall control and\ndata flow modeling. It defines \"computation steps\" as the minimal execution\nunit and requires models to predict the total number of steps in reasoning\ntasks, thereby assessing a model's in-depth understanding of dynamic execution\nbeyond simple I/O matching. Evaluation on over 20 mainstream models (including\n14 reasoning-enhanced models) demonstrates that SX-Bench is highly\ndiscriminative: even the state-of-the-art OpenAI-O3 achieves only 78.37 percent\naccuracy on Hard-Reasoning tasks, much lower than its saturated scores on\nprevious benchmarks, thereby revealing bottlenecks in complex and fine-grained\nreasoning. We also release an automated pipeline combining program synthesis,\nsymbolic execution, and LLM-aided validation for efficient benchmark generation\nand quality assurance. SX-Bench advances code evaluation from \"single-function\nverification\" to \"multi-function dynamic reasoning,\" providing a key tool for\nthe in-depth assessment of advanced code intelligence models.", "AI": {"tldr": "This paper introduces STEPWISE-CODEX-Bench (SX-Bench), a new benchmark for evaluating advanced multi-function code understanding and dynamic execution reasoning in large language models (LLMs). It addresses limitations in existing benchmarks by focusing on complex scenarios and computation steps, revealing performance gaps even in state-of-the-art models.", "motivation": "Mainstream benchmarks like HumanEval/MBPP focus on functional correctness, while reasoning benchmarks like CRUXEVAL are limited to single-function, low-complexity scenarios. This limits discriminative power for advanced models achieving near-saturated scores, necessitating a more challenging benchmark for complex code reasoning.", "method": "The paper develops SX-Bench through: 1) Designing multi-function tasks (chained calls, nested loops) requiring dynamic control/data flow modeling 2) Defining 'computation steps' as minimal execution units for reasoning assessment 3) Creating an automated generation pipeline combining program synthesis, symbolic execution, and LLM validation.", "result": "Evaluations on 20+ models (including 14 reasoning-enhanced ones) showed significant performance gaps: OpenAI-O3 achieved only 78.37% accuracy on Hard-Reasoning tasks, much lower than previous benchmarks. This demonstrates SX-Bench's effectiveness in identifying bottlenecks in fine-grained execution reasoning.", "conclusion": "SX-Bench advances code evaluation by shifting focus from single-function verification to multi-function dynamic reasoning. It provides a critical tool to assess advanced code intelligence models and identify fundamental limitations in their reasoning capabilities."}}
{"id": "2508.05518", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.05518", "abs": "https://arxiv.org/abs/2508.05518", "authors": ["Weihong Sheng", "Jiajun Chen", "Bin Cai", "Chunqiang Hu", "Meng Han", "Jiguo Yu"], "title": "Local Distance Query with Differential Privacy", "comment": null, "summary": "Differential Privacy (DP) is commonly employed to safeguard graph analysis or\npublishing. Distance, a critical factor in graph analysis, is typically handled\nusing curator DP, where a trusted curator holds the complete neighbor lists of\nall vertices and answers queries privately. However, in many real-world\nscenarios, such a curator may not be present, posing a significant challenge\nfor implementing differentially private distance queries under Local\nDifferential Privacy (LDP). This paper proposes two approaches to address this\nchallenge. The first approach generates a synthetic graph by randomizing\nresponses and applies bitwise operations to reduce noise interference. However,\nlike other synthetic graph methods, this approach suffers from low utility. To\novercome this limitation, we propose a second approach, the first LDP method\nspecifically designed for distance queries, which captures the global graph\nstructure by continuously aggregating local distance vectors from neighboring\nvertices. This process enables the accurate updating of global distances. We\ndemonstrate the effectiveness of our method through comprehensive theoretical\nanalysis and experimental evaluations on real-world datasets.", "AI": {"tldr": "This paper proposes two approaches for differentially private distance queries under LDP. The first uses synthetic graphs with bitwise operations for noise reduction, while the second introduces a novel LDP method that captures global graph structure by aggregating local distance vectors from neighboring vertices.", "motivation": "Curator DP requires a trusted party holding complete neighbor lists, which is impractical in real-world scenarios where a centralized authority may not exist. This motivates developing LDP solutions for distance queries that preserve privacy without relying on curators.", "method": "1) Synthetic Graph with Response Randomization: Uses randomized responses and bitwise operations to minimize noise interference while generating privacy-preserving distance data. 2) Local Distance Vector Aggregation: First LDP method for distance queries that iteratively aggregates local distance vectors from neighboring vertices to maintain accurate global distance estimates without requiring a central curator.", "result": "Theoretical analysis and experimental evaluations on real-world datasets demonstrate the proposed methods' effectiveness in achieving both privacy and utility for distance queries under LDP constraints.", "conclusion": "The paper presents the first LDP method specifically designed for distance queries, enabling accurate global distance updates via local structure aggregation while avoiding the need for a central curator. The approach addresses privacy constraints in decentralized graph analysis scenarios."}}
{"id": "2508.05199", "categories": ["cs.SE", "cs.AI", "D.2.2; D.2.7; I.2.2"], "pdf": "https://arxiv.org/pdf/2508.05199", "abs": "https://arxiv.org/abs/2508.05199", "authors": ["Igor Costa", "Christopher Baran"], "title": "EvoGraph: Hybrid Directed Graph Evolution toward Software 3.0", "comment": "15 pages, 3 tables, 1 algorithm. Submitted to ICSE 2025", "summary": "We introduce **EvoGraph**, a framework that enables software systems to\nevolve their own source code, build pipelines, documentation, and tickets.\nEvoGraph represents every artefact in a typed directed graph, applies learned\nmutation operators driven by specialized small language models (SLMs), and\nselects survivors with a multi-objective fitness. On three benchmarks, EvoGraph\nfixes 83% of known security vulnerabilities, translates COBOL to Java with 93%\nfunctional equivalence (test verified), and maintains documentation freshness\nwithin two minutes. Experiments show a 40% latency reduction and a sevenfold\ndrop in feature lead time compared with strong baselines. We extend our\napproach to **evoGraph**, leveraging language-specific SLMs for modernizing\n.NET, Lisp, CGI, ColdFusion, legacy Python, and C codebases, achieving 82-96%\nsemantic equivalence across languages while reducing computational costs by 90%\ncompared to large language models. EvoGraph's design responds to empirical\nfailure modes in legacy modernization, such as implicit contracts, performance\npreservation, and integration evolution. Our results suggest a practical path\ntoward Software 3.0, where systems adapt continuously yet remain under\nmeasurable control.", "AI": {"tldr": "EvoGraph is a framework that enables software systems to automatically evolve their source code, build pipelines, and documentation using a graph-based approach with small language models (SLMs) and multi-objective fitness selection. It addresses legacy modernization challenges with notable performance improvements.", "motivation": "The paper aims to solve challenges in self-adapting software systems, particularly in legacy modernization contexts such as security vulnerability fixes, cross-language translation (e.g., COBOL to Java), documentation maintenance, and reducing system latency and deployment lead times.", "method": "EvoGraph represents software artifacts in a typed directed graph, applies learned mutation operators via specialized SLMs, and selects surviving changes through a multi-objective fitness strategy. The approach is extended using language-specific SLMs to modernize diverse legacy systems while minimizing computational costs.", "result": "EvoGraph fixes 83% of known security vulnerabilities, achieves 93% test-verified functional equivalence in COBOL-to-Java translation, maintains documentation within 2 minutes, and reduces latency and feature lead time by 40% and 7\u00d7, respectively. The extended version achieves 82-96% semantic equivalence across languages with 90% lower costs than large language models.", "conclusion": "EvoGraph demonstrates a scalable and cost-effective framework for Software 3.0, enabling continuous system adaptation while maintaining measurable control, effectively addressing failure modes in legacy software modernization through empirical methods."}}
{"id": "2508.05545", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.05545", "abs": "https://arxiv.org/abs/2508.05545", "authors": ["Leon Garza", "Anantaa Kotal", "Aritran Piplai", "Lavanya Elluri", "Prajit Das", "Aman Chadha"], "title": "PRvL: Quantifying the Capabilities and Risks of Large Language Models for PII Redaction", "comment": null, "summary": "Redacting Personally Identifiable Information (PII) from unstructured text is\ncritical for ensuring data privacy in regulated domains. While earlier\napproaches have relied on rule-based systems and domain-specific Named Entity\nRecognition (NER) models, these methods fail to generalize across formats and\ncontexts. Recent advances in Large Language Models (LLMs) offer a promising\nalternative, yet the effect of architectural and training choices on redaction\nperformance remains underexplored. LLMs have demonstrated strong performance in\ntasks that require contextual language understanding, including the redaction\nof PII in free-form text. Prior work suggests that with appropriate adaptation,\nLLMs can become effective contextual privacy learners. However, the\nconsequences of architectural and training choices for PII Redaction remain\nunderexplored. In this work, we present a comprehensive analysis of LLMs as\nprivacy-preserving PII Redaction systems. We evaluate a range of LLM\narchitectures and training strategies for their effectiveness in PII Redaction.\nOur analysis measures redaction performance, semantic preservation, and PII\nleakage, and compares these outcomes against latency and computational cost.\nThe results provide practical guidance for configuring LLM-based redactors that\nare accurate, efficient, and privacy-aware. To support reproducibility and\nreal-world deployment, we release PRvL, an open-source suite of fine-tuned\nmodels, and evaluation tools for general-purpose PII Redaction. PRvL is built\nentirely on open-source LLMs and supports multiple inference settings for\nflexibility and compliance. It is designed to be easily customized for\ndifferent domains and fully operable within secure, self-managed environments.\nThis enables data owners to perform redactions without relying on third-party\nservices or exposing sensitive content beyond their own infrastructure.", "AI": {"tldr": "The paper evaluates LLMs for PII redaction, analyzing architectural/train choices, and introduces PRvL, an open-source tool for privacy-preserving redaction.", "motivation": "Addresses limitations of rule-based and domain-specific NER models in generalizing PII redaction across formats and contexts, exploring LLMs as a versatile alternative within regulated domains.", "method": "Comprehensive analysis of diverse LLM architectures and training strategies through redaction performance, semantic preservation, PII leakage measures, and benchmarks against latency/infrastructure costs.", "result": "PRvL open-source suite was developed with fine-tuned models customizable for domains, achieving accurate/efficient PII redaction while minimizing leakage. Practical deployment guidelines for secure/self-managed systems were provided.", "conclusion": "LLMs demonstrate potential as effective privacy-preserving redaction systems when configured with domain-optimized architectures. PRvL enables third-party-free, secure customization for real-world PII redaction needs."}}
{"id": "2508.05301", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.05301", "abs": "https://arxiv.org/abs/2508.05301", "authors": ["Victoria Torres Bosch", "Ronny Seiger", "Manuela Albert Albiol", "Antoni Mestre Gascon", "Pedro Jose Valderas Aranda"], "title": "A Conceptual Model and Methodology for Sustainability-aware, IoT-enhanced Business Processes", "comment": "Submitted to Information Systems Frontiers (1572-9419)", "summary": "The real-time data collection and automation capabilities offered by the\nInternet of Things (IoT) are revolutionizing and transforming Business\nProcesses (BPs) into IoT-enhanced BPs, showing high potential for improving\nsustainability. Although already studied in Business Process Management (BPM),\nsustainability research has primarily focused on environmental concerns.\nHowever, achieving a holistic and lasting impact requires a systematic approach\nto address sustainability beyond the environmental dimension. This work\nproposes a conceptual model and a structured methodology with the goal of\nanalyzing the potential of IoT to measure and improve the sustainability of\nBPs. The conceptual model formally represents key sustainability concepts,\nlinking BPM and IoT by highlighting how IoT devices support and contribute to\nsustainability. The methodology guides the systematic analysis of existing BPs,\nidentifies opportunities, and implements sustainability-aware, IoT-enhanced\nBPs. The approach is illustrated through a running example from the tourism\ndomain and a case study in healthcare.", "AI": {"tldr": "The paper proposes a conceptual model and methodology to analyze IoT's potential for improving sustainability in business processes beyond environmental aspects, demonstrated through tourism and healthcare case studies.", "motivation": "Current BPM and IoT research focuses heavily on environmental sustainability, but a holistic approach is needed to address all sustainability dimensions (economic, environmental, social) systematically for lasting business process impacts.", "method": "1) Developed a conceptual model linking BPM and IoT sustainability concepts\n2) Created a structured methodology for: \n   - Analyzing existing business processes\n   - Identifying sustainability opportunities\n   - Implementing IoT-enhanced sustainable processes\n3) Validated approach through tourism domain examples and a healthcare case study.", "result": "Demonstrated practical implementation of the approach in healthcare using IoT sensors for resource optimization and in tourism for sustainability monitoring, showing the methodology effectively identifies and implements process improvements across multiple domains.", "conclusion": "The proposed model and methodology enable systematic analysis and optimization of business processes through IoT to achieve comprehensive sustainability improvements. Success in healthcare and tourism suggests applicability across diverse sectors requiring balanced environmental, economic, and social sustainability considerations."}}
