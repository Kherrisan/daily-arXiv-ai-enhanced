{"id": "2508.16619", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16619", "abs": "https://arxiv.org/abs/2508.16619", "authors": ["Rahul Mishra", "Sudhanshu Kumar Jha", "Naresh Kshetri", "Bishnu Bhusal", "Mir Mehedi Rahman", "Md Masud Rana", "Aimina Ali Eli", "Khaled Aminul Islam", "Bishwo Prakash Pokharel"], "title": "nodeWSNsec: A hybrid metaheuristic approach for reliable security and node deployment in WSNs", "comment": "12 pages, 9 figures", "summary": "Efficient and reliable node deployment in Wireless Sensor Networks is crucial\nfor optimizing coverage of the area, connectivity among nodes, and energy\nefficiency. This paper proposes a hybrid meta heuristic approach combining a\nGenetic Algorithm (GA) and Particle Swarm Optimization (PSO) to address the\nchallenges of energy efficient and reliable node deployment. The GA PSO hybrid\nleverages GAs strong exploration capabilities and PSOs rapid convergence,\nachieving an optimum stability between coverage and energy consumption. The\nperformance of the proposed approach is evaluated against GA and PSO alone and\nthe innovatory meta heuristic based Competitive Multi Objective Marine\nPredators Algorithm (CMOMPA) across varying sensing ranges. Simulation results\ndemonstrate that GA PSO requires 15% to 25% fewer sensor nodes and maintains\n95% or more area coverage while maintaining the connectivity in comparison to\nstandalone GA or PSO algorithm. The proposed algorithm also dominates CMOMPA\nwhen compared for long sensing and communication range in terms of higher\ncoverage, improved connectivity, and reduced deployment time while requiring\nfewer sensor nodes. This study also explores key trade offs in WSN deployment\nand highlights future research directions, including heterogeneous node\ndeployment, mobile WSNs, and enhanced multi objective optimization techniques.\nThe findings underscore the effectiveness of hybrid meta heuristics in\nimproving WSN performance, offering a promising approach for real world\napplications such as environmental monitoring, smart cities, smart agriculture,\ndisaster response, and IIoT."}
{"id": "2508.16625", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16625", "abs": "https://arxiv.org/abs/2508.16625", "authors": ["Rijha Safdar", "Danyail Mateen", "Syed Taha Ali", "M. Umer Ashfaq", "Wajahat Hussain"], "title": "Data and Context Matter: Towards Generalizing AI-based Software Vulnerability Detection", "comment": null, "summary": "The performance of AI-based software vulnerability detection systems is often\nlimited by their poor generalization to unknown codebases. In this research, we\nexplore the impact of data quality and model architecture on the\ngeneralizability of vulnerability detection systems. By generalization we mean\nability of high vulnerability detection performance across different C/C++\nsoftware projects not seen during training. Through a series of experiments, we\ndemonstrate that improvements in dataset diversity and quality substantially\nenhance detection performance. Additionally, we compare multiple encoder-only\nand decoder-only models, finding that encoder based models outperform in terms\nof accuracy and generalization. Our model achieves 6.8% improvement in recall\non the benchmark BigVul[1] dataset, also outperforming on unseen projects,\nhence showing enhanced generalizability. These results highlight the role of\ndata quality and model selection in the development of robust vulnerability\ndetection systems. Our findings suggest a direction for future systems having\nhigh cross-project effectiveness."}
{"id": "2508.16637", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16637", "abs": "https://arxiv.org/abs/2508.16637", "authors": ["Abraham Itzhak Weinberg"], "title": "Passive Hack-Back Strategies for Cyber Attribution: Covert Vectors in Denied Environment", "comment": null, "summary": "Attributing cyberattacks remains a central challenge in modern cybersecurity,\nparticularly within denied environments where defenders have limited visibility\ninto attacker infrastructure and are restricted by legal or operational rules\nof engagement. This perspective examines the strategic value of passive\nhack-back techniques that enable covert attribution and intelligence collection\nwithout initiating direct offensive actions. Key vectors include tracking\nbeacons, honeytokens, environment-specific payloads, and supply-chain-based\ntraps embedded within exfiltrated or leaked assets. These approaches rely on\nthe assumption that attackers will interact with compromised data in traceable\nways, allowing defenders to gather signals without violating engagement\npolicies. The paper also explores the role of Artificial Intelligence (AI) in\nenhancing passive hack-back operations. Topics include the deployment of\nautonomous agents for forensic reconnaissance, the use of Large Language Models\n(LLMs) to generate dynamic payloads, and Adversarial Machine Learning (AML)\ntechniques for evasion and counter-deception. A dedicated section discusses the\nimplications of quantum technologies in this context, both as future threats to\ncryptographic telemetry and as potential tools for stealthy communication and\npost-quantum resilience. Finally, the paper advocates for hybrid defensive\nframeworks that combine passive attribution with delayed or conditional active\nresponses, while maintaining compliance with legal, ethical, and operational\nconstraints."}
{"id": "2508.16662", "categories": ["cs.CR", "cs.CY", "cs.NI", "cs.SE", "K.6.5; C.2.0; D.4.6"], "pdf": "https://arxiv.org/pdf/2508.16662", "abs": "https://arxiv.org/abs/2508.16662", "authors": ["Alexander Tabalipa"], "title": "Bridging the Mobile Trust Gap: A Zero Trust Framework for Consumer-Facing Applications", "comment": "43 pages, 5 figures, 9 tables. Working Paper - Version 1.0. Submitted\n  under a CC BY-SA 4.0 license. Also available as an SSRN Working Paper.\n  Feedback and collaboration are welcome", "summary": "Zero Trust Architecture (ZTA) has become a widely adopted model for securing\nenterprise environments, promoting continuous verification and minimal trust\nacross systems. However, its application in mobile contexts remains limited,\ndespite mobile applications now accounting for most global digital interactions\nand being increasingly targeted by sophisticated threats. Existing Zero Trust\nframeworks developed by organisations such as the National Institute of\nStandards and Technology (NIST) and the Cybersecurity and Infrastructure\nSecurity Agency (CISA) primarily focus on enterprise-managed infrastructure,\nassuming organisational control over devices, networks, and identities. This\npaper addresses a critical gap by proposing an extended Zero Trust model\ndesigned for mobile applications operating in untrusted, user-controlled\nenvironments. Using a design science methodology, the study introduced a\nsix-pillar framework that supports runtime enforcement of trust through\ncontrols including device integrity, user identity validation, data protection,\nsecure application programming interface (API) usage, behavioural monitoring,\nand live application protection. Each pillar was mapped to relevant regulatory\nand security standards to support compliance. A phased implementation roadmap\nand maturity assessment model were also developed to guide adoption across\nvarying organisational contexts. The proposed model offers a practical and\nstandards-aligned approach to securing mobile applications beyond\npre-deployment controls, aligning real-time enforcement with Zero Trust\nprinciples. This contribution expands the operational boundaries of ZTA and\nprovides organisations with a deployable path to reduce fraud, enhance\ncompliance, and address emerging mobile security challenges. Future research\nmay include empirical validation of the framework and cross-sector application\ntesting."}
{"id": "2508.16671", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16671", "abs": "https://arxiv.org/abs/2508.16671", "authors": ["Mingyang Zhou", "Quanming Yao", "Lun Du", "Lanning Wei", "Da Zheng"], "title": "Reflective Paper-to-Code Reproduction Enabled by Fine-Grained Verification", "comment": null, "summary": "Reproducing machine learning papers is essential for scientific progress but\nremains challenging for both humans and automated agents. Existing agent-based\nmethods often struggle to fully and accurately reproduce implementation details\nsuch as mathematical formulas and algorithmic logic. Previous studies show that\nreflection with explicit feedback improves agent performance. However, current\npaper reproduction methods fail to effectively adopt this strategy. This gap\nmainly arises from the diverse paper patterns, complex method modules, and\nvaried configurations encountered in research papers. Motivated by how humans\nuse systematic checklists to efficiently debug complex code, we propose\n\\textbf{RePro}, a \\textbf{Re}flective Paper-to-Code \\textbf{Repro}duction\nframework that automatically extracts a paper's fingerprint, referring to a\ncomprehensive set of accurate and atomic criteria serving as high-quality\nsupervisory signals. The framework first generates code based on the extracted\ninformation, and then leverages the fingerprint within iterative verification\nand refinement loop. This approach systematically detects discrepancies and\nproduces targeted revisions to align generated code with the paper's\nimplementation details. Extensive experiments on the PaperBench Code-Dev\nbenchmark have been conducted, RePro achieves 13.0\\% performance gap over\nbaselines, and it correctly revises complex logical and mathematical criteria\nin reflecting, on which the effectiveness is obvious."}
{"id": "2508.16761", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16761", "abs": "https://arxiv.org/abs/2508.16761", "authors": ["Nesrine Benchoubane", "Olfa Ben Yahia", "William Ferguson", "Gurkan Gur", "Sumit Chakravarty", "Gregory Falco", "Gunes Karabulut Kurt"], "title": "Securing Heterogeneous Network (HetNet) Communications for Wildfire Management: Mitigating the Effects of Adversarial and Environmental Threats", "comment": null, "summary": "In the face of adverse environmental conditions and cyber threats, robust\ncommunication systems for critical applications such as wildfire management and\ndetection demand secure and resilient architectures. This paper presents a\nnovel framework that considers both adversarial factors, building resilience\ninto a heterogeneous network (HetNet) integrating Low Earth Orbit (LEO)\nsatellite constellation with High-Altitude Platform Ground Stations (HAPGS) and\nLow-Altitude Platforms (LAPS), tailored to support wildfire management\noperations. Building upon our previous work on secure-by-component approach for\nlink segment security, we extend protection to the communication layer by\nsecuring both Radio Frequency (RF)/Free Space Optics (FSO) management and\ndifferent links. Through a case study, we quantify how environmental stressors\nimpact secrecy capacity and expose the system to passive adversaries. Key\nfindings demonstrate that atmospheric attenuation and beam misalignment can\nnotably degrade secrecy capacity across both short- and long-range\ncommunication links, while high-altitude eavesdroppers face less signal\ndegradation, increasing their interception capability. Moreover, increasing\ntransmit power to counter environmental losses can inadvertently improve\neavesdropper reception, thereby reducing overall link confidentiality. Our work\nnot only highlights the importance of protecting networks from these dual\nthreats but also aligns with the IEEE P3536 Standard for Space System\nCybersecurity Design, ensuring resilience and the prevention of mission\nfailures."}
{"id": "2508.16678", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.16678", "abs": "https://arxiv.org/abs/2508.16678", "authors": ["Konrad Cinkusz", "Jarosław A. Chudziak", "Ewa Niewiadomska-Szynkiewicz"], "title": "Cognitive Agents Powered by Large Language Models for Agile Software Project Management", "comment": null, "summary": "This paper investigates the integration of cognitive agents powered by Large\nLanguage Models (LLMs) within the Scaled Agile Framework (SAFe) to reinforce\nsoftware project management. By deploying virtual agents in simulated software\nenvironments, this study explores their potential to fulfill fundamental roles\nin IT project development, thereby optimizing project outcomes through\nintelligent automation. Particular emphasis is placed on the adaptability of\nthese agents to Agile methodologies and their transformative impact on\ndecision-making, problem-solving, and collaboration dynamics. The research\nleverages the CogniSim ecosystem, a platform designed to simulate real-world\nsoftware engineering challenges, such as aligning technical capabilities with\nbusiness objectives, managing interdependencies, and maintaining project\nagility. Through iterative simulations, cognitive agents demonstrate advanced\ncapabilities in task delegation, inter-agent communication, and project\nlifecycle management. By employing natural language processing to facilitate\nmeaningful dialogues, these agents emulate human roles and improve the\nefficiency and precision of Agile practices. Key findings from this\ninvestigation highlight the ability of LLM-powered cognitive agents to deliver\nmeasurable improvements in various metrics, including task completion times,\nquality of deliverables, and communication coherence. These agents exhibit\nscalability and adaptability, ensuring their applicability across diverse and\ncomplex project environments. This study underscores the potential of\nintegrating LLM-powered agents into Agile project management frameworks as a\nmeans of advancing software engineering practices. This integration not only\nrefines the execution of project management tasks but also sets the stage for a\nparadigm shift in how teams collaborate and address emerging challenges."}
{"id": "2508.16765", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16765", "abs": "https://arxiv.org/abs/2508.16765", "authors": ["GodsGift Uzor", "Hasan Al-Qudah", "Ynes Ineza", "Abdul Serwadda"], "title": "Guarding Your Conversations: Privacy Gatekeepers for Secure Interactions with Cloud-Based AI Models", "comment": "2025 19th International Conference on Semantic Computing (ICSC)", "summary": "The interactive nature of Large Language Models (LLMs), which closely track\nuser data and context, has prompted users to share personal and private\ninformation in unprecedented ways. Even when users opt out of allowing their\ndata to be used for training, these privacy settings offer limited protection\nwhen LLM providers operate in jurisdictions with weak privacy laws, invasive\ngovernment surveillance, or poor data security practices. In such cases, the\nrisk of sensitive information, including Personally Identifiable Information\n(PII), being mishandled or exposed remains high. To address this, we propose\nthe concept of an \"LLM gatekeeper\", a lightweight, locally run model that\nfilters out sensitive information from user queries before they are sent to the\npotentially untrustworthy, though highly capable, cloud-based LLM. Through\nexperiments with human subjects, we demonstrate that this dual-model approach\nintroduces minimal overhead while significantly enhancing user privacy, without\ncompromising the quality of LLM responses."}
{"id": "2508.16684", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.16684", "abs": "https://arxiv.org/abs/2508.16684", "authors": ["Vikranth Udandarao", "Nipun Misra"], "title": "Democratizing AI Development: Local LLM Deployment for India's Developer Ecosystem in the Era of Tokenized APIs", "comment": "for survey results, check\n  https://docs.google.com/spreadsheets/d/1t0eV9oURaiu2HfARWo6sriBO0eC8bHUyZNN7CgK2NAk/edit?usp=sharing", "summary": "India's developer community faces significant barriers to sustained\nexperimentation and learning with commercial Large Language Model (LLM) APIs,\nprimarily due to economic and infrastructural constraints. This study\nempirically evaluates local LLM deployment using Ollama as an alternative to\ncommercial cloud-based services for developer-focused applications. Through a\nmixed-methods analysis involving 180 Indian developers, students, and AI\nenthusiasts, we find that local deployment enables substantially greater\nhands-on development and experimentation, while reducing costs by 33% compared\nto commercial solutions. Developers using local LLMs completed over twice as\nmany experimental iterations and reported deeper understanding of advanced AI\narchitectures. Our results highlight local deployment as a critical enabler for\ninclusive and accessible AI development, demonstrating how technological\naccessibility can enhance learning outcomes and innovation capacity in\nresource-constrained environments."}
{"id": "2508.16843", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16843", "abs": "https://arxiv.org/abs/2508.16843", "authors": ["Kamel Kamel", "Keshav Sood", "Hridoy Sankar Dutta", "Sunil Aryal"], "title": "A Survey of Threats Against Voice Authentication and Anti-Spoofing Systems", "comment": "This paper will be submitted to the Computer Science Review", "summary": "Voice authentication has undergone significant changes from traditional\nsystems that relied on handcrafted acoustic features to deep learning models\nthat can extract robust speaker embeddings. This advancement has expanded its\napplications across finance, smart devices, law enforcement, and beyond.\nHowever, as adoption has grown, so have the threats. This survey presents a\ncomprehensive review of the modern threat landscape targeting Voice\nAuthentication Systems (VAS) and Anti-Spoofing Countermeasures (CMs), including\ndata poisoning, adversarial, deepfake, and adversarial spoofing attacks. We\nchronologically trace the development of voice authentication and examine how\nvulnerabilities have evolved in tandem with technological advancements. For\neach category of attack, we summarize methodologies, highlight commonly used\ndatasets, compare performance and limitations, and organize existing literature\nusing widely accepted taxonomies. By highlighting emerging risks and open\nchallenges, this survey aims to support the development of more secure and\nresilient voice authentication systems."}
{"id": "2508.16688", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16688", "abs": "https://arxiv.org/abs/2508.16688", "authors": ["Ankur Tomar", "Hengyue Liang", "Indranil Bhattacharya", "Natalia Larios", "Francesco Carbone"], "title": "Cybernaut: Towards Reliable Web Automation", "comment": null, "summary": "The emergence of AI-driven web automation through Large Language Models\n(LLMs) offers unprecedented opportunities for optimizing digital workflows.\nHowever, deploying such systems within industry's real-world environments\npresents four core challenges: (1) ensuring consistent execution, (2)\naccurately identifying critical HTML elements, (3) meeting human-like accuracy\nin order to automate operations at scale and (4) the lack of comprehensive\nbenchmarking data on internal web applications. Existing solutions are\nprimarily tailored for well-designed, consumer-facing websites (e.g.,\nAmazon.com, Apple.com) and fall short in addressing the complexity of\npoorly-designed internal web interfaces. To address these limitations, we\npresent Cybernaut, a novel framework to ensure high execution consistency in\nweb automation agents designed for robust enterprise use. Our contributions are\nthreefold: (1) a Standard Operating Procedure (SOP) generator that converts\nuser demonstrations into reliable automation instructions for linear browsing\ntasks, (2) a high-precision HTML DOM element recognition system tailored for\nthe challenge of complex web interfaces, and (3) a quantitative metric to\nassess execution consistency. The empirical evaluation on our internal\nbenchmark demonstrates that using our framework enables a 23.2% improvement\n(from 72% to 88.68%) in task execution success rate over the browser_use.\nCybernaut identifies consistent execution patterns with 84.7% accuracy,\nenabling reliable confidence assessment and adaptive guidance during task\nexecution in real-world systems. These results highlight Cybernaut's\neffectiveness in enterprise-scale web automation and lay a foundation for\nfuture advancements in web automation."}
{"id": "2508.16868", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.16868", "abs": "https://arxiv.org/abs/2508.16868", "authors": ["Joshua Mashburn", "Johann Knechtel", "Florian Klemme", "Hussam Amrouch", "Ozgur Sinanoglu", "Paul V. Gratz"], "title": "Targeted Wearout Attacks in Microprocessor Cores", "comment": "13 pages, 11 figures, submitted to IEEE International Symposium on\n  High-Performance Computer Architecture 2026 (HPCA-32)", "summary": "Negative-Bias Temperature Instability is a dominant aging mechanism in\nnanoscale CMOS circuits such as microprocessors. With this aging mechanism, the\nrate of device aging is dependent not only on overall operating conditions,\nsuch as heat, but also on user controllable inputs to the transistors. This\ndependence on input implies a possible timing fault-injection attack wherein a\ntargeted path of logic is intentionally degraded through the purposeful,\nsoftware-driven actions of an attacker, rendering a targeted bit effectively\nstuck.\n  In this work, we describe such an attack mechanism, which we dub a\n\"$\\textbf{Targeted Wearout Attack}$\", wherein an attacker with sufficient\nknowledge of the processor core, executing a carefully crafted software program\nwith only user privilege, is able to degrade a functional unit within the\nprocessor with the aim of eliciting a particular desired incorrect calculation\nin a victim application. Here we give a general methodology for the attack. We\nthen demonstrate a case study where a targeted path within the fused\nmultiply-add pipeline in a RISC-V CPU sees a $>7x$ increase in wear over time\nthan would be experienced under typical workloads. We show that an attacker\ncould leverage such an attack, leading to targeted and silent data corruption\nin a co-running victim application using the same unit."}
{"id": "2508.16708", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16708", "abs": "https://arxiv.org/abs/2508.16708", "authors": ["Shufeng Chen", "Halima El Badaoui", "Mariat James Elizebeth", "Takuya Nakashima", "Siddartha Khastgir", "Paul Jennings"], "title": "A Scalable Framework for the Management of STPA Requirements: a Case Study on eVTOL Operations", "comment": null, "summary": "System-Theoretic Process Analysis (STPA) is a recommended method for\nanalysing complex systems, capable of identifying thousands of safety\nrequirements often missed by traditional techniques such as Failure Mode and\nEffects Analysis (FMEA) and Fault Tree Analysis (FTA). However, the absence of\na structured framework for managing and prioritising these requirements\npresents challenges, particularly in fast-paced development environments. This\npaper introduces a scalable framework for prioritising STPA-derived\nrequirements. The framework integrates outputs from each STPA step and\nincorporates expert evaluations based on four key factors: implementation time,\ncost, requirement type, and regulatory coverage. To reduce subjectivity,\nMonte-Carlo Simulation (MCS) is employed to calculate and stabilise requirement\nrankings. An automation toolchain supports the framework, enabling dynamic\nmapping of prioritised requirements in a scaling matrix. This visualisation\naids decision-making and ensures traceability across development phases. The\nframework is applicable from early conceptualisation to more advanced stages,\nenhancing its utility in iterative system development. The framework was\nvalidated through a real-world case study focused on Electric Vertical Take-off\nand Landing (eVTOL) operations, conducted in collaboration with the UK Civil\nAviation Authority. The findings contributed directly to CAP3141, a Civil\nAviation Publication that identifies systemic operational risks and safety\nmitigations for regulators, operators, and vertiports. The prioritisation\nprocess supported decision-making by helping stakeholders identify and manage\nhigh-impact requirements efficiently. This work contributes a practical\nsolution for managing STPA outputs, bridging gaps in requirement prioritisation\nand supporting safety-critical development in emerging technologies."}
{"id": "2508.16941", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16941", "abs": "https://arxiv.org/abs/2508.16941", "authors": ["Yu Cheng", "Xiaofang Qi", "Yanhui Li"], "title": "Investigating red packet fraud in Android applications: Insights from user reviews", "comment": "This manuscript has been accepted for publication in Cybersecurity\n  (Springer Nature). The final version of record will be published by Springer\n  Nature and will be accessible online. This version is the Author Accepted\n  Manuscript (AAM) and may differ from the final published version", "summary": "With the popularization of smartphones, red packets have been widely used in\nmobile apps. However, the issues of fraud associated with them have also become\nincreasingly prominent. As reported in user reviews from mobile app markets,\nmany users have complained about experiencing red packet fraud and being\npersistently troubled by fraudulent red packets. To uncover this phenomenon, we\nconduct the first investigation into an extensive collection of user reviews on\napps with red packets. In this paper, we first propose a novel automated\napproach, ReckDetector, for effectively identifying apps with red packets from\napp markets. We then collect over 360,000 real user reviews from 334 apps with\nred packets available on Google Play and three popular alternative Android app\nmarkets. We preprocess the user reviews to extract those related to red packets\nand fine-tune a pre-trained BERT model to identify negative reviews. Finally,\nbased on semantic analysis, we have summarized six distinct categories of red\npacket fraud issues reported by users. Through our study, we found that red\npacket fraud is highly prevalent, significantly impacting user experience and\ndamaging the reputation of apps. Moreover, red packets have been widely\nexploited by unscrupulous app developers as a deceptive incentive mechanism to\nentice users into completing their designated tasks, thereby maximizing their\nprofits."}
{"id": "2508.16713", "categories": ["cs.SE", "cs.AI", "hep-ex"], "pdf": "https://arxiv.org/pdf/2508.16713", "abs": "https://arxiv.org/abs/2508.16713", "authors": ["Mohammad Atif", "Kriti Chopra", "Ozgur Kilic", "Tianle Wang", "Zhihua Dong", "Charles Leggett", "Meifeng Lin", "Paolo Calafiura", "Salman Habib"], "title": "CelloAI: Leveraging Large Language Models for HPC Software Development in High Energy Physics", "comment": "12 pages, 2 figures", "summary": "Next-generation High Energy Physics (HEP) experiments will generate\nunprecedented data volumes, necessitating High Performance Computing (HPC)\nintegration alongside traditional high-throughput computing. However, HPC\nadoption in HEP is hindered by the challenge of porting legacy software to\nheterogeneous architectures and the sparse documentation of these complex\nscientific codebases. We present CelloAI, a locally hosted coding assistant\nthat leverages Large Language Models (LLMs) with retrieval-augmented generation\n(RAG) to support HEP code documentation and generation. This local deployment\nensures data privacy, eliminates recurring costs and provides access to large\ncontext windows without external dependencies. CelloAI addresses two primary\nuse cases, code documentation and code generation, through specialized\ncomponents. For code documentation, the assistant provides: (a) Doxygen style\ncomment generation for all functions and classes by retrieving relevant\ninformation from RAG sources (papers, posters, presentations), (b) file-level\nsummary generation, and (c) an interactive chatbot for code comprehension\nqueries. For code generation, CelloAI employs syntax-aware chunking strategies\nthat preserve syntactic boundaries during embedding, improving retrieval\naccuracy in large codebases. The system integrates callgraph knowledge to\nmaintain dependency awareness during code modifications and provides\nAI-generated suggestions for performance optimization and accurate refactoring.\nWe evaluate CelloAI using real-world HEP applications from ATLAS, CMS, and DUNE\nexperiments, comparing different embedding models for code retrieval\neffectiveness. Our results demonstrate the AI assistant's capability to enhance\ncode understanding and support reliable code generation while maintaining the\ntransparency and safety requirements essential for scientific computing\nenvironments."}
{"id": "2508.16991", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16991", "abs": "https://arxiv.org/abs/2508.16991", "authors": ["Ekzhin Ear"], "title": "Towards Principled Analysis and Mitigation of Space Cyber Risks", "comment": "PhD Dissertation", "summary": "Space infrastructures have become an underpinning of modern society, but\ntheir associated cyber risks are little understood. This Dissertation advances\nthe state-of-the-art via four contributions. (i) It introduces an innovative\nframework for characterizing real-world cyber attacks against space\ninfrastructures, or space cyber attacks, including a novel methodology for\ncoping with missing data and three novel metrics. A case study demonstrates the\nusefulness of the framework on 108 real-world space cyber attacks. (ii) This\nDissertation characterizes the state-of-the-practice in space cyber risk\nanalysis and mitigation, namely the Notional Risk Scores (NRS) within the Space\nAttack Research and Tactic Analysis (SPARTA) framework. (iii) We propose a set\nof desired properties that should be satisfied by any competent space cyber\nrisk analysis and mitigation tool and applies them to assess two industrial\nspace cyber risk analysis and mitigation tools. (iv) The study introduces a\nnovel framework to analyze and mitigate space cyber risks by explicitly\nmodeling space cyber attack cascading effects and presenting algorithms for\nmission risk analysis and mission hardening. We demonstrate the usefulness of\nthe framework by applying it to analyze and mitigate space cyber risks, with\ntestbed-based validation."}
{"id": "2508.16771", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.16771", "abs": "https://arxiv.org/abs/2508.16771", "authors": ["Yifan Zhang", "Chen Huang", "Yueke Zhang", "Jiahao Zhang", "Toby Jia-Jun Li", "Collin McMillan", "Kevin Leach", "Yu Huang"], "title": "EyeMulator: Improving Code Language Models by Mimicking Human Visual Attention", "comment": null, "summary": "Code language models (so-called CodeLLMs) are now commonplace in software\ndevelopment. As a general rule, CodeLLMs are trained by dividing training\nexamples into input tokens and then learn importance of those tokens in a\nprocess called machine attention. Machine attention is based solely on input\ntoken salience to output token examples during training. Human software\ndevelopers are different, as humans intuitively know that some tokens are more\nsalient than others. While intuition itself is ineffable and a subject of\nphilosophy, clues about salience are present in human visual attention, since\npeople tend to look at more salient words more often. In this paper, we present\nEyeMulator, a technique for training CodeLLMs to mimic human visual attention\nwhile training for various software development tasks. We add special weights\nfor each token in each input example to the loss function used during LLM\nfine-tuning. We draw these weights from observations of human visual attention\nderived from a previously-collected publicly-available dataset of eye-tracking\nexperiments in software engineering tasks. These new weights ultimately induce\nchanges in the attention of the subject LLM during training, resulting in a\nmodel that does not need eye-tracking data during inference. Our evaluation\nshows that EyeMulator outperforms strong LLM baselines on several tasks such as\ncode translation, completion and summarization. We further show an ablation\nstudy that demonstrates the improvement is due to subject models learning to\nmimic human attention."}
{"id": "2508.17043", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.17043", "abs": "https://arxiv.org/abs/2508.17043", "authors": ["Shayesta Naziri", "Xu Wang", "Guangsheng Yu", "Christy Jie Liang", "Wei Ni"], "title": "ZAPS: A Zero-Knowledge Proof Protocol for Secure UAV Authentication with Flight Path Privacy", "comment": "11 Pages, 8 figures, Journal", "summary": "The increasing deployment of Unmanned Aerial Vehicles (UAVs) for military,\ncommercial, and logistics applications has raised significant concerns\nregarding flight path privacy. Conventional UAV communication systems often\nexpose flight path data to third parties, making them vulnerable to tracking,\nsurveillance, and location inference attacks. Existing encryption techniques\nprovide security but fail to ensure complete privacy, as adversaries can still\ninfer movement patterns through metadata analysis. To address these challenges,\nwe propose a zk-SNARK(Zero-Knowledge Succinct Non-Interactive Argument of\nKnowledge)-based privacy-preserving flight path authentication and verification\nframework. Our approach ensures that a UAV can prove its authorisation,\nvalidate its flight path with a control centre, and comply with regulatory\nconstraints without revealing any sensitive trajectory information. By\nleveraging zk-SNARKs, the UAV can generate cryptographic proofs that verify\ncompliance with predefined flight policies while keeping the exact path and\nlocation undisclosed. This method mitigates risks associated with real-time\ntracking, identity exposure, and unauthorised interception, thereby enhancing\nUAV operational security in adversarial environments. Our proposed solution\nbalances privacy, security, and computational efficiency, making it suitable\nfor resource-constrained UAVs in both civilian and military applications."}
{"id": "2508.16853", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16853", "abs": "https://arxiv.org/abs/2508.16853", "authors": ["Pratyush Nidhi Sharma", "Lauren Wright", "Anne Herfurth", "Munsif Sokiyna", "Pratyaksh Nidhi Sharma", "Sethu Das", "Mikko Siponen"], "title": "DevLicOps: A Framework for Mitigating Licensing Risks in AI-Generated Code", "comment": "18 pages, 1 figure, 2 Tables", "summary": "Generative AI coding assistants (ACAs) are widely adopted yet pose serious\nlegal and compliance risks. ACAs can generate code governed by restrictive\nopen-source licenses (e.g., GPL), potentially exposing companies to litigation\nor forced open-sourcing. Few developers are trained in these risks, and legal\nstandards vary globally, especially with outsourcing. Our article introduces\nDevLicOps, a practical framework that helps IT leaders manage ACA-related\nlicensing risks through governance, incident response, and informed tradeoffs.\nAs ACA adoption grows and legal frameworks evolve, proactive license compliance\nis essential for responsible, risk-aware software development in the AI era."}
{"id": "2508.17071", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.17071", "abs": "https://arxiv.org/abs/2508.17071", "authors": ["Sufyan Al-Janabi"], "title": "Post-Quantum Blockchain: Challenges and Opportunities", "comment": null, "summary": "Blockchain is a Distributed Ledger Technology (DLT) that offers numerous\nbenefits including decentralization, transparency, efficiency, and reduced\ncosts. Hence, blockchain has been included in many fields. Blockchain relies on\ncryptographic protocols (especially public-key cryptography and hash functions)\nto achieve many essential sub-routines. However, the increased progress of\nquantum computation and algorithms has threatened the security of many\ntraditional cryptosystems. Therefore, this represents a serious risk for the\nexisting blockchain technology. For example, SHA-256 and the Elliptic Curve\nDigital Signature Algorithm (ECDSA) cryptosystems can be compromised by Shor s\nand Grover s quantum algorithms in the foreseeable future. Post-Quantum\nCryptography (PQC) is a basic solution for resisting these quantum attacks.\nApplying PQC to blockchains results in creating Post-Quantum Blockchains (PQB).\nThus, this paper aims to review the threats imposed by quantum computers on\nclassical blockchain technology and provide useful guidelines on PQB security\nto blockchain researchers. The paper focuses on the challenges and\nopportunities of future work direction in this field."}
{"id": "2508.16860", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16860", "abs": "https://arxiv.org/abs/2508.16860", "authors": ["Md Afif Al Mamun", "Gias Uddin", "Lan Xia", "Longyu Zhang"], "title": "TriagerX: Dual Transformers for Bug Triaging Tasks with Content and Interaction Based Rankings", "comment": "This work is currently under review at IEEE Transactions on Software\n  Engineering. The replication package will be made publicly available upon\n  acceptance", "summary": "Pretrained Language Models or PLMs are transformer-based architectures that\ncan be used in bug triaging tasks. PLMs can better capture token semantics than\ntraditional Machine Learning (ML) models that rely on statistical features\n(e.g., TF-IDF, bag of words). However, PLMs may still attend to less relevant\ntokens in a bug report, which can impact their effectiveness. In addition, the\nmodel can be sub-optimal with its recommendations when the interaction history\nof developers around similar bugs is not taken into account. We designed\nTriagerX to address these limitations. First, to assess token semantics more\nreliably, we leverage a dual-transformer architecture. Unlike current\nstate-of-the-art (SOTA) baselines that employ a single transformer\narchitecture, TriagerX collects recommendations from two transformers with each\noffering recommendations via its last three layers. This setup generates a\nrobust content-based ranking of candidate developers. TriagerX then refines\nthis ranking by employing a novel interaction-based ranking methodology, which\nconsiders developers' historical interactions with similar fixed bugs. Across\nfive datasets, TriagerX surpasses all nine transformer-based methods, including\nSOTA baselines, often improving Top-1 and Top-3 developer recommendation\naccuracy by over 10%. We worked with our large industry partner to successfully\ndeploy TriagerX in their development environment. The partner required both\ndeveloper and component recommendations, with components acting as proxies for\nteam assignments-particularly useful in cases of developer turnover or team\nchanges. We trained TriagerX on the partner's dataset for both tasks, and it\noutperformed SOTA baselines by up to 10% for component recommendations and 54%\nfor developer recommendations."}
{"id": "2508.17121", "categories": ["cs.CR", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.17121", "abs": "https://arxiv.org/abs/2508.17121", "authors": ["Zhenliang Gan", "Xiaoxiao Hu", "Sheng Li", "Zhenxing Qian", "Xinpeng Zhang"], "title": "SyncGuard: Robust Audio Watermarking Capable of Countering Desynchronization Attacks", "comment": null, "summary": "Audio watermarking has been widely applied in copyright protection and source\ntracing. However, due to the inherent characteristics of audio signals,\nwatermark localization and resistance to desynchronization attacks remain\nsignificant challenges. In this paper, we propose a learning-based scheme named\nSyncGuard to address these challenges. Specifically, we design a frame-wise\nbroadcast embedding strategy to embed the watermark in arbitrary-length audio,\nenhancing time-independence and eliminating the need for localization during\nwatermark extraction. To further enhance robustness, we introduce a\nmeticulously designed distortion layer. Additionally, we employ dilated\nresidual blocks in conjunction with dilated gated blocks to effectively capture\nmulti-resolution time-frequency features. Extensive experimental results show\nthat SyncGuard efficiently handles variable-length audio segments, outperforms\nstate-of-the-art methods in robustness against various attacks, and delivers\nsuperior auditory quality."}
{"id": "2508.16903", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16903", "abs": "https://arxiv.org/abs/2508.16903", "authors": ["Yijun Lu", "Hironori Washizaki", "Naoyasu Ubayashi", "Nobukazu Yoshioka", "Chenhao Wu", "Masanari Kondo", "Yuyin Ma", "Jiong Dong", "Jianjin Zhao", "Dongqi Han"], "title": "Mind the Gap: A Decade-Scale Empirical Study of Multi-Stakeholder Dynamics in VR Ecosystem", "comment": null, "summary": "In the development and evolution of VR ecosystem, platform stakeholders\ncontinuously adapt their products in response to user and technical feedback,\noften reflected in subtle shifts in discussion topics or system updates. A\ncomprehensive understanding of these changes is essential for identifying gaps\nbetween user expectations and developer actions, which can guide more effective\nquality assurance and user-centered innovation. While previous studies have\nanalyzed either user reviews or developer discussions in isolation, such\napproaches typically fail to reveal how specific user concerns are (or are not)\naddressed by corresponding technical activities. To address this limitation,\nour study introduces a multi-view empirical framework that systematically\ncompares and aligns stakeholder perspectives. By applying topic modeling and\nquantitative impact analysis to 944,320 user reviews and 389,477 developer\nposts, we identify not only the overlap in concerns (e.g., performance, input\nmethods), but also clear gaps in areas like inclusivity and community safety\n(e.g., LGBTQ+ representation, child-friendly content). Our findings show that\nwhile users repeatedly raise such issues, they are rarely discussed in\ndeveloper forums. These insights enable data-driven recommendations for closing\nthe user-developer gap in VR ecosystems, offering practical implications for\nplatform governance and the design of next-generation VR systems."}
{"id": "2508.17155", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17155", "abs": "https://arxiv.org/abs/2508.17155", "authors": ["Derek Lilienthal", "Sanghyun Hong"], "title": "Mind the Gap: Time-of-Check to Time-of-Use Vulnerabilities in LLM-Enabled Agents", "comment": "Pre-print", "summary": "Large Language Model (LLM)-enabled agents are rapidly emerging across a wide\nrange of applications, but their deployment introduces vulnerabilities with\nsecurity implications. While prior work has examined prompt-based attacks\n(e.g., prompt injection) and data-oriented threats (e.g., data exfiltration),\ntime-of-check to time-of-use (TOCTOU) remain largely unexplored in this\ncontext. TOCTOU arises when an agent validates external state (e.g., a file or\nAPI response) that is later modified before use, enabling practical attacks\nsuch as malicious configuration swaps or payload injection. In this work, we\npresent the first study of TOCTOU vulnerabilities in LLM-enabled agents. We\nintroduce TOCTOU-Bench, a benchmark with 66 realistic user tasks designed to\nevaluate this class of vulnerabilities. As countermeasures, we adapt detection\nand mitigation techniques from systems security to this setting and propose\nprompt rewriting, state integrity monitoring, and tool-fusing. Our study\nhighlights challenges unique to agentic workflows, where we achieve up to 25%\ndetection accuracy using automated detection methods, a 3% decrease in\nvulnerable plan generation, and a 95% reduction in the attack window. When\ncombining all three approaches, we reduce the TOCTOU vulnerabilities from an\nexecuted trajectory from 12% to 8%. Our findings open a new research direction\nat the intersection of AI safety and systems security."}
{"id": "2508.17161", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.17161", "abs": "https://arxiv.org/abs/2508.17161", "authors": ["Julyanara R. Silva", "Carlos Eduardo C. Dantas", "Marcelo A. Maia"], "title": "What Developers Ask to ChatGPT in GitHub Pull Requests? an Exploratory Study", "comment": "12 pages, 3 figures", "summary": "The emergence of Large Language Models (LLMs), such as ChatGPT, has\nintroduced a new set of tools to support software developers in solving pro-\ngramming tasks. However, our understanding of the interactions (i.e., prompts)\nbetween developers and ChatGPT that result in contributions to the codebase\nremains limited. To explore this limitation, we conducted a manual evaluation\nof 155 valid ChatGPT share links extracted from 139 merged Pull Requests (PRs),\nrevealing the interactions between developers and reviewers with ChatGPT that\nled to merges into the main codebase. Our results produced a catalog of 14\ntypes of ChatGPT requests categorized into four main groups. We found a\nsignificant number of requests involving code review and the implementation of\ncode snippets based on specific tasks. Developers also sought to clarify doubts\nby requesting technical explanations or by asking for text refinements for\ntheir web pages. Furthermore, we verified that prompts involving code\ngeneration generally required more interactions to produce the desired answer\ncompared to prompts requesting text review or technical information."}
{"id": "2508.17222", "categories": ["cs.CR", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.17222", "abs": "https://arxiv.org/abs/2508.17222", "authors": ["Jiale Liu", "Jiahao Zhang", "Suhang Wang"], "title": "Exposing Privacy Risks in Graph Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is a powerful technique for enhancing\nLarge Language Models (LLMs) with external, up-to-date knowledge. Graph RAG has\nemerged as an advanced paradigm that leverages graph-based knowledge structures\nto provide more coherent and contextually rich answers. However, the move from\nplain document retrieval to structured graph traversal introduces new,\nunder-explored privacy risks. This paper investigates the data extraction\nvulnerabilities of the Graph RAG systems. We design and execute tailored data\nextraction attacks to probe their susceptibility to leaking both raw text and\nstructured data, such as entities and their relationships. Our findings reveal\na critical trade-off: while Graph RAG systems may reduce raw text leakage, they\nare significantly more vulnerable to the extraction of structured entity and\nrelationship information. We also explore potential defense mechanisms to\nmitigate these novel attack surfaces. This work provides a foundational\nanalysis of the unique privacy challenges in Graph RAG and offers insights for\nbuilding more secure systems."}
{"id": "2508.17343", "categories": ["cs.SE", "cs.AI", "D.2"], "pdf": "https://arxiv.org/pdf/2508.17343", "abs": "https://arxiv.org/abs/2508.17343", "authors": ["Abhik Roychoudhury"], "title": "Agentic AI for Software: thoughts from Software Engineering community", "comment": "4 pages", "summary": "AI agents have recently shown significant promise in software engineering.\nMuch public attention has been transfixed on the topic of code generation from\nLarge Language Models (LLMs) via a prompt. However, software engineering is\nmuch more than programming, and AI agents go far beyond instructions given by a\nprompt.\n  At the code level, common software tasks include code generation, testing,\nand program repair. Design level software tasks may include architecture\nexploration, requirements understanding, and requirements enforcement at the\ncode level. Each of these software tasks involves micro-decisions which can be\ntaken autonomously by an AI agent, aided by program analysis tools. This\ncreates the vision of an AI software engineer, where the AI agent can be seen\nas a member of a development team.\n  Conceptually, the key to successfully developing trustworthy agentic AI-based\nsoftware workflows will be to resolve the core difficulty in software\nengineering - the deciphering and clarification of developer intent.\nSpecification inference, or deciphering the intent, thus lies at the heart of\nmany software tasks, including software maintenance and program repair. A\nsuccessful deployment of agentic technology into software engineering would\ninvolve making conceptual progress in such intent inference via agents.\n  Trusting the AI agent becomes a key aspect, as software engineering becomes\nmore automated. Higher automation also leads to higher volume of code being\nautomatically generated, and then integrated into code-bases. Thus to deal with\nthis explosion, an emerging direction is AI-based verification and validation\n(V & V) of AI generated code. We posit that agentic software workflows in\nfuture will include such AIbased V&V."}
{"id": "2508.17296", "categories": ["cs.CR", "81P68", "F.2; F.1; E.3"], "pdf": "https://arxiv.org/pdf/2508.17296", "abs": "https://arxiv.org/abs/2508.17296", "authors": ["Adi Mutha", "Jitendra Sandu"], "title": "Literature Review of the Effect of Quantum Computing on Cryptocurrencies using Blockchain Technology", "comment": "21 pages", "summary": "With the advent of quantum computing, cryptocurrencies that rely on\nblockchain technology face mounting cryptographic vulnerabilities. This paper\npresents a comprehensive literature review evaluating how quantum algorithms,\nspecifically Shors and Grovers, could disrupt the foundational security\nmechanisms of cryptocurrencies. Shors algorithm poses a threat to public-key\ncryptographic schemes by enabling efficient factorization and discrete\nlogarithm solving, thereby endangering digital signature systems. Grovers\nalgorithm undermines hash-based functions, increasing the feasibility of fifty\none percent attacks and hash collisions. By examining the internal mechanisms\nof major cryptocurrencies such as Bitcoin, Ethereum, Litecoin, Monero, and\nZcash, this review identifies specific vulnerabilities in transaction and\nconsensus processes. It further analyses the current hardware limitations of\nquantum systems and estimates when such attacks could become feasible. In\nanticipation, it investigates countermeasures including Post-Quantum\nCryptography (PQC), Quantum Key Distribution (QKD), and protocol-level\nmodifications such as memory-intensive proof-of-work algorithms and\nmulti-signature schemes. The discussion integrates recent advancements in\nquantum error correction, hardware scalability, and NIST-standardized\ncryptographic algorithms. This review concludes that while quantum computers\nare not yet advanced enough to pose an immediate threat, proactive integration\nof quantum-resistant solutions is essential. The findings underscore the urgent\nneed for cryptocurrencies to adopt post-quantum cryptographic standards to\npreserve the decentralized trust, integrity, and security that define\nblockchain-based digital cryptocurrencies."}
{"id": "2508.17344", "categories": ["cs.SE", "cs.LG", "cs.PF", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.17344", "abs": "https://arxiv.org/abs/2508.17344", "authors": ["Rajrupa Chattaraj", "Sridhar Chimalakonda", "Vibhu Saujanya Sharma", "Vikrant Kaulgud"], "title": "Who Wins the Race? (R Vs Python) - An Exploratory Study on Energy Consumption of Machine Learning Algorithms", "comment": "18 pages including references, 5 figures", "summary": "The utilization of Machine Learning (ML) in contemporary software systems is\nextensive and continually expanding. However, its usage is energy-intensive,\ncontributing to increased carbon emissions and demanding significant resources.\nWhile numerous studies examine the performance and accuracy of ML, only a\nlimited few focus on its environmental aspects, particularly energy\nconsumption. In addition, despite emerging efforts to compare energy\nconsumption across various programming languages for specific algorithms and\ntasks, there remains a gap specifically in comparing these languages for\nML-based tasks. This paper aims to raise awareness of the energy costs\nassociated with employing different programming languages for ML model training\nand inference. Through this empirical study, we measure and compare the energy\nconsumption along with run-time performance of five regression and five\nclassification tasks implemented in Python and R, the two most popular\nprogramming languages in this context. Our study results reveal a statistically\nsignificant difference in costs between the two languages in 95% of the cases\nexamined. Furthermore, our analysis demonstrates that the choice of programming\nlanguage can influence energy efficiency significantly, up to 99.16% during\nmodel training and up to 99.8% during inferences, for a given ML task."}
{"id": "2508.17304", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.17304", "abs": "https://arxiv.org/abs/2508.17304", "authors": ["Muhammad Ibn Ziauddin", "Rownak Rahad Rabbi", "SM Mehrab", "Fardin Faiyaz", "Mosarrat Jahan"], "title": "An Efficient Recommendation Filtering-based Trust Model for Securing Internet of Things", "comment": null, "summary": "Trust computation is crucial for ensuring the security of the Internet of\nThings (IoT). However, current trust-based mechanisms for IoT have limitations\nthat impact data security. Sliding window-based trust schemes cannot ensure\nreliable trust computation due to their inability to select appropriate window\nlengths. Besides, recent trust scores are emphasized when considering the\neffect of time on trust. This can cause a sudden change in overall trust score\nbased on recent behavior, potentially misinterpreting an honest service\nprovider as malicious and vice versa. Moreover, clustering mechanisms used to\nfilter recommendations in trust computation often lead to slower results. In\nthis paper, we propose a robust trust model to address these limitations. The\nproposed approach determines the window length dynamically to guarantee\naccurate trust computation. It uses the harmonic mean of average trust score\nand time to prevent sudden fluctuations in trust scores. Additionally, an\nefficient personalized subspace clustering algorithm is used to exclude\nrecommendations. We present a security analysis demonstrating the resiliency of\nthe proposed scheme against bad-mouthing, ballot-stuffing, and on-off attacks.\nThe proposed scheme demonstrates a competitive performance in detecting\nbad-mouthing attacks, while outperforming existing works with an approximately\n44% improvement in accuracy for detecting on-off attacks. It maintains its\neffectiveness even when the percentage of on-off attackers increases and in\nscenarios where multiple attacks occur simultaneously. Additionally, the\nproposed scheme reduces the recommendation filtering time by 95%."}
{"id": "2508.17713", "categories": ["cs.SE", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.17713", "abs": "https://arxiv.org/abs/2508.17713", "authors": ["Zhihao Xu", "Shikai Guo", "Guilin Zhao", "Peiyu Zou", "Siwen Wang", "Qian Ma", "Hui Li", "Furui Zhan"], "title": "Code Difference Guided Fuzzing for FPGA Logic Synthesis Compilers via Bayesian Optimization", "comment": null, "summary": "Field Programmable Gate Arrays (FPGAs) play a crucial role in Electronic\nDesign Automation (EDA) applications, which have been widely used in\nsafety-critical environments, including aerospace, chip manufacturing, and\nmedical devices. A critical step in FPGA development is logic synthesis, which\nenables developers to translate their software designs into hardware net lists,\nwhich facilitates the physical implementation of the chip, detailed timing and\npower analysis, gate-level simulation, test vector generation, and optimization\nand consistency checking. However, bugs or incorrect implementations in FPGA\nlogic synthesis compilers may lead to unexpected behaviors in target\nwapplications, posing security risks. Therefore, it is crucial to eliminate\nsuch bugs in FPGA logic synthesis compilers. The effectiveness of existing\nworks is still limited by its simple, blind mutation strategy. To address this\nchallenge, we propose a guided mutation strategy based on Bayesian optimization\ncalled LSC-Fuzz to detect bugs in FPGA logic synthesis compilers. Specifically,\nLSC-Fuzz consists of three components: the test-program generation component,\nthe Bayesian diversity selection component, and the equivalent check component.\nBy performing test-program generation and Bayesian diversity selection,\nLSC-Fuzz generates diverse and complex HDL code, thoroughly testing the FPGA\nlogic synthesis compilers using equivalent check to detect bugs. Through three\nmonths, LSC-Fuzz has found 16 bugs, 12 of these has been confirmed by official\ntechnical support."}
{"id": "2508.17329", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.17329", "abs": "https://arxiv.org/abs/2508.17329", "authors": ["Xiaoyan Zhang", "Dongyang Lyu", "Xiaoqi Li"], "title": "Risk Assessment and Security Analysis of Large Language Models", "comment": null, "summary": "As large language models (LLMs) expose systemic security challenges in high\nrisk applications, including privacy leaks, bias amplification, and malicious\nabuse, there is an urgent need for a dynamic risk assessment and collaborative\ndefence framework that covers their entire life cycle. This paper focuses on\nthe security problems of large language models (LLMs) in critical application\nscenarios, such as the possibility of disclosure of user data, the deliberate\ninput of harmful instructions, or the models bias. To solve these problems, we\ndescribe the design of a system for dynamic risk assessment and a hierarchical\ndefence system that allows different levels of protection to cooperate. This\npaper presents a risk assessment system capable of evaluating both static and\ndynamic indicators simultaneously. It uses entropy weighting to calculate\nessential data, such as the frequency of sensitive words, whether the API call\nis typical, the realtime risk entropy value is significant, and the degree of\ncontext deviation. The experimental results show that the system is capable of\nidentifying concealed attacks, such as role escape, and can perform rapid risk\nevaluation. The paper uses a hybrid model called BERT-CRF (Bidirectional\nEncoder Representation from Transformers) at the input layer to identify and\nfilter malicious commands. The model layer uses dynamic adversarial training\nand differential privacy noise injection technology together. The output layer\nalso has a neural watermarking system that can track the source of the content.\nIn practice, the quality of this method, especially important in terms of\ncustomer service in the financial industry."}
{"id": "2508.17719", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.17719", "abs": "https://arxiv.org/abs/2508.17719", "authors": ["Akhila Sri Manasa Venigalla", "Sridhar Chimalakonda"], "title": "DocFetch - Towards Generating Software Documentation from Multiple Software Artifacts", "comment": "12 pages, 7 Figures, 4 Tables", "summary": "Software Documentation plays a major role in the usage and development of a\nproject. Widespread adoption of open source software projects contributes to\nlarger and faster development of the projects, making it difficult to maintain\nthe associated documentation. Existing automated approaches to generate\ndocumentation largely focus on source code. However, information useful for\ndocumentation is observed to be scattered across various artifacts that\nco-evolve with the source code. Leveraging this information across multiple\nartifacts can reduce the effort involved in maintaining documentation. Hence,\nwe propose DocFetch, to generate different types of documentation from multiple\nsoftware artifacts. We employ a multi-layer prompt based LLM and generate\nstructured documentation corresponding to different documentation types for the\ndata consolidated in DocMine dataset. We evaluate the performance of DocFetch\nusing a manually curated groundtruth dataset by analysing the artifacts in\nDocMine. The evaluation yields a highest BLEU-4 score of 43.24% and ROUGE-L\nscore of 0.39 for generation of api-related and file-related information from\nfive documentation sources. The generation of other documentation type related\ninformation also reported BLEU-4 scores close to 30% indicating good\nperformance of the approach. Thus,DocFetch can be employed to\nsemi-automatically generate documentation, and helps in comprehending the\nprojects with minimal effort in maintaining the documentation."}
{"id": "2508.17414", "categories": ["cs.CR", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.17414", "abs": "https://arxiv.org/abs/2508.17414", "authors": ["Temesgen Kitaw Damenu", "İnci Zaim Gökbay", "Alexandra Covaci", "Shujun Li"], "title": "Cyber Security Educational Games for Children: A Systematic Literature Review", "comment": null, "summary": "Educational games have been widely used to teach children about cyber\nsecurity. This systematic literature review reveals evidence of positive\nlearning outcomes, after analysing 91 such games reported in 68 papers\npublished between 2010 and 2024. However, critical gaps have also been\nidentified regarding the design processes and the methodological rigour,\nincluding lack of systematic design, misalignment between proposed and achieved\nlearning outcomes, rare use of control groups, limited discussions on ethical\nconsiderations, and underutilisation of emerging technologies. We recommend\nmultiple future research directions, e.g., a hybrid approach to game design and\nevaluation that combines bottom-up and top-down approaches."}
{"id": "2508.17720", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.17720", "abs": "https://arxiv.org/abs/2508.17720", "authors": ["Ziqi Guan", "Xin Yin", "Zhiyuan Peng", "Chao Ni"], "title": "RepoTransAgent: Multi-Agent LLM Framework for Repository-Aware Code Translation", "comment": null, "summary": "Repository-aware code translation is critical for modernizing legacy systems,\nenhancing maintainability, and enabling interoperability across diverse\nprogramming languages. While recent advances in large language models (LLMs)\nhave improved code translation quality, existing approaches face significant\nchallenges in practical scenarios: insufficient contextual understanding,\ninflexible prompt designs, and inadequate error correction mechanisms. These\nlimitations severely hinder accurate and efficient translation of complex,\nreal-world code repositories. To address these challenges, we propose\nRepoTransAgent, a novel multi-agent LLM framework for repository-aware code\ntranslation. RepoTransAgent systematically decomposes the translation process\ninto specialized subtasks-context retrieval, dynamic prompt construction, and\niterative code refinement-each handled by dedicated agents. Our approach\nleverages retrieval-augmented generation (RAG) for contextual information\ngathering, employs adaptive prompts tailored to varying repository scenarios,\nand introduces a reflection-based mechanism for systematic error correction. We\nevaluate RepoTransAgent on hundreds of Java-C# translation pairs from six\npopular open-source projects. Experimental results demonstrate that\nRepoTransAgent significantly outperforms state-of-the-art baselines in both\ncompile and pass rates. Specifically, RepoTransAgent achieves up to 55.34%\ncompile rate and 45.84% pass rate. Comprehensive analysis confirms the\nrobustness and generalizability of RepoTransAgent across different LLMs,\nestablishing its effectiveness for real-world repository-aware code\ntranslation."}
{"id": "2508.17481", "categories": ["cs.CR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.17481", "abs": "https://arxiv.org/abs/2508.17481", "authors": ["Priyanka Prakash Surve", "Asaf Shabtai", "Yuval Elovici"], "title": "SoK: Cybersecurity Assessment of Humanoid Ecosystem", "comment": null, "summary": "Humanoids are progressing toward practical deployment across healthcare,\nindustrial, defense, and service sectors. While typically considered\ncyber-physical systems (CPSs), their dependence on traditional networked\nsoftware stacks (e.g., Linux operating systems), robot operating system (ROS)\nmiddleware, and over-the-air update channels, creates a distinct security\nprofile that exposes them to vulnerabilities conventional CPS models do not\nfully address. Prior studies have mainly examined specific threats, such as\nLiDAR spoofing or adversarial machine learning (AML). This narrow focus\noverlooks how an attack targeting one component can cascade harm throughout the\nrobot's interconnected systems. We address this gap through a systematization\nof knowledge (SoK) that takes a comprehensive approach, consolidating\nfragmented research from robotics, CPS, and network security domains. We\nintroduce a seven-layer security model for humanoid robots, organizing 39 known\nattacks and 35 defenses across the humanoid ecosystem-from hardware to\nhuman-robot interaction. Building on this security model, we develop a\nquantitative 39x35 attack-defense matrix with risk-weighted scoring, validated\nthrough Monte Carlo analysis. We demonstrate our method by evaluating three\nreal-world robots: Pepper, G1 EDU, and Digit. The scoring analysis revealed\nvarying security maturity levels, with scores ranging from 39.9% to 79.5%\nacross the platforms. This work introduces a structured, evidence-based\nassessment method that enables systematic security evaluation, supports\ncross-platform benchmarking, and guides prioritization of security investments\nin humanoid robotics."}
{"id": "2508.17851", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.17851", "abs": "https://arxiv.org/abs/2508.17851", "authors": ["Patrick Loic Foalem", "Leuson Da Silva", "Foutse Khomh", "Heng Li", "Ettore Merlo"], "title": "Logging Requirement for Continuous Auditing of Responsible Machine Learning-based Applications", "comment": null, "summary": "Machine learning (ML) is increasingly applied across industries to automate\ndecision-making, but concerns about ethical and legal compliance remain due to\nlimited transparency, fairness, and accountability. Monitoring through logging\na long-standing practice in traditional software offers a potential means for\nauditing ML applications, as logs provide traceable records of system behavior\nuseful for debugging, performance analysis, and continuous auditing.\nsystematically auditing models for compliance or accountability. The findings\nunderscore the need for enhanced logging practices and tooling that\nsystematically integrate responsible AI metrics. Such practices would support\nthe development of auditable, transparent, and ethically responsible ML\nsystems, aligning with growing regulatory requirements and societal\nexpectations. By highlighting specific deficiencies and opportunities, this\nwork provides actionable guidance for both practitioners and tool developers\nseeking to strengthen the accountability and trustworthiness of ML\napplications."}
{"id": "2508.17674", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.17674", "abs": "https://arxiv.org/abs/2508.17674", "authors": ["Qiming Guo", "Jinwen Tang", "Xingran Huang"], "title": "Attacking LLMs and AI Agents: Advertisement Embedding Attacks Against Large Language Models", "comment": "7 pages, 2 figures", "summary": "We introduce Advertisement Embedding Attacks (AEA), a new class of LLM\nsecurity threats that stealthily inject promotional or malicious content into\nmodel outputs and AI agents. AEA operate through two low-cost vectors: (1)\nhijacking third-party service-distribution platforms to prepend adversarial\nprompts, and (2) publishing back-doored open-source checkpoints fine-tuned with\nattacker data. Unlike conventional attacks that degrade accuracy, AEA subvert\ninformation integrity, causing models to return covert ads, propaganda, or hate\nspeech while appearing normal. We detail the attack pipeline, map five\nstakeholder victim groups, and present an initial prompt-based self-inspection\ndefense that mitigates these injections without additional model retraining.\nOur findings reveal an urgent, under-addressed gap in LLM security and call for\ncoordinated detection, auditing, and policy responses from the AI-safety\ncommunity."}
{"id": "2508.17882", "categories": ["cs.SE", "cs.SC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.17882", "abs": "https://arxiv.org/abs/2508.17882", "authors": ["Izudin Dzafic", "Rabih A. Jabr"], "title": "modelSolver: A Symbolic Model-Driven Solver for Power Network Simulation and Monitoring", "comment": null, "summary": "The development of advanced software tools for power system analysis requires\nextensive programming expertise. Even when using open-source tools, programming\nskills are essential to modify built-in models. This can be particularly\nchallenging for domain experts who lack coding proficiency. This paper\nintroduces modelSolver, a software solution with a new framework centered\naround symbolic mathematical modeling. The proposed paradigm facilitates\ndefining models through intuitive mathematical expressions, thus eliminating\nthe need for traditional programming constructs such as arrays, loops, and\nsparse matrix computations. The modelSolver focuses on power flow and state\nestimation using an open-box approach, which allows users to specify custom\nmodels using either real or complex variables. Unlike existing tools that rely\non hard-coded models, modelSolver enables the representation of a wide range of\nadvanced functionalities, including power flow with voltage regulators and load\ntap changers, continuation power flow, and Gauss-Newton state estimation with\nequality constraints. Compatibility with MATPOWER is ensured via a converter\nthat automates importing data files. The framework prioritizes model-driven\ndevelopment and empowers domain experts to focus on power system modeling\nwithout programming barriers. It aims to simplify power system computations,\nmaking them more accessible to students, scientists, and practitioners."}
{"id": "2508.17809", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.17809", "abs": "https://arxiv.org/abs/2508.17809", "authors": ["Abdullah Sahruri", "Martin Margala"], "title": "TLGLock: A New Approach in Logic Locking Using Key-Driven Charge Recycling in Threshold Logic Gates", "comment": "To appear in the 33rd IFIP/IEEE International Conference on Very\n  Large Scale Integration (VLSI-SoC 2025)", "summary": "Logic locking remains one of the most promising defenses against hardware\npiracy, yet current approaches often face challenges in scalability and design\noverhead. In this paper, we present TLGLock, a new design paradigm that\nleverages the structural expressiveness of Threshold Logic Gates (TLGs) and the\nenergy efficiency of charge recycling to enforce key-dependent functionality at\nthe gate level. By embedding the key into the gate's weighted logic and\nutilizing dynamic charge sharing, TLGLock provides a stateless and compact\nalternative to conventional locking techniques. We implement a complete\nsynthesis-to-locking flow and evaluate it using ISCAS, ITC, and MCNC\nbenchmarks. Results show that TLGLock achieves up to 30% area, 50% delay, and\n20% power savings compared to latch-based locking schemes. In comparison with\nXOR and SFLL-HD methods, TLGLock offers up to 3x higher SAT attack resistance\nwith significantly lower overhead. Furthermore, randomized key-weight\nexperiments demonstrate that TLGLock can reach up to 100% output corruption\nunder incorrect keys, enabling tunable security at minimal cost. These results\nposition TLGLock as a scalable and resilient solution for secure hardware\ndesign."}
{"id": "2508.17900", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17900", "abs": "https://arxiv.org/abs/2508.17900", "authors": ["Mohammed O. Alannsary"], "title": "A Defect Classification Framework for AI-Based Software Systems (AI-ODC)", "comment": "Article, 19 pages, 6 figures, 8 tables,", "summary": "Artificial Intelligence has gained a lot of attention recently, it has been\nutilized in several fields ranging from daily life activities, such as\nresponding to emails and scheduling appointments, to manufacturing and\nautomating work activities. Artificial Intelligence systems are mainly\nimplemented as software solutions, and it is essential to discover and remove\nsoftware defects to assure its quality using defect analysis which is one of\nthe major activities that contribute to software quality. Despite the\nproliferation of AI-based systems, current defect analysis models fail to\ncapture their unique attributes. This paper proposes a framework inspired by\nthe Orthogonal Defect Classification (ODC) paradigm and enables defect analysis\nof Artificial Intelligence systems while recognizing its special attributes and\ncharacteristics. This study demonstrated the feasibility of modifying ODC for\nAI systems to classify its defects. The ODC was adjusted to accommodate the\nData, Learning, and Thinking aspects of AI systems which are newly introduced\nclassification dimensions. This adjustment involved the introduction of an\nadditional attribute to the ODC attributes, the incorporation of a new severity\nlevel, and the substitution of impact areas with characteristics pertinent to\nAI systems. The framework was showcased by applying it to a publicly available\nMachine Learning bug dataset, with results analyzed through one-way and two-way\nanalysis. The case study indicated that defects occurring during the Learning\nphase were the most prevalent and were significantly linked to high-severity\nclassifications. In contrast, defects identified in the Thinking phase had a\ndisproportionate effect on trustworthiness and accuracy. These findings\nillustrate AIODC's capability to identify high-risk defect categories and\ninform focused quality assurance measures."}
{"id": "2508.17853", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.17853", "abs": "https://arxiv.org/abs/2508.17853", "authors": ["Saeed Alshehhi"], "title": "Software Unclonable Functions for IoT Devices Identification and Security", "comment": null, "summary": "In the evolving landscape of IoT ecosystem, distinguishing between legitimate\nand compromised devices is a critical challenge. This research investigates the\neffectiveness of hardware performance counter (HPC)-derived signatures'\nuniqueness under the umbrella of a concept that we introduced as software\nunclonable functions (SUFs)."}
{"id": "2508.17912", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.17912", "abs": "https://arxiv.org/abs/2508.17912", "authors": ["Mohammed O. Alannsary"], "title": "Evaluating Citizen Satisfaction with Saudi Arabia's E-Government Services: A Standards-Based, Theory-Informed Approach", "comment": "38 pages, 1 figure, 16 tables, journal research paper", "summary": "As digital government platforms become central to public service delivery,\nunderstanding citizen assessment is crucial for enhancing usability, trust, and\ninclusivity. This study investigates citizen satisfaction with the e-government\nservices in Saudi Arabia through a quality-in-use framework based on ISO/IEC\n25010 and ISO/IEC 25022 standards, interpreted through the lens of the Unified\nTheory of Acceptance and Use of Technology (UTAUT). A structured questionnaire\nwas administered to 500 citizens, yielding 276 valid responses. Satisfaction\nwas evaluated across four dimensions: overall satisfaction, feature\nsatisfaction, trust, and emotional engagement (pleasure). The findings\ndemonstrate consistently high levels of satisfaction regarding usability and\ntrust, aligning with Saudi Arabia's top-tier global ranking in e-government\ndevelopment. However, the results also highlight persistent challenges related\nto service clarity and system responsiveness. Emotional engagement was limited,\nindicating that users perceive these services primarily as functional tools\nrather than as engaging digital experiences. The study offers valuable insights\nfor policymakers and contributes to the theoretical integration of\nstandards-based and behavioral adoption models in the context of citizenship."}
{"id": "2508.17856", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.17856", "abs": "https://arxiv.org/abs/2508.17856", "authors": ["Tiezhu Sun", "Marco Alecci", "Aleksandr Pilgun", "Yewei Song", "Xunzhu Tang", "Jordan Samhi", "Tegawendé F. Bissyandé", "Jacques Klein"], "title": "MalLoc: Toward Fine-grained Android Malicious Payload Localization via LLMs", "comment": "Accepted at ICSME 2025, NIER Track", "summary": "The rapid evolution of Android malware poses significant challenges to the\nmaintenance and security of mobile applications (apps). Traditional detection\ntechniques often struggle to keep pace with emerging malware variants that\nemploy advanced tactics such as code obfuscation and dynamic behavior\ntriggering. One major limitation of these approaches is their inability to\nlocalize malicious payloads at a fine-grained level, hindering precise\nunderstanding of malicious behavior. This gap in understanding makes the design\nof effective and targeted mitigation strategies difficult, leaving mobile apps\nvulnerable to continuously evolving threats.\n  To address this gap, we propose MalLoc, a novel approach that leverages the\ncode understanding capabilities of large language models (LLMs) to localize\nmalicious payloads at a fine-grained level within Android malware. Our\nexperimental results demonstrate the feasibility and effectiveness of using\nLLMs for this task, highlighting the potential of MalLoc to enhance precision\nand interpretability in malware analysis. This work advances beyond traditional\ndetection and classification by enabling deeper insights into behavior-level\nmalicious logic and opens new directions for research, including dynamic\nmodeling of localized threats and targeted countermeasure development."}
{"id": "2508.17988", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.17988", "abs": "https://arxiv.org/abs/2508.17988", "authors": ["Eduardo de Conto", "Blaise Genest", "Arvind Easwaran", "Nicholas Ng", "Shweta Menon"], "title": "DesCartes Builder: A Tool to Develop Machine-Learning Based Digital Twins", "comment": "5 pages, 4 figures. Accepted at EDTconf 2025", "summary": "Digital twins (DTs) are increasingly utilized to monitor, manage, and\noptimize complex systems across various domains, including civil engineering. A\ncore requirement for an effective DT is to act as a fast, accurate, and\nmaintainable surrogate of its physical counterpart, the physical twin (PT). To\nthis end, machine learning (ML) is frequently employed to (i) construct\nreal-time DT prototypes using efficient reduced-order models (ROMs) derived\nfrom high-fidelity simulations of the PT's nominal behavior, and (ii)\nspecialize these prototypes into DT instances by leveraging historical sensor\ndata from the target PT. Despite the broad applicability of ML, its use in DT\nengineering remains largely ad hoc. Indeed, while conventional ML pipelines\noften train a single model for a specific task, DTs typically require multiple,\ntask- and domain-dependent models. Thus, a more structured approach is required\nto design DTs.\n  In this paper, we introduce DesCartes Builder, an open-source tool to enable\nthe systematic engineering of ML-based pipelines for real-time DT prototypes\nand DT instances. The tool leverages an open and flexible visual data flow\nparadigm to facilitate the specification, composition, and reuse of ML models.\nIt also integrates a library of parameterizable core operations and ML\nalgorithms tailored for DT design. We demonstrate the effectiveness and\nusability of DesCartes Builder through a civil engineering use case involving\nthe design of a real-time DT prototype to predict the plastic strain of a\nstructure."}
{"id": "2508.17884", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.17884", "abs": "https://arxiv.org/abs/2508.17884", "authors": ["Toby Murray"], "title": "PhantomLint: Principled Detection of Hidden LLM Prompts in Structured Documents", "comment": null, "summary": "Hidden LLM prompts have appeared in online documents with increasing\nfrequency. Their goal is to trigger indirect prompt injection attacks while\nremaining undetected from human oversight, to manipulate LLM-powered automated\ndocument processing systems, against applications as diverse as r\\'esum\\'e\nscreeners through to academic peer review processes. Detecting hidden LLM\nprompts is therefore important for ensuring trust in AI-assisted human decision\nmaking.\n  This paper presents the first principled approach to hidden LLM prompt\ndetection in structured documents. We implement our approach in a prototype\ntool called PhantomLint. We evaluate PhantomLint against a corpus of 3,402\ndocuments, including both PDF and HTML documents, and covering academic paper\npreprints, CVs, theses and more. We find that our approach is generally\napplicable against a wide range of methods for hiding LLM prompts from visual\ninspection, has a very low false positive rate (approx. 0.092%), is practically\nuseful for detecting hidden LLM prompts in real documents, while achieving\nacceptable performance."}
{"id": "2508.18003", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18003", "abs": "https://arxiv.org/abs/2508.18003", "authors": ["Robert Heumüller", "Frank Ortmeier"], "title": "Previously on... Automating Code Review", "comment": "Preprint currently under review", "summary": "Modern Code Review (MCR) is a standard practice in software engineering, yet\nit demands substantial time and resource investments. Recent research has\nincreasingly explored automating core review tasks using machine learning (ML)\nand deep learning (DL). As a result, there is substantial variability in task\ndefinitions, datasets, and evaluation procedures. This study provides the first\ncomprehensive analysis of MCR automation research, aiming to characterize the\nfield's evolution, formalize learning tasks, highlight methodological\nchallenges, and offer actionable recommendations to guide future research.\nFocusing on the primary code review tasks, we systematically surveyed 691\npublications and identified 24 relevant studies published between May 2015 and\nApril 2024. Each study was analyzed in terms of tasks, models, metrics,\nbaselines, results, validity concerns, and artifact availability. In\nparticular, our analysis reveals significant potential for standardization,\nincluding 48 task metric combinations, 22 of which were unique to their\noriginal paper, and limited dataset reuse. We highlight challenges and derive\nconcrete recommendations for examples such as the temporal bias threat, which\nare rarely addressed so far. Our work contributes to a clearer overview of the\nfield, supports the framing of new research, helps to avoid pitfalls, and\npromotes greater standardization in evaluation practices."}
{"id": "2508.17913", "categories": ["cs.CR", "cs.ET", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.17913", "abs": "https://arxiv.org/abs/2508.17913", "authors": ["Yagmur Yigit", "Mehmet Ali Erturk", "Kerem Gursu", "Berk Canberk"], "title": "PRZK-Bind: A Physically Rooted Zero-Knowledge Authentication Protocol for Secure Digital Twin Binding in Smart Cities", "comment": "6 pages, 4 figures, 2 tables, Accepted by IEEE Global Communications\n  Conference (GLOBECOM) 2025", "summary": "Digital twin (DT) technology is rapidly becoming essential for smart city\necosystems, enabling real-time synchronisation and autonomous decision-making\nacross physical and digital domains. However, as DTs take active roles in\ncontrol loops, securely binding them to their physical counterparts in dynamic\nand adversarial environments remains a significant challenge. Existing\nauthentication solutions either rely on static trust models, require\ncentralised authorities, or fail to provide live and verifiable\nphysical-digital binding, making them unsuitable for latency-sensitive and\ndistributed deployments. To address this gap, we introduce PRZK-Bind, a\nlightweight and decentralised authentication protocol that combines\nSchnorr-based zero-knowledge proofs with elliptic curve cryptography to\nestablish secure, real-time correspondence between physical entities and DTs\nwithout relying on pre-shared secrets. Simulation results show that PRZK-Bind\nsignificantly improves performance, offering up to 4.5 times lower latency and\n4 times reduced energy consumption compared to cryptography-heavy baselines,\nwhile maintaining false acceptance rates more than 10 times lower. These\nfindings highlight its suitability for future smart city deployments requiring\nefficient, resilient, and trustworthy DT authentication."}
{"id": "2508.18070", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18070", "abs": "https://arxiv.org/abs/2508.18070", "authors": ["Karolina M. Milano", "Wesley K. G. Assunção", "Bruno B. P. Cafeo"], "title": "A Large-Scale Study on Developer Engagement and Expertise in Configurable Software System Projects", "comment": null, "summary": "Modern systems operate in multiple contexts making variability a fundamental\naspect of Configurable Software Systems (CSSs). Variability, implemented via\npre-processor directives (e.g., #ifdef blocks) interleaved with other code and\nspread across files, complicates maintenance and increases error risk. Despite\nits importance, little is known about how variable code is distributed among\ndevelopers or whether conventional expertise metrics adequately capture\nvariable code proficiency. This study investigates developers' engagement with\nvariable versus mandatory code, the concentration of variable code workload,\nand the effectiveness of expertise metrics in CSS projects. We mined\nrepositories of 25 CSS projects, analyzing 450,255 commits from 9,678\ndevelopers. Results show that 59% of developers never modified variable code,\nwhile about 17% were responsible for developing and maintaining 83% of it. This\nindicates a high concentration of variable code expertise among a few\ndevelopers, suggesting that task assignments should prioritize these\nspecialists. Moreover, conventional expertise metrics performed\npoorly--achieving only around 55% precision and 50% recall in identifying\ndevelopers engaged with variable code. Our findings highlight an unbalanced\ndistribution of variable code responsibilities and underscore the need to\nrefine expertise metrics to better support task assignments in CSS projects,\nthereby promoting a more equitable workload distribution."}
{"id": "2508.17964", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.17964", "abs": "https://arxiv.org/abs/2508.17964", "authors": ["Yuhe Lu", "Zhongwen Li", "Xiaoqi Li"], "title": "MoveScanner: Analysis of Security Risks of Move Smart Contracts", "comment": null, "summary": "As blockchain technology continues to evolve, the security of smart contracts\nhas increasingly drawn attention from both academia and industry. The Move\nlanguage, with its unique resource model and linear type system, provides a\nsolid foundation for the security of digital assets. However, smart contracts\nstill face new security challenges due to developer programming errors and the\npotential risks associated with cross-module interactions. This paper\nsystematically analyzes the limitations of existing security tools within the\nMove ecosystem and reveals their unique vulnerability patterns. To address\nthese issues, it introduces MoveScanner, a static analysis tool based on a\ncontrol flow graph and data flow analysis architecture. By incorporating\ncross-module call graph tracking, MoveScanner can effectively identify five key\ntypes of security vulnerabilities, including resource leaks, weak permission\nmanagement, and arithmetic overflows. In terms of design, MoveScanner adheres\nto a modular principle, supports bytecode-level analysis and multi-chain\nadaptation, and introduces innovative resource trajectory tracking algorithms\nand capability matrix analysis methods, thereby significantly reducing the\nfalse positive rate. Empirical results show that MoveScanner achieved 88.2%\ndetection accuracy in benchmark testing, filling the gap in security tools in\nthe Move ecosystem. Furthermore, this paper identifies twelve new types of\nsecurity risks based on the resource-oriented programming paradigm and provides\na theoretical foundation and practical experience for the development of smart\ncontract security mechanisms. Future work will focus on combining formal\nverification and dynamic analysis techniques to build a security protection\nframework covering the entire contract lifecycle"}
{"id": "2508.18073", "categories": ["cs.SE", "cs.DL"], "pdf": "https://arxiv.org/pdf/2508.18073", "abs": "https://arxiv.org/abs/2508.18073", "authors": ["Joenio Marques da Costa", "Christina von Flach"], "title": "Debian in the Research Software Ecosystem: A Bibliometric Analysis", "comment": "5 pages; 3 figures; 2 tables; to be published in DebConf25 Academic\n  Track https://www.diverse-team.fr/debconf25-academictrack", "summary": "Context: The Debian system has historically participated in academic works\nand scientific projects, with well-known examples including NeuroDebian, Debian\nMed, Debsources, Debian Science, and Debian GIS, where the scientific relevance\nof Debian and its contribution to the Research Software ecosystem are evident.\n  Objective: The objective of this study is to investigate the Debian system\nthrough academic publications, with the aim of classifying articles, mapping\nresearch, identifying trends, and finding opportunities.\n  Method: The study is based on a bibliometric analysis starting with an\ninitial search for the term \"Debian\" in the titles, abstracts, or keywords of\nacademic publications, using the Scopus database. This analysis calculates\nmetrics of co-citation, co-authorship, and word co-occurrence, and is guided by\na set of research questions and criteria for inclusion and exclusion to conduct\nthe bibliometric analysis.\n  Results: The study includes a set of articles published across various fields\nof knowledge, providing a map of the academic publication space about Debian.\nThe study's data will be available in a public repository, reporting\ndemographic and bibliometric trends, including the most cited articles, active\ncountries, researchers, and popular conferences.\n  Conclusion: Results includes a bibliometric and demographic analysis\nidentified in publications about Debian, shedding light on the intellectual\nstructure of academic research. The results of the analyses can help\nresearchers gain an overview of existing trends in publications about Debian\nand identify areas that require more attention from the scientific community."}
{"id": "2508.18109", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.18109", "abs": "https://arxiv.org/abs/2508.18109", "authors": ["Lingxiao Wang", "Wenjing Dang", "Mengyao Zhang", "Yue Wang", "Xianzong Wu", "Sen Chen"], "title": "Aligning Core Aspects: Improving Vulnerability Proof-of-Concepts via Cross-Source Insights", "comment": null, "summary": "For vulnerabilities, Proof-of-Concept (PoC) plays an irreplaceable role in\ndemonstrating the exploitability. PoC reports may include critical information\nsuch as specific usage, test platforms, and more, providing essential insights\nfor researchers. However, in reality, due to various PoC templates across PoC\nplatforms, PoC reports extensively suffer from information deficiency, leading\nthe suboptimal quality and limited usefulness. Fortunately, we found that\ninformation deficiency of PoC reports could be mitigated by the completion from\nmultiple sources given the same referred vulnerability. In this paper, we\nconduct the first study on the deficiency of information in PoC reports across\npublic platforms. We began by collecting 173,170 PoC reports from 4 different\nplatforms and defined 8 key aspects that PoCs should contain. By integrating\nrule-based matching and a fine-tuned BERT-NER model for extraction of key\naspects, we discovered that all PoC reports available on public platforms have\nat least one missing key aspect. Subsequently, we developed a multi-source\ninformation fusion method to complete the missing aspect information in PoC\nreports by leveraging CVE entries and related PoC reports from different\nsources. Finally, we successfully completed 69,583 PoC reports (40.18% of all\nreports)."}
{"id": "2508.18089", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18089", "abs": "https://arxiv.org/abs/2508.18089", "authors": ["Karine Even-Mendoza", "Alexander Brownlee", "Alina Geiger", "Carol Hanna", "Justyna Petke", "Federica Sarro", "Dominik Sobania"], "title": "LLM-Guided Genetic Improvement: Envisioning Semantic Aware Automated Software Evolution", "comment": null, "summary": "Genetic Improvement (GI) of software automatically creates alternative\nsoftware versions that are improved according to certain properties of\ninterests (e.g., running-time). Search-based GI excels at navigating large\nprogram spaces, but operates primarily at the syntactic level. In contrast,\nLarge Language Models (LLMs) offer semantic-aware edits, yet lack goal-directed\nfeedback and control (which is instead a strength of GI). As such, we propose\nthe investigation of a new research line on AI-powered GI aimed at\nincorporating semantic aware search. We take a first step at it by augmenting\nGI with the use of automated clustering of LLM edits. We provide initial\nempirical evidence that our proposal, dubbed PatchCat, allows us to\nautomatically and effectively categorize LLM-suggested patches. PatchCat\nidentified 18 different types of software patches and categorized newly\nsuggested patches with high accuracy. It also enabled detecting NoOp edits in\nadvance and, prospectively, to skip test suite execution to save resources in\nmany cases. These results, coupled with the fact that PatchCat works with\nsmall, local LLMs, are a promising step toward interpretable, efficient, and\ngreen GI. We outline a rich agenda of future work and call for the community to\njoin our vision of building a principled understanding of LLM-driven mutations,\nguiding the GI search process with semantic signals."}
{"id": "2508.18148", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18148", "abs": "https://arxiv.org/abs/2508.18148", "authors": ["Haijian Ma", "Daizong Liu", "Xiaowen Cai", "Pan Zhou", "Yulai Xie"], "title": "Learning from Few Samples: A Novel Approach for High-Quality Malcode Generation", "comment": "18pages,5 figures,emnlp", "summary": "Intrusion Detection Systems (IDS) play a crucial role in network security\ndefense. However, a significant challenge for IDS in training detection models\nis the shortage of adequately labeled malicious samples. To address these\nissues, this paper introduces a novel semi-supervised framework\n\\textbf{GANGRL-LLM}, which integrates Generative Adversarial Networks (GANs)\nwith Large Language Models (LLMs) to enhance malicious code generation and SQL\nInjection (SQLi) detection capabilities in few-sample learning scenarios.\nSpecifically, our framework adopts a collaborative training paradigm where: (1)\nthe GAN-based discriminator improves malicious pattern recognition through\nadversarial learning with generated samples and limited real samples; and (2)\nthe LLM-based generator refines the quality of malicious code synthesis using\nreward signals from the discriminator. The experimental results demonstrate\nthat even with a limited number of labeled samples, our training framework is\nhighly effective in enhancing both malicious code generation and detection\ncapabilities. This dual enhancement capability offers a promising solution for\ndeveloping adaptive defense systems capable of countering evolving cyber\nthreats."}
{"id": "2508.18106", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18106", "abs": "https://arxiv.org/abs/2508.18106", "authors": ["Keke Lian", "Bin Wang", "Lei Zhang", "Libo Chen", "Junjie Wang", "Ziming Zhao", "Yujiu Yang", "Haotong Duan", "Haoran Zhao", "Shuang Liao", "Mingda Guo", "Jiazheng Quan", "Yilu Zhong", "Chenhao He", "Zichuan Chen", "Jie Wu", "Haoling Li", "Zhaoxuan Li", "Jiongchi Yu", "Hui Li", "Dong Zhang"], "title": "A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code", "comment": null, "summary": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks are inadequate, as they focus on isolated code\nsnippets, employ unstable evaluation methods that lack reproducibility, and\nfail to connect the quality of input context with the security of the output.\nTo address these gaps, we introduce A.S.E (AI Code Generation Security\nEvaluation), a benchmark for repository-level secure code generation. A.S.E\nconstructs tasks from real-world repositories with documented CVEs, preserving\nfull repository context like build systems and cross-file dependencies. Its\nreproducible, containerized evaluation framework uses expert-defined rules to\nprovide stable, auditable assessments of security, build quality, and\ngeneration stability. Our evaluation of leading LLMs on A.S.E reveals three key\nfindings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The\nsecurity gap between proprietary and open-source models is narrow;\nQwen3-235B-A22B-Instruct attains the top security score. (3) Concise,\n``fast-thinking'' decoding strategies consistently outperform complex,\n``slow-thinking'' reasoning for security patching."}
{"id": "2508.18155", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.18155", "abs": "https://arxiv.org/abs/2508.18155", "authors": ["Muhammad Ali Nadeem", "Bishwo Prakash Pokharel", "Naresh Kshetri", "Achyut Shankar", "Gokarna Sharma"], "title": "$AutoGuardX$: A Comprehensive Cybersecurity Framework for Connected Vehicles", "comment": "16 pages, 3 figures, 8 tables", "summary": "The rapid integration of Internet of Things (IoT) and interconnected systems\nin modern vehicles not only introduced a new era of convenience, automation,\nand connected vehicles but also elevated their exposure to sophisticated cyber\nthreats. This is especially evident in US and Canada, where cyber-enabled auto\ntheft has surged in recent years, revealing the limitations of existing\nsecurity measures for connected vehicles. In response, this paper proposes\n$AutoGuardX$, a comprehensive cybersecurity framework designed specifically for\nconnected vehicles. $AutoGuardX$ combines key elements from existing recognized\nstandards for vehicle security, such as ISO/SAE 21434 and ISO 26262, with\nadvanced technologies, including machine learning-based anomaly detection, IoT\nsecurity protocols, and encrypted communication channels. The framework\naddresses major attack vectors like relay attacks, controller area network\n(CAN) bus intrusions, and vulnerabilities introduced by emerging technologies\nsuch as 5G and quantum computing. $AutoGuardX$ is extensively evaluated through\nsecurity simulations across a mix of Sedans and SUVs from four major vehicle\nbrands manufactured between 2019 and 2023. The results demonstrate the\nframework's adaptability, scalability, and practical effectiveness against\nexisting and emerging threats."}
{"id": "2508.16625", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16625", "abs": "https://arxiv.org/abs/2508.16625", "authors": ["Rijha Safdar", "Danyail Mateen", "Syed Taha Ali", "M. Umer Ashfaq", "Wajahat Hussain"], "title": "Data and Context Matter: Towards Generalizing AI-based Software Vulnerability Detection", "comment": null, "summary": "The performance of AI-based software vulnerability detection systems is often\nlimited by their poor generalization to unknown codebases. In this research, we\nexplore the impact of data quality and model architecture on the\ngeneralizability of vulnerability detection systems. By generalization we mean\nability of high vulnerability detection performance across different C/C++\nsoftware projects not seen during training. Through a series of experiments, we\ndemonstrate that improvements in dataset diversity and quality substantially\nenhance detection performance. Additionally, we compare multiple encoder-only\nand decoder-only models, finding that encoder based models outperform in terms\nof accuracy and generalization. Our model achieves 6.8% improvement in recall\non the benchmark BigVul[1] dataset, also outperforming on unseen projects,\nhence showing enhanced generalizability. These results highlight the role of\ndata quality and model selection in the development of robust vulnerability\ndetection systems. Our findings suggest a direction for future systems having\nhigh cross-project effectiveness."}
{"id": "2508.18230", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18230", "abs": "https://arxiv.org/abs/2508.18230", "authors": ["Chitraksh Singh", "Monisha Dhanraj", "Ken Huang"], "title": "KillChainGraph: ML Framework for Predicting and Mapping ATT&CK Techniques", "comment": "8 pages, 3 figures", "summary": "The escalating complexity and volume of cyberattacks demand proactive\ndetection strategies that go beyond traditional rule-based systems. This paper\npresents a phase-aware, multi-model machine learning framework that emulates\nadversarial behavior across the seven phases of the Cyber Kill Chain using the\nMITRE ATT&CK Enterprise dataset. Techniques are semantically mapped to phases\nvia ATTACK-BERT, producing seven phase-specific datasets. We evaluate LightGBM,\na custom Transformer encoder, fine-tuned BERT, and a Graph Neural Network\n(GNN), integrating their outputs through a weighted soft voting ensemble.\nInter-phase dependencies are modeled using directed graphs to capture attacker\nmovement from reconnaissance to objectives. The ensemble consistently achieved\nthe highest scores, with F1-scores ranging from 97.47% to 99.83%, surpassing\nGNN performance (97.36% to 99.81%) by 0.03%--0.20% across phases. This\ngraph-driven, ensemble-based approach enables interpretable attack path\nforecasting and strengthens proactive cyber defense."}
{"id": "2508.16662", "categories": ["cs.CR", "cs.CY", "cs.NI", "cs.SE", "K.6.5; C.2.0; D.4.6"], "pdf": "https://arxiv.org/pdf/2508.16662", "abs": "https://arxiv.org/abs/2508.16662", "authors": ["Alexander Tabalipa"], "title": "Bridging the Mobile Trust Gap: A Zero Trust Framework for Consumer-Facing Applications", "comment": "43 pages, 5 figures, 9 tables. Working Paper - Version 1.0. Submitted\n  under a CC BY-SA 4.0 license. Also available as an SSRN Working Paper.\n  Feedback and collaboration are welcome", "summary": "Zero Trust Architecture (ZTA) has become a widely adopted model for securing\nenterprise environments, promoting continuous verification and minimal trust\nacross systems. However, its application in mobile contexts remains limited,\ndespite mobile applications now accounting for most global digital interactions\nand being increasingly targeted by sophisticated threats. Existing Zero Trust\nframeworks developed by organisations such as the National Institute of\nStandards and Technology (NIST) and the Cybersecurity and Infrastructure\nSecurity Agency (CISA) primarily focus on enterprise-managed infrastructure,\nassuming organisational control over devices, networks, and identities. This\npaper addresses a critical gap by proposing an extended Zero Trust model\ndesigned for mobile applications operating in untrusted, user-controlled\nenvironments. Using a design science methodology, the study introduced a\nsix-pillar framework that supports runtime enforcement of trust through\ncontrols including device integrity, user identity validation, data protection,\nsecure application programming interface (API) usage, behavioural monitoring,\nand live application protection. Each pillar was mapped to relevant regulatory\nand security standards to support compliance. A phased implementation roadmap\nand maturity assessment model were also developed to guide adoption across\nvarying organisational contexts. The proposed model offers a practical and\nstandards-aligned approach to securing mobile applications beyond\npre-deployment controls, aligning real-time enforcement with Zero Trust\nprinciples. This contribution expands the operational boundaries of ZTA and\nprovides organisations with a deployable path to reduce fraud, enhance\ncompliance, and address emerging mobile security challenges. Future research\nmay include empirical validation of the framework and cross-sector application\ntesting."}
{"id": "2508.17856", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.17856", "abs": "https://arxiv.org/abs/2508.17856", "authors": ["Tiezhu Sun", "Marco Alecci", "Aleksandr Pilgun", "Yewei Song", "Xunzhu Tang", "Jordan Samhi", "Tegawendé F. Bissyandé", "Jacques Klein"], "title": "MalLoc: Toward Fine-grained Android Malicious Payload Localization via LLMs", "comment": "Accepted at ICSME 2025, NIER Track", "summary": "The rapid evolution of Android malware poses significant challenges to the\nmaintenance and security of mobile applications (apps). Traditional detection\ntechniques often struggle to keep pace with emerging malware variants that\nemploy advanced tactics such as code obfuscation and dynamic behavior\ntriggering. One major limitation of these approaches is their inability to\nlocalize malicious payloads at a fine-grained level, hindering precise\nunderstanding of malicious behavior. This gap in understanding makes the design\nof effective and targeted mitigation strategies difficult, leaving mobile apps\nvulnerable to continuously evolving threats.\n  To address this gap, we propose MalLoc, a novel approach that leverages the\ncode understanding capabilities of large language models (LLMs) to localize\nmalicious payloads at a fine-grained level within Android malware. Our\nexperimental results demonstrate the feasibility and effectiveness of using\nLLMs for this task, highlighting the potential of MalLoc to enhance precision\nand interpretability in malware analysis. This work advances beyond traditional\ndetection and classification by enabling deeper insights into behavior-level\nmalicious logic and opens new directions for research, including dynamic\nmodeling of localized threats and targeted countermeasure development."}
