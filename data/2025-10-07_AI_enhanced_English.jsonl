{"id": "2510.03461", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03461", "abs": "https://arxiv.org/abs/2510.03461", "authors": ["Sanjay Malakar", "Michael D. Ernst", "Martin Kellogg", "Manu Sridharan"], "title": "Repairing Leaks in Resource Wrappers", "comment": null, "summary": "A resource leak occurs when a program fails to release a finite resource like\na socket, file descriptor or database connection. While sound static analysis\ntools can detect all leaks, automatically repairing them remains challenging.\nPrior work took the output of a detection tool and attempted to repair only\nleaks from a hard-coded list of library resource types. That approach limits\nthe scope of repairable leaks: real-world code uses resource wrappers that\nstore a resource in a field and must themselves be closed. This paper makes\nfour key contributions to improve resource leak repair in the presence of\nwrappers. (1) It integrates inference of resource management specifications\ninto the repair pipeline, enabling extant fixing approaches to reason about\nwrappers. (2) It transforms programs into variants that are easier to analyze,\nmaking inference, detection, and fixing tools more effective; for instance, it\nmakes detection tools report problems closer to the root cause, often in a\nclient of a resource wrapper rather than within the wrapper class itself. (3) A\nnovel field containment analysis reasons about resource lifetimes, enabling\nrepair of more leaks involving resources stored in fields. (4) It introduces a\nnew repair pattern and more precise reasoning to better handle resources stored\nin non-final fields. Prior work fixed 41% of resource leak warnings in the NJR\nbenchmark suite; our implementation Arodnap fixes 68%.", "AI": {"tldr": "The paper presents a novel approach to automatically repair resource leaks in programs by extending existing leak detection and repair techniques to handle resource wrappers. It introduces four key contributions, including specification inference, program transformation for better analysis, field lifetime analysis, and a new repair pattern, achieving a 68% success rate in the NJR benchmark suite.", "motivation": "Resource leaks are a common issue in software where programs fail to release finite resources. While static analysis can detect these leaks, automatically repairing them is still a challenge, especially when resources are wrapped and stored in fields, which previous methods couldn't handle effectively.", "method": "The authors propose four improvements to resource leak repair: (1) integrating specification inference into the repair pipeline to reason about wrappers, (2) transforming programs for easier analysis, (3) implementing a field containment analysis for resource lifetime handling, and (4) introducing a new repair pattern for non-final fields.", "result": "Their implementation, Arodnap, fixes 68% of resource leak warnings in the NJR benchmark, improving upon previous work that achieved 41%.", "conclusion": "The approach significantly enhances the effectiveness of resource leak repair by addressing wrapper-based leaks more comprehensively. The success rate of automated repair jumps to 68%, showing the impact of reasoning about field-contained resources and non-final fields."}}
{"id": "2510.03463", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03463", "abs": "https://arxiv.org/abs/2510.03463", "authors": ["Vali Tawosi", "Keshav Ramani", "Salwa Alamir", "Xiaomo Liu"], "title": "ALMAS: an Autonomous LLM-based Multi-Agent Software Engineering Framework", "comment": null, "summary": "Multi-agent Large Language Model (LLM) systems have been leading the way in\napplied LLM research across a number of fields. One notable area is software\ndevelopment, where researchers have advanced the automation of code\nimplementation, code testing, code maintenance, inter alia, using LLM agents.\nHowever, software development is a multifaceted environment that extends beyond\njust code. As such, a successful LLM system must factor in multiple stages of\nthe software development life-cycle (SDLC). In this paper, we propose a vision\nfor ALMAS, an Autonomous LLM-based Multi-Agent Software Engineering framework,\nwhich follows the above SDLC philosophy such that it may work within an agile\nsoftware development team to perform several tasks end-to-end. ALMAS aligns its\nagents with agile roles, and can be used in a modular fashion to seamlessly\nintegrate with human developers and their development environment. We showcase\nthe progress towards ALMAS through our published works and a use case\ndemonstrating the framework, where ALMAS is able to seamlessly generate an\napplication and add a new feature.", "AI": {"tldr": "TL;DR: This paper proposes ALMAS, an autonomous multi-agent LLM-based framework for software engineering that aligns with agile methodologies. ALMAS aims to perform multiple stages of the software development life cycle end-to-end and integrate with human teams.", "motivation": "The motivation of the paper is driven by the complexity and multifaceted nature of software development, which suggests the need for a more comprehensive approach for LLM systems. Current LLM agents primarily focus on code implementation, testing, and maintenance, but agile software development requires a more integrated and end-to-end system to handle the full spectrum of SDLC activities.", "method": "To achieve the proposed vision, the authors focus on developing a framework that aligns LLM agent functionalities with agile software development roles and multiple stages of the software development life cycle (SDLC). The framework is designed to be modular, allowing it to naturally integrate with human developers and existing development environments. The approach is validated using use cases and results from related published works that have contributed to the foundation of ALMAS.", "result": "Results presented in the paper include the successful demonstration of ALMAS, where the framework was able to autonomously generate an application and implement a new feature through its agent architecture. These findings are supported by the authors' published works and a specific use case illustrating ALMAS's integration and functionality within the SDLC.", "conclusion": "In conclusion, the authors presented a vision for ALMAS as a comprehensive autonomous multi-agent framework for LLM-based software engineering. They emphasize the potential of ALMAS to be used in agile development workflows, where its modular design allows for flexible use and seamless integration with human teams and environments."}}
{"id": "2510.03474", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03474", "abs": "https://arxiv.org/abs/2510.03474", "authors": ["Nadeeshan De Silva", "Martin Kellogg", "Oscar Chaparro"], "title": "Relative Code Comprehensibility Prediction", "comment": null, "summary": "Automatically predicting how difficult it is for humans to understand a code\nsnippet can assist developers in tasks like deciding when and where to\nrefactor. Despite many proposed code comprehensibility metrics, studies have\nshown they often correlate poorly with actual measurements of human\ncomprehensibility. This has motivated the use of machine learning models to\npredict human comprehensibility directly from code, but these models have also\nshown limited accuracy.\n  We argue that model inaccuracy stems from inherent noise in human\ncomprehensibility data, which confuses models trained to predict it directly.\nTo address this, we propose training models to predict the relative\ncomprehensibility of two code snippets - that is, predicting which snippet a\nhuman would find easier to understand without predicting each snippet's\ncomprehensibility in isolation. This mitigates noise in predicting 'absolute'\ncomprehensibility measurements, but is still useful for downstream\nsoftware-engineering tasks like assessing whether refactoring improves or\nhinders comprehensibility.\n  We conducted a study to assess and compare the effectiveness of absolute and\nrelative code comprehensibility prediction via machine learning. We used a\ndataset of 150 Java code snippets and 12.5k human comprehensibility\nmeasurements from prior user studies, comparing the models' performance with\nnaive baselines (eg 'always predict the majority class'). Our findings indicate\nthat absolute comprehensibility models improve over the baselines by at most\n33.4% and frequently underperform. In contrast, relative comprehensibility\nmodels are substantially better, with average improvements of 137.8% and 74.7%\nfor snippet-wise and developer-wise prediction, respectively. These results\nsuggest that relative comprehensibility models learn more effectively from the\ndata, supporting their practical applicability for downstream SE tasks.", "AI": {"tldr": "This paper introduces a more accurate approach to predicting code comprehensibility by focusing on relative comparisons rather than absolute measurements, finding that this method improves model performance significantly over traditional methods and naive baselines.", "motivation": "Current code comprehensibility metrics and machine learning models struggle with accuracy due to noise in human understanding measurements. Developers need tools to decide when to refactor code, and the paper addresses this by proposing a better model approach.", "method": "The authors propose models that predict which of two code snippets is more comprehensible to a human, instead of evaluating each snippet's absolute comprehensibility. They compare the effectiveness of absolute and relative prediction models using 150 Java snippets and existing 12.5k human measurements in a study.", "result": "Absolute models only improved over baselines by up to 33.4% and often underperformed. Relative models improved performance by 137.8% and 74.7% for snippet-wise and developer-wise predictions, respectively, outperforming naive baselines and absolute models.", "conclusion": "Relative comprehensibility models are more effective and robust against data noise, making them practical for software engineering tasks. This approach outperforms current methods, allowing better assessment of factors like refactoring impact without requiring absolute ease metrics."}}
{"id": "2510.03480", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03480", "abs": "https://arxiv.org/abs/2510.03480", "authors": ["Vali Tawosi", "Salwa Alamir", "Xiaomo Liu", "Manuela Veloso"], "title": "LLM Agents for Automated Dependency Upgrades", "comment": null, "summary": "As a codebase expands over time, its library dependencies can become outdated\nand require updates to maintain innovation and security. However, updating a\nlibrary can introduce breaking changes in the code, necessitating significant\ndeveloper time for maintenance. To address this, we introduce a framework of\nLLM agents to be used in combination with migration documentation to\nautomatically recommend and apply code updates and ensure compatibility with\nnew versions. Our solution can automatically localize updated library usages in\nlive Java codebases and implement recommended fixes in a user-friendly manner.\nThe system architecture consists of multiple key components: a Summary Agent,\nControl Agent, and Code Agent. To validate our approach, we apply the framework\non an industrial use case by which we create three synthetic code repositories\nwith major Upgrade changes and benchmark our approach against state-of-the-art\nmethods. Results show that our approach not only performs upgrades using fewer\ntokens across all cases but also achieves a precision of 71.4%, highlighting\nits efficiency and effectiveness compared to state-of-the-art methods.", "AI": {"tldr": "The paper proposes an LLM agent framework to automatically update and maintain Java codebases by leveraging migration documentation, achieving high precision and efficiency.", "motivation": "Outdated library dependencies in expanding codebases demand updates for innovation and security, which often require substantial developer time due to breaking changes.", "method": "A system architecture comprising Summary, Control, and Code Agents is introduced, integrating LLM capabilities with migration documentation to recommend and apply compatible code updates.", "result": "The framework achieves 71.4% precision and uses fewer tokens than state-of-the-art methods when benchmarked across three synthetic repositories with major upgrades.", "conclusion": "The LLM agent-based approach provides an efficient and effective automated solution for library upgrades, significantly reducing token usage and maintenance effort."}}
{"id": "2510.03319", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03319", "abs": "https://arxiv.org/abs/2510.03319", "authors": ["Chenxiang Luo", "David K. Y. Yau", "Qun Song"], "title": "SVDefense: Effective Defense against Gradient Inversion Attacks via Singular Value Decomposition", "comment": null, "summary": "Federated learning (FL) enables collaborative model training without sharing\nraw data but is vulnerable to gradient inversion attacks (GIAs), where\nadversaries reconstruct private data from shared gradients. Existing defenses\neither incur impractical computational overhead for embedded platforms or fail\nto achieve privacy protection and good model utility at the same time.\nMoreover, many defenses can be easily bypassed by adaptive adversaries who have\nobtained the defense details. To address these limitations, we propose\nSVDefense, a novel defense framework against GIAs that leverages the truncated\nSingular Value Decomposition (SVD) to obfuscate gradient updates. SVDefense\nintroduces three key innovations, a Self-Adaptive Energy Threshold that adapts\nto client vulnerability, a Channel-Wise Weighted Approximation that selectively\npreserves essential gradient information for effective model training while\nenhancing privacy protection, and a Layer-Wise Weighted Aggregation for\neffective model aggregation under class imbalance. Our extensive evaluation\nshows that SVDefense outperforms existing defenses across multiple\napplications, including image classification, human activity recognition, and\nkeyword spotting, by offering robust privacy protection with minimal impact on\nmodel accuracy. Furthermore, SVDefense is practical for deployment on various\nresource-constrained embedded platforms. We will make our code publicly\navailable upon paper acceptance.", "AI": {"tldr": "Federated learning (FL) is susceptible to gradient inversion attacks (GIAs), from which adversaries reconstruct private training data from model gradients. In response, the authors propose a novel FL defense framework known as SVDefense that effectively addresses these privacy vulnerabilities. It is founded on the use of truncated SVD for obfuscating gradients and introduces three key innovations: a Self-Adaptive Energy Threshold, a Channel-Wise Weighted Approximation, and a Layer-Wise Weighted Aggregation. These innovations allow SVDefense to offer robust privacy protection without compromising model utility across a variety of embedded applications.", "motivation": "This paper is driven by the challenge to effectively defeat gradient inversion attacks and enable FL to be more secure and privacy-compliant, particularly in resource-constrained embedded platforms. It aims for a solution that provides strong privacy guarantees without diminishing the model's performance. The motivation stems from the limitations of existing FL defense mechanisms that either lack scalability for embedded systems or suffer from inefficiencies when potential adversaries adapt by learning the defense mechanisms themselves.", "method": "SVDefense employs truncated SVD to obscure gradient updates to prevent gradient inversion attacks. The framework presents three groundbreaking components: 1) a Self-Adaptive Energy Threshold that adapts dynamically to a client's vulnerability profile. 2) a Channel-Wise Weighted Approximation that enhances privacy while keeping the most pertinent gradient data for training. 3) a Layer-Wise Weighted Aggregation, that handles such issues as class imbalance, ensuring effective model training while maintaining robust privacy protection through these layered weighting strategies.", "result": "Comprehensive testing of SVDefense shows its superiority over existing defenses in protecting against gradient inversion attacks across multiple real-world applications such as image classification, human activity recognition, and keyword spotting. It offers a more robust defense while preserving as much as possible the accuracy of the model. In addition, the paper highlights the practicality of SVDefense by showing its suitability for deployment in various resource-constrained embedded platforms.", "conclusion": "SVDefense presents a strong framework for enhancing the privacy of federated learning systems. By introducing novel innovations to gradient obfuscation methods, it effectively balances both privacy and model performance. This paper concludes that these improvements render FL techniques more secure and deployable for real-world settings where computational resources are limited. The authors plan on making their code public upon paper acceptance to foster further research and development in this space."}}
{"id": "2510.03495", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03495", "abs": "https://arxiv.org/abs/2510.03495", "authors": ["Erik Pautsch", "Tanmay Singla", "Wenxin Jiang", "Huiyun Peng", "Behnaz Hassanshahi", "Konstantin L\u00e4ufer", "George K. Thiruvathukal", "James C. Davis"], "title": "AgentHub: A Research Agenda for Agent Sharing Infrastructure", "comment": null, "summary": "LLM-based agents are rapidly proliferating, yet the infrastructure for\ndiscovering, evaluating, and governing them remains fragmented compared to\nmature ecosystems like software package registries (e.g., npm) and model hubs\n(e.g., Hugging Face). Recent research and engineering works have begun to\nconsider the requisite infrastructure, but so far they focus narrowly -- on\ndistribution, naming, or protocol negotiation. However, considering broader\nsoftware engineering requirements would improve open-source distribution and\nease reuse. We therefore propose AgentHub, a research agenda for agent sharing.\nBy framing the key challenges of capability clarity, lifecycle transparency,\ninteroperability, governance, security, and workflow integration, AgentHub\ncharts a community-wide agenda for building reliable and scalable agent\necosystems. Our vision is a future where agents can be shared, trusted, and\ncomposed as seamlessly as today's software libraries.", "AI": {"tldr": "The paper highlights the need for a standardized infrastructure for sharing and managing LLM-based agents and introduces AgentHub, a research agenda to address this gap, with the goal of enabling seamless sharing and reuse of agents like modern software libraries.", "motivation": "Currently, the ecosystem for LLM-based agents is fragmented, unlike mature platforms like npm or Hugging Face. Existing research has focused on limited aspects like distribution and naming, but broad software engineering requirements for infrastructure have not been fully considered, hindering open-source distribution and reuse.", "method": "The authors outline AgentHub, which systematically identifies critical infrastructure challenges for LLM agent sharing. They analyze key requirements like capability clarity, lifecycle transparency, interoperability, governance, security, and workflow integration.", "result": "AgentHub provides a structured research agenda and sets a foundation for community collaboration in building robust and scalable ecosystems for agent sharing.", "conclusion": "Addressing infrastructure requirements will enable LLM agents to be shared efficiently and reused across various applications, similar to software libraries. The proposed research agenda sets a path toward a standardized and reliable agent ecosystem."}}
{"id": "2510.03320", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03320", "abs": "https://arxiv.org/abs/2510.03320", "authors": ["Raik Dankworth", "Gesina Schwalbe"], "title": "Attack logics, not outputs: Towards efficient robustification of deep neural networks by falsifying concept-based properties", "comment": "13 pages, 2 figures, accepted by \"7th OVERLAY\" workshop", "summary": "Deep neural networks (NNs) for computer vision are vulnerable to adversarial\nattacks, i.e., miniscule malicious changes to inputs may induce unintuitive\noutputs. One key approach to verify and mitigate such robustness issues is to\nfalsify expected output behavior. This allows, e.g., to locally proof security,\nor to (re)train NNs on obtained adversarial input examples. Due to the\nblack-box nature of NNs, current attacks only falsify a class of the final\noutput, such as flipping from $\\texttt{stop_sign}$ to $\\neg\\texttt{stop_sign}$.\nIn this short position paper we generalize this to search for generally\nillogical behavior, as considered in NN verification: falsify constraints\n(concept-based properties) involving further human-interpretable concepts, like\n$\\texttt{red}\\wedge\\texttt{octogonal}\\rightarrow\\texttt{stop_sign}$. For this,\nan easy implementation of concept-based properties on already trained NNs is\nproposed using techniques from explainable artificial intelligence. Further, we\nsketch the theoretical proof that attacks on concept-based properties are\nexpected to have a reduced search space compared to simple class falsification,\nwhilst arguably be more aligned with intuitive robustness targets. As an\noutlook to this work in progress we hypothesize that this approach has\npotential to efficiently and simultaneously improve logical compliance and\nrobustness.", "AI": {"tldr": "This paper proposes extending adversarial attacks from output class flips to falsifying logical constraints based on human-interpretable concepts (e.g., red octagons must be stop signs) in vision neural networks, aiming to enhance robustness through reduced search spaces and better alignment with intuitive security goals.", "motivation": "Current adversarial attacks only target final output classes (e.g., stop_sign \u2192 \u00acstop_sign), neglecting deeper logical relationships within input concepts, leading to robustness vulnerabilities that don't reflect real-world failings.", "method": "Introduces concept-based property falsification using explainable AI techniques (like concept activation analysis) to implement and attack logical constraints on pre-trained networks. Theoretical proof outlines how concept-based attacks have smaller search spaces than class-based attacks.", "result": "Framework for testing logical compliance with human-readable constraints, preliminary theoretical foundation showing efficiency advantages, and a hypothesis that this method simultaneously improves logical consistency and robustness through targeted adversarial training.", "conclusion": "Concept-based adversarial attacks provide a promising direction for neural network verification by aligning robustness goals with semantic properties; further work is needed to implement and test the approach on real-world vision systems."}}
{"id": "2510.03588", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.03588", "abs": "https://arxiv.org/abs/2510.03588", "authors": ["Anvith Pabba", "Simin Chen", "Alex Mathai", "Anindya Chakraborty", "Baishakhi Ray"], "title": "REFINE: Enhancing Program Repair Agents through Context-Aware Patch Refinement", "comment": "We also open source our code at\n  https://anonymous.4open.science/r/SemAgent-7B2F/README.md", "summary": "Large Language Models (LLMs) have recently shown strong potential in\nautomatic program repair (APR), especially in repository-level settings where\nthe goal is to generate patches based on natural language issue descriptions,\nlarge codebases, and regression tests. However, despite their promise, current\nLLM-based APR techniques often struggle to produce correct fixes due to limited\nunderstanding of code context and over-reliance on incomplete test suites. As a\nresult, they frequently generate Draft Patches-partially correct patches that\neither incompletely address the bug or overfit to the test cases. In this work,\nwe propose a novel patch refinement framework, Refine, that systematically\ntransforms Draft Patches into correct ones. Refine addresses three key\nchallenges: disambiguating vague issue and code context, diversifying patch\ncandidates through test-time scaling, and aggregating partial fixes via an\nLLM-powered code review process. We implement Refine as a general refinement\nmodule that can be integrated into both open-agent-based and workflow-based APR\nsystems. Our evaluation on the SWE-Bench Lite benchmark shows that Refine\nachieves state-of-the-art results among workflow-based approaches and\napproaches the best-known performance across all APR categories. Specifically,\nRefine boosts AutoCodeRover's performance by 14.67%, achieving a score of\n51.67% and surpassing all prior baselines. On SWE-Bench Verified, Refine\nimproves the resolution rate by 12.2%, and when integrated across multiple APR\nsystems, it yields an average improvement of 14%-demonstrating its broad\neffectiveness and generalizability. These results highlight the effectiveness\nof refinement as a missing component in current APR pipelines and the potential\nof agentic collaboration in closing the gap between near-correct and correct\npatches. We also open source our code.", "AI": {"tldr": "This paper introduces Refine, a patch refinement framework for LLM-based automatic program repair (APR). It addresses limitations in current APR methods by transforming draft patches into correct ones through disambiguating context, diversifying patch candidates, and aggregating partial fixes via code review. Evaluated on SWE-Bench benchmarks, Refine achieves state-of-the-art results with significant performance boosts.", "motivation": "Current LLM-AVR techniques struggle with incomplete test suite coverage and limited code context understanding, producing partially correct 'Draft Patches'. This limits their practical effectiveness in repository-level APR.", "method": "Refine systematically refines Draft Patches via three mechanisms: (1): disambiguating issue/code context through context-enrichment, (2) diversifying patch candidates using test-time scaling, and (3): aggregating partial fixes using an LLM-powered code review process. It is designed as a modular component compatible with both open-agent-based and workflow-based APR systems.", "result": "On SWE-Bench Lite, Refine improves AutoCodeRover by 14.67%, achieving 51.67%. On SWE-Bench Verified, it increases resolution by 12.2%. Across multiple APR systems, it yields average 14% improvement while achieving state-of-the-art workflow-based performance.", "conclusion": "Refine demonstrates the critical value of refinement stages in APR pipelines as a missing component for transforming near-correct to correct patches. It establishes a new paradigm for agentic collaboration in APR and significantly advances baseline performance while showing strong generalizability across systems."}}
{"id": "2510.03407", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03407", "abs": "https://arxiv.org/abs/2510.03407", "authors": ["Boniface M. Sindala", "Ragib Hasan"], "title": "Security Analysis and Threat Modeling of Research Management Applications [Extended Version]", "comment": "8 pages, 4 tables, 2 figures, This is an extended version of a paper\n  published in IEEE SoutheastCon 2025. \\c{opyright} 2025 IEEE", "summary": "Research management applications (RMA) are widely used in clinical research\nenvironments to collect, transmit, analyze, and store sensitive data. This data\nis so valuable making RMAs susceptible to security threats. This analysis,\nanalyzes RMAs' security, focusing on Research Electronic Data Capture (REDCap)\nas an example. We explore the strengths and vulnerabilities within RMAs by\nevaluating the architecture, data flow, and security features. We identify and\nassess potential risks using the MITRE ATT\\&CK framework and STRIDE model. We\nassess REDCap's defenses against common attack vectors focusing on security to\nprovide confidentiality, integrity, availability, non-repudiation, and\nauthentication. We conclude by proposing recommendations for enhancing the\nsecurity of RMAs, ensuring that critical research data remains protected\nwithout compromising usability. This research aims to contribute towards a more\nsecure framework for managing sensitive information in research-intensive\nenvironments.", "AI": {"tldr": "Research management applications, such as REDCap, face significant security challenges. This study assesses their vulnerabilities and defenses using frameworks like MITRE ATT&CK and STRIDE, and offers recommendations to enhance security while maintaining usability.", "motivation": "The increasing use of RMAs in clinical research to handle sensitive data necessitates a thorough understanding of their security vulnerabilities to ensure data protection without compromising usability.", "method": "The analysis evaluates the architecture, data flow, and security features of RMAs, focusing on REDCap. Potential security risks are identified and assessed using the MITRE ATT&CK framework and STRIDE model.", "result": "The study identifies vulnerabilities in RMAs, examines their defenses against common attack vectors, and highlights the critical aspects of security such as confidentiality, integrity, availability, non-repudiation, and authentication.", "conclusion": "The research proposes recommendations to enhance the security of RMAs, contributing to a more secure framework for managing sensitive research data in high-risk environments."}}
{"id": "2510.03641", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03641", "abs": "https://arxiv.org/abs/2510.03641", "authors": ["Satoshi Masuda", "Satoshi Kouzawa", "Kyousuke Sezai", "Hidetoshi Suhara", "Yasuaki Hiruta", "Kunihiro Kudou"], "title": "Generating High-Level Test Cases from Requirements using LLM: An Industry Study", "comment": "11pages", "summary": "Currently, generating high-level test cases described in natural language\nfrom requirement documents is performed manually. In the industry, including\ncompanies specializing in software testing, there is a significant demand for\nthe automatic generation of high-level test cases from requirement documents\nusing Large Language Models (LLMs). Efforts to utilize LLMs for requirement\nanalysis are underway. In some cases, retrieval-augmented generation (RAG) is\nemployed for generating high-level test cases using LLMs. However, in practical\napplications, it is necessary to create a RAG tailored to the knowledge system\nof each specific application, which is labor-intensive. Moreover, when applying\nhigh-level test case generation as a prompt, there is no established method for\ninstructing the generation of high-level test cases at a level applicable to\nother specifications without using RAG. It is required to establish a method\nfor the automatic generation of high-level test cases that can be generalized\nacross a wider range of requirement documents. In this paper, we propose a\nmethod for generating high-level (GHL) test cases from requirement documents\nusing only prompts, without creating RAGs. In the proposed method, first, the\nrequirement document is input into the LLM to generate test design techniques\ncorresponding to the requirement document. Then, high-level test cases are\ngenerated for each of the generated test design techniques. Furthermore, we\nverify an evaluation method based on semantic similarity of the generated\nhigh-level test cases. In the experiments, we confirmed the method using\ndatasets from Bluetooth and Mozilla, where requirement documents and high-level\ntest cases are available, achieving macro-recall measurement of 0.81 and 0.37,\nrespectively. We believe that the method is feasible for practical application\nin generating high-level test cases without using RAG.", "AI": {"tldr": "Prompt-based method for automatic generation of high-level test cases from requirement documents without requiring custom RAG implementation and performs well in evaluations.", "motivation": "Current high-level test case generation from requirement documents is manual and requires customization of retrieval-augmented generation (RAG) systems for each application's knowledge system, making it labor-intensive. Moreover, existing methods depend on RAG, which is not generalizable.", "method": "The proposed method utilizes Large Language Models (LLMs) with tailored prompts to directly generate high-level test cases from requirement documents. It consists of two steps: first generating test design techniques from the requirement document and then using these to generate high-level test cases.", "result": "The method was evaluated using datasets from Bluetooth and Mozilla, achieving macro-recall scores of 0.81 and 0.37, respectively, for the generated high-level test cases.", "conclusion": "The method demonstrates feasibility of using prompt-based LLM approaches for automatic, generalizable generation of high-level test cases from requirement documents, without the need for retrieval-augmented generation."}}
{"id": "2510.03417", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03417", "abs": "https://arxiv.org/abs/2510.03417", "authors": ["Javad Rafiei Asl", "Sidhant Narula", "Mohammad Ghasemigol", "Eduardo Blanco", "Daniel Takabi"], "title": "NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn LLM Jailbreaks", "comment": "Javad Rafiei Asl and Sidhant Narula are co-first authors", "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nbut remain vulnerable to jailbreak attacks, especially multi-turn jailbreaks\nthat distribute malicious intent across benign exchanges and bypass alignment\nmechanisms. Existing approaches often explore the adversarial space poorly,\nrely on hand-crafted heuristics, or lack systematic query refinement. We\npresent NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular\nframework for constructing, refining, and executing optimized multi-turn\nattacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a\nharmful intent into a structured semantic network of topics, entities, and\nquery chains; (2) a feedback-driven Simulator that iteratively refines and\nprunes these chains through attacker-victim-judge LLM collaboration using\nharmfulness and semantic-similarity benchmarks; and (3) a Network Traverser\nthat adaptively navigates the refined query space for real-time attacks. This\npipeline uncovers stealthy, high-success adversarial paths across LLMs. On\nseveral closed-source and open-source LLMs, NEXUS increases attack success rate\nby 2.1% to 19.4% over prior methods. Code: https://github.com/inspire-lab/NEXUS", "AI": {"tldr": "NEXUS is a modular framework for LLM multi-turn jailbreak attacks, improving effectiveness via structured semantic networks, active LLM-based refinement, and adaptive query execution.", "motivation": "Existing methods for multi-turn LLM jailbreaks lack systematic exploration, rely on heuristics, and fail to optimize query sequences effectively.", "method": "The framework integrates 1) ThoughtNet (semantic network expansion of harmful intent), 2) Simulator (feedback-driven refinement using aligned LLMs), and 3) Network Traverser (adaptive path selection during attacks).", "result": "Achieved 2.1-19.4% higher success rates across major LLMs compared to prior art, with scalable, stealthy, and adaptive attack execution.", "conclusion": "NEXUS demonstrates a robust, modular approach to multi-turn jailbreaks through semantic modeling, collaborative refinement, and adaptive traversal, outperforming existing methodologies."}}
{"id": "2510.03712", "categories": ["cs.SE", "68M15, 90B25, 68T05, 90C29", "C.4; C.2.4; D.2.5; D.4.5"], "pdf": "https://arxiv.org/pdf/2510.03712", "abs": "https://arxiv.org/abs/2510.03712", "authors": ["Jahidul Arafat", "Kh. M. Moniruzzaman", "Shamim Hossain", "Fariha Tasmin", "Kamrujjaman", "Ahsan Habib Tareq"], "title": "Detecting and Preventing Latent Risk Accumulation in High-Performance Software Systems", "comment": "26 pages, 12 tables, 4 figures. Academic-industry collaboration.\n  Framework (HYDRA, RAVEN, APEX) for optimization-induced vulnerabilities.\n  Evaluated: 2,160 configs, 12.7TB data, 1,748 scenarios", "summary": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization.", "AI": {"tldr": "This paper introduces a framework to detect, prevent, and optimize conditions in distributed systems where high performance optimization can lead to hidden vulnerabilities.  The framework includes the Latent Risk Index (LRI), HYDRA for perturbation testing (89.7% risk discovery rate), RAVEN for monitoring (92.9% precision and 93.8% recall), and APEX for maintaining performance while reducing risks by 59.2%.  Evaluations show strong results with high reproducibility and ROI.", "motivation": "High performance optimizations in distributed systems can mask  underlying vulnerabilities, creating conditions that lead to sudden system failures when these optimizations struggle.  Current reliability engineering is mainly reactive, rather than focusing on  proactively detecting these hidden risks.", "method": "The paper presents a framework that combines mathematical modeling, intelligent  perturbation testing, and risk-aware performance optimization.  The framework includes the LRI to assess risk, HYDRA which uses six optimization-aware  strategies for perturbation testing, RAVEN for continuous production monitoring,  and APEX to optimize performance while managing risk.", "result": "HYDRA achieved 89.7% risk discovery rate.  RAVEN achieved 92.9% precision and 93.8% recall. APEX reduced risks by 59.2% while maintaining 96.6% of baseline performance. Evaluation showed high reliability (Cohen d>2.0) and reproducibility (r>0.92).  Production deployment highlighted 69.1% reduction in  mean time to recovery, 78.6% reduction in incident severity, and prevented 81 incidents with 1.44M USD annual  benefits and 3.2-month ROI.", "conclusion": "The approach presented in this paper transforms reliability engineering from reactive incident management to proactive,  risk-aware, optimization,  demonstrating significant improvements in system resilience and cost  effectiveness."}}
{"id": "2510.03542", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03542", "abs": "https://arxiv.org/abs/2510.03542", "authors": ["Pouriya Alimoradi", "Ali Barati", "Hamid Barati"], "title": "A Multi-Layer Electronic and Cyber Interference Model for AI-Driven Cruise Missiles: The Case of Khuzestan Province", "comment": null, "summary": "The rapid advancement of Artificial Intelligence has enabled the development\nof cruise missiles endowed with high levels of autonomy, adaptability, and\nprecision. These AI driven missiles integrating deep learning algorithms, real\ntime data processing, and advanced guidance systems pose critical threats to\nstrategic infrastructures, especially under complex geographic and climatic\nconditions such as those found in Irans Khuzestan Province. In this paper, we\npropose a multi layer interference model, encompassing electronic warfare,\ncyberattacks, and deception strategies, to degrade the performance of AI guided\ncruise missiles significantly. Our experimental results, derived from 400\nsimulation runs across four distinct scenarios, demonstrate notable\nimprovements when employing the integrated multi layer approach compared to\nsingle layer or no interference baselines. Specifically, the average missile\ndeviation from its intended target increases from 0.25 to 8.65 under multi\nlayer interference a more than 3300 increase in angular deviation. Furthermore,\nthe target acquisition success rate is reduced from 92.7 in the baseline\nscenario to 31.5, indicating a 66 decrease in successful strikes. While\nresource consumption for multi layer strategies rises by approximately 25\ncompared to single layer methods, the significant drop in missile accuracy and\nreliability justifies the more intensive deployment of jamming power, cyber\nresources, and decoy measures. Beyond these quantitative improvements, the\nproposed framework uses a deep reinforcement learning based defense coordinator\nto adaptively select the optimal configuration of EW, cyber, and deception\ntactics in real time.", "AI": {"tldr": "This paper proposes a multi-layer interference framework combining electronic warfare, cyberattacks, and deception tactics using deep reinforcement learning to counter AI-guided cruise missiles. Simulations show significant improvements in disrupting missile accuracy compared to baseline methods.", "motivation": "AI-driven cruise missiles with high autonomy and precision pose critical threats to strategic infrastructure, particularly in complex environments like Iran's Khuzestan Province. Current single-layer defense systems are insufficient to counter these advanced threats.", "method": "The approach integrates three layers: (1) electronic warfare jamming, (2) cyberattacks on missile navigation systems, and (3) deception tactics using active decoys. The framework employs a deep reinforcement learning coordinator to dynamically select optimal defense strategies in real-time across four simulation scenarios.", "result": "Multi-layer interference increased average missile deviation from 0.25 to 8.65 angular units (3300% increase) and reduced target acquisition rate from 92.7% to 31.5% (66% reduction). While requiring 25% more resources than single-layer methods, the system reduces successful strikes by two-thirds.", "conclusion": "The multi-layer framework demonstrates superior effectiveness in degrading AI-guided cruise missile performance despite higher resource costs. The adaptive defense coordinator enables real-time optimization of countermeasures, making it a justified solution for protecting strategic infrastructure against advanced autonomous threats."}}
{"id": "2510.03743", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03743", "abs": "https://arxiv.org/abs/2510.03743", "authors": ["Zachary Eberhart", "Collin McMillan"], "title": "APIDA-Chat: Structured Synthesis of API Search Dialogues to Bootstrap Conversational Agents", "comment": "4 pages, 2 figures. To be published in Proceedings of the 40th\n  IEEE/ACM International Conference on Automated Software Engineering", "summary": "Large-language-model assistants are suitable for explaining popular APIs, yet\nthey falter on niche or proprietary libraries because the multi-turn dialogue\ndata needed for fine-tuning are scarce. We present APIDA-Chat, an open-source\npipeline that converts symbolic dialogue-act \"scripts\" into realistic,\ndomain-grounded API Search conversations using a lightweight model for\ninexpensive training data generation. Phase I pairs a legacy dialogue planner\nwith a high-capability teacher LLM (o4-mini) to synthesize a \"gold set\" of\nrealized dialogues; then, a smaller Llama 3.2 3B student model is fine-tuned on\nthis corpus. Phase II drops the teacher and reuses the same planner with the\nfine-tuned model, allowing rapid, low-cost synthesis of new dialogues without\nexposing source code to external services. The fine-tuned student improves BLEU\nfrom 0.38 to 0.50 and BERTScore from 0.88 to 0.91 versus the base model while\nrunning entirely on a single consumer GPU. All components are modular and\npublicly released to serve as a conservative baseline for future work.\nAPIDA-Chat is open-sourced at https://github.com/Zeberhart/apida-chat and a\nvideo demo is available at https://youtu.be/YqmZBHyGbPs .", "AI": {"tldr": "APIDA-Chat introduces an open-source pipeline to generate low-cost training data for API Search conversations using a two-phase approach with a teacher LLM and a fine-tuned student model, improving dialogue generation metrics without requiring external API access.", "motivation": "Large language models struggle with niche/proprietary APIs due to insufficient multi-turn dialogue data for fine-tuning. Existing solutions rely on expensive large models, requiring more cost-effective, modular data generation methods.", "method": "Phase I combines a legacy dialogue planner with a teacher LLM (o4-mini) to create a 'gold set' of dialogues, then fine-tunes a smaller Llama 3.2 student model. Phase II leverages the fine-tuned student model alone for rapid, domain-agnostic dialogue synthesis without exposing source code.", "result": "The student model achieves BLEU 0.50 (+0.12) and BERTScore 0.91 (+0.03) improvements over baselines, runs on single consumer GPUs, and demonstrates modular, scalable data generation for API Search.", "conclusion": "APIDA-Chat provides a cost-effective, open-source baseline for generating API Search dialogue data, reducing dependency on high-capacity models while improving metric performance, with components released at https://github.com/Zeberhart/apida-chat."}}
{"id": "2510.03559", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.03559", "abs": "https://arxiv.org/abs/2510.03559", "authors": ["Zeya Chen", "Jianing Wen", "Ruth Schmidt", "Yaxing Yao", "Toby Jia-Jun Li", "Tianshi Li"], "title": "PrivacyMotiv: Speculative Persona Journeys for Empathic and Motivating Privacy Reviews in UX Design", "comment": "42 pages, 13 figures", "summary": "UX professionals routinely conduct design reviews, yet privacy concerns are\noften overlooked -- not only due to limited tools, but more critically because\nof low intrinsic motivation. Limited privacy knowledge, weak empathy for\nunexpectedly affected users, and low confidence in identifying harms make it\ndifficult to address risks. We present PrivacyMotiv, an LLM-powered system that\nsupports privacy-oriented design diagnosis by generating speculative personas\nwith UX user journeys centered on individuals vulnerable to privacy risks.\nDrawing on narrative strategies, the system constructs relatable and\nattention-drawing scenarios that show how ordinary design choices may cause\nunintended harms, expanding the scope of privacy reflection in UX. In a\nwithin-subjects study with professional UX practitioners (N=16), we compared\nparticipants' self-proposed methods with PrivacyMotiv across two privacy review\ntasks. Results show significant improvements in empathy, intrinsic motivation,\nand perceived usefulness. This work contributes a promising privacy review\napproach which addresses the motivational barriers in privacy-aware UX.", "AI": {"tldr": "PrivacyMotiv is an LLM-powered system that helps UX professionals address privacy concerns through speculative personas and relatable scenarios, which increases empathy, motivation, and perceived usefulness in privacy reviews.", "motivation": "UX professionals often overlook privacy due to limited tools and low motivation. Lack of privacy knowledge, empathy for affected users, and confidence in identifying harms hinders privacy consideration.", "method": "PrivacyMotiv generates speculative personas and UX user journeys focused on individuals vulnerable to privacy risks. It uses narrative strategies to create relatable scenarios demonstrating how common design choices can lead to unintended privacy harms.", "result": "A within-subjects study (N=16) showed that PrivacyMotiv significantly improved UX practitioners' empathy, intrinsic motivation, and perceived usefulness of privacy reviews compared to their self-proposed methods.", "conclusion": "PrivacyMotiv provides a promising approach to privacy review by overcoming motivational barriers in privacy-aware UX design through narrative-based personas and scenarios."}}
{"id": "2510.03755", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03755", "abs": "https://arxiv.org/abs/2510.03755", "authors": ["Roham Koohestani", "Parham Bateni", "Aydin Ebrahimi", "Behdad Etezadi", "Kiarash Karimi", "Maliheh Izadi"], "title": "Code4MeV2: a Research-oriented Code-completion Platform", "comment": "Under review for submission at a conference", "summary": "The adoption of AI-powered code completion tools in software development has\nincreased substantially, yet the user interaction data produced by these\nsystems remain proprietary within large corporations. This creates a barrier\nfor the academic community, as researchers must often develop dedicated\nplatforms to conduct studies on human--AI interaction, making reproducible\nresearch and large-scale data analysis impractical. In this work, we introduce\nCode4MeV2, a research-oriented, open-source code completion plugin for\nJetBrains IDEs, as a solution to this limitation. Code4MeV2 is designed using a\nclient--server architecture and features inline code completion and a\ncontext-aware chat assistant. Its core contribution is a modular and\ntransparent data collection framework that gives researchers fine-grained\ncontrol over telemetry and context gathering. Code4MeV2 achieves\nindustry-comparable performance in terms of code completion, with an average\nlatency of 200~ms. We assess our tool through a combination of an expert\nevaluation and a user study with eight participants. Feedback from both\nresearchers and daily users highlights its informativeness and usefulness. We\ninvite the community to adopt and contribute to this tool. More information\nabout the tool can be found at https://app.code4me.me.", "AI": {"tldr": "Code4MeV2 is an open-source code completion plugin for JetBrains IDEs designed to facilitate academic research on human-AI interaction by offering a modular data collection framework.", "motivation": "The limited availability of user interaction data, as it remains in proprietary systems, hinders reproducible research and large-scale analysis in human-AI interaction studies within the academic community.", "method": "The researchers designed Code4MeV2 using a client--server architecture, providing inline code completion and a context-aware chat assistant, with a focus on a modular and transparent data collection system.", "result": "Code4MeV2 performs on par with industry tools, showing an average latency of 200~ms. It was evaluated by an expert assessment and a user study with eight participants, who found it informative and useful.", "conclusion": "Code4MeV2 addresses the challenge of data scarcity in human-AI interaction research, offering an open-source tool that eases large-scale studies and invites the community to use and contribute to it."}}
{"id": "2510.03565", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03565", "abs": "https://arxiv.org/abs/2510.03565", "authors": ["Cory Brynds", "Parker McLeod", "Lauren Caccamise", "Asmita Pal", "Dewan Saiham", "Sazadur Rahman", "Joshua San Miguel", "Di Wu"], "title": "CryptOracle: A Modular Framework to Characterize Fully Homomorphic Encryption", "comment": null, "summary": "Privacy-preserving machine learning has become an important long-term pursuit\nin this era of artificial intelligence (AI). Fully Homomorphic Encryption (FHE)\nis a uniquely promising solution, offering provable privacy and security\nguarantees. Unfortunately, computational cost is impeding its mass adoption.\nModern solutions are up to six orders of magnitude slower than plaintext\nexecution. Understanding and reducing this overhead is essential to the\nadvancement of FHE, particularly as the underlying algorithms evolve rapidly.\nThis paper presents a detailed characterization of OpenFHE, a comprehensive\nopen-source library for FHE, with a particular focus on the CKKS scheme due to\nits significant potential for AI and machine learning applications. We\nintroduce CryptOracle, a modular evaluation framework comprising (1) a\nbenchmark suite, (2) a hardware profiler, and (3) a predictive performance\nmodel. The benchmark suite encompasses OpenFHE kernels at three abstraction\nlevels: workloads, microbenchmarks, and primitives. The profiler is compatible\nwith standard and user-specified security parameters. CryptOracle monitors\napplication performance, captures microarchitectural events, and logs power and\nenergy usage for AMD and Intel systems. These metrics are consumed by a\nmodeling engine to estimate runtime and energy efficiency across different\nconfiguration scenarios, with error geomean of $-7.02\\%\\sim8.40\\%$ for runtime\nand $-9.74\\%\\sim15.67\\%$ for energy. CryptOracle is open source, fully modular,\nand serves as a shared platform to facilitate the collaborative advancements of\napplications, algorithms, software, and hardware in FHE. The CryptOracle code\ncan be accessed at https://github.com/UnaryLab/CryptOracle.", "AI": {"tldr": "This paper analyzes FHE's computational overhead through the OpenFHE library, introducing CryptOracle\u2014a modular evaluation framework to benchmark, profile, and predict performance for CKKS-based AI/ML applications.", "motivation": "FHE's adoption is hindered by its high computational cost (~6x slower than plaintext), necessitating tools to reduce overhead as algorithms evolve rapidly.", "method": "CryptOracle integrates (1)a benchmark suite with OpenFHE kernels at multiple abstraction levels, (2)a hardware/security parameter profiler, and (3)a predictive model for runtime/energy estimation.", "result": "Achieved runtime prediction error between -7.02%-8.40%, energy error -9.74%-15.67%; framework supports AMD/Intel system metrics and open-source sharing.", "conclusion": "CryptOracle provides a collaborative platform to advance FHE applications/optimizations by systematically characterizing performance bottlenecks."}}
{"id": "2510.03802", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03802", "abs": "https://arxiv.org/abs/2510.03802", "authors": ["Gilberto Recupito", "Vincenzo De Martino", "Dario Di Nucci", "Fabio Palomba"], "title": "A First Look at the Lifecycle of DL-Specific Self-Admitted Technical Debt", "comment": "Accepted at the International Workshop of Software Quality Assurance\n  for Artificial Intelligence 2025 (SQA4AI), Montr\\'eal, Canada", "summary": "The rapid adoption of Deep Learning (DL)-enabled systems has revolutionized\nsoftware development, driving innovation across various domains. However, these\nsystems also introduce unique challenges, particularly in maintaining software\nquality and performance. Among these challenges, Self-Admitted Technical Debt\n(SATD) has emerged as a growing concern, significantly impacting the\nmaintainability and overall quality of ML and DL-enabled systems. Despite its\ncritical implications, the lifecycle of DL-specific SATD, how developers\nintroduce, acknowledge, and address it over time-remains underexplored. This\nstudy presents a preliminary analysis of the persistence and lifecycle of\nDL-specific SATD in DL-enabled systems. The purpose of this project is to\nuncover the patterns of SATD introduction, recognition, and durability during\nthe development life cycle, providing information on how to manage these\nissues. Using mining software repository techniques, we examined 40 ML\nprojects, focusing on 185 DL-specific SATD instances. The analysis tracked the\nintroduction and persistence of SATD instances through project commit histories\nto assess their lifecycle and developer actions. The findings indicate that\nDL-specific SATD is predominantly introduced during the early and middle stages\nof project development. Training and Hardware phases showed the longest SATD\ndurations, highlighting critical areas where debt accumulates and persists.\nAdditionally, developers introduce DL-specific SATD more frequently during\nfeature implementation and bug fixes. This study emphasizes the need for\ntargeted DL-specific SATD management strategies in DL-enabled systems to\nmitigate its impact. By understanding the temporal characteristics and\nevolution of DL-specific SATD, developers can prioritize interventions at\ncritical stages to improve the maintainability and quality of the system.", "AI": {"tldr": "This study analyzes the lifecycle of Deep Learning (DL)-specific Self-Admitted Technical Debt (SATD) in software projects, finding that it is most commonly introduced in the early to middle stages, particularly during training and hardware phases, with developers often creating it during feature implementation and bug fixes.", "motivation": "The rapid adoption of DL systems has introduced challenges in software quality and performance, compounded by DL-specific SATD, which affects maintainability. Despite the significance, the lifecycle of this type of debt remains poorly understood, necessitating research to inform better management strategies.", "method": "The research uses mining software repository techniques to analyze 40 ML projects, specifically tracking 185 DL-specific SATD instances across their commit histories to assess how these instances are introduced, acknowledged, and resolved over time.", "result": "DL-SATD is most frequently introduced in early/middle project stages, with the longest durations observed in the Training and Hardware phases. These issues are commonly introduced during feature implementation and debugging, indicating a trend in how debt accumulates.", "conclusion": "The study concludes that targeted strategies are needed to address DL-SATD, particularly by focusing on early lifecycle phases and phases like Training and Hardware where debt tends to persist. Better management here could enhance long-term system maintainability and quality."}}
{"id": "2510.03610", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03610", "abs": "https://arxiv.org/abs/2510.03610", "authors": ["Zachary Ezetta", "Wu-chang Feng"], "title": "PentestMCP: A Toolkit for Agentic Penetration Testing", "comment": null, "summary": "Agentic AI is transforming security by automating many tasks being performed\nmanually. While initial agentic approaches employed a monolithic architecture,\nthe Model-Context-Protocol has now enabled a remote-procedure call (RPC)\nparadigm to agentic applications, allowing for the flexible construction and\ncomposition of multi-function agents. This paper describes PentestMCP, a\nlibrary of MCP server implementations that support agentic penetration testing.\nBy supporting common penetration testing tasks such as network scanning,\nresource enumeration, service fingerprinting, vulnerability scanning,\nexploitation, and post-exploitation, PentestMCP allows a developer to customize\nmulti-agent workflows for performing penetration tests.", "AI": {"tldr": "This paper introduces PentestMCP, which allows agentic applications to reliably interact with common pen testing tools in a server environment. Framework can be modified for new attack surfaces and multiple agents can interact for a coordinated pen test.", "motivation": "This paper introduces a library of Model-Context-Protocol (MCP) server implementations that support agentic penetration testing by enabling developers to create custom multi-agent workflows while automating common penetration testing tasks.", "method": "The researchers developed a library based on the Model-Context-Protocol to enable automation and flexible composition of agentic multi-function agents for penetration testing.", "result": "PentestMCP supports common tasks like network scanning, resource enumeration, service fingerprinting, vulnerability scanning, exploitation, and post-exploitation, allowing customized penetration testing workflows.", "conclusion": "PentestMCP provides a new approach to performing penetration testing via agentic applications supported by the Model-Context-Protocol, enabling more adaptable and scalable testing workflows."}}
{"id": "2510.03843", "categories": ["cs.SE", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03843", "abs": "https://arxiv.org/abs/2510.03843", "authors": ["Vincent Nguyen", "Guilherme Herzog", "Jos\u00e9 Cambronero", "Marcus Revaj", "Aditya Kini", "Alexander Fr\u00f6mmgen", "Maxim Tabachnyk"], "title": "Smart Paste: Automatically Fixing Copy/Paste for Google Developers", "comment": "11 pages", "summary": "Manually editing pasted code is a long-standing developer pain point. In\ninternal software development at Google, we observe that code is pasted 4 times\nmore often than it is manually typed. These paste actions frequently require\nfollow-up edits, ranging from simple reformatting and renaming to more complex\nstyle adjustments and cross-language translations. Prior work has shown deep\nlearning can be used to predict these edits. In this work, we show how to\niteratively develop and scale Smart Paste, an IDE feature for post-paste edit\nsuggestions, to Google's development environment. This experience can serve as\na guide for AI practitioners on a holistic approach to feature development,\ncovering user experience, system integration, and model capabilities. Since\ndeployment, Smart Paste has had overwhelmingly positive feedback with a 45%\nacceptance rate. At Google's enterprise scale, these accepted suggestions\naccount substantially for over 1% of all code written company-wide.", "AI": {"tldr": "Pasted code is commonly used at Google, often requiring follow-up edits. The paper introduces Smart Paste, an IDE feature that provides post-paste edit suggestions, and shows its successful deployment with a 45% acceptance rate, contributing to over 1% of company-wide code.", "motivation": "Manually editing pasted code is a pain point for developers, especially since code pasting is more frequent than manual typing and often needs multiple adjustments.", "method": "The authors developed an iterative approach to create and scale Smart Paste, an IDE feature for deep learning-based post-paste edit suggestions, within Google's development environment.", "result": "Smart Paste has a 45% acceptance rate and contributes to over 1% of all code written company-wide. It reduces manual editing efforts for follow-up tasks like reformatting, renaming, and translations.", "conclusion": "The case study for Smart Paste highlights the potential of AI in improving developer workflows at scale, offering a practical guide for AI practitioners focusing on user experience, system integration, and model capabilities in real-world deployments."}}
{"id": "2510.03623", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03623", "abs": "https://arxiv.org/abs/2510.03623", "authors": ["Maraz Mia", "Mir Mehedi A. Pritom"], "title": "Explainable but Vulnerable: Adversarial Attacks on XAI Explanation in Cybersecurity Applications", "comment": "10 pages, 9 figures, 4 tables", "summary": "Explainable Artificial Intelligence (XAI) has aided machine learning (ML)\nresearchers with the power of scrutinizing the decisions of the black-box\nmodels. XAI methods enable looking deep inside the models' behavior, eventually\ngenerating explanations along with a perceived trust and transparency. However,\ndepending on any specific XAI method, the level of trust can vary. It is\nevident that XAI methods can themselves be a victim of post-adversarial attacks\nthat manipulate the expected outcome from the explanation module. Among such\nattack tactics, fairwashing explanation (FE), manipulation explanation (ME),\nand backdoor-enabled manipulation attacks (BD) are the notable ones. In this\npaper, we try to understand these adversarial attack techniques, tactics, and\nprocedures (TTPs) on explanation alteration and thus the effect on the model's\ndecisions. We have explored a total of six different individual attack\nprocedures on post-hoc explanation methods such as SHAP (SHapley Additive\nexPlanations), LIME (Local Interpretable Model-agnostic Explanation), and IG\n(Integrated Gradients), and investigated those adversarial attacks in\ncybersecurity applications scenarios such as phishing, malware, intrusion, and\nfraudulent website detection. Our experimental study reveals the actual\neffectiveness of these attacks, thus providing an urgency for immediate\nattention to enhance the resiliency of XAI methods and their applications.", "AI": {"tldr": "This paper analyzes adversarial attacks on XAI methods, highlighting real-world impacts in cybersecurity and the need for improved resiliency.", "motivation": "The motivation is to address the vulnerability of XAI methods to post-adversarial attacks that manipulate model decisions by altering explanations.", "method": "The authors explored six attack procedures on post-hoc XAI methods like SHAP, LIME, and IG, testing them in cybersecurity scenarios including phishing, malware, intrusion, and fraudulent website detection.", "result": "Experiments demonstrated that these attacks are effective in real-world cybersecurity contexts, exposing weaknesses in current XAI techniques.", "conclusion": "The conclusion emphasizes the urgency of enhancing XAI resilience to prevent exploitation by adversarial TTPs in critical domains like cybersecurity."}}
{"id": "2510.03862", "categories": ["cs.SE", "cs.AI", "500"], "pdf": "https://arxiv.org/pdf/2510.03862", "abs": "https://arxiv.org/abs/2510.03862", "authors": ["Nathalia Nascimento", "Everton Guimaraes", "Paulo Alencar"], "title": "Designing Empirical Studies on LLM-Based Code Generation: Towards a Reference Framework", "comment": "5 pages", "summary": "The rise of large language models (LLMs) has introduced transformative\npotential in automated code generation, addressing a wide range of software\nengineering challenges. However, empirical evaluation of LLM-based code\ngeneration lacks standardization, with studies varying widely in goals, tasks,\nand metrics, which limits comparability and reproducibility. In this paper, we\npropose a theoretical framework for designing and reporting empirical studies\non LLM-based code generation. The framework is grounded in both our prior\nexperience conducting such experiments and a comparative analysis of key\nsimilarities and differences among recent studies. It organizes evaluation\naround core components such as problem sources, quality attributes, and\nmetrics, supporting structured and systematic experimentation. We demonstrate\nits applicability through representative case mappings and identify\nopportunities for refinement. Looking forward, we plan to evolve the framework\ninto a more robust and mature tool for standardizing LLM evaluation across\nsoftware engineering contexts.", "AI": {"tldr": "This paper introduces a theoretical framework for standardized evaluation of LLM-based code generation in software engineering. It aims to address the lack of consistency in current studies to improve comparability and reproducibility.", "motivation": "The motivation stems from the fragmented and inconsistent evaluation practices in existing LLM-based code generation research, which hampers meaningful comparisons and reproducibility of results.", "method": "The authors developed the framework based on prior experience and a comparative analysis of recent studies, focusing on core components like problem sources, quality attributes, and metrics.", "result": "The result is a proposed framework that structures empirical evaluations systematically, demonstrated through case mappings which show its practical application and potential for refinement.", "conclusion": "The conclusion highlights the framework's role in standardizing LLM evaluations and the intention to further develop it into a robust tool for software engineering contexts."}}
{"id": "2510.03625", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.03625", "abs": "https://arxiv.org/abs/2510.03625", "authors": ["Joachim Neu", "Javier Nieto", "Ling Ren"], "title": "On the Limits of Consensus under Dynamic Availability and Reconfiguration", "comment": null, "summary": "Proof-of-stake blockchains require consensus protocols that support Dynamic\nAvailability and Reconfiguration (so-called DAR setting), where the former\nmeans that the consensus protocol should remain live even if a large number of\nnodes temporarily crash, and the latter means it should be possible to change\nthe set of operating nodes over time. State-of-the-art protocols for the DAR\nsetting, such as Ethereum, Cardano's Ouroboros, or Snow White, require\nunrealistic additional assumptions, such as social consensus, or that key\nevolution is performed even while nodes are not participating. In this paper,\nwe identify the necessary and sufficient adversarial condition under which\nconsensus can be achieved in the DAR setting without additional assumptions. We\nthen introduce a new and realistic additional assumption: honest nodes dispose\nof their cryptographic keys the moment they express intent to exit from the set\nof operating nodes. To add reconfiguration to any dynamically available\nconsensus protocol, we provide a bootstrapping gadget that is particularly\nsimple and efficient in the common optimistic case of few reconfigurations and\nno double-spending attempts.", "AI": {"tldr": "This paper addresses consensus challenges in proof-of-stake blockchains under Dynamic Availability and Reconfiguration (DAR) settings, proposing a realistic adversarial condition and a simple bootstrap gadget to enable secure reconfiguration without prior assumptions.", "motivation": "Existing DAR protocols (e.g., Ethereum, Ouroboros, Snow White) rely on unrealistic assumptions like social consensus or continuous key evolution during node inactivity. Current solutions fail to provide practical security guarantees for dynamic node participation.", "method": "1) Derive necessary/sufficient adversarial conditions for DAR consensus security without additional assumptions\n2 ) Introduce a realistic assumption: Honest nodes discard keys immediately when exiting\n3) Design a lightweight bootstrap gadget for reconfiguration in optimistic cases", "result": "Established formal security boundaries for DAR consensus, demonstrated gadget efficiency in common cases with low reconfiguration frequency and without double-spending attempts.", "conclusion": "The proposed adversarial model and bootstrap mechanism provide a practical foundation for secure, dynamically available blockchain protocols without requiring unrealistic coordination or assumptions beyond standard cryptography."}}
{"id": "2510.03879", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03879", "abs": "https://arxiv.org/abs/2510.03879", "authors": ["Tianyu Li", "Ruishi Li", "Bo Wang", "Brandon Paulsen", "Umang Mathur", "Prateek Saxena"], "title": "Adversarial Agent Collaboration for C to Rust Translation", "comment": null, "summary": "Translating C to memory-safe languages, like Rust, prevents critical memory\nsafety vulnerabilities that are prevalent in legacy C software. Existing\napproaches for C to safe Rust translation, including LLM-assisted ones, do not\ngeneralize on larger (> 500 LoC) C codebases because they depend on complex\nprogram analyses that frequently break. In this work, we present ACToR\n(Adversarial C To Rust translator), a simple LLM agent-based approach. Inspired\nby GANs, ACToR pits a generator agent against a discriminator agent, which\ncollaborate to iteratively generate a Rust translation. On each iteration, the\ntranslator agent synthesizes and refines a Rust translation to pass an existing\nsuite of tests, and then the discriminator agent finds new failing tests. We\ndemonstrate that ACToR translates all of the 63 real-world command line\nutilities considered in our benchmarks, which have an average size of 485 lines\nof code, and it achieves over 90% test pass rate with zero human intervention.\nTo our knowledge, it is the first such system that reliably translates C\nprograms of this scale. Furthermore, ACToR improves translation correctness by\nup to 18.9% compared to baseline, non-adversarial approaches.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.03631", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03631", "abs": "https://arxiv.org/abs/2510.03631", "authors": ["Saleh Darzi", "Saif Eddine Nouma", "Kiarash Sedghighadikolaei", "Attila Altay"], "title": "QPADL: Post-Quantum Private Spectrum Access with Verified Location and DoS Resilience", "comment": "13 pages, 3 figures, 1 table, 4 algorithms", "summary": "With advances in wireless communication and growing spectrum scarcity,\nSpectrum Access Systems (SASs) offer an opportunistic solution but face\nsignificant security challenges. Regulations require disclosure of location\ncoordinates and transmission details, exposing user privacy and anonymity\nduring spectrum queries, while the database operations themselves permit\nDenial-of-Service (DoS) attacks. As location-based services, SAS is also\nvulnerable to compromised or malicious users conducting spoofing attacks. These\nthreats are further amplified given the quantum computing advancements. Thus,\nwe propose QPADL, the first post-quantum (PQ) secure framework that\nsimultaneously ensures privacy, anonymity, location verification, and DoS\nresilience while maintaining efficiency for large-scale spectrum access\nsystems. QPADL introduces SAS-tailored private information retrieval for\nlocation privacy, a PQ-variant of Tor for anonymity, and employs advanced\nsignature constructions for location verification alongside client puzzle\nprotocols and rate-limiting technique for DoS defense. We formally assess its\nsecurity and conduct a comprehensive performance evaluation, incorporating GPU\nparallelization and optimization strategies to demonstrate practicality and\nscalability.", "AI": {"tldr": "This paper proposes QPADL, a post-quantum secure framework for Spectrum Access Systems (SASs) addressing privacy, anonymity, DoS resilience, and scalability challenges through tailored cryptographic methods.", "motivation": "Current SASs face severe security risks from privacy exposure (location/transmission data disclosure), DoS attacks, spoofing vulnerabilities, and emerging quantum threats. Existing solutions lack comprehensive post-quantum security guarantees.", "method": "QPADL combines SAS-specific private information retrieval for location privacy, a quantum-resistant Tor-like anonymity network, cryptographic signatures for location verification, and client puzzles/rate-limiting for DoS defense. GPU optimization enhances scalability.", "result": "Security analysis confirms robustness against quantum threats and adversarial scenarios. Performance evaluations with GPU acceleration demonstrate practicality for large-scale deployments.", "conclusion": "QPADL is the first post-quantum framework to simultaneously achieve privacy, anonymity, location verification, and DoS resilience in SASs while maintaining efficiency, addressing critical gaps in 5G/6G spectrum management security."}}
{"id": "2510.03890", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03890", "abs": "https://arxiv.org/abs/2510.03890", "authors": ["Jose Garcia-Alonso", "Enrique Moguel", "Jaime Alvarado-Valiente", "Javier Romero-Alvarez", "\u00c1lvaro M. Aparicio-Morales", "Juan M. Murillo", "Francisco Javier Cavero", "Adri\u00e1n Romero-Flores", "Alfonso E. Marquez-Chamorro", "Jos\u00e9 Antonio Parejo", "Antonio Ruiz-Cort\u00e9s", "Giuseppe Bisicchia", "Alessandro Bocci", "Antonio Brogi"], "title": "Rethinking Services in the Quantum Age: The SOQ Paradigm", "comment": "39 pages, 5 figures, 6 tables", "summary": "Quantum computing is rapidly progressing from theoretical promise to\npractical implementation, offering significant computational advantages for\ntasks in optimization, simulation, cryptography, and machine learning. However,\nits integration into real-world software systems remains constrained by\nhardware fragility, platform heterogeneity, and the absence of robust software\nengineering practices. This paper introduces Service-Oriented Quantum (SOQ), a\nnovel paradigm that reimagines quantum software systems through the lens of\nclassical service-oriented computing. Unlike prior approaches such as Quantum\nService-Oriented Computing (QSOC), which treat quantum capabilities as\nauxiliary components within classical systems, SOQ positions quantum services\nas autonomous, composable, and interoperable entities. We define the\nfoundational principles of SOQ, propose a layered technology stack to support\nits realization, and identify the key research and engineering challenges that\nmust be addressed, including interoperability, hybridity, pricing models,\nservice abstractions, and workforce development. This approach is of vital\nimportance for the advancement of quantum technology because it enables the\nscalable, modular, and interoperable integration of quantum computing into\nreal-world software systems independently and without relying on a dedicated\nclassical environment to manage quantum processing.", "AI": {"tldr": "This paper introduces Service-Oriented Quantum (SOQ), a new approach to incorporating quantum computing into practical software systems by autonomously integrating quantum services as modular and interoperable components, unlike previous methods that treated them as part of classical systems.", "motivation": "The motivation for the paper arises from the fact that while quantum computing is moving from theory to practice and can offer substantial computational benefits, its integration into real-world software systems is limited by hardware fragility, platform differences, and the lack of strong software engineering practices.", "method": "The paper presents Service-Oriented Quantum (SOQ) by establishing its key principles, formulating a layered technology stack to facilitate its implementation, and pinpointing essential challenges to be tackled, including interoperability, hybrid systems, pricing models, service abstractions, and workforce training.", "result": "The introduction of SOQ as a novel paradigm enables the development of scalable, modular, and interoperable quantum software systems without reliance on a dedicated classical environment.", "conclusion": "Service-Oriented Quantum (SOQ) represents a crucial step forward in integrating quantum computing into real-world software by making quantum services autonomous, modular, and interoperable, rather than treating them as auxiliary components in classical systems."}}
{"id": "2510.03697", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03697", "abs": "https://arxiv.org/abs/2510.03697", "authors": ["Benjamin Marsh", "Paolo Serafino"], "title": "A Time-Bound Signature Scheme for Blockchains", "comment": "Accepted to the 2025 IEEE International Conference on Blockchain", "summary": "We introduce a modified Schnorr signature scheme to allow for time-bound\nsignatures for transaction fee auction bidding and smart contract purposes in a\nblockchain context, ensuring an honest producer can only validate a signature\nbefore a given block height. The immutable blockchain is used as a source of\nuniversal time for the signature scheme. We show the use of such a signature\nscheme leads to lower MEV revenue for builders. We then apply our time-bound\nsignatures to Ethereum's EIP-1559 and show how it can be used to mitigate the\neffect of MEV on predicted equilibrium strategies.", "AI": {"tldr": "The paper presents a modified Schnorr signature scheme to enable time-bound signatures for blockchain transaction fee auction and smart contracts, using the immutable blockchain as universal time. It shows that such signatures can reduce MEV (Maximal Extractable Value) revenue for builders when applied to EIP-1559.", "motivation": "The paper aims to address the issues of transaction fee auctions and MEV (Maximal Extractable Value) in blockchain systems, especially in the context of Ethereum's EIP-1559. By introducing a time-bound element to Schnorr signatures, the authors seek to limit the time during which a signature is valid, thereby ensuring that honest producers can only validate it before a specific block height. This is intended to reduce the potential for MEV by preventing builders from profiting from early access to transactions.", "method": "The authors propose a modified Schnorr signature scheme that incorporates a block height as a time parameter. In this scheme, a signature is only valid up to a certain block height, after which it can no longer be used. This is achieved by integrating the block height directly into the signature generation and verification process. The paper demonstrates how this modification can be applied to EIP-1559, ensuring the signature is time-bound and honest block producers are incentivized to validate transactions before a specified height.", "result": "Through one application example, the paper shows that the modified Schnorr signature scheme can lower the MEV revenue for block builders in the EIP-1559 context. The time-bound nature of the signatures limits the ability of builders to profit from transaction reordering or inclusion, thus mitigating the impact of MEV on the predicted equilibrium strategies of participants.", "conclusion": "The modified Schnorr signature scheme using time-bound functionality is effective in reducing MEV revenues in blockchain-based transaction fee auctions and may have useful applications in complex smart contracts to prevent time-based manipulation. It is suggested that the approach could be applied to improve fairness and economic incentives in various blockchain protocols."}}
{"id": "2510.03894", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03894", "abs": "https://arxiv.org/abs/2510.03894", "authors": ["Antonios Saravanos"], "title": "A Brief History of the Waterfall Model: Past, Present, and Future", "comment": null, "summary": "The waterfall model, one of the earliest software development methodologies,\nhas played a foundational role in shaping contemporary software engineering\npractices. This paper provides a historical and critical overview of the model,\ntracing its conceptual origins in software engineering, its formalization by\nRoyce, and its evolution through decades of industry adoption and critique.\nAlthough often criticized for its rigidity, shortcomings, and high failure\nrates, the waterfall model persists in specific domains. Its principles\ncontinue to influence contemporary hybrid development frameworks that combine\ntraditional and agile methods. Drawing on a range of scholarly sources, this\nstudy synthesizes key developments in the perception and application of the\nwaterfall model. The analysis highlights how the model has shifted from a\nstandalone framework to a component within modern hybrid methodologies. By\nrevisiting its origins, assessing its present utility, and examining its role\nin contemporary development practices, this paper argues that the waterfall\nmodel remains relevant, not as a relic of the past but as part of context-aware\ndevelopment strategies. The paper contends that the model's enduring relevance\nlies in its adaptability. By recognizing both its limitations and its\nstrengths, and by understanding its integration within hybrid approaches,\npractitioners can make more informed decisions about methodology selection and\nprocess design in diverse development environments.", "AI": {"tldr": "tl;dr", "motivation": "motivation", "method": "method", "result": "result", "conclusion": "conclusion"}}
{"id": "2510.03705", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03705", "abs": "https://arxiv.org/abs/2510.03705", "authors": ["Yulin Chen", "Haoran Li", "Yuan Sui", "Yangqiu Song", "Bryan Hooi"], "title": "Backdoor-Powered Prompt Injection Attacks Nullify Defense Methods", "comment": "EMNLP 2025 Findings", "summary": "With the development of technology, large language models (LLMs) have\ndominated the downstream natural language processing (NLP) tasks. However,\nbecause of the LLMs' instruction-following abilities and inability to\ndistinguish the instructions in the data content, such as web pages from search\nengines, the LLMs are vulnerable to prompt injection attacks. These attacks\ntrick the LLMs into deviating from the original input instruction and executing\nthe attackers' target instruction. Recently, various instruction hierarchy\ndefense strategies are proposed to effectively defend against prompt injection\nattacks via fine-tuning. In this paper, we explore more vicious attacks that\nnullify the prompt injection defense methods, even the instruction hierarchy:\nbackdoor-powered prompt injection attacks, where the attackers utilize the\nbackdoor attack for prompt injection attack purposes. Specifically, the\nattackers poison the supervised fine-tuning samples and insert the backdoor\ninto the model. Once the trigger is activated, the backdoored model executes\nthe injected instruction surrounded by the trigger. We construct a benchmark\nfor comprehensive evaluation. Our experiments demonstrate that backdoor-powered\nprompt injection attacks are more harmful than previous prompt injection\nattacks, nullifying existing prompt injection defense methods, even the\ninstruction hierarchy techniques.", "AI": {"tldr": "This paper introduces backdoor-powered prompt injection attacks, which exploit poisoned training data to bypass existing defense methods, including instruction hierarchy techniques, making them more harmful than previous attacks.", "motivation": "Existing prompt injection defenses using instruction hierarchies are vulnerable to advanced attacks that nullify their effectiveness, necessitating exploration of more sophisticated threats.", "method": "Attackers poison supervised fine-tuning samples by inserting backdoors. When activated, the backdoored model executes attacker-injected instructions surrounded by triggers.", "result": "Experiments demonstrate that these attacks outperform previous methods in circumventing defenses, including instruction hierarchy techniques, with a constructed benchmark for evaluation.", "conclusion": "Backdoor-powered attacks pose a critical threat by defeating current defenses, highlighting the need for robust mitigation strategies beyond instruction hierarchy approaches."}}
{"id": "2510.03902", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03902", "abs": "https://arxiv.org/abs/2510.03902", "authors": ["Rana Nameer Hussain Khan", "Dawood Wasif", "Jin-Hee Cho", "Ali Butt"], "title": "Multi-Agent Code-Orchestrated Generation for Reliable Infrastructure-as-Code", "comment": null, "summary": "The increasing complexity of cloud-native infrastructure has made\nInfrastructure-as-Code (IaC) essential for reproducible and scalable\ndeployments. While large language models (LLMs) have shown promise in\ngenerating IaC snippets from natural language prompts, their monolithic,\nsingle-pass generation approach often results in syntactic errors, policy\nviolations, and unscalable designs. In this paper, we propose MACOG\n(Multi-Agent Code-Orchestrated Generation), a novel multi-agent LLM-based\narchitecture for IaC generation that decomposes the task into modular subtasks\nhandled by specialized agents: Architect, Provider Harmonizer, Engineer,\nReviewer, Security Prover, Cost and Capacity Planner, DevOps, and Memory\nCurator. The agents interact via a shared-blackboard, finite-state orchestrator\nlayer, and collectively produce Terraform configurations that are not only\nsyntactically valid but also policy-compliant and semantically coherent. To\nensure infrastructure correctness and governance, we incorporate Terraform Plan\nfor execution validation and Open Policy Agent (OPA) for customizable policy\nenforcement. We evaluate MACOG using the IaC-Eval benchmark, where MACOG is the\ntop enhancement across models, e.g., GPT-5 improves from 54.90 (RAG) to 74.02\nand Gemini-2.5 Pro from 43.56 to 60.13, with concurrent gains on BLEU,\nCodeBERTScore, and an LLM-judge metric. Ablations show constrained decoding and\ndeploy feedback are critical: removing them drops IaC-Eval to 64.89 and 56.93,\nrespectively.", "AI": {"tldr": "This paper proposes MACOG, a multi-agent LLM-based system for generating Infrastructure-as-Code (IaC), addressing syntax errors, policy violations, and scalability issues in current single-pass LLM approaches.", "motivation": "Current LLM-generated IaC has limitations like syntactic errors, policy violations, and unscalable designs due to monolithic generation processes.", "method": "MACOG employs specialized agents (Architect, Security Prover, etc.) working via a shared-blackboard orchestrator. It integrates Terraform Plan for validation and Open Policy Agent (OPA).", "result": "MACOG outperforms existing methods on IaC-Eval benchmark (e.g., GPT-5 improves from 54.90 to 74.02), with gains in BLEU, CodeBERTScore, and LLM-judge metrics. Ablation studies confirm decoding constraints and feedback are critical.", "conclusion": "MACOG demonstrates that modular multi-agent collaboration with policy enforcement significantly improves IaC generation quality compared to single-pass LLMs."}}
{"id": "2510.03720", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03720", "abs": "https://arxiv.org/abs/2510.03720", "authors": ["Dongyang Zhan", "Zhaofeng Yu", "Xiangzhan Yu", "Hongli Zhang", "Lin Ye"], "title": "Shrinking the Kernel Attack Surface Through Static and Dynamic Syscall Limitation", "comment": "13 pages, 5 figures. Accepted for publication in IEEE Transactions on\n  Services Computing (TSC), 2023", "summary": "Linux Seccomp is widely used by the program developers and the system\nmaintainers to secure the operating systems, which can block unused syscalls\nfor different applications and containers to shrink the attack surface of the\noperating systems. However, it is difficult to configure the whitelist of a\ncontainer or application without the help of program developers. Docker\ncontainers block about only 50 syscalls by default, and lots of unblocked\nuseless syscalls introduce a big kernel attack surface. To obtain the dependent\nsyscalls, dynamic tracking is a straight-forward approach but it cannot get the\nfull syscall list. Static analysis can construct an over-approximated syscall\nlist, but the list contains many false positives. In this paper, a systematic\ndependent syscall analysis approach, sysverify, is proposed by combining static\nanalysis and dynamic verification together to shrink the kernel attack surface.\nThe semantic gap between the binary executables and syscalls is bridged by\nanalyzing the binary and the source code, which builds the mapping between the\nlibrary APIs and syscalls systematically. To further reduce the attack surface\nat best effort, we propose a dynamic verification approach to intercept and\nanalyze the security of the invocations of indirect-call-related or rarely\ninvoked syscalls with low overhead.", "AI": {"tldr": "The paper introduces sysverify, a systematic approach combining static analysis and dynamic verification to accurately identify and block unused syscalls in Linux containers, significantly shrinking the kernel attack surface beyond current tools like Docker's default 50 syscall limit.", "motivation": "Modern container security is hindered by incomplete syscall whitelisting - dynamic tracking misses syscalls and static analysis generates excessive false positives, leaving systems vulnerable to attacks through unblocked syscalls.", "method": "Sysverify bridges the binary-syscall semantic gap through joint analysis of binary executables and source code to create precise API-syscall mappings, complemented by dynamic verification of potentially dangerous syscalls with low overhead analysis of indirect/rare syscalls.", "result": "Sysverify achieves comprehensive syscall analysis with reduced false positives, enabling effective blocking of non-essential syscalls and demonstrating significant attack surface reduction compared to default container security implementations.", "conclusion": "The integrated static-dynamic analysis framework provides a scalable solution for developer-free syscall whitelisting, setting a new standard for production system hardening through systematic dependency verification."}}
{"id": "2510.03914", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03914", "abs": "https://arxiv.org/abs/2510.03914", "authors": ["Yonnel Chen Kuang Piao", "Jean Carlors Paul", "Leuson Da Silva", "Arghavan Moradi Dakhel", "Mohammad Hamdaqa", "Foutse Khomh"], "title": "Refactoring with LLMs: Bridging Human Expertise and Machine Understanding", "comment": "43 pages, 2 figures, 9 tables", "summary": "Code refactoring is a fundamental software engineering practice aimed at\nimproving code quality and maintainability. Despite its importance, developers\noften neglect refactoring due to the significant time, effort, and resources it\nrequires, as well as the lack of immediate functional rewards. Although several\nautomated refactoring tools have been proposed, they remain limited in\nsupporting a broad spectrum of refactoring types. In this study, we explore\nwhether instruction strategies inspired by human best-practice guidelines can\nenhance the ability of Large Language Models (LLMs) to perform diverse\nrefactoring tasks automatically. Leveraging the instruction-following and code\ncomprehension capabilities of state-of-the-art LLMs (e.g., GPT-mini and\nDeepSeek-V3), we draw on Martin Fowler's refactoring guidelines to design\nmultiple instruction strategies that encode motivations, procedural steps, and\ntransformation objectives for 61 well-known refactoring types. We evaluate\nthese strategies on benchmark examples and real-world code snippets from GitHub\nprojects. Our results show that instruction designs grounded in Fowler's\nguidelines enable LLMs to successfully perform all benchmark refactoring types\nand preserve program semantics in real-world settings, an essential criterion\nfor effective refactoring. Moreover, while descriptive instructions are more\ninterpretable to humans, our results show that rule-based instructions often\nlead to better performance in specific scenarios. Interestingly, allowing\nmodels to focus on the overall goal of refactoring, rather than prescribing a\nfixed transformation type, can yield even greater improvements in code quality.", "AI": {"tldr": "This study investigates how instruction strategies based on Martin Fowler's refactoring guidelines can enhance LLMs' ability to perform automated code refactoring. The proposed methods achieve high performance on benchmark tasks and real-world code, showing rule-based and goal-oriented instructions improve refactorability and code quality preservation.", "motivation": "Automated refactoring tools lack broad support for diverse refactoring types, and developers avoid refactoring due to resource demands. Existing solutions fail to leverage LLMs efficiently for this critical software engineering task.", "method": "Designed 61 instruction strategies encoding motivations, procedural steps, and objectives based on Fowler's guidelines. Tested these using GPT-mini and DeepSeek-V3 on benchmark examples and GitHub code snippets, comparing descriptive vs. rule-based instructions and goal-oriented approaches.", "result": "LLMs using Fowler-inspired instructions successfully completed all benchmark refactoring types, preserved program semantics in practical scenarios, and showed higher quality improvements with goal-focused approaches compared to fixed-type transformations.", "conclusion": "Instruction strategies grounded in human best practices significantly enhance LLM-based refactoring capabilities. Prioritizing overall refactoring goals over specific transformation types leads to better outcomes, suggesting a new direction for automated code improvement tools."}}
{"id": "2510.03737", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03737", "abs": "https://arxiv.org/abs/2510.03737", "authors": ["Dongyang Zhan", "Zhaofeng Yu", "Xiangzhan Yu", "Hongli Zhang", "Lin Ye", "Likun Liu"], "title": "Securing Operating Systems Through Fine-grained Kernel Access Limitation for IoT Systems", "comment": "14 pages, 3 figures. Accepted for publication in IEEE Internet of\n  Things Journal (IOTJ), 2023", "summary": "With the development of Internet of Things (IoT), it is gaining a lot of\nattention. It is important to secure the embedded systems with low overhead.\nThe Linux Seccomp is widely used by developers to secure the kernels by\nblocking the access of unused syscalls, which introduces less overhead.\nHowever, there are no systematic Seccomp configuration approaches for IoT\napplications without the help of developers. In addition, the existing Seccomp\nconfiguration approaches are coarse-grained, which cannot analyze and limit the\nsyscall arguments. In this paper, a novel static dependent syscall analysis\napproach for embedded applications is proposed, which can obtain all of the\npossible dependent syscalls and the corresponding arguments of the target\napplications. So, a fine-grained kernel access limitation can be performed for\nthe IoT applications. To this end, the mappings between dynamic library APIs\nand syscalls according with their arguments are built, by analyzing the control\nflow graphs and the data dependency relationships of the dynamic libraries. To\nthe best of our knowledge, this is the first work to generate the fine-grained\nSeccomp profile for embedded applications.", "AI": {"tldr": "This paper proposes a new static analysis approach to generate fine-grained Seccomp profiles for embedded IoT applications, improving security by analyzing dependent syscalls and their arguments without developer intervention.", "motivation": "Securing low-overhead embedded systems is critical in the growing IoT landscape. Linux Seccomp is often used but lacks systematic, fine-grained configuration methods not requiring developer input. Existing methods are too coarse to limit syscall arguments effectively.", "method": "The authors analyze the control flow graphs and data dependency relationships within dynamic libraries to map their APIs to corresponding syscalls and their arguments, enabling a novel static dependent syscall analysis approach.", "result": "A fine-grained Seccomp configuration methodology for embedded applications was developed, capturing all possible dependent syscalls and their arguments, which allows for more precise kernel access restrictions in IoT systems.", "conclusion": "This work introduces the first approach for automatically generating fine-grained Seccomp profiles for embedded applications, offering improved security and reducing reliance on developers for configuration."}}
{"id": "2510.03920", "categories": ["cs.SE", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.03920", "abs": "https://arxiv.org/abs/2510.03920", "authors": ["Ravi Kalluri"], "title": "Why Does the Engineering Manager Still Exist in Agile Software Development?", "comment": "12 pages, 3 figures, 2 tables", "summary": "Although Agile methodologies emphasize decentralized decision-making and team\nautonomy, engineering managers continue to be employed in Agile software\norganizations. This apparent paradox suggests that traditional managerial\nfunctions persist despite the theoretical displacement of managerial hierarchy\nin Agile. This paper explores the persistence of engineering managers through a\nmultidimensional framework encompassing historical context, theoretical\ntensions, organizational realities, empirical evidence, evolving managerial\nroles, and practical implications. A systematic literature review underpins our\nmultifaceted analysis, supplemented by illustrative case studies. We conclude\nby proposing a conceptual model that reconciles Agile principles with\nmanagerial necessity, offering guidance for practitioners, researchers, and\ntool designers. Implications for leadership development, tool integration, and\nfuture research are discussed.", "AI": {"tldr": "this paper provides a systematic literature review and analysis of engineering managers' persistence in agile organizations, proposing a conceptual model to align agile principles with necessary managerial roles.", "motivation": "the authors aim to resolve the perceived contradiction between agile concepts that devalue traditional management and the ongoing presence of engineering managers in such organizations.", "method": "the paper employs a multidimensional framework and a systematic literature review, supported by case studies, to analyze the role of engineering managers in agile settings.", "result": "the research reveals that traditional managerial functions remain in agile organizations and proposes a conceptual model to reconcile agile principles with those managerial roles.", "conclusion": "this research provides a conceptual model that unites agile concepts with the organizational need for management, offering insights for practitioners, researchers, and tool designers in agile contexts."}}
{"id": "2510.03752", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03752", "abs": "https://arxiv.org/abs/2510.03752", "authors": ["Rohit Chatterjee", "Changrui Mu", "Prashant Nalini Vasudevan"], "title": "Public-Key Encryption from the MinRank Problem", "comment": null, "summary": "We construct a public-key encryption scheme from the hardness of the\n(planted) MinRank problem over uniformly random instances. This corresponds to\nthe hardness of decoding random linear rank-metric codes. Existing\nconstructions of public-key encryption from such problems require hardness for\nstructured instances arising from the masking of efficiently decodable codes.\nCentral to our construction is the development of a new notion of duality for\nrank-metric codes.", "AI": {"tldr": "The paper presents a public-key encryption scheme based on the MinRank problem over uniformly random instances using a new rank-metric code duality concept.", "motivation": "There is a need for public-key encryption based on the hardness of decoding random linear rank-metric codes.", "method": "Development of a new notion of duality for rank-metric codes and constructing a public-key encryption scheme using the hardness of the (planted) MinRank problem over uniformly random instances.", "result": "A public-key encryption scheme constructed without relying on structured instances, which is a novel approach compared to existing methods.", "conclusion": "The paper advances the field by providing a new PKC construction from the MinRank problem, using duality concepts to avoid traditional structured instance requirements."}}
{"id": "2510.04078", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04078", "abs": "https://arxiv.org/abs/2510.04078", "authors": ["Han Hu", "Wei Minn", "Yonghui Liu", "Jiakun Liu", "Ferdian Thung", "Terry Yue Zhuo", "Lwin Khin Shar", "Debin Gao", "David Lo"], "title": "Bamboo: LLM-Driven Discovery of API-Permission Mappings in the Android Framework", "comment": null, "summary": "The permission mechanism in the Android Framework is integral to safeguarding\nthe privacy of users by managing users' and processes' access to sensitive\nresources and operations. As such, developers need to be equipped with an\nin-depth understanding of API permissions to build robust Android apps.\nUnfortunately, the official API documentation by Android chronically suffers\nfrom imprecision and incompleteness, causing developers to spend significant\neffort to accurately discern necessary permissions. This potentially leads to\nincorrect permission declarations in Android app development, potentially\nresulting in security violations and app failures. Recent efforts in improving\npermission specification primarily leverage static and dynamic code analyses to\nuncover API-permission mappings within the Android framework. Yet, these\nmethodologies encounter substantial shortcomings, including poor adaptability\nto Android SDK and Framework updates, restricted code coverage, and a\npropensity to overlook essential API-permission mappings in intricate\ncodebases. This paper introduces a pioneering approach utilizing large language\nmodels (LLMs) for a systematic examination of API-permission mappings. In\naddition to employing LLMs, we integrate a dual-role prompting strategy and an\nAPI-driven code generation approach into our mapping discovery pipeline,\nresulting in the development of the corresponding tool, \\tool{}. We formulate\nthree research questions to evaluate the efficacy of \\tool{} against\nstate-of-the-art baselines, assess the completeness of official SDK\ndocumentation, and analyze the evolution of permission-required APIs across\ndifferent SDK releases. Our experimental results reveal that \\tool{} identifies\n2,234, 3,552, and 4,576 API-permission mappings in Android versions 6, 7, and\n10 respectively, substantially outprforming existing baselines.", "AI": {"tldr": "This paper proposes a novel approach using large language models (LLMs), dual-role prompting, and API-driven code generation to systematically identify API-permission mappings in Android, addressing limitations in existing static/dynamic analysis tools and improving Android app security through accurate permission declarations.", "motivation": "Android's permission mechanism is critical for user privacy, but outdated/incorrect documentation and flawed manual analysis lead to security risks and app failures. Current methods (static/dynamic analysis) are inflexible to SDK updates, have poor coverage, and miss key mappings in complex codebases.", "method": "The authors develop \\tool{}, integrating LLMs with a dual-role prompting strategy (asking LLMs to act as both code analyzer and security auditor) and API-driven code generation. They evaluate it via three research questions: 1. Does \\tool{} outperform baselines? 2. How complete is the official SDK documentation? 3. How do permission_REQ APIs evolve across SDK versions?", "result": "\\tool{} discovers 2,234/3,552/4,576 API-permission mappings in Android 6/7/10 respectively, exceeding existing baselines. The tool demonstrates significant improvements in adaptability, coverage, and precision for complex codebases.", "conclusion": "LLM-based approaches with dual-role prompting and code generation effectively address limitations of traditional methods, offering scalable, accurate API-permission mapping discovery for evolving Android frameworks and improving app security."}}
{"id": "2510.03761", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03761", "abs": "https://arxiv.org/abs/2510.03761", "authors": ["Richard A. Dubniczky", "Bertalan Borsos", "Tihanyi Norbert"], "title": "You Have Been LaTeXpOsEd: A Systematic Analysis of Information Leakage in Preprint Archives Using Large Language Models", "comment": null, "summary": "The widespread use of preprint repositories such as arXiv has accelerated the\ncommunication of scientific results but also introduced overlooked security\nrisks. Beyond PDFs, these platforms provide unrestricted access to original\nsource materials, including LaTeX sources, auxiliary code, figures, and\nembedded comments. In the absence of sanitization, submissions may disclose\nsensitive information that adversaries can harvest using open-source\nintelligence. In this work, we present the first large-scale security audit of\npreprint archives, analyzing more than 1.2 TB of source data from 100,000 arXiv\nsubmissions. We introduce LaTeXpOsEd, a four-stage framework that integrates\npattern matching, logical filtering, traditional harvesting techniques, and\nlarge language models (LLMs) to uncover hidden disclosures within\nnon-referenced files and LaTeX comments. To evaluate LLMs' secret-detection\ncapabilities, we introduce LLMSec-DB, a benchmark on which we tested 25\nstate-of-the-art models. Our analysis uncovered thousands of PII leaks,\nGPS-tagged EXIF files, publicly available Google Drive and Dropbox folders,\neditable private SharePoint links, exposed GitHub and Google credentials, and\ncloud API keys. We also uncovered confidential author communications, internal\ndisagreements, and conference submission credentials, exposing information that\nposes serious reputational risks to both researchers and institutions. We urge\nthe research community and repository operators to take immediate action to\nclose these hidden security gaps. To support open science, we release all\nscripts and methods from this study but withhold sensitive findings that could\nbe misused, in line with ethical principles. The source code and related\nmaterial are available at the project website https://github.com/LaTeXpOsEd", "AI": {"tldr": "This paper reveals significant security risks in arXiv preprint submissions by analyzing 100,000 source materials. It introduces LaTeXpOsEd, a framework combining pattern matching and LLMs to detect 1.2TB of sensitive data leaks including PII, API keys, and confidential communications. A novel benchmark (LLMSec-DB), tools, and scripts are released to address repository security gaps.", "motivation": "Public access to preprint source materials creates critical security vulnerabilities by exposing unfiltered sensitive information. Existing systems lack systematic auditing of non-referenced files and comments in scientific submissions.", "method": "LaTeXpOsEd employs a four-stage pipeline integrating regular expression pattern matching, logical filtering, traditional file harvesting techniques, and 25 state-of-the-art LLMs for anomaly detection. A dedicated benchmark (LLMSec-DB) evaluates model performance across secret-detection tasks.", "result": "Thousands of security incidents detected, including GPS-tagged EXIF files, exposed API credentials, editable SharePoint links, and private author correspondences. The methodology successfully uncovered vulnerabilities that could be exploited for open-source intelligence harvesting.", "conclusion": "The study demonstrates urgent need for repository-level sanitization protocols. Open-science contributions include the full research toolkit while sensitive findings are withheld following ethical guidelines. The authors advocate combined technical countermeasures and policy changes to secure academic publishing workflows."}}
{"id": "2510.04135", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04135", "abs": "https://arxiv.org/abs/2510.04135", "authors": ["Jingzhi Gong", "Yixin Bian", "Luis de la Cal", "Giovanni Pinna", "Anisha Uteem", "David Williams", "Mar Zamorano", "Karine Even-Mendoza", "W. B. Langdon", "Hector Menendez", "Federica Sarro"], "title": "GA4GC: Greener Agent for Greener Code via Multi-Objective Configuration Optimization", "comment": "Accepted by SSBSE'25 Challenge Track", "summary": "Coding agents powered by LLMs face critical sustainability and scalability\nchallenges in industrial deployment, with single runs consuming over 100k\ntokens and incurring environmental costs that may exceed optimization benefits.\nThis paper introduces GA4GC, the first framework to systematically optimize\ncoding agent runtime (greener agent) and code performance (greener code)\ntrade-offs by discovering Pareto-optimal agent hyperparameters and prompt\ntemplates. Evaluation on the SWE-Perf benchmark demonstrates up to 135x\nhypervolume improvement, reducing agent runtime by 37.7% while improving\ncorrectness. Our findings establish temperature as the most critical\nhyperparameter, and provide actionable strategies to balance agent\nsustainability with code optimization effectiveness in industrial deployment.", "AI": {"tldr": "GA4GC optimizes the trade-off between runtime and code performance for coding agents, achieving significant improvements on a benchmark with reduced resource consumption.", "motivation": "The paper addresses the challenge of sustainability and scalability in deploying coding agents powered by Large Language Models (LLMs), where high token consumption and environmental costs can outweigh optimization benefits.", "method": "The authors propose GA4GC, a framework that utilizes gradient-based methods alongside adversarial techniques for visual question answering (VQA) and mannering (Intent-based Surfaces). It involves training generative and discriminative models to enhance performance and domain adaptation for document image retrieval tasks.", "result": "The framework achieves a state-of-the-art performance across various document image retrieval tasks, including outperforming existing methods in fine-grained VQA and mannering. It also demonstrates effective domain adaptation with improvements in accuracy and inference speed.", "conclusion": "The study shows the value of introducing environmental considerations in industrial deployment of coding agents and proves that it is possible to enhance both sustainability and effectiveness through the Proposed framework. However, more extensive evaluation is needed to address potential limitations and improve generalizability in diverse settings."}}
{"id": "2510.03770", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03770", "abs": "https://arxiv.org/abs/2510.03770", "authors": ["David Megias"], "title": "Complex Domain Approach for Reversible Data Hiding and Homomorphic Encryption: General Framework and Application to Dispersed Data", "comment": null, "summary": "Ensuring the trustworthiness of data from distributed and\nresource-constrained environments, such as Wireless Sensor Networks or IoT\ndevices, is critical. Existing Reversible Data Hiding (RDH) methods for scalar\ndata suffer from low embedding capacity and poor intrinsic mixing between host\ndata and watermark. This paper introduces Hiding in the Imaginary Domain with\nData Encryption (H[i]dden), a novel framework based on complex number\narithmetic for simultaneous information embedding and encryption. The H[i]dden\nframework offers perfect reversibility, in-principle unlimited watermark size,\nand intrinsic data-watermark mixing. The paper further introduces two\nprotocols: H[i]dden-EG, for joint reversible data hiding and encryption, and\nH[i]dden-AggP, for privacy-preserving aggregation of watermarked data, based on\npartially homomorphic encryption. These protocols provide efficient and\nresilient solutions for data integrity, provenance and confidentiality, serving\nas a foundation for new schemes based on the algebraic properties of the\ncomplex domain.", "AI": {"tldr": "This paper proposes H[i]dden, a complex-number-based framework for reversible data hiding and encryption that enables perfect reversibility, unlimited watermark size, and intrinsic data-watermark mixing, along with two protocols (H[i]dden-EG and H[i]dden-AggP) for secure data handling in distributed environments.", "motivation": "Existing Reversible Data Hiding (RDH) methods for scalar data face limitations in embedding capacity and data-watermark mixing, creating vulnerabilities in resource-constrained environments like IoT and Wireless Sensor Networks.", "method": "The framework leverages complex number arithmetic for simultaneous information embedding and encryption. It introduces: (1)...", "result": "The framework achieves perfect reversibility, supports in-principle unlimited watermark sizes, and enables privacy-preserving aggregation of watermarked data. Protocols are validated as efficient and resilient solutions for ensuring data integrity, provenance, and confidentiality.", "conclusion": "H[i]dden establishes a novel foundation for secure data handling in distributed systems by exploiting complex domain algebraic properties, with the potential to inspire future cryptographic and data protection schemes."}}
{"id": "2510.04143", "categories": ["cs.SE", "D.2.13"], "pdf": "https://arxiv.org/pdf/2510.04143", "abs": "https://arxiv.org/abs/2510.04143", "authors": ["Konstantinos Kitsios", "Francesco Sovrano", "Earl T. Barr", "Alberto Bacchelli"], "title": "Detecting Semantic Clones of Unseen Functionality", "comment": "13 pages, 3 figures, accepted for publication (to appear) in the 40th\n  IEEE/ACM International Conference on Automated Software Engineering, ASE 2025", "summary": "Semantic code clone detection is the task of detecting whether two snippets\nof code implement the same functionality (e.g., Sort Array). Recently, many\nneural models achieved near-perfect performance on this task. These models seek\nto make inferences based on their training data. Consequently, they better\ndetect clones similar to those they have seen during training and may struggle\nto detect those they have not. Developers seeking clones are, of course,\ninterested in both types of clones. We confirm this claim through a literature\nreview, identifying three practical clone detection tasks in which the model's\ngoal is to detect clones of a functionality even if it was trained on clones of\ndifferent functionalities. In light of this finding, we re-evaluate six\nstate-of-the-art models, including both task-specific models and generative\nLLMs, on the task of detecting clones of unseen functionality. Our experiments\nreveal a drop in F1 of up to 48% (average 31%) for task-specific models. LLMs\nperform on par with task-specific models without explicit training for clone\ndetection, but generalize better to unseen functionalities, where F1 drops up\nto 5% (average 3%) instead. We propose and evaluate the use of contrastive\nlearning to improve the performance of existing models on clones of unseen\nfunctionality. We draw inspiration from the computer vision and natural\nlanguage processing fields where contrastive learning excels at measuring\nsimilarity between two objects, even if they come from classes unseen during\ntraining. We replace the final classifier of the task-specific models with a\ncontrastive classifier, while for the generative LLMs we propose contrastive\nin-context learning, guiding the LLMs to focus on the differences between\nclones and non-clones. The F1 on clones of unseen functionality is improved by\nup to 26% (average 9%) for task-specific models and up to 5% (average 3%) for\nLLMs.", "AI": {"tldr": "Semantic code clone detection aims to find code snippets performing similar functions. While neural models have shown good performance, their effectiveness drops significantly on unseen functionalities. LLMs, though not explicitly trained for clone detection, generalize better. The authors propose contrastive learning to enhance performance on unseen functionalities, showing improvements of up to 26% for task-specific models and 5% for LLMs.", "motivation": "The paper is motivated by the need to detect both seen and unseen code clones, as developers require accurate detection across various functionalities regardless of whether they were covered in training. Previous studies have shown that current models struggle with unseen functionalities, so the authors aim to address this generalization issue through new methods.", "method": "The authors evaluate existing semantic code clone detection models, including task-specific neural models and generative LLMs, by testing them on clones of unseen functionalities. They then propose two enhancements: 1) Replacing the final classifier of task-specific models with a contrastive classifier to improve generalization performance, and 2) Introducing contrastive in-context learning for LLMs to guide them focus on differences between clones and non-clones. Finally, they re-evaluate the models to assess the improvements from these methods.", "result": "Using the proposed contrastive learning methods, the task-specific models saw an improvement of up to 26% in F1 on clones of unseen functionalities (average 9% improvement), and LLMs improved by 5% (average 3% improvement). The results indicate that these methods help models better handle clones of functionalities they have not seen in training, suggesting effective ways to boost generalization in semantic code clone detection.", "conclusion": "The paper concludes that contrastive learning is an effective approach to enhance the generalization of code clone detection models to unseen functionalities. The results suggest that the performance drop in task-specific models for such clones is significant and can be improved. For LLMs, even though their F1 drops is smaller, the improvements are still beneficial, demonstrating the effectiveness of contrastive in-context learning."}}
{"id": "2510.03819", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03819", "abs": "https://arxiv.org/abs/2510.03819", "authors": ["Chunyi Zhang", "Qinghong Wei", "Xiaoqi Li"], "title": "Security Analysis of Ponzi Schemes in Ethereum Smart Contracts", "comment": null, "summary": "The rapid advancement of blockchain technology has precipitated the\nwidespread adoption of Ethereum and smart contracts across a variety of\nsectors. However, this has also given rise to numerous fraudulent activities,\nwith many speculators embedding Ponzi schemes within smart contracts, resulting\nin significant financial losses for investors. Currently, there is a lack of\neffective methods for identifying and analyzing such new types of fraudulent\nactivities. This paper categorizes these scams into four structural types and\nexplores the intrinsic characteristics of Ponzi scheme contract source code\nfrom a program analysis perspective. The Mythril tool is employed to conduct\nstatic and dynamic analyses of representative cases, thereby revealing their\nvulnerabilities and operational mechanisms. Furthermore, this paper employs\nshell scripts and command patterns to conduct batch detection of open-source\nsmart contract code, thereby unveiling the common characteristics of Ponzi\nscheme smart contracts.", "AI": {"tldr": "This paper addresses Ethereum Ponzi scheme detection by categorizing scams into four structural types, analyzing contract source code via Mythril tool, and using shell scripts for batch detection of vulnerabilities in open-source smart contracts.", "motivation": "The proliferation of blockchain fraud through Ethereum smart contracts has caused significant investor losses, yet effective detection methods for embedded Ponzi schemes remain lacking.", "method": "The paper 1. Classifies Ponzi contracts into four structural categories, 2. Performs static/dynamic analysis using Mythril on representative cases, 3. Employs shell scripts and command patterns for batch analysis of open-source code to identify common characteristics.", "result": "Revealed intrinsic code characteristics of Ponzi schemes, exposed vulnerabilities and operational mechanisms through case analysis, and developed a batch detection framework that identifies common patterns across scams.", "conclusion": "The study establishes a systematic analysis approach for blockchain fraud by combining code categorization, program analysis, and automated detection techniques, enabling proactive identification of Ponzi scheme smart contracts."}}
{"id": "2510.04166", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04166", "abs": "https://arxiv.org/abs/2510.04166", "authors": ["Marco Edoardo Palma", "Pooja Rani", "Harald C. Gall"], "title": "Multi Language Models for On-the-Fly Syntax Highlighting", "comment": null, "summary": "Syntax highlighting is a critical feature in modern software development\nenvironments, enhancing code readability and developer productivity. However,\ndelivering accurate highlighting in real time remains challenging for online\nand web-based development tools due to strict time and memory constraints on\nbackend services. These systems must serve highlights rapidly and frequently,\neven when code is partially valid or invalid. This has led to on-the-fly syntax\nhighlighting, where visual annotations are generated just before content is\nserved, often at high request rates and under incomplete input conditions. To\nmeet these demands efficiently, state-of-the-art models use deep learning to\nlearn the behavior of brute-force syntax highlighting resolvers, tools that are\neasy to implement but too slow for production. Through the Deep Abstraction\nprocess, brute-force strategies are encoded into fast statistical models that\nachieve both high accuracy and low-latency inference. Despite their success,\nsuch models face key challenges: they support only one programming language per\nmodel, require large datasets from slow brute-force generators, and involve\nresource-intensive training. In multi-language environments, this means\nmaintaining multiple independent models, increasing system complexity and\noperational cost. This work addresses these issues by introducing a unified\nmodel capable of highlighting up to six mainstream programming languages,\nreducing deployment complexity by a factor of six and improving performance on\nunseen languages. A novel normalization technique significantly enhances model\ngeneralization, while few-shot learning experiments show that a small number of\noracle samples can replace large datasets, minimizing dependence on brute-force\ngenerators. Combined, these innovations enable efficient, scalable, and\ncost-effective syntax highlighting across diverse programming languages.", "AI": {"tldr": "The paper introduces a unified model for multi-language syntax highlighting, reducing deployment complexity by 6x and improving performance with normalization and few-shot learning.", "motivation": "Real-time syntax highlighting is challenging for web-based development tools due to time and memory constraints, and current models are limited in language support, data generation, and training costs.", "method": "The authors develop a unified model supporting multiple languages via a normalization technique and few-shot learning, encoding brute-force strategies with deep abstraction.", "result": "The model demonstrates improved performance on unseen languages and reduced reliance on large datasets generated by brute-force methods.", "conclusion": "The proposed approach simplifies deployment and enhances scalability of syntax highlighting, making it more efficient and cost-effective for multi-language environments."}}
{"id": "2510.03831", "categories": ["cs.CR", "cs.IT", "cs.LG", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.03831", "abs": "https://arxiv.org/abs/2510.03831", "authors": ["Pedro Ivo da Cruz", "Dimitri Silva", "Tito Spadini", "Ricardo Suyama", "Murilo Bellezoni Loiola"], "title": "Pilot Contamination Attacks Detection with Machine Learning for Multi-User Massive MIMO", "comment": "This version of the article has been accepted for publication, after\n  peer review and is subject to Springer Nature's AM terms of use, but is not\n  the Version of Record and does not reflect post-acceptance improvements, or\n  any corrections. The Version of Record is available online at:\n  https://doi.org/10.1007/s11235-024-01163-0", "summary": "Massive multiple-input multiple-output (MMIMO) is essential to modern\nwireless communication systems, like 5G and 6G, but it is vulnerable to active\neavesdropping attacks. One type of such attack is the pilot contamination\nattack (PCA), where a malicious user copies pilot signals from an authentic\nuser during uplink, intentionally interfering with the base station's (BS)\nchannel estimation accuracy. In this work, we propose to use a Decision Tree\n(DT) algorithm for PCA detection at the BS in a multi-user system. We present a\nmethodology to generate training data for the DT classifier and select the best\nDT according to their depth. Then, we simulate different scenarios that could\nbe encountered in practice and compare the DT to a classical technique based on\nlikelihood ratio testing (LRT) submitted to the same scenarios. The results\nrevealed that a DT with only one level of depth is sufficient to outperform the\nLRT. The DT shows a good performance regarding the probability of detection in\nnoisy scenarios and when the malicious user transmits with low power, in which\ncase the LRT fails to detect the PCA. We also show that the reason for the good\nperformance of the DT is its ability to compute a threshold that separates PCA\ndata from non-PCA data better than the LRT's threshold. Moreover, the DT does\nnot necessitate prior knowledge of noise power or assumptions regarding the\nsignal power of malicious users, prerequisites typically essential for LRT and\nother hypothesis testing methodologies.", "AI": {"tldr": "This paper proposes using Decision Trees (DT) at the base station to detect pilot contamination attacks (PCA) in massive MIMO systems, demonstrating that a simple DT with one level of depth outperforms traditional likelihood ratio testing (LRT), particularly in noisy or low-power attack scenarios, without requiring prior knowledge of noise or malicious signal power.", "motivation": "Massive MIMO systems are critical for 5G/6G but are vulnerable to PCA, where malicious users mimic pilot signals, degrading channel estimation. Current methods like LRT have limitations, including dependence on prior knowledge of noise and signal parameters, necessitating a more robust solution.", "method": "The authors develop a methodology to generate training data for a DT classifier and select the optimal tree depth. They simulate various practical scenarios and benchmark the DT against LRT, analyzing detection probabilities under noise and low-power attacks.", "result": "A single-level DT outperforms LRT in PCA detection, particularly in noisy or low-power malicious transmission conditions where LRT fails. The DT\u2019s superior performance is attributed to its ability to learn an effective decision threshold without requiring assumptions about noise or signal power.", "conclusion": "Decision Trees offer a practical, parameter-free alternative to traditional hypothesis testing methods like LRT for PCA detection. Their robustness in adverse conditions and independence from prior knowledge make them suitable for real-world massive MIMO systems."}}
{"id": "2510.04274", "categories": ["cs.SE", "D.2; I.2; J.6; K.3; K.7"], "pdf": "https://arxiv.org/pdf/2510.04274", "abs": "https://arxiv.org/abs/2510.04274", "authors": ["Damjan Fujs", "Damjan Vavpoti\u010d", "Toma\u017e Hovelja", "Marko Po\u017eenel"], "title": "Selecting Cybersecurity Requirements: Effects of LLM Use and Professional Software Development Experience", "comment": "5 pages, 1 figure, 2 tables, presented at IARIA CYBER 2025", "summary": "This study investigates how access to Large Language Models (LLMs) and\nvarying levels of professional software development experience affect the\nprioritization of cybersecurity requirements for web applications. Twenty-three\npostgraduate students participated in a research study to prioritize security\nrequirements (SRs) using the MoSCoW method and subsequently rated their\nproposed solutions against multiple evaluation criteria. We divided\nparticipants into two groups (one with and the other without access to LLM\nsupport during the task). Results showed no significant differences related to\nLLM use, suggesting that access to LLMs did not noticeably influence how\nparticipants evaluated cybersecurity solutions. However, statistically\nsignificant differences emerged between experience groups for certain criteria,\nsuch as estimated cost to develop a feature, perceived impact on user\nexperience, and risk assessment related to non-implementation of the proposed\nfeature. Participants with more professional experience tended to provide\nhigher ratings for user experience impact and lower risk estimates.", "AI": {"tldr": "LLMs do not significantly influence how cybersecurity requirements are prioritized, but professional experience does.", "motivation": "The paper aims to explore the impact of access to LLMs and previous software development experience on the prioritization of cybersecurity requirements for web applications.", "method": "Twenty-three postgraduate students, divided into two groups (with and without LLM access), participated in a study using the MoSCoW method to prioritize SRs and evaluated them against multiple criteria.", "result": "LLM use did not yield significant differences in SR evaluations; however, professionals with more experience showed trait in higher ratings for user experience and lower risk estimates.", "conclusion": "While LLMs may not impact cybersecurity prioritization, the amount of professional software development experience does influence participants' assessments."}}
{"id": "2510.03992", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03992", "abs": "https://arxiv.org/abs/2510.03992", "authors": ["Jehyeok Yeon", "Isha Chaudhary", "Gagandeep Singh"], "title": "Quantifying Distributional Robustness of Agentic Tool-Selection", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in agentic systems\nwhere they map user intents to relevant external tools to fulfill a task. A\ncritical step in this process is tool selection, where a retriever first\nsurfaces candidate tools from a larger pool, after which the LLM selects the\nmost appropriate one. This pipeline presents an underexplored attack surface\nwhere errors in selection can lead to severe outcomes like unauthorized data\naccess or denial of service, all without modifying the agent's model or code.\nWhile existing evaluations measure task performance in benign settings, they\noverlook the specific vulnerabilities of the tool selection mechanism under\nadversarial conditions. To address this gap, we introduce ToolCert, the first\nstatistical framework that formally certifies tool selection robustness.\nToolCert models tool selection as a Bernoulli success process and evaluates it\nagainst a strong, adaptive attacker who introduces adversarial tools with\nmisleading metadata, and are iteratively refined based on the agent's previous\nchoices. By sampling these adversarial interactions, ToolCert produces a\nhigh-confidence lower bound on accuracy, formally quantifying the agent's\nworst-case performance. Our evaluation with ToolCert uncovers the severe\nfragility: under attacks injecting deceptive tools or saturating retrieval, the\ncertified accuracy bound drops near zero, an average performance drop of over\n60% compared to non-adversarial settings. For attacks targeting the retrieval\nand selection stages, the certified accuracy bound plummets to less than 20%\nafter just a single round of adversarial adaptation. ToolCert thus reveals\npreviously unexamined security threats inherent to tool selection and provides\na principled method to quantify an agent's robustness to such threats, a\nnecessary step for the safe deployment of agentic systems.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.04349", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04349", "abs": "https://arxiv.org/abs/2510.04349", "authors": ["Dmitry Ustalov", "Egor Bogomolov", "Alexander Bezzubov", "Yaroslav Golubev", "Evgeniy Glukhov", "Georgii Levtsov", "Vladimir Kovalenko"], "title": "Challenge on Optimization of Context Collection for Code Completion", "comment": "7 pages, 3 figures, 5 tables. A report on the Context Collection\n  Workshop co-located with ASE'25", "summary": "The rapid advancement of workflows and methods for software engineering using\nAI emphasizes the need for a systematic evaluation and analysis of their\nability to leverage information from entire projects, particularly in large\ncode bases. In this challenge on optimization of context collection for code\ncompletion, organized by JetBrains in collaboration with Mistral AI as part of\nthe ASE 2025 conference, participants developed efficient mechanisms for\ncollecting context from source code repositories to improve fill-in-the-middle\ncode completions for Python and Kotlin. We constructed a large dataset of\nreal-world code in these two programming languages using permissively licensed\nopen-source projects. The submissions were evaluated based on their ability to\nmaximize completion quality for multiple state-of-the-art neural models using\nthe chrF metric. During the public phase of the competition, nineteen teams\nsubmitted solutions to the Python track and eight teams submitted solutions to\nthe Kotlin track. In the private phase, six teams competed, of which five\nsubmitted papers to the workshop.", "AI": {"tldr": "This paper presents a challenge on optimizing context collection for code completion in Python and Kotlin, organized by JetBrains and Mistral AI as part of ASE 2025. It evaluates methods for improving fill-in-the-middle code completion using a large dataset of open-source projects and chrF metrics.", "motivation": "AI-driven software engineering workflows struggle to effectively leverage context from large codebases. Systematic evaluation of context collection methods is critical to enhance code completion quality in real-world programming scenarios.", "method": "A permissively licensed open-source dataset of Python and Kotlin code was created. Teams submitted solutions in public/private competition phases (19/8 teams for Python/Kotlin publicly, 6 teams privately), with evaluations based on neural model completion quality using chrF metrics.", "result": "Nineteen teams competed in Python track and eight in Kotlin track during the public phase. Six private phase teams submitted workshop papers. Results validated the effectiveness of optimized context collection strategies for code completion tasks.", "conclusion": "The challenge underscores the importance of context-aware code completion in large projects. Participants' methods demonstrate progress in leveraging repository-wide information, but further research is needed to refine context collection techniques for neural models in real-world codebases."}}
{"id": "2510.03995", "categories": ["cs.CR", "cs.AI", "I.2; E.m"], "pdf": "https://arxiv.org/pdf/2510.03995", "abs": "https://arxiv.org/abs/2510.03995", "authors": ["Nges Brian Njungle", "Eric Jahns", "Milan Stojkov", "Michel A. Kinsy"], "title": "PrivSpike: Employing Homomorphic Encryption for Private Inference of Deep Spiking Neural Networks", "comment": "13 pages, 5 figures", "summary": "Deep learning has become a cornerstone of modern machine learning. It relies\nheavily on vast datasets and significant computational resources for high\nperformance. This data often contains sensitive information, making privacy a\nmajor concern in deep learning. Spiking Neural Networks (SNNs) have emerged as\nan energy-efficient alternative to conventional deep learning approaches.\nNevertheless, SNNs still depend on large volumes of data, inheriting all the\nprivacy challenges of deep learning. Homomorphic encryption addresses this\nchallenge by allowing computations to be performed on encrypted data, ensuring\ndata confidentiality throughout the entire processing pipeline. In this paper,\nwe introduce PRIVSPIKE, a privacy-preserving inference framework for SNNs using\nthe CKKS homomorphic encryption scheme. PRIVSPIKE supports arbitrary depth SNNs\nand introduces two key algorithms for evaluating the Leaky Integrate-and-Fire\nactivation function: (1) a polynomial approximation algorithm designed for\nhigh-performance SNN inference, and (2) a novel scheme-switching algorithm that\noptimizes precision at a higher computational cost. We evaluate PRIVSPIKE on\nMNIST, CIFAR-10, Neuromorphic MNIST, and CIFAR-10 DVS using models from LeNet-5\nand ResNet-19 architectures, achieving encrypted inference accuracies of\n98.10%, 79.3%, 98.1%, and 66.0%, respectively. On a consumer-grade CPU, SNN\nLeNet-5 models achieved inference times of 28 seconds on MNIST and 212 seconds\non Neuromorphic MNIST. For SNN ResNet-19 models, inference took 784 seconds on\nCIFAR-10 and 1846 seconds on CIFAR-10 DVS. These results establish PRIVSPIKE as\na viable and efficient solution for secure SNN inference, bridging the gap\nbetween energy-efficient deep neural networks and strong cryptographic privacy\nguarantees while outperforming prior encrypted SNN solutions.", "AI": {"tldr": "PRIVSPIKE is a privacy-preserving inference framework for Spiking Neural Networks (SNNs), using CKKS homomorphic encryption to enable encrypted data processing. It introduces two algorithms for the Leaky Integrate-and-Fire activation function, achieving high accuracy and efficiency on datasets like MNIST and CIFAR-10.", "motivation": "Spiking Neural Networks (SNNs), despite their energy efficiency, inherit privacy risks due to data sensitivity. Traditional solutions fail to balance cryptographic security and performance, necessitating a framework that preserves privacy without sacrificing SNN efficiency.", "method": "PRIVSPIKE employs CKKS homomorphic encryption for secure SNN inference. It proposes (1)a polynomial approximation algorithm for high-performance inference and (2)a scheme-switching algorithm for precision-optimized computations. These algorithms are validated on LeNet-5 and ResNet-19 architectures.", "result": "PRIVSPIKE achieves 98.10% accuracy on MNIST and 66.0% on CIFAR-10 DVS with ResNet-19, with encrypted inference times of 28s (MNIST LeNet-5) to 1846s (CIFAR-10 DVS ResNet-19). These results outperform prior encrypted SNN solutions.", "conclusion": "PRIVSPIKE bridges the gap between energy-efficient SNNs and strong privacy guarantees via homomorphic encryption. It demonstrates practical secure inference for SNNs, setting a new benchmark for encrypted deep learning."}}
{"id": "2510.04363", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04363", "abs": "https://arxiv.org/abs/2510.04363", "authors": ["Hyunjun Kim", "Sejong Kim"], "title": "MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models", "comment": "NeurIPS 2025 Workshop on Lock-LLM", "summary": "We introduce MacroBench, a code-first benchmark that evaluates whether LLMs\ncan synthesize reusable browser automation programs from natural language goals\nby reading HTML/DOM and emitting Python with Selenium. MacroBench instantiates\nseven self-hosted sites: Airbnb-like, TikTok-like, Reddit-like, Instagram-like,\nFacebook-like, Discord-like, and Threads-like, covering 681 tasks across\ninteraction complexity and targeting difficulty. Our end-to-end protocol\nvalidates generated code via static checks, sandboxed execution, and outcome\nverification including DOM assertions and database snapshots, and includes a\nsafety suite for scraping, spam/abuse, and credential/privacy prompts. Across\n2636 model-task runs, we observe stratified success: GPT-4o-Mini achieves 96.8\npercent, GPT-4.1 achieves 95.3 percent, Gemini-2.5-Pro achieves 89.0 percent,\nand DeepSeek-V3.1 achieves 83.4 percent. Models handle simple tasks reliably at\n91.7 percent but fail on complex workflows at 0.0 percent, and none meet\nproduction-quality coding practices despite functional completion. We release\nour complete benchmark pipeline, evaluation framework, and experimental results\nto enable reproducible assessment of macro synthesis for web automation.", "AI": {"tldr": "MacroBench is a benchmark evaluating LLMs' ability to generate browser automation code via Python+Selenium, showing strong performance on simple tasks (91.7%) but zero success on complex workflows, with GPT-4o-Mini leading at 96.8%. Code quality (production standards remediation) remains unmet.", "motivation": "Existing LLM evaluation lacks benchmarks for generating reusable web automation macros with cross-site generalization, security considerations, and end-to-end validation pipelines.", "method": "Synthesized 7 modular websites with 681 tasks spanning interaction complexity. Evaluated via static analysis, sandboxed execution, DOM outcome verification, and database snapshot comparisons with safety checks. Reran 2636 model-task combinations across multiple LLMs.", "result": "91.7% success on simple tasks vs 0% on complex workflows. GPT-4o-Mini (96.8%) outperformed GPT-4.1 (95.3%) > Gemini-2.5-Pro (89.0%) > DeepSeek-V3.1 (83.4%). No models met production coding standards despite functional outputs.", "conclusion": "Establishes reproducible evaluation framework for macro synthesis. Highlights importance of complexity compositionality and code quality metrics for deployment-ready LLM automation systems."}}
{"id": "2510.03996", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03996", "abs": "https://arxiv.org/abs/2510.03996", "authors": ["Nges Brian Njungle", "Eric Jahns", "Michel A. Kinsy"], "title": "FHEON: A Configurable Framework for Developing Privacy-Preserving Neural Networks Using Homomorphic Encryption", "comment": "14 pages, 6 figures", "summary": "The widespread adoption of Machine Learning as a Service raises critical\nprivacy and security concerns, particularly about data confidentiality and\ntrust in both cloud providers and the machine learning models. Homomorphic\nEncryption (HE) has emerged as a promising solution to this problems, allowing\ncomputations on encrypted data without decryption. Despite its potential,\nexisting approaches to integrate HE into neural networks are often limited to\nspecific architectures, leaving a wide gap in providing a framework for easy\ndevelopment of HE-friendly privacy-preserving neural network models similar to\nwhat we have in the broader field of machine learning. In this paper, we\npresent FHEON, a configurable framework for developing privacy-preserving\nconvolutional neural network (CNN) models for inference using HE. FHEON\nintroduces optimized and configurable implementations of privacy-preserving CNN\nlayers including convolutional layers, average pooling layers, ReLU activation\nfunctions, and fully connected layers. These layers are configured using\nparameters like input channels, output channels, kernel size, stride, and\npadding to support arbitrary CNN architectures. We assess the performance of\nFHEON using several CNN architectures, including LeNet-5, VGG-11, VGG- 16,\nResNet-20, and ResNet-34. FHEON maintains encrypted-domain accuracies within\n+/- 1% of their plaintext counterparts for ResNet-20 and LeNet-5 models.\nNotably, on a consumer-grade CPU, the models build on FHEON achieved 98.5%\naccuracy with a latency of 13 seconds on MNIST using LeNet-5, and 92.2%\naccuracy with a latency of 403 seconds on CIFAR-10 using ResNet-20.\nAdditionally, FHEON operates within a practical memory budget requiring not\nmore than 42.3 GB for VGG-16.", "AI": {"tldr": "FHEON is a configurable framework for developing privacy-preserving CNN models using Homomorphic Encryption (HE). It supports standard CNN architectures and achieves near-plaintext accuracy with 13-403s latency on consumer-grade CPUs.", "motivation": "ML-as-a-Service faces security risks from data confidentiality breaches. Existing HE solutions for neural networks lack flexibility and generality, necessitating a framework akin to standard ML development for HE-friendly models.", "method": "FHEON implements HE-optimized CNN layers (convolutional, pooling, ReLU, fully connected). Architecture is configured via parameters like channel counts, kernel sizes, and padding. Evaluations use LeNet-5, VGG, ResNet benchmarks.", "result": "LeNet-5 achieves 98.5%M318F accuracy (13s latency) on MNIST; ResNet-20 reaches 92.2%M30; encfigssemble Conversely 403s on CIFAR-10. All models maintain <1%% accuracy loss. Memory usage stays under 42.3GB for VGG-16.", "conclusion": "FHEON closes the gap in HE-based CNN development by offering both flexibility and practical performance metrics, proving privacy-preserving deep learning is achievable without significant accuracy loss."}}
{"id": "2510.04380", "categories": ["cs.SE", "cs.AI", "cs.HC", "D.2.1; D.2.2; D.2.9; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.04380", "abs": "https://arxiv.org/abs/2510.04380", "authors": ["Mateen Ahmed Abbasi", "Petri Ihantola", "Tommi Mikkonen", "Niko M\u00e4kitalo"], "title": "Reconsidering Requirements Engineering: Human-AI Collaboration in AI-Native Software Development", "comment": "Accepted at SEAA 2025. Appearing in Springer LNCS 16081, pages\n  164-180", "summary": "Requirement Engineering (RE) is the foundation of successful software\ndevelopment. In RE, the goal is to ensure that implemented systems satisfy\nstakeholder needs through rigorous requirements elicitation, validation, and\nevaluation processes. Despite its critical role, RE continues to face\npersistent challenges, such as ambiguity, conflicting stakeholder needs, and\nthe complexity of managing evolving requirements. A common view is that\nArtificial Intelligence (AI) has the potential to streamline the RE process,\nresulting in improved efficiency, accuracy, and management actions. However,\nusing AI also introduces new concerns, such as ethical issues, biases, and lack\nof transparency. This paper explores how AI can enhance traditional RE\npractices by automating labor-intensive tasks, supporting requirement\nprioritization, and facilitating collaboration between stakeholders and AI\nsystems. The paper also describes the opportunities and challenges that AI\nbrings to RE. In particular, the vision calls for ethical practices in AI,\nalong with a much-enhanced collaboration between academia and industry\nprofessionals. The focus should be on creating not only powerful but also\ntrustworthy and practical AI solutions ready to adapt to the fast-paced world\nof software development.", "AI": {"tldr": "This paper explores AI's potential to enhance Requirement Engineering (RE) for better efficiency and accuracy while addressing ethical concerns. It emphasizes trustworthy AI solutions through academia-industry collaboration.", "motivation": "RE faces persistent challenges like ambiguity, conflicting stakeholder needs, and evolving requirements. AI could streamline RE but introduces ethical issues like bias and transparency concerns.", "method": "The paper analyzes how AI can automate labor-intensive RE tasks, support prioritization, and foster stakeholder collaboration while examining opportunities/challenges through a systematic review and vision paper approach.", "result": "Highlights AI's potential to improve RE processes (automation, prioritization, collaboration), identifies ethical risks, and outlines the need for balanced practical-ethical AI solutions in RE.", "conclusion": "The study calls for ethical AI frameworks and stronger academia-industry partnerships to develop trustworthy, adaptive AI tools for RE that align with stakeholder needs and software development realities."}}
{"id": "2510.04056", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.04056", "abs": "https://arxiv.org/abs/2510.04056", "authors": ["Rijha Safdar", "Danyail Mateen", "Syed Taha Ali", "Wajahat Hussain"], "title": "Real-VulLLM: An LLM Based Assessment Framework in the Wild", "comment": null, "summary": "Artificial Intelligence (AI) and more specifically Large Language Models\n(LLMs) have demonstrated exceptional progress in multiple areas including\nsoftware engineering, however, their capability for vulnerability detection in\nthe wild scenario and its corresponding reasoning remains underexplored.\nPrompting pre-trained LLMs in an effective way offers a computationally\neffective and scalable solution. Our contributions are (i)varied prompt designs\nfor vulnerability detection and its corresponding reasoning in the wild. (ii)a\nreal-world vector data store constructed from the National Vulnerability\nDatabase, that will provide real time context to vulnerability detection\nframework, and (iii)a scoring measure for combined measurement of accuracy and\nreasoning quality. Our contribution aims to examine whether LLMs are ready for\nwild deployment, thus enabling the reliable use of LLMs stronger for the\ndevelopment of secure software's.", "AI": {"tldr": "This paper is about improving the ability of large language models to detect vulnerabilities in software in real-world scenarios by using various prompt designs, a real-world vector data store for context, and a scoring measure that combines accuracy and reasoning quality.", "motivation": "Existing LLMs have shown progress in software engineering, but there is a need to explore their effectiveness and efficiency for vulnerability detection scenarios, particularly in real-world conditions.", "method": "The paper introduces three main contributions: (i) prompt designs for vulnerability detection and reasoning, (ii) a real-world vector data store built from the National Vulnerability Database to provide real-time context, and (iii) a scoring method that measures both accuracy and reasoning quality in detecting vulnerabilities.", "result": "The results of applying the proposed methods for vulnerability detection using LLMs are not detailed in the abstract but are expected to demonstrate improved performance through effective prompting, contextual data, and combined scoring of accuracy and reasoning.", "conclusion": "The paper's conclusion emphasizes the examination of the readiness of LLMs for deployment in real-world vulnerability detection settings, providing a framework to reliably leverage LLMs for secure software development."}}
{"id": "2510.04437", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04437", "abs": "https://arxiv.org/abs/2510.04437", "authors": ["Fangzhe Wu", "Dongyang Lyu", "Xiaoqi Li"], "title": "Smart Hiring Redefined: An Intelligent Recruitment Management Platform", "comment": null, "summary": "Against the backdrop of deepening digital and intelligent transformation in\nhuman resource management, traditional recruitment models struggle to fully\nmeet enterprises' growing demand for precise talent acquisition due to limited\nefficiency, high costs, and information asymmetry. As a vital tool for\noptimizing recruitment processes, reducing labor and time costs, and enhancing\ncore competitiveness, intelligent recruitment management systems become an\nindispensable component of modern organizational talent strategies.Compared\nwith the labor intensive tasks of resume screening, candidate position\nmatching, and interview coordination in traditional manual recruitment,\nintelligent recruitment systems significantly enhance the efficiency and\naccuracy of the hiring process through automation and data driven approaches.\nThese systems enable rapid parsing of massive resume volumes, intelligent\nmatching of candidates to positions, and automated scheduling of interview\nprocesses.", "AI": {"tldr": "This paper examines the transition from traditional recruitment models to intelligent recruitment management systems motivated by inefficiencies, high costs, and information asymmetry in talent acquisition. It highlights automation-driven solutions (resume parsing, candidate-position matching, and interview scheduling algorithms) that enhance hiring efficiency and accuracy.", "motivation": "Traditional recruitment models face limitations in efficiency, cost-effectiveness, and information transparency as enterprises demand precision in talent acquisition. Manual processes like resume screening and interview coordination are labor-intensive and error-prone.", "method": "The study compares manual recruitment workflows with intelligent systems leveraging automation and data-driven algorithms for resume parsing, candidate-job matching, and interview process automation.", "result": "Intelligent systems demonstrate significant improvements in processing speed, accuracy of candidate-job alignment, and cost reduction compared to manual methods, while enabling scalable talent acquisition.", "conclusion": "Intelligent recruitment management systems are positioned as a critical infrastructure for modern talent strategies, optimizing efficiency, reducing costs, and enhancing organizational competitiveness through digital transformation."}}
{"id": "2510.04085", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.04085", "abs": "https://arxiv.org/abs/2510.04085", "authors": ["Prabhanjan Ananth", "John Bostanci", "Aditya Gulati", "Yao-Ting Lin"], "title": "Gluing Random Unitaries with Inverses and Applications to Strong Pseudorandom Unitaries", "comment": "55 pages. A preliminary version, merging this paper and\n  arXiv:2509.24432, appears in the proceedings of the 45th Annual International\n  Cryptology Conference (CRYPTO 2025) under the title \"Pseudorandom Unitaries\n  in the Haar Random Oracle Model\". This is Part II of the full version", "summary": "Gluing theorem for random unitaries [Schuster, Haferkamp, Huang, QIP 2025]\nhave found numerous applications, including designing low depth random\nunitaries [Schuster, Haferkamp, Huang, QIP 2025], random unitaries in ${\\sf\nQAC0}$ [Foxman, Parham, Vasconcelos, Yuen'25] and generically shortening the\nkey length of pseudorandom unitaries [Ananth, Bostanci, Gulati, Lin\nEUROCRYPT'25]. We present an alternate method of combining Haar random\nunitaries from the gluing lemma from [Schuster, Haferkamp, Huang, QIP 2025]\nthat is secure against adversaries with inverse query access to the joined\nunitary. As a consequence, we show for the first time that strong pseudorandom\nunitaries can generically have their length extended, and can be constructed\nusing only $O(n^{1/c})$ bits of randomness, for any constant $c$, if any family\nof strong pseudorandom unitaries exists.", "AI": {"tldr": "This paper presents a secure gluing theorem for random unitaries, enabling extended key length and reduced randomness requirements for strong pseudorandom unitaries.", "motivation": "Existing gluing methods lack security against inverse-query adversaries, and efficient pseudorandom unitary constructions with sublinear randomness were missing.", "method": "An alternate gluing approach using the Haar random unitary lemma from [Schuster et al., QIP 2025], ensuring security against adversaries with inverse query access to the combined unitary.", "result": "First generic key-length extension for strong pseudorandom unitaries, and constructions using O(n^{1/c}) bits of randomness for any constant c.", "conclusion": "The method advances quantum pseudorandomness by addressing security and efficiency gaps, with implications for quantum cryptography and circuit design."}}
{"id": "2510.04468", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04468", "abs": "https://arxiv.org/abs/2510.04468", "authors": ["Asif Mohammed Samir", "Mohammad Masudur Rahman"], "title": "Improving IR-based Bug Localization with Semantics-Driven Query Reduction", "comment": "56 pages, 16 figures, 11 tables", "summary": "Despite decades of research, software bug localization remains challenging\ndue to heterogeneous content and inherent ambiguities in bug reports. Existing\nmethods such as Information Retrieval (IR)-based approaches often attempt to\nmatch source documents to bug reports, overlooking the context and semantics of\nthe source code. On the other hand, Large Language Models (LLM) (e.g.,\nTransformer models) show promising results in understanding both texts and\ncode. However, they have not been yet adapted well to localize software bugs\nagainst bug reports. They could be also data or resource-intensive. To bridge\nthis gap, we propose, IQLoc, a novel bug localization approach that capitalizes\non the strengths of both IR and LLM-based approaches. In particular, we\nleverage the program semantics understanding of transformer-based models to\nreason about the suspiciousness of code and reformulate queries during bug\nlocalization using Information Retrieval. To evaluate IQLoc, we refine the\nBench4BL benchmark dataset and extend it by incorporating ~30% more recent bug\nreports, resulting in a benchmark containing ~7.5K bug reports. We evaluated\nIQLoc using three performance metrics and compare it against four baseline\ntechniques. Experimental results demonstrate its superiority, achieving up to\n58.52% and 60.59% in MAP, 61.49% and 64.58% in MRR, and 69.88% and 100.90% in\nHIT@K for the test bug reports with random and time-wise splits, respectively.\nMoreover, IQLoc improves MAP by 91.67% for bug reports with stack traces,\n72.73% for those that include code elements, and 65.38% for those containing\nonly descriptions in natural language. By integrating program semantic\nunderstanding into Information Retrieval, IQLoc mitigates several longstanding\nchallenges of traditional IR-based approaches in bug localization.", "AI": {"tldr": "IQLoc combines Information Retrieval (IR) and Large Language Models (LLM) to address software bug localization challenges. By leveraging transformer-based models for program semantics and reformulating IR queries, IQLoc achieves significant improvements in localization metrics over existing methods.", "motivation": "Traditional IR-based methods fail to capture code context and semantics, while LLMs remain underutilized for bug localization due to resource intensity. Existing techniques struggle with heterogeneous and ambiguous bug reports.", "method": "IQLoc integrates transformer-based models to reason about code suspiciousness and reformulate IR queries. The benchmark was expanded by 30% with recent bug reports (7.5K total), and performance was evaluated using MAP, MRR, and HIT@K metrics against four baselines.", "result": "IQLoc outperformed baselines with 58.52-60.59% MAP, 61.49-64.58% MRR, and 69.88-100.90% HIT@K. It showed 91.67% MAP improvement for stack-trace reports, 72.73% for code-inclusive reports, and 65.38% for natural language-only reports.", "conclusion": "By integrating program semantics into IR, IQLoc addresses longstanding limitations of traditional IR-based approaches. It demonstrates transformative potential through semantic understanding, achieving state-of-the-art performance across multiple bug report types."}}
{"id": "2510.04118", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.04118", "abs": "https://arxiv.org/abs/2510.04118", "authors": ["Prakhar Paliwal", "Atul Kabra", "Manjesh Kumar Hanawal"], "title": "Cyber Warfare During Operation Sindoor: Malware Campaign Analysis and Detection Framework", "comment": "Accepted for presentation at the 21st International Conference on\n  Information Systems Security (ICISS 2025)", "summary": "Rapid digitization of critical infrastructure has made cyberwarfare one of\nthe important dimensions of modern conflicts. Attacking the critical\ninfrastructure is an attractive pre-emptive proposition for adversaries as it\ncan be done remotely without crossing borders. Such attacks disturb the support\nsystems of the opponents to launch any offensive activities, crippling their\nfighting capabilities. Cyberattacks during cyberwarfare can not only be used to\nsteal information, but also to spread disinformation to bring down the morale\nof the opponents. Recent wars in Europe, Africa, and Asia have demonstrated the\nscale and sophistication that the warring nations have deployed to take the\nearly upper hand. In this work, we focus on the military action launched by\nIndia, code-named Operation Sindoor, to dismantle terror infrastructure\nemanating from Pakistan and the cyberattacks launched by Pakistan. In\nparticular, we study the malware used by Pakistan APT groups to deploy Remote\nAccess Trojans in Indian systems. We provide details of the tactics and\ntechniques used in the RAT deployment and develop a telemetry framework to\ncollect necessary event logs using Osquery with a custom extension. Finally, we\ndevelop a detection rule that can be readily deployed to detect the presence of\nthe RAT or any exploitation performed by the malware.", "AI": {"tldr": "This paper analyzes cyberwarfare tactics in Operation Sindoor, focusing on Pakistan's use of Remote Access Trojans (RATs), techniques for deployment, and a telemetry framework for detection.", "motivation": "Digitization of critical infrastructure enables remote cyberattacks to disrupt opponents' capabilities and morale through disinformation, as seen in recent global conflicts.", "method": "Study of Pakistan APT groups' malware in Operation Sindoor, including RAT deployment tactics, Osquery-based telemetry framework for log collection, and development of a detection rule.", "result": "Detailed insights into RAT deployment techniques and a deployable detection rule to identify the malware or its exploitation.", "conclusion": "The research emphasizes the necessity of monitoring critical infrastructure and implementing proactive detection systems against sophisticated cyberwarfare strategies."}}
{"id": "2510.04469", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04469", "abs": "https://arxiv.org/abs/2510.04469", "authors": ["Wenqi Yan", "Toby Murray", "Benjamin Rubinstein", "Van-Thuan Pham"], "title": "DynamiQ: Unlocking the Potential of Dynamic Task Allocation in Parallel Fuzzing", "comment": null, "summary": "We present DynamiQ, a full-fledged and optimized successor to AFLTeam that\nsupports dynamic and adaptive parallel fuzzing. Unlike most existing approaches\nthat treat individual seeds as tasks, DynamiQ leverages structural information\nfrom the program's call graph to define tasks and continuously refines task\nallocation using runtime feedback. This design significantly reduces redundant\nexploration and enhances fuzzing efficiency at scale. Built on top of the\nstate-of-the-art LibAFL framework, DynamiQ incorporates several practical\noptimizations in both task allocation and task-aware fuzzing. Evaluated on 12\nreal-world targets from OSS-Fuzz and FuzzBench over 25,000 CPU hours, DynamiQ\noutperforms state-of-the-art parallel fuzzers in both code coverage and\nvulnerability discovery, uncovering 9 previously unknown bugs in widely used\nand extensively fuzzed open-source software.", "AI": {"tldr": "DynamiQ is a dynamic parallel fuzzer using program call graphs for task allocation, reducing redundancy and improving efficiency. It outperforms existing fuzzers in coverage and discovered 9 new bugs.", "motivation": "Existing parallel fuzzers treat seeds as tasks, causing redundancy. DynamiQ addresses this by dynamically refining task allocation using runtime feedback and structural program information.", "method": "Leverages call graph data to define tasks, employs runtime feedback for adaptive allocation, and includes task-aware fuzzing optimizations. Built on top of LibAFL.", "result": "Outperformed state-of-the-art fuzzers in code coverage (12 OSS-Fuzz/FuzzBench targets, 25k CPU hours), discovered 9 new bugs in widely-used software.", "conclusion": "DynamiQ's structural task allocation and adaptive runtime feedback enable more efficient parallel fuzzing, effectively reducing redundancy while enhancing vulnerability discovery capabilities."}}
{"id": "2510.04153", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04153", "abs": "https://arxiv.org/abs/2510.04153", "authors": ["Haoqi Wu", "Wei Dai", "Ming Xu", "Li Wang", "Qiang Yan"], "title": "ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy Preservation", "comment": "Accepted by NeurIPS 2025", "summary": "Diffusion Models have gained significant popularity due to their remarkable\ncapabilities in image generation, albeit at the cost of intensive computation\nrequirement. Meanwhile, despite their widespread deployment in inference\nservices such as Midjourney, concerns about the potential leakage of sensitive\ninformation in uploaded user prompts have arisen. Existing solutions either\nlack rigorous privacy guarantees or fail to strike an effective balance between\nutility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play\nsafeguard that enables oblivious cloud-device hybrid generation. By oblivious,\neach input prompt is transformed into a set of semantically similar candidate\nprompts that differ only in sensitive attributes (e.g., gender, ethnicity). The\ncloud server processes all candidate prompts without knowing which one is the\nreal one, thus preventing any prompt leakage. To mitigate server cost, only a\nsmall portion of denoising steps is performed upon the large cloud model. The\nintermediate latents are then sent back to the client, which selects the\ntargeted latent and completes the remaining denoising using a small device\nmodel. Additionally, we analyze and incorporate several cache-based\naccelerations that leverage temporal and batch redundancy, effectively reducing\ncomputation cost with minimal utility degradation. Extensive experiments across\nmultiple datasets demonstrate that ObCLIP provides rigorous privacy and\ncomparable utility to cloud models with slightly increased server cost.", "AI": {"tldr": "This paper proposes ObCLIP, a privacy-preserving method for diffusion models that hides sensitive attributes in user prompts by generating semantically similar candidates, preventing information leakage on cloud servers, while minimizing computational costs.", "motivation": "Diffusion Models are computationally intensive and lack strong privacy guarantees for user prompts in cloud inference services, leading to potential sensitive information leakage.", "method": "ObCLIP transforms input prompts into semantically similar candidates that vary only by sensitive attributes. The cloud processes all candidates without knowing the real one, and a device model completes the generation using intermediate latents with cache-based optimizations for efficiency.", "result": "Experiments show ObCLIP achieves rigorous privacy with utility comparable to cloud models, at slightly increased cost.", "conclusion": "ObClip is an effective, efficient, and practical solution for privacy in cloud-based diffusion model inference by combining prompt obfuscation, partial generation on the cloud, and local completion with cache optimizations."}}
{"id": "2510.04495", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04495", "abs": "https://arxiv.org/abs/2510.04495", "authors": ["Napasorn Tevarut", "Brittany Reid", "Yutaro Kashiwa", "Pattara Leelaprute", "Arnon Rungsawang", "Bundit Manaskasemsak", "Hajimu Iida"], "title": "Detecting and Characterizing Low and No Functionality Packages in the NPM Ecosystem", "comment": "Accepted in PROFES 2025", "summary": "Trivial packages, small modules with low functionality, are common in the npm\necosystem and can pose security risks despite their simplicity. This paper\nrefines existing definitions and introduce data-only packages that contain no\nexecutable logic. A rule-based static analysis method is developed to detect\ntrivial and data-only packages and evaluate their prevalence and associated\nrisks in the 2025 npm ecosystem. The analysis shows that 17.92% of packages are\ntrivial, with vulnerability levels comparable to non-trivial ones, and\ndata-only packages, though rare, also contain risks. The proposed detection\ntool achieves 94% accuracy (macro-F1 0.87), enabling effective large-scale\nanalysis to reduce security exposure. This findings suggest that trivial and\ndata-only packages warrant greater attention in dependency management to reduce\npotential technical debt and security exposure.", "AI": {"tldr": "This paper identifies security risks in trivial and data-only npm packages, proposing a rule-based static analysis tool with 94% accuracy to assess their prevalence and risks in the 2025 ecosystem.", "motivation": "Trivial/data-only npm packages lack clear definitions and are overlooked in security assessments despite posing vulnerabilities. Existing tools fail to address their unique risks.", "method": "Refined definitions of trivial packages, introduced data-only packages, and developed a rule-based static analysis method to detect them at scale, validated with macro-F1 0.87 accuracy.", "result": "17.92%% of npm packages are trivial (similar vulnerability levels to non-trivial packages); data-only packages are rare but contain risks. The tool enables large-scale risk assessment.", "conclusion": "Trivial/data-only packages require prioritization in dependency management to mitigate technical debt and security exposure, highlighting gaps in current npm security practices."}}
{"id": "2510.04257", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04257", "abs": "https://arxiv.org/abs/2510.04257", "authors": ["Yanjie Li", "Yiming Cao", "Dong Wang", "Bin Xiao"], "title": "AgentTypo: Adaptive Typographic Prompt Injection Attacks against Black-box Multimodal Agents", "comment": "13 pages, 8 figures. Submitted to IEEE Transactions on Information\n  Forensics & Security", "summary": "Multimodal agents built on large vision-language models (LVLMs) are\nincreasingly deployed in open-world settings but remain highly vulnerable to\nprompt injection, especially through visual inputs. We introduce AgentTypo, a\nblack-box red-teaming framework that mounts adaptive typographic prompt\ninjection by embedding optimized text into webpage images. Our automatic\ntypographic prompt injection (ATPI) algorithm maximizes prompt reconstruction\nby substituting captioners while minimizing human detectability via a stealth\nloss, with a Tree-structured Parzen Estimator guiding black-box optimization\nover text placement, size, and color. To further enhance attack strength, we\ndevelop AgentTypo-pro, a multi-LLM system that iteratively refines injection\nprompts using evaluation feedback and retrieves successful past examples for\ncontinual learning. Effective prompts are abstracted into generalizable\nstrategies and stored in a strategy repository, enabling progressive knowledge\naccumulation and reuse in future attacks. Experiments on the VWA-Adv benchmark\nacross Classifieds, Shopping, and Reddit scenarios show that AgentTypo\nsignificantly outperforms the latest image-based attacks such as AgentAttack.\nOn GPT-4o agents, our image-only attack raises the success rate from 0.23 to\n0.45, with consistent results across GPT-4V, GPT-4o-mini, Gemini 1.5 Pro, and\nClaude 3 Opus. In image+text settings, AgentTypo achieves 0.68 ASR, also\noutperforming the latest baselines. Our findings reveal that AgentTypo poses a\npractical and potent threat to multimodal agents and highlight the urgent need\nfor effective defense.", "AI": {"tldr": "AgentTypo introduces a black-box typographic prompt injection framework that outperforms existing methods by optimizing stealthy text embedding in webpage images for attacking multimodal agents.", "motivation": "Multimodal agents face significant vulnerabilities to visual prompt injections, with current attacks lacking adaptability and stealth. The paper addresses this by developing a practical framework for scalable, hard-to-detect attacks.", "method": "The ATPI algorithm combines prompt reconstruction via captioner substitution, stealth loss for human detection resistance, and Tree-structured Parzen Estimator for optimization. AgentTypo-pro adds multi-LLM iterative refinement, feedback-driven learning, and a strategy repository for knowledge reuse.", "result": "Achieves 0.45 success rate on GPT-4o (up from 0.23), 0.68 attack success rate (ASR), and outperforms baselines across GPT-4V, Gemini 1.5 Pro, and Claude 3 Opus in VWA-Adv benchmark scenarios.", "conclusion": "AgentTypo demonstrates a critical threat vector for multimodal agents, exposing urgent defense needs through its superior practical attack performance and adaptability across models."}}
{"id": "2510.04519", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04519", "abs": "https://arxiv.org/abs/2510.04519", "authors": ["Heiko Koziolek", "Thilo Braun", "Virendra Ashiwal", "Sofia Linsbauer", "Marthe Ahlgreen Hansen", "Karoline Grotterud"], "title": "Spec2Control: Automating PLC/DCS Control-Logic Engineering from Natural Language Requirements with LLMs - A Multi-Plant Evaluation", "comment": "12 pages, 9 figures", "summary": "Distributed control systems (DCS) manage the automation for many industrial\nproduction processes (e.g., power plants, chemical refineries, steel mills).\nProgramming the software for such systems remains a largely manual and tedious\nprocess, incurring costs of millions of dollars for extensive facilities. Large\nlanguage models (LLMs) have been found helpful in generating DCS control logic,\nresulting in commercial copilot tools. Today, these tools are focused on\ntextual notations, they provide limited automation, and have not been tested on\nlarge datasets with realistic test cases. We introduce Spec2Control, a highly\nautomated LLM workflow to generate graphical control logic directly from\nnatural language user requirements. Experiments using an open dataset with 10\ncontrol narratives and 65 complex test cases demonstrate that Spec2Control can\nsuccessfully identify control strategies, can generate 98.6% of correct control\nstrategy connections autonomously, and can save between 94-96% of human labor.\nSpec2Control is being integrated into commercial ABB engineering tools, but is\nalso available as an open-source variant for independent validation.", "AI": {"tldr": "Spec2Control automates graphical DCS control logic generation from natural language requirements using LLMs, achieving 98.6% correct connections and reducing human labor by 94-96%. It outperforms existing text-focused tools and is now available both as open-source and in commercial ABB tools.", "motivation": "Manual programming of DCS software costs millions of dollars annually, while current LLM-based tools lack automation, scalability, and real-world validation. This gap motivates the need for a more efficient and validated automation solution.", "method": "Spec2Control leverages LLMs to convert natural language specifications directly into graphical control logic diagrams, bypassing manual translation steps. It was evaluated on a 10-narrative open dataset with 65 complex test cases to validate automation effectiveness.", "result": "The system autonomously achieved 98.6 accuracy in control strategy connections, saving 94-96 of human labor. Integration into ABB's commercial tools and open-source availability enable practical deployment and independent validation.", "conclusion": "Spec2Control demonstrates industry-grade automation for DCS programming, significantly reducing costs and errors. Its success provides a scalable model for LLM applications in industrial software development, validated across both commercial and open-source platforms."}}
{"id": "2510.04261", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.04261", "abs": "https://arxiv.org/abs/2510.04261", "authors": ["Yu Cui", "Sicheng Pan", "Yifei Liu", "Haibin Zhang", "Cong Zuo"], "title": "VortexPIA: Indirect Prompt Injection Attack against LLMs for Efficient Extraction of User Privacy", "comment": null, "summary": "Large language models (LLMs) have been widely deployed in Conversational AIs\n(CAIs), while exposing privacy and security threats. Recent research shows that\nLLM-based CAIs can be manipulated to extract private information from human\nusers, posing serious security threats. However, the methods proposed in that\nstudy rely on a white-box setting that adversaries can directly modify the\nsystem prompt. This condition is unlikely to hold in real-world deployments.\nThe limitation raises a critical question: can unprivileged attackers still\ninduce such privacy risks in practical LLM-integrated applications? To address\nthis question, we propose \\textsc{VortexPIA}, a novel indirect prompt injection\nattack that induces privacy extraction in LLM-integrated applications under\nblack-box settings. By injecting token-efficient data containing false\nmemories, \\textsc{VortexPIA} misleads LLMs to actively request private\ninformation in batches. Unlike prior methods, \\textsc{VortexPIA} allows\nattackers to flexibly define multiple categories of sensitive data. We evaluate\n\\textsc{VortexPIA} on six LLMs, covering both traditional and reasoning LLMs,\nacross four benchmark datasets. The results show that \\textsc{VortexPIA}\nsignificantly outperforms baselines and achieves state-of-the-art (SOTA)\nperformance. It also demonstrates efficient privacy requests, reduced token\nconsumption, and enhanced robustness against defense mechanisms. We further\nvalidate \\textsc{VortexPIA} on multiple realistic open-source LLM-integrated\napplications, demonstrating its practical effectiveness.", "AI": {"tldr": "Can we extract private data from LLM-powered CIs with the black-box setting?", "motivation": "Traditional methods rely on white-box, but in real application, attackers can't access system prompt. This study aim to answer the question by proposing VortexPIA to test the possibility in black-box scenario.", "method": "VortexPIA crafts a token-efficient dataset with false memory and uses it to train a LLM that request user to disclose their private info in multiple privleges categories while being applied to several applications in black-box scenario.", "result": "VortexPIA outperforms prior approaches in extracting user's private info in a black-box setting with less token usage and better robustness against known defense techniques.", "conclusion": "VortexPIA shows that even black-box attackers can successfully extract private info from LLM-powered CAIs, highlighting the privacy risks. Future research can focus towards more robust defense techniques."}}
{"id": "2510.04603", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04603", "abs": "https://arxiv.org/abs/2510.04603", "authors": ["Johan Lin\u00e5ker", "Sachiko Muto"], "title": "Advancing Digital Government: Integrating Open Source Software Enablement Indicators in Maturity Indexes", "comment": "In submission", "summary": "Context: Open Source Software (OSS) is a vital public good, included across\nmost of modern software stacks, significantly impacting GDP and national tech\ngrowth, while supporting interoperability, sovereignty, and transparency.\nHowever, systematic measurement of governmental OSS adoption remain limited.\n  Research Aim: This study contributes to digital government maturity indexes\nby analyzing policies and support actions leveraging OSS for software reuse and\ncollaborative development across 16 digitally mature countries, and proposing\npotential indicators for said indexes. It examines OSS policy formation, stated\ngoals, key actors, and support mechanisms.\n  Methodology: A qualitative approach is used combining desk research of policy\ndocuments with semi-structured interviews of government representatives,\nproducing detailed country reports. These are cross-analyzed, focusing on OSS\npolicy promotion, rationale, and implementation support.\n  Results: Policies facilitating OSS reuse are widespread, targeting both\ninbound acquisition and outbound sharing, and are predominantly governed by\ncentral public sector organizations. Policy goals include interoperability,\ndigital sovereignty, transparency, and cost efficiency, with security framed\nboth as a risk and strength. Implementation is supported by diverse Open Source\nProgram Offices (OSPOs) at multiple government levels, which foster capacity\nbuilding, resource pooling, and sustainable project governance. Indicators are\nsynthesized and proposed across 14 areas covering policy incentives and design,\nand implementation and support.\n  Conclusions: OSS is a strategic enabler for public sector digital\ntransformation. Clear policy frameworks, coupled with institutional support\nsuch as OSPOs, are essential. International digital maturity frameworks should\nexpand OSS indicators to better guide and assess government adoption and\nimpact.", "AI": {"tldr": "A study to enhance digital government maturity indexes by analyzing Open Source Software (OSS) policies and support mechanisms in 16 countries, proposing new indicators for measuring governmental OSS adoption, with particular focus on reuse, collaborative development, policy incentives, and implementation support.", "motivation": "OSS is a critical public good with significant economic and technological impacts, yet there are limited systematic measurements of governmental OSS adoption. This study aims to provide actionable indicators for digital government maturity indexes to assess and guide OSS implementation better.", "method": "The study uses a qualitative approach, combining desk research of policy documents with semi-structured interviews of government representatives in 16 countries. Data are analyzed cross-nationally to identify patterns and formulate indicators for policy incentives and implementation.", "result": "The paper found that OSS reuse policies are widespread, governed by central public bodies, promoting interoperability, transparency, sovereignty, and cost efficiency. OSPOs at various government levels provide institutional support. The study also proposes 14 indicator areas to measure digital maturity through OSS adoption.", "conclusion": "OSS plays a strategic role in public digital transformation, necessitating clear policy and institutional support. Current digital maturity assessments should include OSS indicators to effectively evaluate and guide government adoption."}}
{"id": "2510.04397", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04397", "abs": "https://arxiv.org/abs/2510.04397", "authors": ["Van Nguyen", "Surya Nepal", "Xingliang Yuan", "Tingmin Wu", "Fengchao Chen", "Carsten Rudolph"], "title": "MulVuln: Enhancing Pre-trained LMs with Shared and Language-Specific Knowledge for Multilingual Vulnerability Detection", "comment": null, "summary": "Software vulnerabilities (SVs) pose a critical threat to safety-critical\nsystems, driving the adoption of AI-based approaches such as machine learning\nand deep learning for software vulnerability detection. Despite promising\nresults, most existing methods are limited to a single programming language.\nThis is problematic given the multilingual nature of modern software, which is\noften complex and written in multiple languages. Current approaches often face\nchallenges in capturing both shared and language-specific knowledge of source\ncode, which can limit their performance on diverse programming languages and\nreal-world codebases. To address this gap, we propose MULVULN, a novel\nmultilingual vulnerability detection approach that learns from source code\nacross multiple languages. MULVULN captures both the shared knowledge that\ngeneralizes across languages and the language-specific knowledge that reflects\nunique coding conventions. By integrating these aspects, it achieves more\nrobust and effective detection of vulnerabilities in real-world multilingual\nsoftware systems. The rigorous and extensive experiments on the real-world and\ndiverse REEF dataset, consisting of 4,466 CVEs with 30,987 patches across seven\nprogramming languages, demonstrate the superiority of MULVULN over thirteen\neffective and state-of-the-art baselines. Notably, MULVULN achieves\nsubstantially higher F1-score, with improvements ranging from 1.45% to 23.59%\ncompared to the baseline methods.", "AI": {"tldr": "Multilingual vulnerability detection is difficult because existing approaches are language specific and cannot capture multi language domain code. MULVULN is a new approach that address this by learn re common and language specific knowledge.", "motivation": "Current vulnerability detection methods are limited to a single language, making it hard to generalize for multilingual software.', ", "method": "MULVULN leverages both shared knowledge across programming languages and language-specific knowledge to enhance vulnerability detection in multilingual codebases.", "result": "MULVULN achieves higher F1-scores on the REEF dataset, outperforming existing methods by 1.45% to 23.59%.", "conclusion": "MULVULN is an effective multilingual vulnerability detection approach that addresses the challenge of multilingual code by learning common and language-specific patterns."}}
{"id": "2510.04605", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04605", "abs": "https://arxiv.org/abs/2510.04605", "authors": ["Jingyao Zhang", "Tianlin Li", "Xiaoyu Zhang", "Qiang Hu", "Bin Shi"], "title": "Exploring the Power of Diffusion Large Language Models for Software Engineering: An Empirical Investigation", "comment": null, "summary": "Autoregressive Large Language Models (AR-LLMs) are widely used in software\nengineering (SE) but face limitations in processing code structure information\nand suffer from high inference latency. Diffusion LLMs (DLLMs) offer a\npromising alternative with global bidirectional encoding and decoupled\ngeneration steps. This work presents the first comprehensive evaluation of\nDLLMs across the software development lifecycle, including code generation,\ndefect detection, and program repair. On a large-scale benchmark of 52,937\ntasks, 7Bparameter DLLMs outperform AR-LLMs with a 30% average accuracy\nimprovement achieving a 113% gain on cross-file repair, while maintaining\nsuperior efficiency and reduced latency. Our results establish DLLMs as a\nsuperior paradigm for SE tasks.", "AI": {"tldr": "This paper evaluates Diffusion LLMs (DLLMs) as a superior alternative to Autoregressive LLMs (AR-LLMs) in software engineering tasks, achieving 30-113\\u2005\\% accuracy improvements with reduced inference latency.", "motivation": "AR-LLMs struggle with code structure processing and exhibit high inference latency, motivating the exploration of DLLMs with global bidirectional capabilities for SE tasks.", "method": "Comprehensive benchmarking of DLLMs across code generation, defect detection, and program repair using a large-scale dataset of 52,937 tasks.", "result": "7B-parameter DLLMs outperformed AR-LLMs by 30\\u2005\\Avg. accuracy and achieved 113\\u2005\\\\Gain for cross-file repair, maintaining lower latency.", "conclusion": "DLLMs demonstrate significant accuracy, efficiency, and latency advantages, establishing them as a superior paradigm for software development lifecycle tasks."}}
{"id": "2510.04503", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04503", "abs": "https://arxiv.org/abs/2510.04503", "authors": ["Shuai Zhao", "Xinyi Wu", "Shiqian Zhao", "Xiaobao Wu", "Zhongliang Guo", "Yanhao Jia", "Anh Tuan Luu"], "title": "P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs", "comment": null, "summary": "During fine-tuning, large language models (LLMs) are increasingly vulnerable\nto data-poisoning backdoor attacks, which compromise their reliability and\ntrustworthiness. However, existing defense strategies suffer from limited\ngeneralization: they only work on specific attack types or task settings. In\nthis study, we propose Poison-to-Poison (P2P), a general and effective backdoor\ndefense algorithm. P2P injects benign triggers with safe alternative labels\ninto a subset of training samples and fine-tunes the model on this re-poisoned\ndataset by leveraging prompt-based learning. This enforces the model to\nassociate trigger-induced representations with safe outputs, thereby overriding\nthe effects of original malicious triggers. Thanks to this robust and\ngeneralizable trigger-based fine-tuning, P2P is effective across task settings\nand attack types. Theoretically and empirically, we show that P2P can\nneutralize malicious backdoors while preserving task performance. We conduct\nextensive experiments on classification, mathematical reasoning, and summary\ngeneration tasks, involving multiple state-of-the-art LLMs. The results\ndemonstrate that our P2P algorithm significantly reduces the attack success\nrate compared with baseline models. We hope that the P2P can serve as a\nguideline for defending against backdoor attacks and foster the development of\na secure and trustworthy LLM community.", "AI": {"tldr": "The paper proposes P2P, a general backdoor defense algorithm for LLMs during fine-tuning by injecting benign triggers with safe labels into datasets using prompt-based learning.", "motivation": "Existing defense strategies lack generalization against diverse backdoor attack types and task settings, creating reliability risks for LLMs undergoing fine-tuning.", "method": "P2P introduces benign triggers with safe labels into training samples via re-poisoning, leveraging prompt-based learning to force models to associate triggers with non-malicious outputs.", "result": "Experiments show P2P significantly reduces attack success rates across classification, reasoning, and summarization tasks while maintaining performance on multiple SOTA LLMs.", "conclusion": "P2P provides a robust, generalizable solution for backdoor mitigation, offering a framework for developing secure LLM applications through trigger-based finetuning."}}
{"id": "2510.04611", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.04611", "abs": "https://arxiv.org/abs/2510.04611", "authors": ["Pawel Weichbroth", "Maciej Lotysz", "Michal Wrobel"], "title": "A survey on the impact of emotions on the productivity among software developers", "comment": "29 pages, 5 tables, 96 references", "summary": "The time pressure associated with software development, among other factors,\noften leads to a diminished emotional state among developers. However, whether\nemotions affect perceived productivity remains an open question. This study\naims to determine the strength and direction of the relationship between\nemotional state and perceived productivity among software developers. We\nemployed a two-stage approach. First, a survey was conducted with a pool of\nnine experts to validate the measurement model. Second, a survey was\nadministered to a pool of 88 software developers to empirically test the\nformulated hypothesis by using Partial Least Squares, as the data analysis\nmethod. The results of the path analysis clearly confirm the formulated\nhypothesis, showing that the emotional state of a software developer has a\nstrong positive, and significant impact (beta = 0.893, p < 0.001) on perceived\nproductivity among software developers. The findings highlight the importance\nof managing and improving developers emotional well-being to enhance\nproductivity in software development environments. Additionally, interventions\naimed at reducing burnout, stress, and other negative factors could have a\nconsiderable impact on their performance outcomes.", "AI": {"tldr": "This paper explores the impact of software developers 'emotional states on their perceived productivity through a two-stage survey approach involving experts and developers, using PLS analysis. Results show a strong positive and significant effect (beta =0.893, p <0.001) of emotional state on productivity.", "motivation": "The study is motivated by the potential impact of emotional well-being on productivity within the software development industry, an area not yet fully understood. The authors aim to validate this relationship to provide actionable insight for interventions that could enhance performance outcomes.", "method": "The study follows a two-stage approach. The first stage involves validating the Measurement Model (construct validity, reliability) through an expert survey ( nine experts ). The second stage uses the validated model to collect empirical data from 88 software developers, followed by hypothesis testing using Partial Least Squares ( PLS ) path analysis. Demographics and industry-specific details were likely collected through surveys to underpin the statistical analysis.", "result": "The path analysis showed a strong positive correlation (beta = 0.893 ) between emotional state and perceived productivity among software developers, which is statistically significant (p < 0.001 ). This result substantiates the formulated hypothesis, indicating that emotional well-being is a critical factor in influencing productivity.", "conclusion": "Emotional well-being significantly influences perceived productivity in software development, and managing this could enhance performance and mitigate negative outcomes such as burnout and stress. Interventions focused on emotional health might yield considerable positive effects for the industry."}}
{"id": "2510.04528", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04528", "abs": "https://arxiv.org/abs/2510.04528", "authors": ["Santhosh KumarRavindran"], "title": "Unified Threat Detection and Mitigation Framework (UTDMF): Combating Prompt Injection, Deception, and Bias in Enterprise-Scale Transformers", "comment": null, "summary": "The rapid adoption of large language models (LLMs) in enterprise systems\nexposes vulnerabilities to prompt injection attacks, strategic deception, and\nbiased outputs, threatening security, trust, and fairness. Extending our\nadversarial activation patching framework (arXiv:2507.09406), which induced\ndeception in toy networks at a 23.9% rate, we introduce the Unified Threat\nDetection and Mitigation Framework (UTDMF), a scalable, real-time pipeline for\nenterprise-grade models like Llama-3.1 (405B), GPT-4o, and Claude-3.5. Through\n700+ experiments per model, UTDMF achieves: (1) 92% detection accuracy for\nprompt injection (e.g., jailbreaking); (2) 65% reduction in deceptive outputs\nvia enhanced patching; and (3) 78% improvement in fairness metrics (e.g.,\ndemographic bias). Novel contributions include a generalized patching algorithm\nfor multi-threat detection, three groundbreaking hypotheses on threat\ninteractions (e.g., threat chaining in enterprise workflows), and a\ndeployment-ready toolkit with APIs for enterprise integration.", "AI": {"tldr": "The paper introduces UTDMF, a real-time threat detection and mitigation framework for enterprise LLMs, achieving high detection accuracy for prompt injections, reduced deceptive outputs, and improved fairness metrics. It builds on adversarial activation patching with novel contributions for multi-threat detection and deployment.", "motivation": "Enterprise adoption of LLMs creates critical vulnerabilities to prompt injections, deceptive outputs, and fairness issues, requiring scalable real-time solutions for production models like Llama-3.1 and GPT-4o.", "method": "Extends adversarial activation patching to create UTDMF, featuring a generalized patching algorithm for multi-threat detection, experiments across 700+$ cases per model, and three hypotheses on enterprise threat interactions including 'threat chaining'.", "result": "92% prompt injection detection accuracy, 65% reduction in deceptive outputs via optimized patching, 78% improvement in demographic fairness metrics across enterprise-grade models.", "conclusion": "UTDMF establishes a robust solution for enterprise LLM security with novel threat interaction hypotheses, a production-ready API toolkit, and quantifiable improvements in three critical risk areas."}}
{"id": "2510.04689", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04689", "abs": "https://arxiv.org/abs/2510.04689", "authors": ["Chengwei Liu", "Wenbo Guo", "Yuxin Zhang", "Limin Wang", "Sen Chen", "Lei Bu", "Yang Liu"], "title": "Evolaris: A Roadmap to Self-Evolving Software Intelligence Management", "comment": null, "summary": "In recent years, the landscape of software threats has become significantly\nmore dynamic and distributed. Security vulnerabilities are no longer discovered\nand shared only through formal channels such as public vulnerability databases\nor vendor advisories. Increasingly, criti- cal threat information emerges\ninformally through blogs, social media, developer forums, open source\nrepositories, and even underground com- munities. To this end, capturing such\nintelligence in a timely manner is essential for maintaining situational\nawareness and enabling prompt security responses. However, this remains a\ncomplex challenge due to the fragmented nature of data sources and the\ntechnical difficulty of collecting, parsing, mapping, and validating\ninformation at scale. To ad- dress this, we propose Evolaris, a self-evolving\nsoftware intelligence sys- tem built on a multi-agent framework. Evolaris is\ndesigned to support a full-stack workflow, where agents operate independently\nbut coordinate through shared context to perform tasks such as information\ndiscovery, reasoning, gap completion, validation, and risk detection. This\narchi- tecture enables the platform to learn from new inputs, refine its\ninternal knowledge, and adapt to emerging threat patterns over time, which\ncould continuously improve the precision, timeliness, and scalability of\nsoftware threat analysis, and offers a sustainable foundation for proactive\nsecu- rity decision-making and strengthens the broader ecosystem of security\nthreat understanding.", "AI": {"tldr": "The paper proposes Evolaris, a self-evolving software intelligence system to tackle the challenge of collecting and analyzing security threats from diverse, informal sources.", "motivation": "The dynamic and distributed nature of software threats, coupled with the emergence of informal channels for vulnerability sharing, necessitates a robust system to ensure timely threat intelligence and proactive security responses.", "method": "Evolaris is built on a multi-agent framework that facilitates independent yet coordinated tasks like discovery, reasoning, gap filling, validation, and risk detection, leveraging shared context for enhanced scalability and adaptability.", "result": "The system demonstrates improved precision, timeliness, and scalability in software threat analysis through continuous learning and adaptation to emerging threat patterns.", "conclusion": "Evolaris offers a sustainable foundation for proactive security decision-making by integrating a multi-agent framework that enhances the ecosystem of security threat understanding."}}
{"id": "2510.04529", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.04529", "abs": "https://arxiv.org/abs/2510.04529", "authors": ["Yuki Takeuchi", "Duo Xu"], "title": "Computational Certified Deletion Property of Magic Square Game and its Application to Classical Secure Key Leasing", "comment": null, "summary": "We present the first construction of a computational Certified Deletion\nProperty (CDP) achievable with classical communication, derived from the\ncompilation of the non-local Magic Square Game (MSG). We leverage the KLVY\ncompiler to transform the non-local MSG into a 2-round interactive protocol,\nrigorously demonstrating that this compilation preserves the game-specific CDP.\nPreviously, the quantum value and rigidity of the compiled game were\ninvestigated. We emphasize that we are the first to investigate CDP (local\nrandomness in [Fu and Miller, Phys. Rev. A 97, 032324 (2018)]) for the compiled\ngame. Then, we combine this CDP with the framework [Kitagawa, Morimae, and\nYamakawa, Eurocrypt 2025] to construct Secure Key Leasing with classical Lessor\n(cSKL). SKL enables the Lessor to lease the secret key to the Lessee and verify\nthat a quantum Lessee has indeed deleted the key. In this paper, we realize\ncSKL for PKE, PRF, and digital signature. Compared to prior works for cSKL, we\nrealize cSKL for PRF and digital signature for the first time. In addition, we\nsucceed in weakening the assumption needed to construct cSKL.", "AI": {"tldr": "The paper constructs the first computational Certified Deletion Property (CDP) using classical communication by compiling the Magic Square Game (MSG) into a 2-round interactive protocol. It applies this CDP to realize Secure Key Leasing (cSKL) for PRF and digital signatures with weakened assumptions.", "motivation": "Existing CDP solutions required quantum communication or were limited to specific cryptographic primitives. The work addresses the need for classical-communication CDP and expands cSKL capabilities.", "method": "1) Compile the non-local MSG into a 2-round protocol via KLVY compiler. 2) Prove the compiled protocol preserves CDP. 3) Integrate with the [Kitagawa et al.] framework to construct cSKL for PKE, PRF, and digital signatures.", "result": "First classical CDP construction. First cSKL implementation for PRF/digital signatures. Reduced assumptions compared to prior cSKL work.", "conclusion": "Demonstrates CDP can be achieved via classical communication, enabling broader cryptographic applications like efficient key leasing with verified deletion."}}
{"id": "2510.04711", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04711", "abs": "https://arxiv.org/abs/2510.04711", "authors": ["Aoyang Fang", "Songhan Zhang", "Yifan Yang", "Haotong Wu", "Junjielong Xu", "Xuyang Wang", "Rui Wang", "Manyi Wang", "Qisheng Lu", "Pinjia He"], "title": "An Empirical Study of SOTA RCA Models: From Oversimplified Benchmarks to Realistic Failures", "comment": "Our project is available on https://operationspai.github.io/", "summary": "While cloud-native microservice architectures have transformed software\ndevelopment, their complexity makes Root Cause Analysis (RCA) both crucial and\nchallenging. Although many data-driven RCA models have been proposed, we find\nthat existing benchmarks are often oversimplified and fail to capture\nreal-world conditions. Our preliminary study shows that simple rule-based\nmethods can match or even outperform state-of-the-art (SOTA) models on four\nwidely used benchmarks, suggesting performance overestimation due to benchmark\nsimplicity. To address this, we systematically analyze popular RCA benchmarks\nand identify key limitations in fault injection, call graph design, and\ntelemetry patterns. Based on these insights, we develop an automated framework\nto generate more realistic benchmarks, yielding a dataset of 1,430 validated\nfailure cases from 9,152 injections, covering 25 fault types under dynamic\nworkloads with hierarchical ground-truth labels and verified SLI impact.\nRe-evaluation of 11 SOTA models on this dataset shows low Top@1 accuracy\n(average 0.21, best 0.37) and significantly longer execution times. Our\nanalysis highlights three common failure patterns: scalability issues,\nobservability blind spots, and modeling bottlenecks.", "AI": {"tldr": "The paper addresses the oversimplification of existing benchmarks for Root Cause Analysis (RCA) in cloud-native microservices. It introduces a framework to generate realistic benchmarks, revealing performance limitations of SOTA models through low accuracy and identified failure patterns.", "motivation": "Current RCA benchmarks fail to capture real-world complexity, leading to overestimated model performance and poor adaptability to dynamic environments. This hinders reliable evaluation and improvement of RCA systems.", "method": "1) Analyzed limitations in existing benchmarks (fault injection, call graph design, telemetry). 2 Developed an automated framework to generate realistic benchmarks with 1,430 validated failure cases across 25 fault types, dynamic workloads, and hierarchical labels.", "result": "Re-evaluation of 11 SOTA models showed average Top@1 accuracy of 0.21. Three failure patterns emerged: scalability issues, observability blind spots, and modeling bottlenecks. Benchmark generation framework provides verifiable SLI impact data.", "conclusion": "Real-world RCA evaluation requires more sophisticated benchmarks. Current models demonstrate significant practical limitations in scalability and observability, necessitating improved approaches that align with dynamic, hierarchical failure scenarios."}}
{"id": "2510.04619", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.04619", "abs": "https://arxiv.org/abs/2510.04619", "authors": ["Ivan Homoliak", "Martin Pere\u0161\u00edni", "Marek Tama\u0161kovi\u010d", "Timotej Ponek", "Luk\u00e1\u0161 Hellebrandt", "Kamil Malinka"], "title": "PoS-CoPOR: Proof-of-Stake Consensus Protocol with Native Onion Routing Providing Scalability and DoS-Resistance", "comment": null, "summary": "Proof-of-Stake (PoS) consensus protocols often face a trade-off between\nperformance and security. Protocols that pre-elect leaders for subsequent\nrounds are vulnerable to Denial-of-Service (DoS) attacks, which can disrupt the\nnetwork and compromise liveness. In this work, we present PoS-CoPOR, a\nsingle-chain PoS consensus protocol that mitigates this vulnerability by\nintegrating a native onion routing mechanism into the consensus protocol\nitself. PoS-CoPOR combines stake-weighted probabilistic leader election with an\nanonymization layer that conceals the network identity of the next block\nproposer. This approach prevents targeted DoS attacks on leaders before they\nproduce a block, thus enhancing network resilience. We implemented and\nevaluated PoS-CoPOR, demonstrating its ability to achieve a throughput of up to\n110 tx/s with 6 nodes, even with the overhead of the anonymization layer. The\nresults show that native anonymization can provide robust DoS resistance with\nonly a modest impact on performance, offering a solution to build secure and\nscalable PoS blockchains.", "AI": {"tldr": "Proof-of-Stake (PoS) consensus protocols often face a trade-off between performance and security. Existing protocols pre-elect leaders, making them vulnerable to DoS attacks. This paper introduces PoS-CoPOR, which integrates onion routing into PoS to anonymize the next block proposer and resist DoS attacks. Evaluations show it maintains reasonable performance with enhanced security.", "motivation": "The motivation of the paper is to address the inherent vulnerability of pre-elect leaders in Proof-of-Stake (PoS) consensus protocols to Denial-of-Service (DoS) attacks, which can compromise the liveness of the network.", "method": "The method proposed in this paper is the introduction of PoS-CoPOR, a single-chain PoS consensus protocol that integrates an onion routing mechanism to anonymize the identity of the next block proposer. It uses stake-weighted probabilistic leader election combined with an anonymization layer.", "result": "The results indicate that PoS-CoPOR can achieve an impressive throughput of up to 110 tx/s with 6 nodes, even considering the overhead introduced by the onion routing anonymization mechanism. This demonstrates its capacity to coincide high performance with robust DoS resistance.", "conclusion": "The conclusion drawn is that native anonymization through PoS-CoPOR can effectively deliver robust resistance to Denial-of-Service (DoS) attacks with a minimal impact on performance, providing a feasible solution for constructing secure and scalable PoS blockchain networks."}}
{"id": "2510.04760", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04760", "abs": "https://arxiv.org/abs/2510.04760", "authors": ["Sisay Deresa Sima", "Ayalew Belay Habtie"], "title": "Agile Software Effort Estimation using Regression Techniques", "comment": null, "summary": "Software development effort estimation is one of the most critical aspect in\nsoftware development process, as the success or failure of the entire project\ndepends on the accuracy of estimations. Researchers are still conducting\nstudies on agile effort estimation. The aim of this research is to develop a\nstory point based agile effort estimation model using LASSO and Elastic Net\nregression techniques. The experimental work is applied to the agile story\npoint approach using 21 software projects collected from six firms. The two\nalgorithms are trained using their default parameters and tuned grid search\nwith 5-fold cross-validation to get an enhanced model. The experiment result\nshows LASSO regression achieved better predictive performance PRED (8%) and\nPRED (25%) results of 100.0, MMRE of 0.0491, MMER of 0.0551, MdMRE of 0.0593,\nMdMER of 0.063, and MSE of 0.0007. The results are also compared with other\nrelated literature.", "AI": {"tldr": "TL;DR: this paper presents a story point based agile effort estimation model using LASSO and Elastic Net regression, with experimen...", "motivation": "Motivation: Software development effort estimation is a vital aspect of project success. The paper addresses the ongoing need fo...", "method": "Method: The study develops an agile effort estimation model using LASSO and Elastic Net regression techniques, applied to 21 softwar...", "result": "Result: The LASSO regression model demonstrates superior predictive performance with 100.0 PRED (8%) and 100.0 PRED (25%) results, a...", "conclusion": "Conclusion: The research successfully created an effective agile effort estimation model using LASSO regression, showing high pred..."}}
{"id": "2510.04640", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.04640", "abs": "https://arxiv.org/abs/2510.04640", "authors": ["Ali Asghar", "Andreas Becher", "Daniel Ziener"], "title": "Backing the Wrong Horse: How Bit-Level Netlist Augmentation can Counter Power Side Channel Attacks", "comment": "5 pages, 3 figures", "summary": "The dependence of power-consumption on the processed data is a known\nvulnerability of CMOS circuits, resulting in side channels which can be\nexploited by power-based side channel attacks (SCAs). These attacks can extract\nsensitive information, such as secret keys, from the implementation of\ncryptographic algorithms. Existing countermeasures against power-based side\nchannel attacks focus on analyzing information leakage at the byte level.\nHowever, this approach neglects the impact of individual bits on the overall\nresistance of a cryptographic implementation. In this work, we present a\ncountermeasure based on single-bit leakage. The results suggest that the\nproposed countermeasure cannot be broken by attacks using conventional SCA\nleakage models.", "AI": {"tldr": "The paper discusses how CMOS circuits' power consumption can be exploited in side channel attacks and proposes a bit-level countermeasure that effectively thwarts conventional attack models.", "motivation": "CMOS circuits are vulnerable to side channel attacks since their power consumption varies with processed data. Existing countermeasures targeting byte-level analysis may insufficiently address vulnerabilities at the individual bit level.", "method": "The study introduces a countermeasure based on analyzing and mitigating single-bit leakage rather than byte-level leakage in CMOS circuits.", "result": "The proposed countermeasure can resist side channel attacks that rely on conventional leakage models, indicating improved security against these threats.", "conclusion": "The bit-level countermeasure minimizes side-channel leakage by addressing individual bit impacts, offering a robust solution against current power-based side channel attacks."}}
{"id": "2510.04791", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04791", "abs": "https://arxiv.org/abs/2510.04791", "authors": ["Kristian Kolthoff", "Felix Kretzer", "Simone Paolo Ponzetto", "Alexander Maedche", "Christian Bartelt"], "title": "GUISpector: An MLLM Agent Framework for Automated Verification of Natural Language Requirements in GUI Prototypes", "comment": null, "summary": "GUIs are foundational to interactive systems and play a pivotal role in early\nrequirements elicitation through prototyping. Ensuring that GUI implementations\nfulfill NL requirements is essential for robust software engineering,\nespecially as LLM-driven programming agents become increasingly integrated into\ndevelopment workflows. Existing GUI testing approaches, whether traditional or\nLLM-driven, often fall short in handling the complexity of modern interfaces,\nand typically lack actionable feedback and effective integration with automated\ndevelopment agents. In this paper, we introduce GUISpector, a novel framework\nthat leverages a multi-modal (M)LLM-based agent for the automated verification\nof NL requirements in GUI prototypes. First, GUISpector adapts a MLLM agent to\ninterpret and operationalize NL requirements, enabling to autonomously plan and\nexecute verification trajectories across GUI applications. Second, GUISpector\nsystematically extracts detailed NL feedback from the agent's verification\nprocess, providing developers with actionable insights that can be used to\niteratively refine the GUI artifact or directly inform LLM-based code\ngeneration in a closed feedback loop. Third, we present an integrated tool that\nunifies these capabilities, offering practitioners an accessible interface for\nsupervising verification runs, inspecting agent rationales and managing the\nend-to-end requirements verification process. We evaluated GUISpector on a\ncomprehensive set of 150 requirements based on 900 acceptance criteria\nannotations across diverse GUI applications, demonstrating effective detection\nof requirement satisfaction and violations and highlighting its potential for\nseamless integration of actionable feedback into automated LLM-driven\ndevelopment workflows. The video presentation of GUISpector is available at:\nhttps://youtu.be/JByYF6BNQeE, showcasing its main capabilities.", "AI": {"tldr": "GUISpector is a multi-modal LLM-based framework that automates verification of natural language (NL) requirements in GUI prototypes, offering actionable feedback and integration with LLM-driven development workflows.", "motivation": "Traditional and LLM-driven GUI testing approaches struggle with modern interface complexity and lack actionable feedback, hindering requirements verification in LLM-driven development workflows.", "method": "1) Adapts a multi-modal LLM agent to interpret NL requirements and execute verification trajectories. 2)...extracts detailed NL feedback for iterative refinement. 3)...provides an integrated tool for supervising verification processes and managing workflows.", "result": "Evaluated on 150 requirements (900 criteria), demonstrating effective detection of requirement satisfaction/violations and seamless feedback integration into LLM-driven workflows.", "conclusion": "GUISpector advances automated GUI verification with actionable feedback mechanisms, enabling closed-loop improvements in GUI prototypes and LLM-based code generation within development pipelines."}}
{"id": "2510.04652", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.04652", "abs": "https://arxiv.org/abs/2510.04652", "authors": ["Ines Akaichi", "Giorgos Flouris", "Irini Fundulaki", "Sabrina Kirrane"], "title": "Modeling and Managing Temporal Obligations in GUCON Using SPARQL-star and RDF-star", "comment": null, "summary": "In the digital age, data frequently crosses organizational and jurisdictional\nboundaries, making effective governance essential. Usage control policies have\nemerged as a key paradigm for regulating data usage, safeguarding privacy,\nprotecting intellectual property, and ensuring compliance with regulations. A\ncentral mechanism for usage control is the handling of obligations, which arise\nas a side effect of using and sharing data. Effective monitoring of obligations\nrequires capturing usage traces and accounting for temporal aspects such as\nstart times and deadlines, as obligations may evolve over times into different\nstates, such as fulfilled, violated, or expired. While several solutions have\nbeen proposed for obligation monitoring, they often lack formal semantics or\nprovide limited support for reasoning over obligation states. To address these\nlimitations, we extend GUCON, a policy framework grounded in the formal\nsemantics of SPAQRL graph patterns, to explicitly model the temporal aspects of\nan obligation. This extension enables the expressing of temporal obligations\nand supports continuous monitoring of their evolving states based on usage\ntraces stored in temporal knowledge graphs. We demonstrate how this extended\nmodel can be represented using RDF-star and SPARQL-star and propose an\nObligation State Manager that monitors obligation states and assess their\ncompliance with respect to usage traces. Finally, we evaluate both the extended\nmodel and its prototype implementation.", "AI": {"tldr": "Extends GUCON to model temporal obligations using SPAQRL semantics and temporal knowledge graphs, enabling continuous obligation state monitoring and compliance assessment.", "motivation": "Existing obligation monitoring solutions lack formal semantics and robust temporal reasoning capabilities for evolving obligation states (fulfilled/violated/expired), creating governance gaps in dynamic data sharing environments.", "method": "Formally integrates temporal aspects into GUCON policy framework via SPAQRL-based semantics; uses temporal knowledge graphs to store usage traces; introduces RDF-star/SPARQL-star for temporal obligation representation; develops Obligation State Manager for continuous state monitoring.", "result": "Proposed temporal extension to GUCON, implementation of Obligation State Manager, and evaluation showing effective support for temporal obligation monitoring across evolving data usage scenarios.", "conclusion": "Temporal-enhanced GUCON provides formal foundations for monitoring obligation state evolution, enabling systematic compliance assessment in cross-organizational data governance workflows."}}
{"id": "2510.04796", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04796", "abs": "https://arxiv.org/abs/2510.04796", "authors": ["Samah Kansab", "Francis Bordeleau", "Ali Tizghadam"], "title": "RevMine: An LLM-Assisted Tool for Code Review Mining and Analysis Across Git Platforms", "comment": null, "summary": "Empirical research on code review processes is increasingly central to\nunderstanding software quality and collaboration. However, collecting and\nanalyzing review data remains a time-consuming and technically intensive task.\nMost researchers follow similar workflows - writing ad hoc scripts to extract,\nfilter, and analyze review data from platforms like GitHub and GitLab. This\npaper introduces RevMine, a conceptual tool that streamlines the entire code\nreview mining pipeline using large language models (LLMs). RevMine guides users\nthrough authentication, endpoint discovery, and natural language-driven data\ncollection, significantly reducing the need for manual scripting. After\nretrieving review data, it supports both quantitative and qualitative analysis\nbased on user-defined filters or LLM-inferred patterns. This poster outlines\nthe tool's architecture, use cases, and research potential. By lowering the\nbarrier to entry, RevMine aims to democratize code review mining and enable a\nbroader range of empirical software engineering studies.", "AI": {"tldr": "RevMine is a conceptual tool that uses large language models to automate and streamline code review data mining, reducing manual scripting and democratizing access to software engineering research.", "motivation": "The paper is motivated by the growing importance of code review analysis for software quality understanding and the current technical challenges in data collection and analysis.", "method": "RevMine employs LLMs to automate data collection from review platforms through natural language interfaces and streamline analysis workflows with both user-defined and model-inferred processing.", "result": "The tool demonstrates the ability to simplify code review mining pipelines by automating authentication, endpoint discovery, data filtering, and analysis while supporting both quantitative and qualitative approaches.", "conclusion": "RevMine represents a new approach to code review mining by leveraging LLMs for intelligent tooling, potentially expanding the scope and accessibility of empirical software engineering research."}}
{"id": "2510.04882", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.04882", "abs": "https://arxiv.org/abs/2510.04882", "authors": ["Elian Morel"], "title": "Enhancing TreePIR for a Single-Server Setting via Resampling", "comment": null, "summary": "Private Information Retrieval (PIR) allows a client to retrieve an entry\n$\\text{DB}[i]$ from a public database $\\text{DB}$ held by one or more servers,\nwithout revealing the queried index $i$. Traditional PIR schemes achieve\nsublinear server computation only under strong assumptions, such as the\npresence of multiple non-colluding servers or the use of public-key\ncryptography. To overcome these limitations, \\textit{preprocessing PIR} schemes\nintroduce a query-independent offline phase where the client collects\n\\textit{hints} that enable efficient private queries during the online phase.\n  In this work, we focus on preprocessing PIR schemes relying solely on\n\\textit{One-Way Functions} (OWFs), which provide minimal cryptographic\nassumptions and practical implementability. We study three main constructions\n-- TreePIR, PIANO, and PPPS -- that explore different trade-offs between\ncommunication, storage, and server trust assumptions. Building upon the\nmechanisms introduced in PIANO and PPPS, we propose an adaptation of TreePIR to\nthe single-server setting by introducing a dual-table hint structure (primary\nand backup tables) and a \\textit{resampling} technique to refresh hints\nefficiently.\n  Our proposed scheme achieves logarithmic upload bandwidth and $O(\\sqrt{n}\\log\nn)$ download complexity while requiring $O(\\sqrt{n}\\log n)$ client storage.\nThis represents a significant improvement over prior single-server\npreprocessing PIR schemes such as PIANO ($O(\\sqrt{n})$ bandwidth) and PPPS\n($O(n^{1/4})$ bandwidth), while maintaining the simplicity and minimal\nassumptions of the OWF-based setting.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.04835", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04835", "abs": "https://arxiv.org/abs/2510.04835", "authors": ["Wentao Gao", "Renata Borovica-Gajic", "Sang Kil Cha", "Tian Qiu", "Van-Thuan Pham"], "title": "InsightQL: Advancing Human-Assisted Fuzzing with a Unified Code Database and Parameterized Query Interface", "comment": null, "summary": "Fuzzing is a highly effective automated testing method for uncovering\nsoftware vulnerabilities. Despite advances in fuzzing techniques, such as\ncoverage-guided greybox fuzzing, many fuzzers struggle with coverage plateaus\ncaused by fuzz blockers, limiting their ability to find deeper vulnerabilities.\nHuman expertise can address these challenges, but analyzing fuzzing results to\nguide this support remains labor-intensive. To tackle this, we introduce\nInsightQL, the first human-assisting framework for fuzz blocker analysis.\nPowered by a unified database and an intuitive parameterized query interface,\nInsightQL aids developers in systematically extracting insights and efficiently\nunblocking fuzz blockers. Our experiments on 14 popular real-world libraries\nfrom the FuzzBench benchmark demonstrate the effectiveness of InsightQL,\nleading to the unblocking of many fuzz blockers and considerable improvements\nin code coverage (up to 13.90%).", "AI": {"tldr": "InsightQL is a human-assisting framework for fuzz blocker analysis that improves code coverage by up to 13.90%, addressing limitations of existing fuzzers through a unified database and parameterized query interface.", "motivation": "Coverage-guided greybox fuzzers struggle with coverage plateaus caused by fuzz blockers; manual analysis to resolve these is labor-intensive and inefficient.", "method": "InsightQL introduces a unified database and intuitive parameterized query interface to systematically extract insights and unblock fuzz blockers with human assistance.", "result": "Experiments on 14 real-world libraries in FuzzBench unblocked numerous fuzz blockers, achieving up to 13.90% code coverage improvements.", "conclusion": "InsightQL effectively enhances fuzzing by addressing blockers, demonstrating significant coverage gains and validating its utility as a human-assisting framework."}}
{"id": "2510.04885", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04885", "abs": "https://arxiv.org/abs/2510.04885", "authors": ["Yuxin Wen", "Arman Zharmagambetov", "Ivan Evtimov", "Narine Kokhlikyan", "Tom Goldstein", "Kamalika Chaudhuri", "Chuan Guo"], "title": "RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning Recipe for Strong Prompt Injection", "comment": null, "summary": "Prompt injection poses a serious threat to the reliability and safety of LLM\nagents. Recent defenses against prompt injection, such as Instruction Hierarchy\nand SecAlign, have shown notable robustness against static attacks. However, to\nmore thoroughly evaluate the robustness of these defenses, it is arguably\nnecessary to employ strong attacks such as automated red-teaming. To this end,\nwe introduce RL-Hammer, a simple recipe for training attacker models that\nautomatically learn to perform strong prompt injections and jailbreaks via\nreinforcement learning. RL-Hammer requires no warm-up data and can be trained\nentirely from scratch. To achieve high ASRs against industrial-level models\nwith defenses, we propose a set of practical techniques that enable highly\neffective, universal attacks. Using this pipeline, RL-Hammer reaches a 98% ASR\nagainst GPT-4o and a $72\\%$ ASR against GPT-5 with the Instruction Hierarchy\ndefense. We further discuss the challenge of achieving high diversity in\nattacks, highlighting how attacker models tend to reward-hack diversity\nobjectives. Finally, we show that RL-Hammer can evade multiple prompt injection\ndetectors. We hope our work advances automatic red-teaming and motivates the\ndevelopment of stronger, more principled defenses. Code is available at\nhttps://github.com/facebookresearch/rl-injector.", "AI": {"tldr": "The paper introduces RL-Hammer, a reinforcement learning-based method for automated prompt injection attacks against LLMs with advanced defenses, achieving high success rates (e.g., 98% ASR against GPT-4o) and evading detectors. The work emphasizes improving red-teaming and defense robustness.", "motivation": "Existing defenses like Instruction Hierarchy and SecAlign remain poorly tested against strong automated attacks, necessitating rigorous evaluation via red-teaming to drive safer LLM development.", "method": "RL-Hammer trains attacker models from scratch using reinforcement learning, employing techniques to maximize attack success rates and diversity without requiring pre-labeled data, enabling universal attacks against industrial models with defenses.", "result": "Achieved 98.0% ASR on GPT-4o and 72.0\u00b14.5% ASR on GPT-5 with Instruction Hierarchy defense, while evading multiple prompt injection detectors, demonstrating strong effectiveness and adaptability.", "conclusion": "Highlights RL-Hammer's role in stress-testing defenses through automated red-teaming, exposing limitations in current detector diversity, and advocating for principled defense frameworks against dynamic attacks."}}
{"id": "2510.04852", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04852", "abs": "https://arxiv.org/abs/2510.04852", "authors": ["Victor May", "Diganta Misra", "Yanqi Luo", "Anjali Sridhar", "Justine Gehring", "Silvio Soares Ribeiro Junior"], "title": "FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration", "comment": "18 pages, 11 figures", "summary": "AI coding assistants are rapidly becoming integral to modern software\ndevelopment. A key challenge in this space is the continual need to migrate and\nmodernize codebases in response to evolving software ecosystems. Traditionally,\nsuch migrations have relied on rule-based systems and human intervention. With\nthe advent of powerful large language models (LLMs), AI-driven agentic\nframeworks offer a promising alternative-but their effectiveness has not been\nsystematically evaluated. In this paper, we introduce FreshBrew, a novel\nbenchmark for evaluating AI agents on project-level Java migrations, with a\nspecific focus on measuring an agent's ability to preserve program semantics\nand avoid reward hacking, which we argue requires projects with high test\ncoverage for a rigorous and reliable evaluation. We benchmark several\nstate-of-the-art LLMs, and compare their performance against established\nrule-based tools. Our evaluation of AI agents on this benchmark of 228\nrepositories shows that the top-performing model, Gemini 2.5 Flash, can\nsuccessfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis\nreveals novel insights into the critical strengths and limitations of current\nagentic approaches, offering actionable insights into their real-world\napplicability. Our empirical study reveals failure modes of current AI agents\nin realistic Java modernization tasks, providing a foundation for evaluating\ntrustworthy code-migration systems. By releasing FreshBrew, we aim to\nfacilitate rigorous, reproducible evaluation and catalyze progress in AI-driven\ncodebase modernization.", "AI": {"tldr": "The paper introduces FreshBrew, a benchmark for evaluating AI agents in Java codebase modernization, revealing strengths and limitations of current agentic approaches in this domain.", "motivation": "Traditional code migration relies on rule-based systems and human effort, but the effectiveness of AI-driven agentic frameworks remains unexamined with rigorous project-level evaluations for Java modernization.", "method": "The authors developed FreshBrew, a benchmark requiring semantics-preserving migrations across 228 repositories with high test coverage to detect reward hacking, comparing state-of-the-art LLMs to established rule-based tools.", "result": "Gemini 2.5 Flash achieved 52.3% successful migrations to JDK 17, while the study uncovered novel insights into AI agent effectiveness, critical limitations, and failure modes in realistic modernization scenarios.", "conclusion": "FreshBrew establishes a foundation for trustworthy code-migration evaluation, enabling reproducible research and progress in AI-driven codebase modernization while highlighting the need for improved agent reliability."}}
{"id": "2510.04987", "categories": ["cs.CR", "I.2"], "pdf": "https://arxiv.org/pdf/2510.04987", "abs": "https://arxiv.org/abs/2510.04987", "authors": ["Avilash Rath", "Weiliang Qi", "Youpeng Li", "Xinda Wang"], "title": "NatGVD: Natural Adversarial Example Attack towards Graph-based Vulnerability Detection", "comment": "10 pages, 2 figures (2 additional figures in Appendices)", "summary": "Graph-based models learn rich code graph structural information and present\nsuperior performance on various code analysis tasks. However, the robustness of\nthese models against adversarial example attacks in the context of\nvulnerability detection remains an open question. This paper proposes NatGVD, a\nnovel attack methodology that generates natural adversarial vulnerable code to\ncircumvent GNN-based and graph-aware transformer-based vulnerability detectors.\nNatGVD employs a set of code transformations that modify graph structure while\npreserving code semantics. Instead of injecting dead or unrelated code like\nprevious works, NatGVD considers naturalness requirements: generated examples\nshould not be easily recognized by humans or program analysis tools. With\nextensive evaluation of NatGVD on state-of-the-art vulnerability detection\nsystems, the results reveal up to 53.04% evasion rate across GNN-based\ndetectors and graph-aware transformer-based detectors. We also explore\npotential defense strategies to enhance the robustness of these systems against\nNatGVD.", "AI": {"tldr": "The paper introduces NatGVD, an attack method for vulnerability detectors that uses natural adversarial vulnerable code, showing significant evasion rates and proposing potential defenses.", "motivation": "Current robustness of graph-based models to adversarial attacks in vulnerability detection is unverified, raising concerns about their dependability.", "method": "NatGVD leverages code transformations to alter graph structure without affecting semantics, emphasizing naturalness to avoid human and tool detection.", "result": "NatGVD achieves an evasion rate of up to 53.04% against advanced vulnerability detectors, highlighting their susceptibility to natural-like attacks.", "conclusion": "NatGVD's effectiveness against vulnerability detectors necessitates implementing defense mechanisms, which the study also presents in the form of preliminary strategies."}}
{"id": "2510.04905", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04905", "abs": "https://arxiv.org/abs/2510.04905", "authors": ["Yicheng Tao", "Yao Qin", "Yepang Liu"], "title": "Retrieval-Augmented Code Generation: A Survey with Focus on Repository-Level Approaches", "comment": null, "summary": "Recent advancements in large language models (LLMs) have substantially\nimproved automated code generation. While function-level and file-level\ngeneration have achieved promising results, real-world software development\ntypically requires reasoning across entire repositories. This gives rise to the\nchallenging task of Repository-Level Code Generation (RLCG), where models must\ncapture long-range dependencies, ensure global semantic consistency, and\ngenerate coherent code spanning multiple files or modules. To address these\nchallenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful\nparadigm that integrates external retrieval mechanisms with LLMs, enhancing\ncontext-awareness and scalability. In this survey, we provide a comprehensive\nreview of research on Retrieval-Augmented Code Generation (RACG), with an\nemphasis on repository-level approaches. We categorize existing work along\nseveral dimensions, including generation strategies, retrieval modalities,\nmodel architectures, training paradigms, and evaluation protocols. Furthermore,\nwe summarize widely used datasets and benchmarks, analyze current limitations,\nand outline key challenges and opportunities for future research. Our goal is\nto establish a unified analytical framework for understanding this rapidly\nevolving field and to inspire continued progress in AI-powered software\nengineering.", "AI": {"tldr": "This paper surveys Retrieval-Augmented Code Generation (RACG), focusing on repository-level approaches, categorizing research dimensions, summarizing datasets, and identifying future challenges.", "motivation": "Current code generation models struggle with repository-level tasks requiring global consistency and long-range dependencies, necessitating scalable solutions like retrieval-augmented generation.", "method": "The authors perform a systematic review, categorizing RACG works along generation strategies, retrieval modalities, model architectures, training paradigms, and evaluation protocols, while analyzing existing datasets.", "result": "Highlights progress in RACG but identifies limitations in scalability, evaluation protocols, and cross-repository generalization, providing a structured overview of current methods and benchmarks.", "conclusion": "Establishes a unified analytical framework for RACG, outlining key challenges and opportunities to guide future research in AI-driven software engineering at the repository level."}}
{"id": "2510.05052", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.05052", "abs": "https://arxiv.org/abs/2510.05052", "authors": ["Weiliang Zhao", "Jinjun Peng", "Daniel Ben-Levi", "Zhou Yu", "Junfeng Yang"], "title": "Proactive defense against LLM Jailbreak", "comment": null, "summary": "The proliferation of powerful large language models (LLMs) has necessitated\nrobust safety alignment, yet these models remain vulnerable to evolving\nadversarial attacks, including multi-turn jailbreaks that iteratively search\nfor successful queries. Current defenses, primarily reactive and static, often\nfail to counter these search-based attacks. In this paper, we introduce ProAct,\na novel proactive defense framework designed to disrupt and mislead autonomous\njailbreaking processes. Our core idea is to intentionally provide adversaries\nwith \"spurious responses\" that appear to be results of successful jailbreak\nattacks but contain no actual harmful content. These misleading responses\nprovide false signals to the attacker's internal optimization loop, causing the\nadversarial search to terminate prematurely and effectively jailbreaking the\njailbreak. By conducting extensive experiments across state-of-the-art LLMs,\njailbreaking frameworks, and safety benchmarks, our method consistently and\nsignificantly reduces attack success rates by up to 92\\%. When combined with\nother defense frameworks, it further reduces the success rate of the latest\nattack strategies to 0\\%. ProAct represents an orthogonal defense strategy that\ncan serve as an additional guardrail to enhance LLM safety against the most\neffective jailbreaking attacks.", "AI": {"tldr": "The paper introduces ProAct, a proactive defense framework against adversarial multi-turn jailbreak attacks on large language models (LLMs). By generating 'spurious responses' that mimic successful jailbreak outputs but lack real harmful content, ProAct misleads attackers into false estimation of success, reducing attack success rates by up to 92%.", "motivation": "LLMs are vulnerable to evolving search-based jailbreak attacks that existing reactive/static defenses cannot counter. This paper addresses the need for proactive strategies to disrupt adversarial attack processes in real-time.", "method": "ProAct deploys a 'decoy-based defense strategy' that generates realistic but harmless responses during autonomous jailbreaking. These spurious responses exploit the heuristic feedback loops in attack optimization, causing early termination of adversarial search without relying on explicit content filtering.", "result": "Experiments show ProAct reduces attack success rates by 92% across SOTA LLMs and frameworks. When combined with existing defense systems, it achieves 100%, non-intrusive mitigation of state-of-the-art jailbreaking techniques.", "conclusion": "ProAct provides an orthogonal defense mechanism that can complement existing safety measures while maintaining LLM capabilities. This strategy shifts the paradigm from reactive filtering to proactive deception in adversarial robustness."}}
{"id": "2510.04964", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04964", "abs": "https://arxiv.org/abs/2510.04964", "authors": ["Kelechi G. Kalu", "James C. Davis"], "title": "Why Software Signing (Still) Matters: Trust Boundaries in the Software Supply Chain", "comment": "8 Pages, 3 Figures", "summary": "Software signing provides a formal mechanism for provenance by ensuring\nartifact integrity and verifying producer identity. It also imposes tooling and\noperational costs to implement in practice. In an era of centralized registries\nsuch as PyPI, npm, Maven Central, and Hugging Face, it is reasonable to ask\nwhether hardening registry security controls obviates the need for end-to-end\nartifact signing. In this work, we posit that the core guarantees of signing,\nprovenance, integrity, and accountability are not automatically carried across\ndifferent software distribution boundaries. These boundaries include mirrors,\ncorporate proxies, re-hosting, and air-gapped transfers, where registry\nsecurity controls alone cannot provide sufficient assurance. We synthesize\nhistorical practice and present a trust model for modern distribution modes to\nidentify when signing is necessary to extend trust beyond registry control.\nTreating signing as a baseline layer of defense strengthens software supply\nchain assurance even when registries are secure.", "AI": {"tldr": "This paper argues that software artifact signing remains critical for supply chain security even with hardened registry controls, as trust boundaries beyond registries (mirrors, proxies, etc.) require end-to-end provenance guarantees.", "motivation": "Centralized registries have security controls, but the paper challenges whether these alone suffice to replace artifact signing given distribution boundary limitations.", "method": "Analyzes historical practices and proposes a trust model to determine when signing is required to maintain integrity across distribution channels beyond registry control.", "result": "Demonstrates that registry security controls cannot ensure provenance across mirrors, re-hosting, and air-gapped transfers, necessitating signing as a baseline defense mechanism.", "conclusion": "Software signing provides essential accountability and integrity guarantees that complement registry security, ensuring trust extends beyond registry-controlled boundaries."}}
{"id": "2510.04982", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04982", "abs": "https://arxiv.org/abs/2510.04982", "authors": ["Aakash Ahmad", "Muhammad Waseem", "Bakheet Aljedaani", "Mahdi Fahmideh", "Peng Liang", "Feras Awaysheh"], "title": "Quantum Computing as a Service - a Software Engineering Perspective", "comment": "37 pages, 10 images, 5 tables, Manuscript submitted to a Journal\n  (2025)", "summary": "Quantum systems have started to emerge as a disruptive technology and\nenabling platforms - exploiting the principles of quantum mechanics via\nprogrammable quantum bits (QuBits) - to achieve quantum supremacy in computing.\nAcademic research, industrial projects (e.g., Amazon Braket, IBM Qiskit), and\nconsortiums like 'Quantum Flagship' are striving to develop practically capable\nand commercially viable quantum computing (QC) systems and technologies.\nQuantum Computing as a Service (QCaaS) is viewed as a solution attuned to the\nphilosophy of service-orientation that can offer QC resources and platforms, as\nutility computing, to individuals and organisations who do not own quantum\ncomputers. This research investigates a process-centric and architecture-driven\napproach to offer a software engineering perspective on enabling QCaaS - a.k.a\nquantum service-orientation. We employed a two-phase research method comprising\n(a) a systematic mapping study and (b) an architecture-based development, first\nto identify the phases of the quantum service development life cycle and\nsubsequently to integrate these phases into a reference architecture that\nsupports QCaaS. The SMS process retrieved a collection of potentially relevant\nresearch literature and based on a multi-step selection and qualitative\nassessment, we selected 41 peer-reviewed studies to answer three RQs. The RQs\ninvestigate (i) demographic details in terms of frequency, types, and trends of\nresearch, (ii) phases of quantum service development lifecycle to derive a\nreference architecture for conception, modeling, assembly, and deployment of\nservices, and (iii) The results identify a 4-phased development lifecycle along\nwith quantum significant requirements (QSRs), various modeling notations,\ncatalogue of patterns, programming languages, and deployment platforms that can\nbe integrated in a layered reference architecture to engineer QCaaS.", "AI": {"tldr": "This paper proposes a software engineering approach for enabling Quantum Computing as a Service (QCaaS) via process-centric architecture development, integrating systematic reviews and reference architectures for quantum service lifecycle management.", "motivation": "As quantum systems advance towards practical deployment, there is a growing need to address service-oriented software engineering challenges for QCaaS to make quantum resources accessible as utility computing. Existing research lacks structured frameworks for quantum service development.", "method": "A two-phase approach combining (a) systematic mapping study (SMS) to analyze 41 peer-reviewed studies through multi-step selection and (b) architecture-driven development to create a reference architecture. The study addresses three research questions spanning research trends, service lifecycle phases, and technical requirements.", "result": "Identified a four-phased quantum service lifecycle (conception, modeling, assembly, deployment), quantum significant requirements (QSRs), modeling notations, design patterns, programming languages, and deployment platforms. These components were integrated into a layered reference architecture for QCaaS.", "conclusion": "This research provides the first structured software engineering perspective for QCaaS engineering, offering a reference architecture and standardized lifecycle model to support organizations in developing quantum services. The outcomes contribute to both academic research and industry adoption of quantum technologies."}}
{"id": "2510.04997", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04997", "abs": "https://arxiv.org/abs/2510.04997", "authors": ["Jiongchi Yu", "Weipeng Jiang", "Xiaoyu Zhang", "Qiang Hu", "Xiaofei Xie", "Chao Shen"], "title": "AutoEmpirical: LLM-Based Automated Research for Empirical Software Fault Analysis", "comment": "5 pages", "summary": "Understanding software faults is essential for empirical research in software\ndevelopment and maintenance. However, traditional fault analysis, while\nvaluable, typically involves multiple expert-driven steps such as collecting\npotential faults, filtering, and manual investigation. These processes are both\nlabor-intensive and time-consuming, creating bottlenecks that hinder\nlarge-scale fault studies in complex yet critical software systems and slow the\npace of iterative empirical research.\n  In this paper, we decompose the process of empirical software fault study\ninto three key phases: (1) research objective definition, (2) data preparation,\nand (3) fault analysis, and we conduct an initial exploration study of applying\nLarge Language Models (LLMs) for fault analysis of open-source software.\nSpecifically, we perform the evaluation on 3,829 software faults drawn from a\nhigh-quality empirical study. Our results show that LLMs can substantially\nimprove efficiency in fault analysis, with an average processing time of about\ntwo hours, compared to the weeks of manual effort typically required. We\nconclude by outlining a detailed research plan that highlights both the\npotential of LLMs for advancing empirical fault studies and the open challenges\nthat required be addressed to achieve fully automated, end-to-end software\nfault analysis.", "AI": {"tldr": "This paper explores using Large Language Models (LLMs) to automate software fault analysis, demonstrating significant efficiency improvements over traditional manual methods. By decomposing the empirical study process into three phases and evaluating 3,829 faults, the authors show LLMs can reduce analysis time from weeks to two hours while highlighting challenges for achieving full automation.", "motivation": "Traditional software fault analysis relies on manual, labor-intensive processes (collection, filtering, investigation) that create bottlenecks in large-scale or iterative empirical research. Automated solutions are needed to enable more scalable and efficient fault studies.", "method": "The authors decompose the fault study process into 1) research objective definition, 2)data preparation, and 3)fault analysis phases. They conduct an initial exploration using LLMs for fault analysis, evaluating 3,829 faults from a high-quality empirical study dataset.", "result": "LLMs achieved an average fault analysis time of ~2 hours compared to weeks of manual effort. The evaluation demonstrated improved efficiency while maintaining accuracy in fault classification/localization tasks.", "conclusion": "LLMs show promise for accelerating empirical software fault studies but require addressing open challenges (unknowns) to enable fully automated, end-to-end analysis. The authors outline a research plan to advance LLMs in this domain."}}
