<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 11]
- [cs.SE](#cs.SE) [Total: 7]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Random Forest Stratified K-Fold Cross Validation on SYN DoS Attack SD-IoV](https://arxiv.org/abs/2509.07016)
*Muhammad Arif Hakimi Zamrai,Kamaludin Mohd Yusof*

Main category: cs.CR

TL;DR: Optimized Random Forest model achieves near-perfect metrics and 0.24s detection time for TCP SYN flood attacks in SD-IoV networks.


<details>
  <summary>Details</summary>
Motivation: Addressing network security vulnerabilities in dynamic Software-Defined Internet of Vehicles (SD-IoV) systems threatened by TCP SYN flood attacks, requiring high-accuracy, low-latency solutions to maintain both security and network efficiency.

Method: A Random Forest Classifier was optimized through feature scaling, label encoding preprocessing, and Stratified K-Fold cross-validation, using 20 estimators and depth-10 tree configuration.

Result: Achieved 0.999998 average accuracy/precision/recall/F1-score with 0.24s detection time, effectively distinguishing normal and malicious traffic in vehicular networks.

Conclusion: This approach establishes a state-of-the-art solution for SYN flood detection in SD-IoV, balancing perfect accuracy with real-time performance requirements critical for vehicular communication systems.

Abstract: In response to the prevalent concern of TCP SYN flood attacks within the
context of Software-Defined Internet of Vehicles (SD-IoV), this study addresses
the significant challenge of network security in rapidly evolving vehicular
communication systems. This research focuses on optimizing a Random Forest
Classifier model to achieve maximum accuracy and minimal detection time,
thereby enhancing vehicular network security. The methodology involves
preprocessing a dataset containing SYN attack instances, employing feature
scaling and label encoding techniques, and applying Stratified K-Fold
cross-validation to target key metrics such as accuracy, precision, recall, and
F1-score. This research achieved an average value of 0.999998 for all metrics
with a SYN DoS attack detection time of 0.24 seconds. Results show that the
fine-tuned Random Forest model, configured with 20 estimators and a depth of
10, effectively differentiates between normal and malicious traffic with high
accuracy and minimal detection time, which is crucial for SD-IoV networks. This
approach marks a significant advancement and introduces a state-of-the-art
algorithm in detecting SYN flood attacks, combining high accuracy with minimal
detection time. It contributes to vehicular network security by providing a
robust solution against TCP SYN flood attacks while maintaining network
efficiency and reliability.

</details>


### [2] [The Signalgate Case is Waiving a Red Flag to All Organizational and Behavioral Cybersecurity Leaders, Practitioners, and Researchers: Are We Receiving the Signal Amidst the Noise?](https://arxiv.org/abs/2509.07053)
*Paul Benjamin Lowry,Gregory D. Moody,Robert Willison,Clay Posey*

Main category: cs.CR

TL;DR: The Signalgate incident exemplifies how human error and governance flaws can undermine organizational security despite robust technical defenses. This paper uses a NIST-based analysis to propose that security strategies must shift toward proactive governance, leadership-driven culture, and human-centric investments to address systemic risks.


<details>
  <summary>Details</summary>
Motivation: The Signalgate incident revealed systemic vulnerabilities in organizational security stemming from human error, governance gaps, and technology misuse. The paper aims to shift the focus from external cyber threats to internal human and organizational factors often overlooked in security practices.

Method: The study employs a case-study approach and systematic review grounded in the NIST Cybersecurity Framework to analyze patterns of human-centric vulnerabilities and governance challenges associated with the Signalgate incident.

Result: The analysis identifies three key findings: (1) Human behavior is the critical vulnerability in organizational security, (2) Leadership tone directly shapes security culture and efficacy, and (3) Overreliance on technical solutions without addressing human/organizational factors leads to ineffective practices. Recommendations include zero-trust architectures, leadership engagement, and structured accountability frameworks.

Conclusion: The paper concludes that organizational security strategies must prioritize addressing governance, cultural, and behavioral risks, particularly during transitions like mergers. Leaders and policymakers need to adopt holistic approaches emphasizing zero-trust architectures, leadership engagement, and human-centric solutions to mitigate systemic vulnerabilities highlighted by the Signalgate incident.

Abstract: The Signalgate incident of March 2025, wherein senior US national security
officials inadvertently disclosed sensitive military operational details via
the encrypted messaging platform Signal, highlights critical vulnerabilities in
organizational security arising from human error, governance gaps, and the
misuse of technology. Although smaller in scale when compared to historical
breaches involving billions of records, Signalgate illustrates critical
systemic issues often overshadowed by a focus on external cyber threats.
Employing a case-study approach and systematic review grounded in the NIST
Cybersecurity Framework, we analyze the incident to identify patterns of
human-centric vulnerabilities and governance challenges common to
organizational security failures. Findings emphasize three critical points. (1)
Organizational security depends heavily on human behavior, with internal actors
often serving as the weakest link despite advanced technical defenses; (2)
Leadership tone strongly influences organizational security culture and
efficacy, and (3) widespread reliance on technical solutions without sufficient
investments in human and organizational factors leads to ineffective practices
and wasted resources. From these observations, we propose actionable
recommendations for enhancing organizational and national security, including
strong leadership engagement, comprehensive adoption of zero-trust
architectures, clearer accountability structures, incentivized security
behaviors, and rigorous oversight. Particularly during periods of
organizational transition, such as mergers or large-scale personnel changes,
additional measures become particularly important. Signalgate underscores the
need for leaders and policymakers to reorient cybersecurity strategies toward
addressing governance, cultural, and behavioral risks.

</details>


### [3] [Sequentially Auditing Differential Privacy](https://arxiv.org/abs/2509.07055)
*Tomás González,Mateo Dulce-Rubio,Aaditya Ramdas,Mónica Ribero*

Main category: cs.CR

TL;DR: This paper introduces an efficient sequential test for differential privacy auditing that reduces sample requirements by 1000× and detects violations faster than existing methods, including identifying DP-SGD issues in under one training run.


<details>
  <summary>Details</summary>
Motivation: Existing batch auditing methods suffer from fixed sample size requirements and inefficiencies (e.g., needing 50K samples or full model training) which limits practical differential privacy auditing.

Method: A sequential testing framework processing output streams with anytime-valid inference and Type I error control to audit differential privacy in black-box mechanisms.

Result: Achieves orders-of-magnitude reduction in sample size (50K → hundreds) and detects DP-SGD violations within a single training run, unlike prior methods requiring full training cycles.

Conclusion: The proposed sequential test outperforms existing methods by significantly reducing required sample sizes and detecting DP violations efficiently, including in under one training run for DP-SGD.

Abstract: We propose a practical sequential test for auditing differential privacy
guarantees of black-box mechanisms. The test processes streams of mechanisms'
outputs providing anytime-valid inference while controlling Type I error,
overcoming the fixed sample size limitation of previous batch auditing methods.
Experiments show this test detects violations with sample sizes that are orders
of magnitude smaller than existing methods, reducing this number from 50K to a
few hundred examples, across diverse realistic mechanisms. Notably, it
identifies DP-SGD privacy violations in \textit{under} one training run, unlike
prior methods needing full model training.

</details>


### [4] [SoK: Security and Privacy of AI Agents for Blockchain](https://arxiv.org/abs/2509.07131)
*Nicolò Romandini,Carlo Mazzocca,Kai Otsuki,Rebecca Montanari*

Main category: cs.CR

TL;DR: This paper is the first to systematically review AI agents for blockchain security and privacy, addressing prior gaps in focused literature and outlining applications, limitations, and future research paths.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the identified gap in literature: while blockchain and AI agent integration is growing, no existing surveys specifically focus on AI-driven systems for blockchain, particularly in security and privacy, leaving non-experts and researchers without structured guidance.

Method: The authors employ a Systematization of Knowledge (SoK) approach, systematically reviewing and organizing existing research at the intersection of AI agents and blockchain, with an emphasis on security and privacy.

Result: The result is a structured SoK that synthesizes current AI applications in blockchain ecosystems, evaluates their security and privacy implications, and highlights limitations and future research directions in the field.

Conclusion: The paper concludes that by presenting the first Systematization of Knowledge on AI-driven blockchain systems, particularly security and privacy aspects, the study offers a comprehensive foundation for future research and applications, addressing the previous lack of focused surveys in this domain.

Abstract: Blockchain and smart contracts have garnered significant interest in recent
years as the foundation of a decentralized, trustless digital ecosystem,
thereby eliminating the need for traditional centralized authorities. Despite
their central role in powering Web3, their complexity still presents
significant barriers for non-expert users. To bridge this gap, Artificial
Intelligence (AI)-based agents have emerged as valuable tools for interacting
with blockchain environments, supporting a range of tasks, from analyzing
on-chain data and optimizing transaction strategies to detecting
vulnerabilities within smart contracts. While interest in applying AI to
blockchain is growing, the literature still lacks a comprehensive survey that
focuses specifically on the intersection with AI agents. Most of the related
work only provides general considerations, without focusing on any specific
domain. This paper addresses this gap by presenting the first Systematization
of Knowledge dedicated to AI-driven systems for blockchain, with a special
focus on their security and privacy dimensions, shedding light on their
applications, limitations, and future research directions.

</details>


### [5] [All You Need Is A Fuzzing Brain: An LLM-Powered System for Automated Vulnerability Detection and Patching](https://arxiv.org/abs/2509.07225)
*Ze Sheng,Qingxiao Xu,Jianwei Huang,Matthew Woodcock,Heqing Huang,Alastair F. Donaldson,Guofei Gu,Jeff Huang*

Main category: cs.CR

TL;DR: Team 'All You Need Is A Fuzzing Brain' placed 4th in DARPA's AIxCC by developing an open-source LLM-powered Cyber Reasoning System that found and patched 28 vulnerabilities, including 6 zero-days, while introducing a new LLM benchmarking leaderboard.


<details>
  <summary>Details</summary>
Motivation: To advance cybersecurity by creating autonomous systems capable of discovering and mitigating vulnerabilities in real-world codebases, as required by DARPA's AIxCC competition.

Method: The team developed an autonomous Cyber Reasoning System (CRS) leveraging Large Language Models (LLMs) to detect vulnerabilities and generate patches, combining fuzzing techniques with AI-based analysis.

Result: The CRS discovered 28 security vulnerabilities in the AIxCC competition (six zero-days) and patched 14, achieving 4th place among seven finalists. They also established a public leaderboard for benchmarking LLMs on vulnerability tasks.

Conclusion: The team's work demonstrates the effectiveness of LLM-powered Cyber Reasoning Systems in identifying and patching security vulnerabilities, contributing to open-source tools and a new public leaderboard for benchmarking future research.

Abstract: Our team, All You Need Is A Fuzzing Brain, was one of seven finalists in
DARPA's Artificial Intelligence Cyber Challenge (AIxCC), placing fourth in the
final round. During the competition, we developed a Cyber Reasoning System
(CRS) that autonomously discovered 28 security vulnerabilities - including six
previously unknown zero-days - in real-world open-source C and Java projects,
and successfully patched 14 of them. The complete CRS is open source at
https://github.com/o2lab/afc-crs-all-you-need-is-a-fuzzing-brain. This paper
provides a detailed technical description of our CRS, with an emphasis on its
LLM-powered components and strategies. Building on AIxCC, we further introduce
a public leaderboard for benchmarking state-of-the-art LLMs on vulnerability
detection and patching tasks, derived from the AIxCC dataset. The leaderboard
is available at https://o2lab.github.io/FuzzingBrain-Leaderboard/.

</details>


### [6] [Paladin: Defending LLM-enabled Phishing Emails with a New Trigger-Tag Paradigm](https://arxiv.org/abs/2509.07287)
*Yan Pang,Wenlong Meng,Xiaojing Liao,Tianhao Wang*

Main category: cs.CR

TL;DR: Paladin detects LLM-generated phishing emails by embedding detectable tags into LLMs during content generation, achieving 90%+ accuracy with low computational overhead.


<details>
  <summary>Details</summary>
Motivation: LLMs enable crafting of linguistically flawless, tailored phishing emails, bypassing traditional detection systems. Existing detection methods either lack reliability (semantic approaches) or suffer from high costs and scalability limits (LLM-based approaches).

Method: Proposed Paladin, which integrates trigger-tag associations into standard LLMs via insertion strategies. When generating phishing content, instrumented LLMs automatically embed detectable tags, enabling efficient detection while maintaining operational stealth.

Result: Outperformed baselines with >90% detection accuracy across all four scenarios, demonstrating superior effectiveness in terms of stealthiness, robustness, and detection capability.

Conclusion: Paladin effectively detects LLM-generated phishing emails with high accuracy across four scenarios, offering an efficient and scalable solution without relying on expensive detection models.

Abstract: With the rapid development of large language models, the potential threat of
their malicious use, particularly in generating phishing content, is becoming
increasingly prevalent. Leveraging the capabilities of LLMs, malicious users
can synthesize phishing emails that are free from spelling mistakes and other
easily detectable features. Furthermore, such models can generate
topic-specific phishing messages, tailoring content to the target domain and
increasing the likelihood of success.
  Detecting such content remains a significant challenge, as LLM-generated
phishing emails often lack clear or distinguishable linguistic features. As a
result, most existing semantic-level detection approaches struggle to identify
them reliably. While certain LLM-based detection methods have shown promise,
they suffer from high computational costs and are constrained by the
performance of the underlying language model, making them impractical for
large-scale deployment.
  In this work, we aim to address this issue. We propose Paladin, which embeds
trigger-tag associations into vanilla LLM using various insertion strategies,
creating them into instrumented LLMs. When an instrumented LLM generates
content related to phishing, it will automatically include detectable tags,
enabling easier identification. Based on the design on implicit and explicit
triggers and tags, we consider four distinct scenarios in our work. We evaluate
our method from three key perspectives: stealthiness, effectiveness, and
robustness, and compare it with existing baseline methods. Experimental results
show that our method outperforms the baselines, achieving over 90% detection
accuracy across all scenarios.

</details>


### [7] [zkUnlearner: A Zero-Knowledge Framework for Verifiable Unlearning with Multi-Granularity and Forgery-Resistance](https://arxiv.org/abs/2509.07290)
*Nan Wang,Nan Wu,Xiangyu Hui,Jiafan Wang,Xin Yuan*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As the demand for exercising the "right to be forgotten" grows, the need for
verifiable machine unlearning has become increasingly evident to ensure both
transparency and accountability. We present {\em zkUnlearner}, the first
zero-knowledge framework for verifiable machine unlearning, specifically
designed to support {\em multi-granularity} and {\em forgery-resistance}.
  First, we propose a general computational model that employs a {\em
bit-masking} technique to enable the {\em selectivity} of existing
zero-knowledge proofs of training for gradient descent algorithms. This
innovation enables not only traditional {\em sample-level} unlearning but also
more advanced {\em feature-level} and {\em class-level} unlearning. Our model
can be translated to arithmetic circuits, ensuring compatibility with a broad
range of zero-knowledge proof systems. Furthermore, our approach overcomes key
limitations of existing methods in both efficiency and privacy. Second, forging
attacks present a serious threat to the reliability of unlearning.
Specifically, in Stochastic Gradient Descent optimization, gradients from
unlearned data, or from minibatches containing it, can be forged using
alternative data samples or minibatches that exclude it. We propose the first
effective strategies to resist state-of-the-art forging attacks. Finally, we
benchmark a zkSNARK-based instantiation of our framework and perform
comprehensive performance evaluations to validate its practicality.

</details>


### [8] [SafeToolBench: Pioneering a Prospective Benchmark to Evaluating Tool Utilization Safety in LLMs](https://arxiv.org/abs/2509.07315)
*Hongfei Xia,Hongru Wang,Zeming Liu,Qian Yu,Yuhang Guo,Haifeng Wang*

Main category: cs.CR

TL;DR: This paper introduces SafeToolBench and SafeInstructTool to proactively enhance LLM safety during external tool use. Their proposed framework outperforms existing methods, offering comprehensive risk assessment and significantly improving tool utilization trustworthiness.


<details>
  <summary>Details</summary>
Motivation: LLMs using external tools face risks like financial loss and privacy leaks from ambiguous/malicious user instructions. Previous retrospective safety evaluations are insufficient for preventing irreversible harm, necessitating prospective risk assessment methodologies.

Method: The paper introduces SafeToolBench, a benchmark for prospective safety assessment of LLM tool utilization, and SafeInstructTool, a framework analyzing risks from three perspectives (User Instruction, Tool Itself, and Joint Instruction-Tool) across nine dimensions. Four LLMs were tested to compare risk mitigation efficacy.

Result: Experiments revealed existing methods fail to detect all tool utilization risks, while SafeInstructTool demonstrates improved safety awareness, reducing harmful tool executions and providing a more robust solution for trustable LLM interactions.

Conclusion: The paper concludes that the proposed SafeInstructTool framework significantly enhances Large Language Models' (LLMs) self-awareness in tool utilization, enabling safer and more trustworthy interactions with external tools. SafeToolBench is introduced as the first benchmark for prospective safety evaluation, addressing the limitations of retrospective methods.

Abstract: Large Language Models (LLMs) have exhibited great performance in autonomously
calling various tools in external environments, leading to better problem
solving and task automation capabilities. However, these external tools also
amplify potential risks such as financial loss or privacy leakage with
ambiguous or malicious user instructions. Compared to previous studies, which
mainly assess the safety awareness of LLMs after obtaining the tool execution
results (i.e., retrospective evaluation), this paper focuses on prospective
ways to assess the safety of LLM tool utilization, aiming to avoid irreversible
harm caused by directly executing tools. To this end, we propose SafeToolBench,
the first benchmark to comprehensively assess tool utilization security in a
prospective manner, covering malicious user instructions and diverse practical
toolsets. Additionally, we propose a novel framework, SafeInstructTool, which
aims to enhance LLMs' awareness of tool utilization security from three
perspectives (i.e., \textit{User Instruction, Tool Itself, and Joint
Instruction-Tool}), leading to nine detailed dimensions in total. We experiment
with four LLMs using different methods, revealing that existing approaches fail
to capture all risks in tool utilization. In contrast, our framework
significantly enhances LLMs' self-awareness, enabling a more safe and
trustworthy tool utilization.

</details>


### [9] [A Decade-long Landscape of Advanced Persistent Threats: Longitudinal Analysis and Global Trends](https://arxiv.org/abs/2509.07457)
*Shakhzod Yuldoshkhujaev,Mijin Jeon,Doowon Kim,Nick Nikiforakis,Hyungjoon Koo*

Main category: cs.CR

TL;DR: This study analyzes 1,509 APT dossiers (2014-2023), identifying global trends in threat activities, attack vectors, and geopolitical links via a hybrid rule-based/LLM methodology, with interactive visualizations.


<details>
  <summary>Details</summary>
Motivation: Prior research on APTs focuses on isolated detection or case studies, with limited longitudinal analysis of scattered dossiers. This work addresses the gap by connecting historical trends and macro patterns.

Method: Systematic analysis of 1,509 APT dossiers (24,215 pages from 2014-2023) using six authoritative sources, combining rule-based information retrieval with large-language-model-based search, and identifying 603 unique APT groups.

Result: Longitudinal shifts in infiltration tactics (e.g., decline in zero-day exploitation since 2016), sector targeting, international influence of events (elections/wars), and 154 affected countries. Findings include 603 unique APT groups and 60,630 identified attack instances.

Conclusion: The work provides a macro overview of APT evolution, revealing interconnected cyber-geopolitical patterns and offering tools like an APT map/flow diagram to enhance understanding of global threat dynamics.

Abstract: An advanced persistent threat (APT) refers to a covert, long-term
cyberattack, typically conducted by state-sponsored actors, targeting critical
sectors and often remaining undetected for long periods. In response,
collective intelligence from around the globe collaborates to identify and
trace surreptitious activities, generating substantial documentation on APT
campaigns publicly available on the web. While prior works predominantly focus
on specific aspects of APT cases, such as detection, evaluation, cyber threat
intelligence, and dataset creation, limited attention has been devoted to
revisiting and investigating these scattered dossiers in a longitudinal manner.
The objective of our study is to fill the gap by offering a macro perspective,
connecting key insights and global trends in past APT attacks. We
systematically analyze six reliable sources-three focused on technical reports
and another three on threat actors-examining 1,509 APT dossiers (24,215 pages)
spanning 2014-2023, and identifying 603 unique APT groups worldwide. To
efficiently unearth relevant information, we employ a hybrid methodology that
combines rule-based information retrieval with large-language-model-based
search techniques. Our longitudinal analysis reveals shifts in threat actor
activities, global attack vectors, changes in targeted sectors, and
relationships between cyberattacks and significant events such as elections or
wars, which provide insights into historical patterns in APT evolution. Over
the past decade, 154 countries have been affected, primarily using malicious
documents and spear phishing as dominant initial infiltration vectors, with a
noticeable decline in zero-day exploitation since 2016. Furthermore, we present
our findings through interactive visualization tools, such as an APT map or
flow diagram, to facilitate intuitive understanding of global patterns and
trends in APT activities.

</details>


### [10] [Biometric Bound Credentials for Age Verification](https://arxiv.org/abs/2509.07465)
*Norman Poh,Daryl Burns*

Main category: cs.CR

TL;DR: BBCreds is a new method for age verification that improves privacy and security by binding credentials to biometric features without storing templates.


<details>
  <summary>Details</summary>
Motivation: The need for accurate and secure age verification systems to combat issues like regulatory compliance, user trust, and protecting minors online has become increasingly important, while legacy systems struggle with drawbacks such as low accuracy, intrusiveness, and security risks.

Method: The solution involves using Biometric Bound Credentials (BBCreds) to cryptographically tie age credentials to an individual's biometric data, eliminating the requirement to store biometric templates.

Result: BBCreds enable exclusive access by the legitimate user, avoid the sharing of credentials, and simultaneously address the challenges faced by legacy and contemporary age verification methods.

Conclusion: This solution demonstrates the viability of private and secure age verification through the use of BBCreds, offering a robust approach that combines cryptographic assurances with biometric features to protect minors and foster trust.

Abstract: Age verification is increasingly critical for regulatory compliance, user
trust, and the protection of minors online. Historically, solutions have
struggled with poor accuracy, intrusiveness, and significant security risks.
More recently, concerns have shifted toward privacy, surveillance, fairness,
and the need for transparent, trustworthy systems. In this paper, we propose
Biometric Bound Credentials (BBCreds) as a privacy-preserving approach that
cryptographically binds age credentials to an individual's biometric features
without storing biometric templates. This ensures only the legitimate,
physically present user can access age-restricted services, prevents credential
sharing, and addresses both legacy and emerging challenges in age verification.
enhances privacy.

</details>


### [11] [Backdoor Attacks and Defenses in Computer Vision Domain: A Survey](https://arxiv.org/abs/2509.07504)
*Bilal Hussain Abbasi,Yanjun Zhang,Leo Zhang,Shang Gao*

Main category: cs.CR

TL;DR: This paper is a comprehensive survey on backdoor attacks and defenses in computer vision, organizing them into a multi-dimensional taxonomy and identifying research gaps.


<details>
  <summary>Details</summary>
Motivation: Backdoor attacks pose significant security risks to machine learning systems, necessitating systematic understanding of attack vectors and defenses to guide future research and practical defense strategies.

Method: The authors establish a taxonomy categorizing existing work along 7 key dimensions (ejection stage, trigger type, labeling strategy, etc.) and conduct a critical evaluation of representative methods across 895+ references.

Result: Analysis reveals strong defense effectiveness against traditional patch attacks but persistent vulnerabilities to input-aware backdoors, transferable encoder attacks, and hardware exploits. Key gaps identified include supply-chain risks, lack of certifiability, and cross-task benchmarks.

Conclusion: The survey establishes a structured framework for threat analysis in secure computer vision, emphasizing the need for layered defenses and proposing 5 practical guidelines for threat-aware evaluation.

Abstract: Backdoor (trojan) attacks embed hidden, controllable behaviors into
machine-learning models so that models behave normally on benign inputs but
produce attacker-chosen outputs when a trigger is present. This survey reviews
the rapidly growing literature on backdoor attacks and defenses in the
computer-vision domain. We introduce a multi-dimensional taxonomy that
organizes attacks and defenses by injection stage (dataset poisoning,
model/parameter modification, inference-time injection), trigger type (patch,
blended/frequency, semantic, transformation), labeling strategy (dirty-label
vs. clean-label / feature-collision), representation stage (instance-specific,
manifold/class-level, neuron/parameter hijacking, distributed encodings), and
target task (classification, detection, segmentation, video, multimodal). For
each axis we summarize representative methods, highlight evaluation practices,
and discuss where defenses succeed or fail. For example, many classical
sanitization and reverse-engineering tools are effective against reusable patch
attacks but struggle with input-aware, sample-specific, or parameter-space
backdoors and with transfer via compromised pre-trained encoders or hardware
bit-flips. We synthesize trends, identify persistent gaps (supply-chain and
hardware threats, certifiable defenses, cross-task benchmarks), and propose
practical guidelines for threat-aware evaluation and layered defenses. This
survey aims to orient researchers and practitioners to the current threat
landscape and pressing research directions in secure computer vision.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [12] [Aspect-Oriented Programming in Secure Software Development: A Case Study of Security Aspects in Web Applications](https://arxiv.org/abs/2509.07449)
*Mterorga Ukor*

Main category: cs.SE

TL;DR: This study evaluates how Aspect-Oriented Programming (AOP) improves security modularity and maintainability in web applications through case studies and metrics, showing AOP introduces minimal performance overhead while enhancing code quality versus traditional OOP and middleware approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional OOP intertwines security logic with business functionality, causing code tangling and reduced maintainability, while web applications face persistent security threats like unauthorized access, data breaches, and injection attacks.

Method: The study uses a case study approach comparing AOP-based implementations of security features with conventional OOP or middleware-based approaches, evaluating code quality metrics, performance metrics, and developer feedback using statistical methods aligned with the ISO/IEC 25010 software quality model.

Result: AOP enhances modularity, reusability, and maintainability of security mechanisms with minimal performance overhead compared to conventional approaches.

Conclusion: The study contributes practical insights for software engineers and researchers seeking to balance security with software quality in web application development.

Abstract: Security remains a critical challenge in modern web applications, where
threats such as unauthorized access, data breaches, and injection attacks
continue to undermine trust and reliability. Traditional Object-Oriented
Programming (OOP) often intertwines security logic with business functionality,
leading to code tangling, scattering, and reduced maintainability. This study
investigates the role of Aspect-Oriented Programming (AOP) in enhancing secure
software development by modularizing cross-cutting security concerns. Using a
case study approach, we compare AOP-based implementations of security features
including authentication, authorization, input validation, encryption, logging,
and session management with conventional OOP or middleware-based approaches.
Data collection involves analyzing code quality metrics (e.g., lines of code,
coupling, cohesion, modularity index, reusability), performance metrics
(response time, throughput, memory usage), and maintainability indicators.
Developer feedback is also incorporated to assess integration and debugging
experiences. Statistical methods, guided by the ISO/IEC 25010 software quality
model, are applied to evaluate differences across implementations. The findings
demonstrate that AOP enhances modularity, reusability, and maintainability of
security mechanisms, while introducing only minimal performance overhead. The
study contributes practical insights for software engineers and researchers
seeking to balance security with software quality in web application
development.

</details>


### [13] [CRACI: A Cloud-Native Reference Architecture for the Industrial Compute Continuum](https://arxiv.org/abs/2509.07498)
*Hai Dinh-Tuan*

Main category: cs.SE

TL;DR: This paper introduces CRACI, a cloud-native architecture addressing industry 4.0's need for flexible, interoperable systems by overcoming the rigidity of traditional IT/OT frameworks.


<details>
  <summary>Details</summary>
Motivation: Traditional hierarchical IT/OT architectures (ISA-95, RAMI 4.0) are rigid, create data silos, and lack cloud-native support, impeding scalable, interoperable Industry 4.0 systems.

Method: Proposes CRACI (Cloud-native Reference Architecture) with decoupled, event-driven design and embedded cross-cutting pillars (Trust, Governance, Observability, Lifecycle Management). Validated through theoretical comparison with standards and quantitative real-world smart manufacturing data.

Result: Demonstrated CRACI's ability to overcome structural limitations of legacy models through theoretical validation and real-world performance comparisons, proving its effectiveness in enabling modern industrial compute continua.

Conclusion: CRACI provides a viable, state-of-the-art architecture that addresses the limitations of traditional models by utilizing the compute continuum to enable scalable, modern industrial systems.

Abstract: The convergence of Information Technology (IT) and Operational Technology
(OT) in Industry 4.0 exposes the limitations of traditional, hierarchical
architectures like ISA-95 and RAMI 4.0. Their inherent rigidity, data silos,
and lack of support for cloud-native technologies impair the development of
scalable and interoperable industrial systems. This paper addresses this issue
by introducing CRACI, a Cloud-native Reference Architecture for the Industrial
Compute Continuum. Among other features, CRACI promotes a decoupled and
event-driven model to enable flexible, non-hierarchical data flows across the
continuum. It embeds cross-cutting concerns as foundational pillars: Trust,
Governance & Policy, Observability, and Lifecycle Management, ensuring quality
attributes are core to the design. The proposed architecture is validated
through a two-fold approach: (1) a comparative theoretical analysis against
established standards, operational models, and academic proposals; and (2) a
quantitative evaluation based on performance data from previously published
real-world smart manufacturing implementations. The results demonstrate that
CRACI provides a viable, state-of-the-art architecture that utilizes the
compute continuum to overcome the structural limitations of legacy models and
enable scalable, modern industrial systems.

</details>


### [14] [PatchSeeker: Mapping NVD Records to their Vulnerability-fixing Commits with LLM Generated Commits and Embeddings](https://arxiv.org/abs/2509.07540)
*Huu Hung Nguyen,Anh Tuan Nguyen,Thanh Le-Cong,Yikun Li,Han Wei Ang,Yide Yin,Frank Liauw,Shar Lwin Khin,Ouh Eng Lieh,Ting Zhang,David Lo*

Main category: cs.SE

TL;DR: PatchSeeker uses LLMs to bridge semantic gaps between NVD descriptions and VFCs via enhanced embeddings and commit message synthesis, outperforming existing methods by 59.3% in MRR and 27.9% in Recall@10 with strong validation on recent CVEs.


<details>
  <summary>Details</summary>
Motivation: NVD records lack direct links to VFCs, and existing approaches struggle with noisy commit messages and superficial semantic analysis. The deep semantic alignment between vulnerability descriptions and VFCs is critical for accurate mapping.

Method: PatchSeeker leverages large language models to generate semantic embeddings from NVD descriptions and enhances sparse commit messages through synthesized summaries, creating richer semantic links between vulnerability reports and VFCs. An ablation study validates component contributions.

Result: PatchSeeker achieves 59.3% higher MRR and 27.9% higher Recall@10 than the Prospector baseline. Extended evaluations on recent CVEs confirm robust performance, with ablation studies confirming the positive impact of commit message enhancement and LLM backbone choices.

Conclusion: The authors present PatchSeeker, a method using LLMs to improve NVD-VFC mapping, demonstrating its effectiveness over existing approaches with significant performance gains while acknowledging remaining challenges for future work.

Abstract: Software vulnerabilities pose serious risks to modern software ecosystems.
While the National Vulnerability Database (NVD) is the authoritative source for
cataloging these vulnerabilities, it often lacks explicit links to the
corresponding Vulnerability-Fixing Commits (VFCs). VFCs encode precise code
changes, enabling vulnerability localization, patch analysis, and dataset
construction. Automatically mapping NVD records to their true VFCs is therefore
critical. Existing approaches have limitations as they rely on sparse, often
noisy commit messages and fail to capture the deep semantics in the
vulnerability descriptions. To address this gap, we introduce PatchSeeker, a
novel method that leverages large language models to create rich semantic links
between vulnerability descriptions and their VFCs. PatchSeeker generates
embeddings from NVD descriptions and enhances commit messages by synthesizing
detailed summaries for those that are short or uninformative. These generated
messages act as a semantic bridge, effectively closing the information gap
between natural language reports and low-level code changes. Our approach
PatchSeeker achieves 59.3% higher MRR and 27.9% higher Recall@10 than the
best-performing baseline, Prospector, on the benchmark dataset. The extended
evaluation on recent CVEs further confirms PatchSeeker's effectiveness.
Ablation study shows that both the commit message generation method and the
selection of backbone LLMs make a positive contribution to PatchSeeker. We also
discuss limitations and open challenges to guide future work.

</details>


### [15] [Bridging the Gap Between Binary and Source Based Package Management in Spack](https://arxiv.org/abs/2509.07728)
*John Gouwar,Gregory Becker,Tamara Dahlgren,Nathan Hanford,Arjun Guha,Todd Gamblin*

Main category: cs.SE

TL;DR: Splicing enables Spack to mix binaries and sources efficiently, avoiding slow rebuilds for HPC workloads while maintaining configurability.


<details>
  <summary>Details</summary>
Motivation: Binary package managers are fast but lack configurability, while source-based ones are slow. Spack's inability to mix unrelated binaries forces unnecessary rebuilds (e.g., for MPI).

Method: Splicing enhances Spack's packaging language and dependency resolution engine to model binary compatibility, reusing existing binaries and integrating them with source builds.

Result: Splicing reduces installation overhead and rebuilds by reusing compatible binaries (even ABI-sensitive ones). It incurs minimal runtime cost while preserving Spack's source-build flexibility.

Conclusion: Splicing extends Spack to solve the binary compatibility issue by enabling seamless mixing of source and binary packages, maintaining flexibility while reducing rebuild times for ABI-sensitive dependencies.

Abstract: Binary package managers install software quickly but they limit
configurability due to rigid ABI requirements that ensure compatibility between
binaries. Source package managers provide flexibility in building software, but
compilation can be slow. For example, installing an HPC code with a new MPI
implementation may result in a full rebuild. Spack, a widely deployed,
HPC-focused package manager, can use source and pre-compiled binaries, but
lacks a binary compatibility model, so it cannot mix binaries not built
together. We present splicing, an extension to Spack that models binary
compatibility between packages and allows seamless mixing of source and binary
distributions. Splicing augments Spack's packaging language and dependency
resolution engine to reuse compatible binaries but maintains the flexibility of
source builds. It incurs minimal installation-time overhead and allows rapid
installation from binaries, even for ABI-sensitive dependencies like MPI that
would otherwise require many rebuilds.

</details>


### [16] [What's Coming Next? Short-Term Simulation of Business Processes from Current State](https://arxiv.org/abs/2509.07747)
*Maksym Avramenko,David Chapela-Campa,Marlon Dumas,Fredrik Milani*

Main category: cs.SE

TL;DR: This paper introduces a short-term simulation method that initializes from the current state of ongoing business processes via event logs, offering more accurate forecasts than traditional warm-up simulations for operational decision-making, especially under dynamic conditions.


<details>
  <summary>Details</summary>
Motivation: Existing simulation methods start from an empty state (tactical decisions), but operational decisions require forecasting short-term performance based on ongoing cases and handling disruptions. Current approaches ignore the current state, leading to less accurate forecasts.

Method: The paper proposes a method to initialize simulations from the current state of ongoing cases and resources, derived from event logs, addressing (1) identifying necessary simulation model information and (2) extracting the current state from logs. This is implemented in a simulation engine.

Result: The proposed approach demonstrates experimentally that short-term simulations initialized with the current state yield more accurate performance forecasts than long-term simulations with warm-up, particularly when dealing with concept drift or bursty performance patterns.

Conclusion: The study concludes that using current state initialization from event logs provides more accurate short-term performance forecasts compared to long-term warm-up simulations, especially under conditions of concept drift or bursty patterns.

Abstract: Business process simulation is an approach to evaluate business process
changes prior to implementation. Existing methods in this field primarily
support tactical decision-making, where simulations start from an empty state
and aim to estimate the long-term effects of process changes. A complementary
use-case is operational decision-making, where the goal is to forecast
short-term performance based on ongoing cases and to analyze the impact of
temporary disruptions, such as demand spikes and shortfalls in available
resources. An approach to tackle this use-case is to run a long-term simulation
up to a point where the workload is similar to the current one (warm-up), and
measure performance thereon. However, this approach does not consider the
current state of ongoing cases and resources in the process. This paper studies
an alternative approach that initializes the simulation from a representation
of the current state derived from an event log of ongoing cases. The paper
addresses two challenges in operationalizing this approach: (1) Given a
simulation model, what information is needed so that a simulation run can start
from the current state of cases and resources? (2) How can the current state of
a process be derived from an event log? The resulting short-term simulation
approach is embodied in a simulation engine that takes as input a simulation
model and a log of ongoing cases, and simulates cases for a given time horizon.
An experimental evaluation shows that this approach yields more accurate
short-term performance forecasts than long-term simulations with warm-up
period, particularly in the presence of concept drift or bursty performance
patterns.

</details>


### [17] [What Were You Thinking? An LLM-Driven Large-Scale Study of Refactoring Motivations in Open-Source Projects](https://arxiv.org/abs/2509.07763)
*Mikel Robredo,Matteo Esposito,Fabio Palomba,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: This paper examines developer refactoring motivations using LLMs and empirical analysis. It finds LLMs match human judgment in 80% of cases but align with existing literature in only 47%, highlighting practical motivations like readability and maintainability, while suggesting hybrid approaches combining LLMs and software metrics for systematic refactoring decisions.


<details>
  <summary>Details</summary>
Motivation: Refactoring is beneficial but resource-intensive. Understanding developer motivations and their correlation with metrics could enhance refactoring practices. Existing literature lacks nuanced explanations, prompting the need for LLMs to uncover deeper rationales.

Method: A large-scale empirical study using LLMs to analyze version control data, comparing LLM-derived motivations against human judgments and motivations from prior literature. Metrics like developer experience and code readability were evaluated for correlation.

Result: LLMs achieved 80%% alignment with human judgments but only 47%% with literature-based motivations. 22%% of LLM-identified motives provided richer details (e.g., readability, structure). Pragmatic motives dominated (simplification, maintainability), though metrics showed weak correlation with motivation categories.

Conclusion: LLMs capture surface-level refactoring motives effectively but lack architectural insight. Integrating LLMs with software metrics offers a hybrid approach to prioritize refactoring, balancing immediate improvements and long-term design goals.

Abstract: Context. Code refactoring improves software quality without changing external
behavior. Despite its advantages, its benefits are hindered by the considerable
cost of time, resources, and continuous effort it demands. Aim. Understanding
why developers refactor, and which metrics capture these motivations, may
support wider and more effective use of refactoring in practice. Method. We
performed a large-scale empirical study to analyze developers refactoring
activity, leveraging Large Language Models (LLMs) to identify underlying
motivations from version control data, comparing our findings with previous
motivations reported in the literature. Results. LLMs matched human judgment in
80% of cases, but aligned with literature-based motivations in only 47%. They
enriched 22% of motivations with more detailed rationale, often highlighting
readability, clarity, and structural improvements. Most motivations were
pragmatic, focused on simplification and maintainability. While metrics related
to developer experience and code readability ranked highest, their correlation
with motivation categories was weak. Conclusions. We conclude that LLMs
effectively capture surface-level motivations but struggle with architectural
reasoning. Their value lies in providing localized explanations, which, when
combined with software metrics, can form hybrid approaches. Such integration
offers a promising path toward prioritizing refactoring more systematically and
balancing short-term improvements with long-term architectural goals.

</details>


### [18] ["We provide our resources in a dedicated repository": Surveying the Transparency of HICSS publications](https://arxiv.org/abs/2509.07851)
*Irdin Pekaric,Giovanni Apruzzese*

Main category: cs.SE

TL;DR: This study analyzes 5579 HICSS papers (2017-2024) to assess how often supplementary research materials are shared in public repositories. Only 3% of 2028 papers with human/technical focus had functional, accessible repositories.


<details>
  <summary>Details</summary>
Motivation: Research transparency and reproducibility require sharing supplementary data/artifacts, yet it's unclear how often HICSS authors implement this practice.

Method: Analyzed all 5579 HICSS papers from 2017-2024, identified 2028 papers involving human subjects (850) or technical implementations (737), and manually checked for public repository links and accessibility.

Result: Only 3.0% (59/2028) of analyzed papers provided functional public repositories with the expected supplementary materials. Tools for replication are publicly shared.

Conclusion: HICSS papers frequently fail to provide accessible supplementary research materials despite their importance for transparency and reproducibility. This highlights a need for improved sharing practices in the research community.

Abstract: Every day, new discoveries are made by researchers from all across the globe
and fields. HICSS is a flagship venue to present and discuss such scientific
advances. Yet, the activities carried out for any given research can hardly be
fully contained in a single document of a few pages-the "paper." Indeed, any
given study entails data, artifacts, or other material that is crucial to truly
appreciate the contributions claimed in the corresponding paper. External
repositories (e.g., GitHub) are a convenient tool to store all such resources
so that future work can freely observe and build upon them -- thereby improving
transparency and promoting reproducibility of research as a whole. In this
work, we scrutinize the extent to which papers recently accepted to HICSS
leverage such repositories to provide supplementary material. To this end, we
collect all the 5579 papers included in HICSS proceedings from 2017-2024. Then,
we identify those entailing either human subject research (850) or technical
implementations (737), or both (147). Finally, we review their text, examining
how many include a link to an external repository-and, inspect its contents.
Overall, out of 2028 papers, only 3\% have a functional and publicly available
repository that is usable by downstream research. We release all our tools.

</details>
