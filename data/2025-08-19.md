<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 46]
- [cs.SE](#cs.SE) [Total: 18]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Code Vulnerability Detection Across Different Programming Languages with AI Models](https://arxiv.org/abs/2508.11710)
*Hael Abdulhakim Ali Humran,Ferdi Sonmez*

Main category: cs.CR

TL;DR: This paper explores the use of transformer-based AI models (CodeBERT and CodeLlama) for code vulnerability detection, demonstrating high accuracy (>97%) via dynamic fine-tuning and hybrid approaches to address precision issues while highlighting generalization across languages and vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Traditional rule-based static analysis tools struggle with context-dependent security vulnerabilities and produce high false positive rates, motivating the need for AI-driven solutions that handle code semantics more effectively.

Method: The methodology involves dataset collection, language normalization, dynamic fine-tuning of transformer models on vulnerability data, and integration of ensemble learning and explainable AI techniques for improved detection.

Result: CodeBERT achieves >97% accuracy matching/better static analyzers, exhibits near-perfect recall but lower precision, and demonstrates generalization across programming languages and vulnerability types when using hybrid models.

Conclusion: AI-based vulnerability detectors show promise in scalability and usability but require further development in robustness, interpretability, and deployment readiness. Hybrid approaches reduce false positives while maintaining effectiveness.

Abstract: Security vulnerabilities present in a code that has been written in diverse
programming languages are among the most critical yet complicated aspects of
source code to detect. Static analysis tools based on rule-based patterns
usually do not work well at detecting the context-dependent bugs and lead to
high false positive rates. Recent developments in artificial intelligence,
specifically the use of transformer-based models like CodeBERT and CodeLlama,
provide light to this problem, as they show potential in finding such flaws
better. This paper presents the implementations of these models on various
datasets of code vulnerability, showing how off-the-shelf models can
successfully produce predictive capacity in models through dynamic fine-tuning
of the models on vulnerable and safe code fragments. The methodology comprises
the gathering of the dataset, normalization of the language, fine-tuning of the
model, and incorporation of ensemble learning and explainable AI. Experiments
show that a well-trained CodeBERT can be as good as or even better than some
existing static analyzers in terms of accuracy greater than 97%. Further study
has indicated that although language models can achieve close-to-perfect
recall, the precision can decrease. A solution to this is given by hybrid
models and validation procedures, which will reduce false positives. According
to the results, the AI-based solutions generalize to different programming
languages and classes of vulnerability. Nevertheless, robustness,
interpretability, and deployment readiness are still being developed. The
results illustrate the probabilities that AI will enhance the trustworthiness
in the usability and scalability of machine-learning-based detectors of
vulnerabilities.

</details>


### [2] [Enhancing GraphQL Security by Detecting Malicious Queries Using Large Language Models, Sentence Transformers, and Convolutional Neural Networks](https://arxiv.org/abs/2508.11711)
*Irash Perera,Hiranya Abeyrathne,Sanjeewa Malalgoda,Arshardh Ifthikar*

Main category: cs.CR

TL;DR: The paper introduces an AI-driven solution to detect malicious GraphQL queries in real-time using static analysis and machine learning (LLMs, SBERT, CNNs, etc.), along with optimizations for production environments.


<details>
  <summary>Details</summary>
Motivation: GraphQL's flexibility poses unique security challenges unaddressed by traditional API mechanisms. Existing solutions like static analysis or rate limiting struggle with context-aware and sophisticated attacks such as denial-of-service, data exfiltration, and injection vulnerabilities.

Method: Our approach integrates static analysis with AI techniques: LLMs generate dynamic schema-based configurations, sentence transformers (SBERT/Doc2Vec) create contextual query embeddings, and classifiers (CNNs, Random Forest, MLPs) identify malicious patterns. The system is optimized via ONNX Runtime and parallel processing for scalability.

Result: Models achieved high accuracy in detecting SQL injection, OS command injection, XSS, and mitigating DoS/SSRF attacks. Evaluations under load confirmed effectiveness in production scenarios and real-time performance.

Conclusion: The research provides a robust, adaptable framework for securing GraphQL APIs against both known and emerging threats, addressing critical limitations in current security tools through AI-driven real-time mitigation.

Abstract: GraphQL's flexibility, while beneficial for efficient data fetching,
introduces unique security vulnerabilities that traditional API security
mechanisms often fail to address. Malicious GraphQL queries can exploit the
language's dynamic nature, leading to denial-of-service attacks, data
exfiltration through injection, and other exploits. Existing solutions, such as
static analysis, rate limiting, and general-purpose Web Application Firewalls,
offer limited protection against sophisticated, context-aware attacks. This
paper presents a novel, AI-driven approach for real-time detection of malicious
GraphQL queries. Our method combines static analysis with machine learning
techniques, including Large Language Models (LLMs) for dynamic schema-based
configuration, Sentence Transformers (SBERT and Doc2Vec) for contextual
embedding of query payloads, and Convolutional Neural Networks (CNNs), Random
Forests, and Multilayer Perceptrons for classification. We detail the system
architecture, implementation strategies optimized for production environments
(including ONNX Runtime optimization and parallel processing), and evaluate the
performance of our detection models and the overall system under load. Results
demonstrate high accuracy in detecting various threats, including SQL
injection, OS command injection, and XSS exploits, alongside effective
mitigation of DoS and SSRF attempts. This research contributes a robust and
adaptable solution for enhancing GraphQL API security.

</details>


### [3] [Privacy-Aware Detection of Fake Identity Documents: Methodology, Benchmark, and Improved Detection Methods (FakeIDet2)](https://arxiv.org/abs/2508.11716)
*Javier Mu√±oz-Haro,Ruben Tolosana,Ruben Vera-Rodriguez,Aythami Morales,Julian Fierrez*

Main category: cs.CR

TL;DR: This study proposes FakeIDet2, a privacy-preserving method for detecting AI-generated fake IDs using patch-based training. They release a public database (FakeIDet2-db) with 900K real/fake ID patches and a reproducible benchmark framework.


<details>
  <summary>Details</summary>
Motivation: The increasing threat of AI-based ID forgeries (print, screen, composite attacks) and the challenge of accessing real ID data for research due to privacy constraints necessitate new approaches to fake ID detection.

Method: The authors developed: 1) A patch-based methodology to anonymize ID data while preserving detectable features 2) FakeIDet2-db: A database of 900K+ patches from 2,000 ID images under varied capture conditions 3) FakeIDet2: A detection method using these patches 4) A benchmark combining physical/synthetic attacks from existing datasets

Result: Created a public dataset with patches from 2,000 ID images covering physical attacks and release a detection method (FakeIDet2) that works effectively with fragmented data while maintaining privacy.

Conclusion: This work addresses the privacy-data scarcity challenge in fake ID detection by enabling research with patch-based representation and provides community resources to advance security solutions against AI-based forgeries.

Abstract: Remote user verification in Internet-based applications is becoming
increasingly important nowadays. A popular scenario for it consists of
submitting a picture of the user's Identity Document (ID) to a service
platform, authenticating its veracity, and then granting access to the
requested digital service. An ID is well-suited to verify the identity of an
individual, since it is government issued, unique, and nontransferable.
However, with recent advances in Artificial Intelligence (AI), attackers can
surpass security measures in IDs and create very realistic physical and
synthetic fake IDs. Researchers are now trying to develop methods to detect an
ever-growing number of these AI-based fakes that are almost indistinguishable
from authentic (bona fide) IDs. In this counterattack effort, researchers are
faced with an important challenge: the difficulty in using real data to train
fake ID detectors. This real data scarcity for research and development is
originated by the sensitive nature of these documents, which are usually kept
private by the ID owners (the users) and the ID Holders (e.g., government,
police, bank, etc.). The main contributions of our study are: 1) We propose and
discuss a patch-based methodology to preserve privacy in fake ID detection
research. 2) We provide a new public database, FakeIDet2-db, comprising over
900K real/fake ID patches extracted from 2,000 ID images, acquired using
different smartphone sensors, illumination and height conditions, etc. In
addition, three physical attacks are considered: print, screen, and composite.
3) We present a new privacy-aware fake ID detection method, FakeIDet2. 4) We
release a standard reproducible benchmark that considers physical and synthetic
attacks from popular databases in the literature.

</details>


### [4] [Assessing User Privacy Leakage in Synthetic Packet Traces: An Attack-Grounded Approach](https://arxiv.org/abs/2508.11742)
*Minhao Jin,Hongyu He,Maria Apostolaki*

Main category: cs.CR

TL;DR: Introduces first privacy benchmark for SynNetGens using membership inference attacks, identifies critical privacy vulnerabilities in existing methods through empirical analysis, and proposes TracePatch as a model-agnostic defense solution.


<details>
  <summary>Details</summary>
Motivation: Existing synthetic traffic generators (SynNetGens) lack concrete privacy guarantees despite improved data fidelity, necessitating a practical method to evaluate their privacy risks empirically.

Method: Developed TraceBleed, a membership inference attack based on contrastive learning and temporal chunking to detect traffic-source leakage. Conducted large-scale experiments across GAN, diffusion, and GPT-based SynNetGens to quantify privacy risks.

Result: SynNetGens leak user-level information (172% better attack accuracy than previous methods), differential privacy degrades fidelity or fails, and increased synthetic data sharing raises leakage by 59%. TracePatch successfully mitigates leakage while preserving data fidelity.

Conclusion: TraceBleed establishes a new benchmark for assessing SynNetGen privacy, revealing critical vulnerabilities. TracePatch provides the first generalizable defense strategy that balances privacy and fidelity without architectural assumptions about the generator model.

Abstract: Current synthetic traffic generators (SynNetGens) promise privacy but lack
comprehensive guarantees or empirical validation, even as their fidelity
steadily improves. We introduce the first attack-grounded benchmark for
assessing the privacy of SynNetGens directly from the traffic they produce. We
frame privacy as membership inference at the traffic-source level--a realistic
and actionable threat for data holders. To this end, we present TraceBleed, the
first attack that exploits behavioral fingerprints across flows using
contrastive learning and temporal chunking, outperforming prior membership
inference baselines by 172%. Our large-scale study across GAN-, diffusion-, and
GPT-based SynNetGens uncovers critical insights: (i) SynNetGens leak user-level
information; (ii) differential privacy either fails to stop these attacks or
severely degrades fidelity; and (iii) sharing more synthetic data amplifies
leakage by 59% on average. Finally, we introduce TracePatch, the first
SynNetGen-agnostic defense that combines adversarial ML with SMT constraints to
mitigate leakage while preserving fidelity.

</details>


### [5] [AegisBlock: A Privacy-Preserving Medical Research Framework using Blockchain](https://arxiv.org/abs/2508.11797)
*Calkin Garg,Omar Rios Cruz,Tessa Andersen,Gaby G. Dagher,Donald Winiecki,Min Long*

Main category: cs.CR

TL;DR: This paper introduces AegisBlock, a patient-centric access control framework that balances data privacy with researcher trust by allowing patients to grant access to their medical records via miner verification, demonstrating scalability and robustness against malicious miners.


<details>
  <summary>Details</summary>
Motivation: The need to maintain patient privacy under HIPAA and similar regulations while enabling legitimate research access to health records motivates the development of a secure, privacy-preserving framework.

Method: AegisBlock employs a blockchain-based verification process where patients approve time-bound record access requests from researchers. Miners validate and enforce access-granting rules, ensuring anonymity and data integrity through decentralized consensus.

Result: Experiments confirm AegisBlock's scalability with expanding patient and hospital counts, handling high miner malicious activity (up to 50%) efficiently without compromising data accuracy or privacy.

Conclusion: AegisBlock effectively reconciles privacy protection with secure data sharing for medical research, offering a reliable framework to maintain anonymity, enforce trust, and support system scalability in real-world applications.

Abstract: Due to HIPAA and other privacy regulations, it is imperative to maintain
patient privacy while conducting research on patient health records. In this
paper, we propose AegisBlock, a patient-centric access controlled framework to
share medical records with researchers such that the anonymity of the patient
is maintained while ensuring the trustworthiness of the data provided to
researchers. AegisBlock allows for patients to provide access to their medical
data, verified by miners. A researcher submits a time-based range query to
request access to records from a certain patient, and upon patient approval,
access will be granted. Our experimental evaluation results show that
AegisBlock is scalable with respect to the number of patients and hospitals in
the system, and efficient with up to 50% of malicious miners.

</details>


### [6] [Securing Sideways: Thwarting Lateral Movement by Implementing Active Directory Tiering](https://arxiv.org/abs/2508.11812)
*Tyler Schroder,Sohee Kim Park*

Main category: cs.CR

TL;DR: The paper addresses security vulnerabilities in Active Directory environments due to compromised credentials and advocates for tiering strategies to prevent theft and limit lateral movement in shared computing networks.


<details>
  <summary>Details</summary>
Motivation: Shared computing environments require centralized identity systems like Active Directory, which are prime targets for cyberattacks. Compromised identities enable credential theft, unauthorized access, and network breaches, leading to significant financial losses (e.g., 16.6B in U.S. cybercrime). Existing literature lacks integrated technical and theoretical guidance for mitigation.

Method: Analyzes real-world cyberattack scenarios to demonstrate how tiering in Active Directory can restrict compromised credential movement, minimize privilege escalation, and prevent large-scale breaches.

Result: Tiering effectively halts lateral movement and advanced cyber-attacks, reducing ransomware risks and limiting attacker access to sensitive resources.

Conclusion: The study bridges the gap between technical practices and theoretical frameworks for AD security, emphasizing that tiering is a critical but non-solitary component of modern cybersecurity. It calls for collaborative development of automated tiering tools to adapt to advancing hardware, cloud, and AI technologies.

Abstract: The advancement of computing equipment and the advances in services over the
Internet has allowed corporations, higher education, and many other
organizations to pursue the shared computing network environment. A requirement
for shared computing environments is a centralized identity system to
authenticate and authorize user access. An organization's digital identity
plane is a prime target for cyber threat actors. When compromised, identities
can be exploited to steal credentials, create unauthorized accounts, and
manipulate permissions-enabling attackers to gain control of the network and
undermine its confidentiality, availability, and integrity. Cybercrime losses
reached a record of 16.6 B in the United States in 2024. For organizations
using Microsoft software, Active Directory is the on-premises identity system
of choice. In this article, we examine the challenge of security compromises in
Active Directory (AD) environments and present effective strategies to prevent
credential theft and limit lateral movement by threat actors. Our proposed
approaches aim to confine the movement of compromised credentials, preventing
significant privilege escalation and theft. We argue that through our
illustration of real-world scenarios, tiering can halt lateral movement and
advanced cyber-attacks, thus reducing ransom escalation. Our work bridges a gap
in existing literature by combining technical guidelines with theoretical
arguments in support of tiering, positioning it as a vital component of modern
cybersecurity strategy even though it cannot function in isolation. As the
hardware advances and the cloud sourced services along with AI is advancing
with unprecedented speed, we think it is important for security experts and the
business to work together and start designing and developing software and
frameworks to classify devices automatically and accurately within the tiered
structure.

</details>


### [7] [Machine Learning-Based AES Key Recovery via Side-Channel Analysis on the ASCAD Dataset](https://arxiv.org/abs/2508.11817)
*Mukesh Poudel,Nick Rahimi*

Main category: cs.CR

TL;DR: This paper demonstrates the effectiveness of machine learning and deep learning models in exploiting side-channel leaks, specifically EM emissions, for partial key recovery in AES-128 implementations, confirming practical vulnerabilities despite theoretical security.


<details>
  <summary>Details</summary>
Motivation: Cryptographic algorithms like AES and RSA are mathematically robust but vulnerable to side-channel attacks due to implementation leaks. The study aims to evaluate ML/DL techniques for efficient partial key recovery to highlight implementation risks.

Method: Used ASCAD fixed (700 traces) and variable (1400 traces) key datasets with 256-class classification targeting the first-round S-box. Trained Random Forest (RF), SVM, CNN, and ResNet. Compared feature importance-based dimensionality reduction and used Key Rank metric for evaluation.

Result: RF with top 100 features achieved Rank 0 using ~50% fewer traces, CNN reached Rank 0 with ~65 traces for fixed-key data, and ResNets excelled on complex datasets but were less efficient for the simpler fixed-key case. Key Rank metric emphasized practical success over standard accuracy.

Conclusion: ML models, particularly CNNs, ResNets, and feature-selected RF with Key Rank evaluation, effectively recover keys from side-channel leaks. This confirms the practical vulnerability of cryptographic implementations, showing side-channel robustness is crucial beyond theoretical security.

Abstract: Cryptographic algorithms like AES and RSA are widely used and they are
mathematically robust and almost unbreakable but its implementation on physical
devices often leak information through side channels, such as electromagnetic
(EM) emissions, potentially compromising said theoretically secure algorithms.
This paper investigates the application of machine learning (ML) techniques and
Deep Learning models to exploit such leakage for partial key recovery. We use
the public ASCAD `fixed' and `variable' key dataset, containing 700 and 1400 EM
traces respectively from an AES-128 implementation on an 8-bit microcontroller.
The problem is framed as a 256-class classification task where we target the
output of the first-round S-box operation, which is dependent on a single key
byte. We evaluate standard classifiers (Random Forest (RF), Support Vector
Machine (SVM)), a Convolutional Neural Network(CNN) and a Residual Neural
Network(ResNet). We also explore the utility of RF-based feature importance for
dimensionality reduction. Crucially, we employ this domain-specific Key Rank
metric for evaluation, showing its necessity over standard classification
accuracy. Our results show that SVM and RF on full features perform poorly in
key ranking. However, RF trained on reduced (top 100) identified via importance
analysis achieves Rank 0 (successful key byte recovery) using almost half the
attack traces. The implemented CNN also achieves Rank 0 efficiently using
approximately 65 attack traces for the fixed-key dataset. The ResNets perform
best on large and complex datasets but may not always be the best choice for
simple fixed key dataset in terms of efficiency. Thus we conclude that models,
particularly CNNs, ResNets and feature-selected RF, coupled with the Key Rank
metric, are an effective tool for side-channel key recovery, confirming the
practical vulnerability of the cryptographic implementations.

</details>


### [8] [Deciphering the Interplay between Attack and Protection Complexity in Privacy-Preserving Federated Learning](https://arxiv.org/abs/2508.11907)
*Xiaojin Zhang,Mingcong Xu,Yiming Li,Wei Chen,Qiang Yang*

Main category: cs.CR

TL;DR: This paper proposes a theoretical framework for analyzing privacy-utility trade-offs in federated learning by formally defining 'Attack Complexity' and 'Protection Complexity' metrics, deriving their mathematical bounds, and revealing scaling relationships with model properties and privacy budgets.


<details>
  <summary>Details</summary>
Motivation: Federated learning's vulnerability to gradient inversion attacks necessitates robust privacy mechanisms. Existing approaches lack quantitative characterization of the interplay between attack efforts and protective measures.

Method: The authors 1) define Attack Complexity (minimum resources needed for data reconstruction below error threshold) and Protection Complexity (distortion from privacy mechanisms), 2) use Maximum Bayesian Privacy (MBP) to derive protection complexity bounds with respect to model dimensionality and privacy budget, and 3) develop rigorous attack complexity bounds influenced by privacy leakage, gradient distortion, model dimensionality, and privacy level.

Result: The paper establishes: 1) Tight theoretical bounds for protection complexity showing its dependence on model dimension; 2) Comprehensive attack complexity bounds demonstrating dependencies on privacy leakage (via R√©nyi divergence), gradient distortion (via signal-to-gradient ratio), model size, and privacy budget; 3) Quantitative privacy-utility trade-off characterization that maps required attacker resources against defender costs.

Conclusion: This theoretical framework provides foundational insights for designing secure federated learning systems by quantifying the complex relationships between attack feasibility, protection effectiveness, data privacy, model utility, and system resources.

Abstract: Federated learning (FL) offers a promising paradigm for collaborative model
training while preserving data privacy. However, its susceptibility to gradient
inversion attacks poses a significant challenge, necessitating robust privacy
protection mechanisms. This paper introduces a novel theoretical framework to
decipher the intricate interplay between attack and protection complexities in
privacy-preserving FL. We formally define "Attack Complexity" as the minimum
computational and data resources an adversary requires to reconstruct private
data below a given error threshold, and "Protection Complexity" as the expected
distortion introduced by privacy mechanisms. Leveraging Maximum Bayesian
Privacy (MBP), we derive tight theoretical bounds for protection complexity,
demonstrating its scaling with model dimensionality and privacy budget.
Furthermore, we establish comprehensive bounds for attack complexity, revealing
its dependence on privacy leakage, gradient distortion, model dimension, and
the chosen privacy level. Our findings quantitatively illuminate the
fundamental trade-offs between privacy guarantees, system utility, and the
effort required for both attacking and defending. This framework provides
critical insights for designing more secure and efficient federated learning
systems.

</details>


### [9] [WebGeoInfer: A Structure-Free and Multi-Stage Framework for Geolocation Inference of Devices Exposing Information](https://arxiv.org/abs/2508.11913)
*Huipeng Yang,Li Yang,Lichuan Ma,Lu Zhou,Junbo Jia,Anyuan Sang,Xinyue Wang*

Main category: cs.CR

TL;DR: This study introduces WebGeoInfer, a novel structure-free framework for automatically inferring geolocation from unstructured web interfaces of remote management devices to address cybersecurity risks from exposed location data.


<details>
  <summary>Details</summary>
Motivation: Remote management devices enable infrastructure monitoring but inadvertently expose sensitive geographical information via management pages, creating significant security vulnerabilities. Automated discovery of geolocation remains challenging due to unstructured, inconsistent, and incomplete data formats.

Method: WebGeoInfer employs multi-stage information enhancement through device page clustering and inter-cluster difference analysis to extract geographical cues without relying on structured data formats. It combines search engine enhancements with Large Language Models (LLMs) for coordinate extraction from unstructured text.

Result: The framework successfully inferred locations for 5,435 devices globally, achieving 96.96% country-level, 88.05% city-level, and 79.70% street-level accuracy, demonstrating its effectiveness across diverse geopolitical populations.

Conclusion: WebGeoInfer provides a robust solution for identifying geographically exposed devices through unstructured interface analysis, enabling more comprehensive cyber-regulatory measures to mitigate location-based security risks.

Abstract: Remote management devices facilitate critical infrastructure monitoring for
administrators but simultaneously increase asset exposure. Sensitive
geographical information overlooked in exposed device management pages poses
substantial security risks. Therefore, identifying devices that reveal location
information due to administrator negligence is crucial for cybersecurity
regulation. Despite the rich information exposed by web interfaces of remote
management devices, automatically discovering geographical locations remains
challenging due to unstructured formats, varying styles, and incomplete
geographical details.
  This study introduces WebGeoInfer, a structure-free geolocation inference
framework utilizing multi-stage information enhancement. WebGeoInfer clusters
similar device web pages and analyzes inter-cluster differences to extract
potential geographical information, bypassing structural limitations. Through
search engine enhancement and Large Language Models mining, the framework
extracts geographical coordinates from identified information. WebGeoInfer
successfully inferred locations for 5,435 devices across 94 countries and 2,056
cities, achieving accuracy rates of 96.96\%, 88.05\%, and 79.70\% at country,
city, and street levels, respectively.

</details>


### [10] [Optimizing Token Choice for Code Watermarking: A RL Approach](https://arxiv.org/abs/2508.11925)
*Zhimeng Guo,Huaisheng Zhu,Siyuan Xu,Hangfan Zhang,Teng Xiao,Minhao Cheng*

Main category: cs.CR

TL;DR: This paper introduces CodeTracer, a reinforcement learning-based adaptive code watermarking framework that biases token choices during next-token prediction while preserving code functionality. Its reward system combines execution feedback with watermark embedding signals, and Gumbel Top-k reparameterization enables gradient optimization of watermark decisions. CodeTracer outperforms existing baselines in detectability and functionality preservation.


<details>
  <summary>Details</summary>
Motivation: Current watermarking systems struggle with the structured, syntactically constrained nature of code. Code generation by LLMs requires watermarks to maintain functional correctness while introducing subtle detectable patterns, necessitating a novel approach to balance these requirements.

Method: CodeTracer uses a policy-driven framework with a parameterized model to bias token choices during generation. The method integrates a reward system combining execution feedback and watermark embedding signals, and employs Gumbel Top-k reparameterization to optimize discrete watermarking decisions via gradient-based reinforcement learning.

Result: CodeTracer demonstrates significant superiority over state-of-the-art methods in both watermark detectability and code functionality preservation through extensive comparative evaluations.

Conclusion: CodeTracer bridges the gap in watermarking structured code generated by LLMs by introducing a reinforcement learning-based approach with innovative reward system and gradient optimization techniques, achieving strong empirical performance on key metrics.

Abstract: The need for detecting LLM-generated code necessitates watermarking systems
capable of operating within its highly structured and syntactically constrained
environment. To address this, we introduce CodeTracer, an innovative adaptive
code watermarking framework underpinned by a novel reinforcement learning
training paradigm. At its core, CodeTracer features a policy-driven approach
that utilizes a parameterized model to intelligently bias token choices during
next-token prediction. This strategy ensures that embedded watermarks maintain
code functionality while exhibiting subtle yet statistically detectable
deviations from typical token distributions. To facilitate policy learning, we
devise a comprehensive reward system that seamlessly integrates execution
feedback with watermark embedding signals, balancing process-level and
outcome-level rewards. Additionally, we employ Gumbel Top-k reparameterization
to enable gradient-based optimization of discrete watermarking decisions.
Extensive comparative evaluations demonstrate CodeTracer's significant
superiority over state-of-the-art baselines in both watermark detectability and
the preservation of generated code's functionality.

</details>


### [11] [The Passwordless Authentication with Passkey Technology from an Implementation Perspective](https://arxiv.org/abs/2508.11928)
*Lien Tran,Boyuan Zhang,Ratchanon Pawanja,Rashid Hussain Khokhar*

Main category: cs.CR

TL;DR: The paper examines Passkey technology for secure passwordless authentication, demonstrating its advantages over TOTP in addressing security vulnerabilities and usability.


<details>
  <summary>Details</summary>
Motivation: Traditional passwords are vulnerable to leaks, phishing, and brute-force attacks, prompting the need for passwordless alternatives like TOTP and Passkeys. TOTP's limitations in resistance to certain threats and user experience necessitate evaluating newer solutions.

Method: Implementation of Passkey technology compared to TOTP, analyzing technical considerations for integration into authentication systems. Evaluation includes security analysis, usability, and performance testing against real-world attack scenarios.

Result: Passkeys surpass TOTP in phishing resistance and security by leveraging decentralized cryptographic storage. They maintain acceptable usability and performance while eliminating dependency on secret codes. TOTP limitations in security and attack susceptibility were empirically validated.

Conclusion: Passkey technology provides superior security without compromising user experience, recommending its adoption as a robust passwordless authentication solution. Future implementations should prioritize Passkey integration over TOTP for enhanced security in authentication systems.

Abstract: With the rise of sophisticated authentication bypass techniques, passwords
are no longer considered a reliable method for securing authentication systems.
In recent years, new authentication technologies have shifted from traditional
password-based logins to passwordless security. Among these, Time-Based
One-Time Passwords (TOTP) remain one of the most widely used mechanisms, while
Passkeys are emerging as a promising alternative with growing adoption. This
paper highlights the key techniques used during the implementation of the
authentication system with Passkey technology. It also suggests considerations
for integrating components during system development to ensure that users can
securely access their accounts with minimal complexity, while still meeting the
requirements of a robust authentication system that balances security,
usability, and performance. Additionally, by examining TOTP and Passkey
mechanisms from an implementation perspective, this work not only addresses
major security concerns such as password leaks, phishing attacks, and
susceptibility to brute-force attacks, but also evaluates the feasibility and
effectiveness of these mechanisms in real-world implementations. This paper
demonstrates the superior security of Passkey technology and its potential for
broader adoption in secure authentication systems.

</details>


### [12] [Design and Implementation of a Controlled Ransomware Framework for Educational Purposes Using Flutter Cryptographic APIs on Desktop PCs and Android Devices](https://arxiv.org/abs/2508.11939)
*James Gu,Ahmed Sartaj,Mohammed Akram Taher Khan,Rashid Hussain Khokhar*

Main category: cs.CR

TL;DR: A ransomware framework using Python and Flutter/Dart for educational experimentation, implementing safeguards for safe analysis of encryption mechanics and victim interactions.


<details>
  <summary>Details</summary>
Motivation: To provide a controlled environment for studying ransomware behavior (specifically WannaCry-inspired mechanics) alongside Android-specific implementation to train cybersecurity professionals through hands-on analysis.

Method: Developed two versions using Python's native crypto APIs and Flutter/Dart with: 1) Directory-restricted encryption 2) Pre-provided RSA private key 3) Limited file extension targeting (.txt/.jpg/.csv/.doc) 4) Open-source codebase for replication

Result: Framework enables systematic analysis of encryption processes, key management, and ransomware attack patterns across desktop and Android platforms with fail-safes preventing actual harm.

Conclusion: The open-source framework offers a pedagogical tool to safely examine ransomware mechanisms, enhancing cybersecurity education through practical reverse engineering and defensive strategy development.

Abstract: This study focuses on the creation and implementation of ransomware for
educational purposes that leverages Python's native cryptographic APIs in a
controlled environment. Additionally, an Android version of the framework is
implemented using Flutter and Dart. For both versions, open-source
cryptographic libraries are utilized. With this framework, researchers can
systematically explore the functionalities of ransomware, including file
encryption processes, cryptographic key management, and victim interaction
dynamics. To ensure safe experimentation, multiple safeguards are incorporated,
such as the ability to restrict the encryption process to a specific directory,
providing the RSA private key for immediate decryption, and narrowing the scope
of targetable files to a carefully curated list (.txt, .jpg, .csv, .doc). This
paper draws inspiration from the infamous WannaCry ransomware and aims to
simulate its behaviour on Android devices. By making the codebase open-source,
it enables users to study, modify, and extend the program for pedagogical
purposes and offers a hands-on tool that can be used to train the next
generation of cybersecurity professionals.

</details>


### [13] [ToxiEval-ZKP: A Structure-Private Verification Framework for Molecular Toxicity Repair Tasks](https://arxiv.org/abs/2508.12035)
*Fei Lin,Tengchao Zhang,Ziyang Gong,Fei-Yue Wang*

Main category: cs.CR

TL;DR: This paper introduces ToxiEval-ZKP, the first structure-private verification framework for molecular toxicity repair using zero-knowledge proofs (ZKPs), enabling secure validation of molecular properties without exposing structural details.


<details>
  <summary>Details</summary>
Motivation: Generative AI in molecular science lacks verifiability and structural privacy protection, especially in toxicity prediction tasks where sensitive molecular designs may be compromised during evaluation.

Method: The authors design a ZKP-compatible circuit integrating evaluation logic, Poseidon-based commitment hashing for data integrity, and nullifier-based replay prevention. This creates an end-to-end system for verifying multidimensional toxicity criteria without revealing molecule structures.

Result: Experiments show ToxiEval-ZKP successfully validates molecular toxicity under complete structural invisibility, achieving strong circuit efficiency, security, and adaptability for both classification and regression tasks.

Conclusion: ToxiEval-ZKP establishes a novel paradigm for trustworthy evaluation in scientific generative AI, addressing critical privacy and verification challenges through zero-knowledge proofs while maintaining rigorous validation standards.

Abstract: In recent years, generative artificial intelligence (GenAI) has demonstrated
remarkable capabilities in high-stakes domains such as molecular science.
However, challenges related to the verifiability and structural privacy of its
outputs remain largely unresolved. This paper focuses on the task of molecular
toxicity repair. It proposes a structure-private verification framework -
ToxiEval-ZKP - which, for the first time, introduces zero-knowledge proof (ZKP)
mechanisms into the evaluation process of this task. The system enables model
developers to demonstrate to external verifiers that the generated molecules
meet multidimensional toxicity repair criteria, without revealing the molecular
structures themselves. To this end, we design a general-purpose circuit
compatible with both classification and regression tasks, incorporating
evaluation logic, Poseidon-based commitment hashing, and a nullifier-based
replay prevention mechanism to build a complete end-to-end ZK verification
system. Experimental results demonstrate that ToxiEval-ZKP facilitates adequate
validation under complete structural invisibility, offering strong circuit
efficiency, security, and adaptability, thereby opening up a novel paradigm for
trustworthy evaluation in generative scientific tasks.

</details>


### [14] [Mitigating Jailbreaks with Intent-Aware LLMs](https://arxiv.org/abs/2508.12072)
*Wei Jie Yeo,Ranjan Satapathy,Erik Cambria*

Main category: cs.CR

TL;DR: Intent-FT is a fine-tuning method teaching LLMs to deduce instruction intent, improving safety without sacrificing task performance while mitigating all evaluated attack categories below 50% success rates.


<details>
  <summary>Details</summary>
Motivation: Large language models remain vulnerable to jailbreak attacks despite safety tuning, highlighting the need for a solution that balances safety and utility in adversarial scenarios.

Method: Intent-FT trains models to infer instruction intent through fine-tuning on adversarial examples, enabling generalization to unseen attacks via explicit intent detection.

Result: Intent-FT reduces all attack success rates <50%, maintains utility on benign inputs with harmful keywords, and transfers learned intent recognition to enhance base model defenses.

Conclusion: Intent-FT demonstrates effective adversarial intent detection that preserves model capabilities, offering a transferable foundation for robust LLM safety systems.

Abstract: Despite extensive safety-tuning, large language models (LLMs) remain
vulnerable to jailbreak attacks via adversarially crafted instructions,
reflecting a persistent trade-off between safety and task performance. In this
work, we propose Intent-FT, a simple and lightweight fine-tuning approach that
explicitly trains LLMs to infer the underlying intent of an instruction before
responding. By fine-tuning on a targeted set of adversarial instructions,
Intent-FT enables LLMs to generalize intent deduction to unseen attacks,
thereby substantially improving their robustness. We comprehensively evaluate
both parametric and non-parametric attacks across open-source and proprietary
models, considering harmfulness from attacks, utility, over-refusal, and impact
against white-box threats. Empirically, Intent-FT consistently mitigates all
evaluated attack categories, with no single attack exceeding a 50\% success
rate -- whereas existing defenses remain only partially effective. Importantly,
our method preserves the model's general capabilities and reduces excessive
refusals on benign instructions containing superficially harmful keywords.
Furthermore, models trained with Intent-FT accurately identify hidden harmful
intent in adversarial attacks, and these learned intentions can be effectively
transferred to enhance vanilla model defenses.

</details>


### [15] [PP-STAT: An Efficient Privacy-Preserving Statistical Analysis Framework using Homomorphic Encryption](https://arxiv.org/abs/2508.12093)
*Hyunmin Choi*

Main category: cs.CR

TL;DR: PP-STAT is a novel HE-based framework enabling efficient privacy-preserving statistical analysis by securely computing advanced metrics (Z-score normalization, skewness, kurtosis, coefficient of variation, Pearson correlation) on encrypted data through two optimizations: Chebyshev-based approximation for inverse square root and pre-normalization scaling to reduce multiplicative depth. It achieves high accuracy (MRE < 2.4x10^-4) on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Cloud computing's rise has increased the demand to outsource statistical analysis while protecting sensitive data (e.g., medical, financial) from privacy breaches. Homomorphic Encryption (HE) offers a solution by allowing computations on encrypted data, addressing urgent needs for secure and privacy-compliant analytics.

Method: PP-STAT leverages HE to perform statistical analysis on encrypted data. It introduces two optimizations: 1) a Chebyshev polynomial approximation strategy for initializing inverse square root operations, overcoming challenges in encrypted numerical computations, and 2) a pre-normalization scaling technique that integrates constant scaling factors into mean and variance calculations to reduce multiplicative depth and minimize bootstrapping costs.

Result: PP-STAT achieves mean relative errors (MRE) below 2.4x10^-4 on encrypted data computations. It successfully computes the Pearson correlation coefficient between 'smoker' attributes and 'charges' (encrypted result: 0.7873, MRE: 2.86x10^-4) while significantly reducing computational overhead through efficient HE operations.

Conclusion: The framework demonstrates practical utility for secure statistical analysis in privacy-sensitive domains by combining HE's confidentiality guarantees with optimized computational techniques, enabling precise and efficient secure analytics for real-world datasets.

Abstract: With the widespread adoption of cloud computing, the need for outsourcing
statistical analysis to third-party platforms is growing rapidly. However,
handling sensitive data such as medical records and financial information in
cloud environments raises serious privacy concerns. In this paper, we present
PP-STAT, a novel and efficient Homomorphic Encryption (HE)-based framework for
privacy-preserving statistical analysis. HE enables computations to be
performed directly on encrypted data without revealing the underlying
plaintext. PP-STAT supports advanced statistical measures, including Z-score
normalization, skewness, kurtosis, coefficient of variation, and Pearson
correlation coefficient, all computed securely over encrypted data. To improve
efficiency, PP-STAT introduces two key optimizations: (1) a Chebyshev-based
approximation strategy for initializing inverse square root operations, and (2)
a pre-normalization scaling technique that reduces multiplicative depth by
folding constant scaling factors into mean and variance computations. These
techniques significantly lower computational overhead and minimize the number
of expensive bootstrapping procedures. Our evaluation on real-world datasets
demonstrates that PP-STAT achieves high numerical accuracy, with mean relative
error (MRE) below 2.4x10-4. Notably, the encrypted Pearson correlation between
the smoker attribute and charges reaches 0.7873, with an MRE of 2.86x10-4.
These results confirm the practical utility of PP-STAT for secure and precise
statistical analysis in privacy-sensitive domains.

</details>


### [16] [AUTOVR: Automated UI Exploration for Detecting Sensitive Data Flow Exposures in Virtual Reality Apps](https://arxiv.org/abs/2508.12187)
*John Y. Kim,Chaoshun Zuo,Yanjie Zhao,Zhiqiang Lin*

Main category: cs.CR

TL;DR: The paper introduces AUTOVR, an automatic framework for testing user interfaces and interactions in Meta Quest VR apps, enhancing privacy through more effective sensitive data exposure detection compared to traditional Android tools.


<details>
  <summary>Details</summary>
Motivation: As VR app development grows on the Meta Quest platform, current headless testing tools lack the capability to thoroughly explore UI elements and events unique to VR, leading to potential privacy and usability issues.

Method: AUTOVR analyzes app binaries to reveal hidden events, resolves generative dependencies between these events, and leverages them to systematically explore VR app interactions. This approach contrasts with conventional Android/GUI testers that rely on surface-level event recognition.

Result: Empirical testing showed AUTOVR triggered 10x more sensitive data exposures than Android Monkey, demonstrating superior testing capability. This leads to more comprehensive privacy vulnerability detection in VR apps.

Conclusion: AUTOVR provides a groundbreaking solution for VR app testing by enabling deeper interaction analysis at the binary level, significantly improving both testing coverage and end-user privacy protection compared to existing platforms.

Abstract: The rise of Virtual Reality (VR) has provided developers with an
unprecedented platform for creating games and applications (apps) that require
distinct inputs, different from those of conventional devices like smartphones.
The Meta Quest VR platform, driven by Meta, has democratized VR app publishing
and attracted millions of users worldwide. However, as the number of published
apps grows, there is a notable lack of robust headless tools for user interface
(UI) exploration and user event testing. To address this need, we present
AUTOVR, an automatic framework for dynamic UI and user event interaction in VR
apps built on the Unity Engine. Unlike conventional Android and GUI testers,
AUTOVR analyzes the app's internal binary to reveal hidden events, resolves
generative event dependencies, and utilizes them for comprehensive exploration
of VR apps. Using sensitive data exposure as a performance metric, we compare
AUTOVR with Android Monkey, a widely used headless Android GUI stress testing
tool. Our empirical evaluation demonstrates AUTOVR's superior performance,
triggering an order of magnitude of more sensitive data exposures and
significantly enhancing the privacy of VR apps.

</details>


### [17] [Ethereum Crypto Wallets under Address Poisoning: How Usable and Secure Are They?](https://arxiv.org/abs/2508.12107)
*Shixuan Guan,Kai Li*

Main category: cs.CR

TL;DR: This study evaluates 53 Ethereum crypto wallets for their ability to prevent address poisoning attacks, revealing communication issues in 12 wallets, high phishing risks in 16, and lack of explicit warnings in most. Only three wallets provide adequate phishing protection with warning messages.


<details>
  <summary>Details</summary>
Motivation: Address poisoning attacks deceive Ethereum users into transferring funds to phishing addresses, causing over $100 million in losses. Evaluating wallet defenses against these attacks is critical to improving security and user protection in cryptocurrency transactions.

Method: The researchers designed experiments to simulate address poisoning attacks and conducted a systematic evaluation of 53 popular Ethereum crypto wallets by analyzing communication with transaction activity providers, phishing transfer display behaviors, and warning mechanisms during suspicious transactions.

Result: 12 wallets failed to download transaction histories due to communication errors. 16 wallets displayed fake token transfers without effective filtering. Most wallets relied on transaction activity providers for phishing detection, but only 3 wallets explicitly warned users of phishing addresses during transactions.

Conclusion: The Ethereum crypto wallet community lacks standardized defenses against address poisoning, with significant gaps in transaction history accuracy and alert mechanisms. While initial mitigation steps are being developed by responding to bug reports, stronger usability and security practices remain necessary to protect users.

Abstract: Blockchain address poisoning is an emerging phishing attack that crafts
"similar-looking" transfer records in the victim's transaction history, which
aims to deceive victims and lure them into mistakenly transferring funds to the
attacker. Recent works have shown that millions of Ethereum users were targeted
and lost over 100 million US dollars.
  Ethereum crypto wallets, serving users in browsing transaction history and
initiating transactions to transfer funds, play a central role in deploying
countermeasures to mitigate the address poisoning attack. However, whether they
have done so remains an open question. To fill the research void, in this
paper, we design experiments to simulate address poisoning attacks and
systematically evaluate the usability and security of 53 popular Ethereum
crypto wallets. Our evaluation shows that there exist communication failures
between 12 wallets and their transaction activity provider, which renders them
unable to download the users' transaction history. Besides, our evaluation also
shows that 16 wallets pose a high risk to their users due to displaying fake
token phishing transfers. Moreover, our further analysis suggests that most
wallets rely on transaction activity providers to filter out phishing
transfers. However, their phishing detection capability varies. Finally, we
found that only three wallets throw an explicit warning message when users
attempt to transfer to the phishing address, implying a significant gap within
the broader Ethereum crypto wallet community in protecting users from address
poisoning attacks.
  Overall, our work shows that more efforts are needed by the Ethereum crypto
wallet developer community to achieve the highest usability and security
standard. Our bug reports have been acknowledged by the developer community,
who are currently developing mitigation solutions.

</details>


### [18] [Systematic Analysis of MCP Security](https://arxiv.org/abs/2508.12538)
*Yongjian Guo,Puzhuo Liu,Wanlun Ma,Zehang Deng,Xiaogang Zhu,Peng Di,Xi Xiao,Sheng Wen*

Main category: cs.CR

TL;DR: This paper introduces MCPLIB, a comprehensive attack library for the Model Context Protocol (MCP), addressing limited security research by categorizing 31 attack methods across four classifications and experimentally analyzing their vulnerabilities to inform robust defense strategies.


<details>
  <summary>Details</summary>
Motivation: The Model Context Protocol (MCP) enables AI agents to interact with external tools but lacks sufficient security research, as current studies provide narrow or qualitative analyses that fail to systematize real-world threats.

Method: 1) Constructed a taxonomy of MCP attacks with four categories (direct/indirect tool injection, malicious user attacks, LLM inherent attacks). 2) Implemented 31 distinct attack methods in the open-source MCPLIB framework. 3) Conducted quantitative evaluation of attack effectiveness through controlled experiments. 4) Analyzed vulnerability patterns from attack outcomes.

Result: Key MCP vulnerabilities include: agents blindly trusting tool descriptions, file-based attacks causing significant impact, chain attacks leveraging shared context, and inability to distinguish external data from executable commands. The library's standardized attack implementations with metrics (e.g., success rates, injection patterns) reveal systematic weaknesses in MCP-based systems.

Conclusion: The paper establishes a foundational attack library and taxonomy for MCP security, demonstrating critical vulnerabilities that necessitate improved context shielding, verification protocols, and attack-resistant design principles for robust AI agent ecosystems.

Abstract: The Model Context Protocol (MCP) has emerged as a universal standard that
enables AI agents to seamlessly connect with external tools, significantly
enhancing their functionality. However, while MCP brings notable benefits, it
also introduces significant vulnerabilities, such as Tool Poisoning Attacks
(TPA), where hidden malicious instructions exploit the sycophancy of large
language models (LLMs) to manipulate agent behavior. Despite these risks,
current academic research on MCP security remains limited, with most studies
focusing on narrow or qualitative analyses that fail to capture the diversity
of real-world threats. To address this gap, we present the MCP Attack Library
(MCPLIB), which categorizes and implements 31 distinct attack methods under
four key classifications: direct tool injection, indirect tool injection,
malicious user attacks, and LLM inherent attack. We further conduct a
quantitative analysis of the efficacy of each attack. Our experiments reveal
key insights into MCP vulnerabilities, including agents' blind reliance on tool
descriptions, sensitivity to file-based attacks, chain attacks exploiting
shared context, and difficulty distinguishing external data from executable
commands. These insights, validated through attack experiments, underscore the
urgency for robust defense strategies and informed MCP design. Our
contributions include 1) constructing a comprehensive MCP attack taxonomy, 2)
introducing a unified attack framework MCPLIB, and 3) conducting empirical
vulnerability analysis to enhance MCP security mechanisms. This work provides a
foundational framework, supporting the secure evolution of MCP ecosystems.

</details>


### [19] [Substituting Proof of Work in Blockchain with Training-Verified Collaborative Model Computation](https://arxiv.org/abs/2508.12138)
*Mohammad Ishzaz Asif Rafid,Morsalin Sakib*

Main category: cs.CR

TL;DR: Introduces a cloud-based collaborative training framework to replace Bitcoin's energy-intensive PoW with sustainable computation using machine learning model training and weighted lottery-based block validation.


<details>
  <summary>Details</summary>
Motivation: Bitcoin's Proof of Work (PoW) mechanism faces criticism for excessive energy consumption and hardware inefficiencies, necessitating sustainable alternatives that maintain blockchain integrity while reducing environmental impact.

Method: 1) Replace PoW with cloud-based collaborative training where miners train horizontal ML models on preprocessed datasets using privacy-preserving methods.
2) Central server evaluates miner contributions using parameter count and model loss reduction metrics.
3) Weighted lottery selects winning miner to validate blocks with SHA-256 hashing and digital signatures ensuring security.

Result: System converts mining energy into productive ML computation while preserving blockchain integrity through digital signatures and SHA-256 hashing. Reduces hardware waste by using cloud infrastructure for training cycles.

Conclusion: The hybrid architecture address sustainability issues of traditional mining by aligning security incentives with computational progress. Creates verifiable proof through valuable machine learning work rather than energy-intensive puzzles.

Abstract: Bitcoin's Proof of Work (PoW) mechanism, while central to achieving
decentralized consensus, has long been criticized for excessive energy use and
hardware inefficiencies \cite{devries2018bitcoin, truby2018decarbonizing}. This
paper introduces a hybrid architecture that replaces Bitcoin's traditional PoW
with a centralized, cloud-based collaborative training framework. In this
model, miners contribute computing resources to train segments of horizontally
scaled machine learning models on preprocessed datasets, ensuring privacy and
generating meaningful outputs \cite{li2017securing}. A central server evaluates
contributions using two metrics: number of parameters trained and reduction in
model loss during each cycle. At the end of every cycle, a weighted lottery
selects the winning miner, who receives a digitally signed certificate. This
certificate serves as a verifiable substitute for PoW and grants the right to
append a block to the blockchain \cite{nakamoto2008bitcoin}. By integrating
digital signatures and SHA-256 hashing \cite{nist2015sha}, the system preserves
blockchain integrity while redirecting energy toward productive computation.
The proposed approach addresses the sustainability concerns of traditional
mining by converting resource expenditure into socially valuable work, aligning
security incentives with real-world computational progress.

</details>


### [20] [Attack Graph Generation on HPC Clusters](https://arxiv.org/abs/2508.12161)
*Ming Li,John Hale*

Main category: cs.CR

TL;DR: The paper proposes using high performance computing (HPC) clusters to address the computational and memory challenges of generating large attack graphs (AGs) in complex networks, demonstrating scalable solutions through experiments.


<details>
  <summary>Details</summary>
Motivation: Traditional attack graph generation suffers from state-space explosion as network assets, device interconnections, and vulnerabilities increase, leading to prohibitive time and memory requirements.

Method: Implementation of AG generators on HPC clusters using distributed computing techniques. The authors evaluate cluster-based AG generation performance through comparative experiments, analyzing load balancing and memory distribution strategies.

Result: Experimental results show that HPC clusters significantly reduce runtime and handle larger AGs compared to single machines, while maintaining memory balance through distributed partitioning.

Conclusion: Cluster computing provides a scalable solution to the performance limitations of AG generation, offering balanced speed and memory improvements. This approach enables practical analysis of complex network security architectures that would otherwise be infeasible with traditional methods.

Abstract: Attack graphs (AGs) are graphical tools to analyze the security of computer
networks. By connecting the exploitation of individual vulnerabilities, AGs
expose possible multi-step attacks against target networks, allowing system
administrators to take preventive measures to enhance their network's security.
As powerful analytical tools, however, AGs are both time- and memory-consuming
to be generated. As the numbers of network assets, interconnections between
devices, as well as vulnerabilities increase, the size and volume of the
resulting AGs grow at a much higher rate, leading to the well-known state-space
explosion. In this paper, we propose the use of high performance computing
(HPC) clusters to implement AG generators. We evaluate the performance through
experiments and provide insights into how cluster environments can help resolve
the issues of slow speed and high memory demands in AG generation in a balanced
way.

</details>


### [21] [Invitation Is All You Need! Promptware Attacks Against LLM-Powered Assistants in Production Are Practical and Dangerous](https://arxiv.org/abs/2508.12175)
*Ben Nassi,Stav Cohen,Or Yair*

Main category: cs.CR

TL;DR: This paper introduces a TARA framework to assess Promptware risks for Gemini-powered assistants, revealing 14 attack scenarios across five threat classes. 73% of threats were high-critical, but Google's mitigations reduced risk to very low-medium.


<details>
  <summary>Details</summary>
Motivation: The paper addresses underappreciated security risks from Promptware (malicious prompts manipulating LLMs), which prior research considered low-severity despite demonstrations of device-escape capabilities.

Method: The researchers developed a novel Threat Analysis and Risk Assessment (TARA) framework. They identified and tested 14 targeted promptware attack scenarios, particularly those involving indirect prompt injection through everyday user interactions like calendar invites and shared documents.

Result: Demonstrated 14 attack scenarios across five classes (context poisoning, memory poisoning, tool misuse, agent/app invocation), including physical consequences like home automation control. 73% of threats rated High-Critical. Google implemented mitigations reducing risk to Very Low-Medium.

Conclusion: Promptware poses significant risks to LLM applications. The TARA framework effectively evaluates threat levels, and targeted mitigations can substantially reduce risks. The findings highlight the importance of proactively addressing prompt injection vulnerabilities as LLMs become more integrated into critical devices.

Abstract: The growing integration of LLMs into applications has introduced new security
risks, notably known as Promptware - maliciously engineered prompts designed to
manipulate LLMs to compromise the CIA triad of these applications. While prior
research warned about a potential shift in the threat landscape for LLM-powered
applications, the risk posed by Promptware is frequently perceived as low. In
this paper, we investigate the risk Promptware poses to users of Gemini-powered
assistants (web application, mobile application, and Google Assistant). We
propose a novel Threat Analysis and Risk Assessment (TARA) framework to assess
Promptware risks for end users. Our analysis focuses on a new variant of
Promptware called Targeted Promptware Attacks, which leverage indirect prompt
injection via common user interactions such as emails, calendar invitations,
and shared documents. We demonstrate 14 attack scenarios applied against
Gemini-powered assistants across five identified threat classes: Short-term
Context Poisoning, Permanent Memory Poisoning, Tool Misuse, Automatic Agent
Invocation, and Automatic App Invocation. These attacks highlight both digital
and physical consequences, including spamming, phishing, disinformation
campaigns, data exfiltration, unapproved user video streaming, and control of
home automation devices. We reveal Promptware's potential for on-device lateral
movement, escaping the boundaries of the LLM-powered application, to trigger
malicious actions using a device's applications. Our TARA reveals that 73% of
the analyzed threats pose High-Critical risk to end users. We discuss
mitigations and reassess the risk (in response to deployed mitigations) and
show that the risk could be reduced significantly to Very Low-Medium. We
disclosed our findings to Google, which deployed dedicated mitigations.

</details>


### [22] [CAN Networks Security in Smart Grids Communication Technologies](https://arxiv.org/abs/2508.12181)
*Ayman W. Baharia,Khaled T. Naga,Hesham S. Abdelfattah,Shady A. Maged,Sherif A. Hammad*

Main category: cs.CR

TL;DR: The paper proposes a CAN bus security solution for smart grids that minimizes computational overhead by centralizing security mechanisms onto a single node, implemented using Code Composer Studio and TM4C1294 evaluation boards.


<details>
  <summary>Details</summary>
Motivation: Smart grids require reliable communication protocols like CAN, but their increasing interconnectivity exposes them to cyber-attacks. Traditional CAN security mechanisms introduce significant computational overhead, causing network delays.

Method: A single dedicated node handles all security processing using Code Composer Studio and TM4C1294 microcontroller evaluation boards, reducing overhead on other CAN nodes while maintaining robustness and real-time capabilities.

Result: Test results demonstrating the effectiveness of this approach will be presented in detail (specific metrics not included in abstract).

Conclusion: Centralized security node implementation offers a promising trade-off between strong CAN network protection and minimal performance impact, though detailed validation remains pending completion of the subsequent discussion.

Abstract: The rapid evolution of smart grids requires effective communication protocols
to transfer data reliably and securely. Controller Area Network (CAN) is one of
the most recognized protocols that offer reliable data transmission in smart
grids due to its robustness, real-time capabilities, and relatively low initial
cost of its required hardware. However, as a smart city becomes more
interconnected, it also becomes more vulnerable to cyber-attacks. As there are
many mechanisms to secure the CAN nodes from attacks, most of those mechanisms
have computational overhead, resulting in more delay in the network. We
implemented a solution that requires almost no overhead to any CAN node
connected to the network. It depends on a single node responsible for securing
the CAN network. This approach seeks to augment network security while reducing
security mechanisms overhead to all CAN network nodes. The methodology and
comprehensive test results will be presented in detail during a subsequent
discussion. The used software for development is Code Composer Studio, and the
used microcontroller evaluation boards (EVB) are TM4C 1294.

</details>


### [23] [Fortifying the Agentic Web: A Unified Zero-Trust Architecture Against Logic-layer Threats](https://arxiv.org/abs/2508.12259)
*Ken Huang,Yasir Mehmood,Hammad Atta,Jerry Huang,Muhammad Zeeshan Baig,Sree Bhargavi Balija*

Main category: cs.CR

TL;DR: The paper introduces a Zero-Trust IAM framework using DIDs/VCs and an ANS to secure the Agentic Web, analyzing security guarantees against LPCI threats. Key innovations include TARE, Causal Chain Auditing, and Dynamic Identity.


<details>
  <summary>Details</summary>
Motivation: Traditional IAM systems lack resilience for the Agentic Web's decentralized agent interactions, while LPCI (Link-Prediction Privacy Intrusion) threats exploit identity patterns to compromise trust.

Method: 1. Protocol-agnostic Agent Name Service (ANS) for identity discovery
2. Trust Fabric with three mechanisms: 
   a. Trust-Adaptive Runtime Environments (TARE)
   b. Causal Chain Auditing for forensic traceability
   c. Dynamic Identity with Behavioral Attestation
3. Formal security model explicitly linking LPCI threats to countermeasures

Result: Formal analysis demonstrates the architecture achieves provable security guarantees against LPCI attacks with a mathematically bounded probability of successful compromise (attacks succeed with probability ‚â§ Œµ, where Œµ is an explicit function of system parameters).

Conclusion: The framework establishes a comprehensive security blueprint for agentic systems, enabling trustworthiness through verifiable identities and adaptive protection layers. The formal model provides a foundation for future security enhancements in decentralized agent ecosystems.

Abstract: This paper presents a Unified Security Architecture that fortifies the
Agentic Web through a Zero-Trust IAM framework. This architecture is built on a
foundation of rich, verifiable agent identities using Decentralized Identifiers
(DIDs) and Verifiable Credentials (VCs), with discovery managed by a
protocol-agnostic Agent Name Service (ANS). Security is operationalized through
a multi-layered Trust Fabric which introduces significant innovations,
including Trust-Adaptive Runtime Environments (TARE), Causal Chain Auditing,
and Dynamic Identity with Behavioral Attestation. By explicitly linking the
LPCI threat to these enhanced architectural countermeasures within a formal
security model, we propose a comprehensive and forward-looking blueprint for a
secure, resilient, and trustworthy agentic ecosystem. Our formal analysis
demonstrates that the proposed architecture provides provable security
guarantees against LPCI attacks with bounded probability of success.

</details>


### [24] [CryptPEFT: Efficient and Private Neural Network Inference via Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2508.12264)
*Saisai Xia,Wenhao Wang,Zihao Wang,Yuhui Zhang,Yier Jin,Dan Meng,Rui Hou*

Main category: cs.CR

TL;DR: CryptPEFT introduces a one-way communication pipeline to reduce encrypted computation in parameter-efficient fine-tuning for private inference while maintaining model accuracy.


<details>
  <summary>Details</summary>
Motivation: Current privacy-preserving PEFT methods face challenges from two-way communication between large models and adapters, leading to high computational and communication burdens. This limits practical use for sensitive applications.

Method: 1) Designs a one-way communication (OWC) architecture where only the adapter requires encryption
2) Develops automated architecture search for optimal OWC-compatible adapters
3) Evaluates the approach using Vision Transformers on image classification tasks

Result: Achieved 85.47% accuracy on CIFAR-100 with 2.26s latency
20.62√ó-291.48√ó speedups over existing methods in WAN/LAN simulations
Maintained parameter efficiency compared to traditional PEFT approaches

Conclusion: CryptPEFT enables secure yet computationally efficient private inference for PEFT systems through its novel OWC architecture and automated adapter optimization, opening new possibilities for deploying ML models with sensitive data constraints.

Abstract: Publicly available large pretrained models (i.e., backbones) and lightweight
adapters for parameter-efficient fine-tuning (PEFT) have become standard
components in modern machine learning pipelines. However, preserving the
privacy of both user inputs and fine-tuned adapters -- often trained on
sensitive data -- during inference remains a significant challenge. Applying
cryptographic techniques, such as multi-party computation (MPC), to PEFT
settings still incurs substantial encrypted computation across both the
backbone and adapter, mainly due to the inherent two-way communication between
them. To address this limitation, we propose CryptPEFT, the first PEFT solution
specifically designed for private inference scenarios. CryptPEFT introduces a
novel one-way communication (OWC) architecture that confines encrypted
computation solely to the adapter, significantly reducing both computational
and communication overhead. To maintain strong model utility under this
constraint, we explore the design space of OWC-compatible adapters and employ
an automated architecture search algorithm to optimize the trade-off between
private inference efficiency and model utility. We evaluated CryptPEFT using
Vision Transformer backbones across widely used image classification datasets.
Our results show that CryptPEFT significantly outperforms existing baselines,
delivering speedups ranging from $20.62\times$ to $291.48\times$ in simulated
wide-area network (WAN) and local-area network (LAN) settings. On CIFAR-100,
CryptPEFT attains 85.47% accuracy with just 2.26 seconds of inference latency.
These findings demonstrate that CryptPEFT offers an efficient and
privacy-preserving solution for modern PEFT-based inference.

</details>


### [25] [Adjustable AprilTags For Identity Secured Tasks](https://arxiv.org/abs/2508.12304)
*Hao Li*

Main category: cs.CR

TL;DR: This paper addresses the need for adjustable AprilTags in public environments to mitigate adversarial attacks, unlike their regulation in private settings.


<details>
  <summary>Details</summary>
Motivation: Fixed AprilTags are insecure in public environments where adversarial attacks can cause harm, necessitating a shift to adjustable solutions.

Method: The paper advocates replacing fixed AprilTags with adjustable ones to alter their visual patterns dynamically and resist adversarial manipulations.

Result: No specific experimental results are mentioned in the abstract; the paper presents the rationale for using adjustable AprilTags in open settings.

Conclusion: Adjustable AprilTags are necessary in public environments to enhance identity security by countering adversarial attacks through dynamic pattern adjustments.

Abstract: Special tags such as AprilTags that facilitate image processing and pattern
recognition are useful in practical applications. In close and private
environments, identity security is unlikely to be an issue because all involved
AprilTags can be completely regulated. However, in open and public
environments, identity security is no longer an issue that can be neglected. To
handle potential harm caused by adversarial attacks, this note advocates
utilization of adjustable AprilTags instead of fixed ones.

</details>


### [26] [Where to Start Alignment? Diffusion Large Language Model May Demand a Distinct Position](https://arxiv.org/abs/2508.12398)
*Zhixin Xie,Xurui Song,Jun Luo*

Main category: cs.CR

TL;DR: This paper analyzes the safety of Diffusion Large Language Models (dLLMs), identifies critical middle tokens for security, and proposes Middle-tOken Safety Alignment (MOSA), a reinforcement learning method that enhances dLLM output safety without compromising utility.


<details>
  <summary>Details</summary>
Motivation: Non-autoregressive dLLMs (e.g., decoding using diffusion processes) exhibit unique generation mechanisms but lack safety alignment frameworks. The paper motivates addressing this gap by highlighting the absence of security studies for dLLMs and the distinct asymmetry in manipulation power between defenders and attackers relative to autoregressive models.

Method: The proposed Middle-tOken Safety Alignment (MOSA) method exploits empirical findings that dLLMs demonstrate a strong sequential token generation pattern. This creates asymmetry where attackers can't easily manipulate middle tokens (critical for safety) while defenders can effectively align them via reinforcement learning strategies focused on those positions.

Result: MOSA demonstrates robust security performance (14.2%-55.4% better than baselines) against eight attacks across two benchmarks. It maintains 94.3%-99.1% of the original utility (coding/math/reasoning scores) compared to autoregressive alignment baselines that lose 35.6%-62.1% of utility.

Conclusion: The paper conclusively demonstrates that dLLMs' diffusion-based sequential generation patterns create security advantages. MOSA effectively leverages this characteristic to achieve middle-token alignment that blocks safety attacks while preserving practical capabilities in reasoning tasks.

Abstract: Diffusion Large Language Models (dLLMs) have recently emerged as a
competitive non-autoregressive paradigm due to their unique training and
inference approach. However, there is currently a lack of safety study on this
novel architecture. In this paper, we present the first analysis of dLLMs'
safety performance and propose a novel safety alignment method tailored to
their unique generation characteristics. Specifically, we identify a critical
asymmetry between the defender and attacker in terms of security. For the
defender, we reveal that the middle tokens of the response, rather than the
initial ones, are more critical to the overall safety of dLLM outputs; this
seems to suggest that aligning middle tokens can be more beneficial to the
defender. The attacker, on the contrary, may have limited power to manipulate
middle tokens, as we find dLLMs have a strong tendency towards a sequential
generation order in practice, forcing the attack to meet this distribution and
diverting it from influencing the critical middle tokens. Building on this
asymmetry, we introduce Middle-tOken Safety Alignment (MOSA), a novel method
that directly aligns the model's middle generation with safe refusals
exploiting reinforcement learning. We implement MOSA and compare its security
performance against eight attack methods on two benchmarks. We also test the
utility of MOSA-aligned dLLM on coding, math, and general reasoning. The
results strongly prove the superiority of MOSA.

</details>


### [27] [LumiMAS: A Comprehensive Framework for Real-Time Monitoring and Enhanced Observability in Multi-Agent Systems](https://arxiv.org/abs/2508.12412)
*Ron Solomon,Yarin Yerushalmi Levi,Lior Vaknin,Eran Aizikovich,Amit Baras,Etai Ohana,Amit Giloni,Shamik Bose,Chiara Picardi,Yuval Elovici,Asaf Shabtai*

Main category: cs.CR

TL;DR: LumiMAS is a new observability framework for multi-agent systems with large language models, designed to monitor system-wide failures. It includes three layers (monitoring/logging, anomaly detection, anomaly explanation), evaluated on seven applications with diverse platforms and failure types, showing effective detection, classification, and root cause analysis.


<details>
  <summary>Details</summary>
Motivation: Current MAS observability frameworks analyze individual agents, neglecting system-wide failures. Integrating large language models in MAS requires better monitoring, interpretation, and failure detection methods to address these global issues.

Method: LumiMAS integrates three layers: (1) a monitoring and logging layer to capture agent activities across MAS workflows, (2) real-time anomaly detection across the system, and (3) anomaly explanation and root cause analysis. Evaluated on seven applications using two MAS platforms and diverse failure scenarios, including hallucinations and bias.

Result: LumiMAS demonstrated effectiveness in detecting, classifying, and diagnosing system-wide anomalies in multi-agent systems. Evaluated on seven applications across two platforms, it excels in identifying rare failures like hallucinations and biases, showing robustness through comprehensive failure simulations.

Conclusion: LumiMAS offers a robust solution for end-to-end observability in multi-agent systems with large language models, addressing system-wide failures through layered analytics and real-world evaluation. Its methodology provides a foundation for improving autonomy in complex environments.

Abstract: The incorporation of large language models in multi-agent systems (MASs) has
the potential to significantly improve our ability to autonomously solve
complex problems. However, such systems introduce unique challenges in
monitoring, interpreting, and detecting system failures. Most existing MAS
observability frameworks focus on analyzing each individual agent separately,
overlooking failures associated with the entire MAS. To bridge this gap, we
propose LumiMAS, a novel MAS observability framework that incorporates advanced
analytics and monitoring techniques. The proposed framework consists of three
key components: a monitoring and logging layer, anomaly detection layer, and
anomaly explanation layer. LumiMAS's first layer monitors MAS executions,
creating detailed logs of the agents' activity. These logs serve as input to
the anomaly detection layer, which detects anomalies across the MAS workflow in
real time. Then, the anomaly explanation layer performs classification and root
cause analysis (RCA) of the detected anomalies. LumiMAS was evaluated on seven
different MAS applications, implemented using two popular MAS platforms, and a
diverse set of possible failures. The applications include two novel
failure-tailored applications that illustrate the effects of a hallucination or
bias on the MAS. The evaluation results demonstrate LumiMAS's effectiveness in
failure detection, classification, and RCA.

</details>


### [28] [A Robust Cross-Domain IDS using BiGRU-LSTM-Attention for Medical and Industrial IoT Security](https://arxiv.org/abs/2508.12470)
*Afrah Gueriani,Hamza Kheddar,Ahmed Cherif Mazari,Mohamed Chahine Ghanem*

Main category: cs.CR

TL;DR: BiGAT-ID is a hybrid transformer-based IDS for IoMT/IIoT that captures temporal dependencies and contextual features, achieving 99.13%/99.34% accuracy on CICIoMT2024 and EdgeIIoTset datasets with efficient inference times.


<details>
  <summary>Details</summary>
Motivation: Growing interconnectivity in IoMT and IIoT systems creates vulnerabilities to sophisticated cyber threats, necessitating robust intrusion detection to protect sensitive data, patient safety, and industrial operations.

Method: The model integrates BiGRU (bidirectional temporal analysis), LSTM (sequential pattern modeling), and multi-head attention (contextual feature enhancement) in a hybrid architecture.

Result: 99.13% detection accuracy on CICIoMT2024 and 99.34% on EdgeIIoTset, with 0.0002s and 0.0001s inference times per instance, respectively, alongside a low false positive rate.

Conclusion: BiGAT-ID demonstrates cross-domain robustness and operational efficiency, making it suitable for real-world heterogeneous IoT environments requiring high-accuracy intrusion detection.

Abstract: The increased Internet of Medical Things IoMT and the Industrial Internet of
Things IIoT interconnectivity has introduced complex cybersecurity challenges,
exposing sensitive data, patient safety, and industrial operations to advanced
cyber threats. To mitigate these risks, this paper introduces a novel
transformer-based intrusion detection system IDS, termed BiGAT-ID a hybrid
model that combines bidirectional gated recurrent units BiGRU, long short-term
memory LSTM networks, and multi-head attention MHA. The proposed architecture
is designed to effectively capture bidirectional temporal dependencies, model
sequential patterns, and enhance contextual feature representation. Extensive
experiments on two benchmark datasets, CICIoMT2024 medical IoT and EdgeIIoTset
industrial IoT demonstrate the model's cross-domain robustness, achieving
detection accuracies of 99.13 percent and 99.34 percent, respectively.
Additionally, the model exhibits exceptional runtime efficiency, with inference
times as low as 0.0002 seconds per instance in IoMT and 0.0001 seconds in IIoT
scenarios. Coupled with a low false positive rate, BiGAT-ID proves to be a
reliable and efficient IDS for deployment in real-world heterogeneous IoT
environments

</details>


### [29] [ChamaleoNet: Programmable Passive Probe for Enhanced Visibility on Erroneous Traffic](https://arxiv.org/abs/2508.12496)
*Zhihao Wang,Alessandro Cornacchia,Andrea Bianco,Idilio Drago,Paolo Giaccone,Dingde Jiang,Marco Mellia*

Main category: cs.CR

TL;DR: ChamaleoNet is an SDN-based transparent monitoring system that detects harmful network traffic (misconfigured hosts, infected systems, external scans) by filtering out well-formed traffic and integrating with honeypots, achieving 96% traffic reduction for scalability.


<details>
  <summary>Details</summary>
Motivation: Existing production networks lack visibility into unsolicited/erroneous traffic, making it hard to detect misconfigurations, temporary failures, or security attacks. Privacy and scalability also pose engineering challenges for monitoring solutions.

Method: The paper leverages SDN to implement network-wide traffic filtering that only processes erroneous packets. It uses in-hardware flow rules to reduce controller traffic by 96% while maintaining privacy-first design. The system can seamlessly integrate with honeypots to engage with malicious traffic sources.

Result: ChamaleoNet successfully filters out 96% of controller-bound traffic via SDN hardware, enabling scalable detection of internal misconfigurations (servers/clients), infected hosts, and external attack radiation. It offers open-source implementation with low operational overhead.

Conclusion: By focusing on erroneous traffic with SDN-based filtering and honeypot integration, ChamaleoNet improves network visibility and scalability without compromising privacy. It provides effective detection of both internal vulnerabilities and external adversarial reconnaissance attempts.

Abstract: Traffic visibility remains a key component for management and security
operations. Observing unsolicited and erroneous traffic, such as unanswered
traffic or errors, is fundamental to detect misconfiguration, temporary
failures or attacks. ChamaleoNet transforms any production network into a
transparent monitor to let administrators collect unsolicited and erroneous
traffic directed to hosts, whether offline or active, hosting a server or a
client, protected by a firewall, or unused addresses. ChamaleoNet is programmed
to ignore well-formed traffic and collect only erroneous packets, including
those generated by misconfigured or infected internal hosts, and those sent by
external actors which scan for services. Engineering such a system poses
several challenges, from scalability to privacy. Leveraging the SDN paradigm,
ChamaleoNet processes the traffic flowing through a campus/corporate network
and focuses on erroneous packets only, lowering the pressure on the collection
system while respecting privacy regulations by design. ChamaleoNet enables the
seamless integration with active deceptive systems like honeypots that can
impersonate unused hosts/ports/services and engage with senders. The SDN
in-hardware filtering reduces the traffic to the controller by 96%, resulting
in a scalable solution, which we offer as open source. Simple analytics unveil
internal misconfigured and infected hosts, identify temporary failures, and
enhance visibility on external radiation produced by attackers looking for
vulnerable services.

</details>


### [30] [The Hidden Cost of Correlation: Rethinking Privacy Leakage in Local Differential Privacy](https://arxiv.org/abs/2508.12539)
*Sandaru Jayawardana,Sennur Ulukus,Ming Ding,Kanchana Thilakarathna*

Main category: cs.CR

TL;DR: This paper systematically evaluates correlation-induced privacy leakage (CPL) in five LDP mechanisms across four real-world datasets, identifies flaws in current assumptions and metrics, and proposes the first theoretical framework and benchmarks for quantifying CPL in general (Œµ,Œ¥)-LDP mechanisms to improve privacy-utility trade-offs.


<details>
  <summary>Details</summary>
Motivation: Existing CPL analysis under LDP is limited by unverified assumptions and pure LDP (Œ¥=0) constraints, leaving unclear their validity or real-world leakage severity. The authors aim to quantify CPL's impact and improve theoretical benchmarks for practical data governance.

Method: 1) Statistical analysis of GRR, RAPPOR, OUE, OLH, and Exponential mechanisms on real datasets (Census Income, Adult, MovieLens, UCI Diabetes). 2) Derive theoretical CPL bounds for any (Œµ,Œ¥)-LDP mechanism. 3) Validate framework through empirical comparison and propose two correlation-aware benchmarks.

Result: 1) Current CPL metrics overestimate/ignore leakage. 2) Approximated LDP mechanisms (Œ¥‚â†0) amplify leakage compared to pure LDP. 3) Theoretical framework accurately captures statistical leakage patterns. 4) Novel benchmarks reveal significant CPL inefficiencies in existing mechanisms.

Conclusion: CPL is a critical, underestimated factor in LDP systems. The proposed framework enables precise analysis of approximated LDP mechanisms, and benchmarks provide tangible tools for optimizing privacy-preserving data collection strategies.

Abstract: Local differential privacy (LDP) has emerged as a promising paradigm for
privacy-preserving data collection in distributed systems, where users
contribute multi-dimensional records with potentially correlated attributes.
Recent work has highlighted that correlation-induced privacy leakage (CPL)
plays a critical role in shaping the privacy-utility trade-off under LDP,
especially when correlations exist among attributes. Nevertheless, it remains
unclear to what extent the prevailing assumptions and proposed solutions are
valid and how significant CPL is in real-world data. To address this gap, we
first perform a comprehensive statistical analysis of five widely used LDP
mechanisms -- GRR, RAPPOR, OUE, OLH and Exponential mechanism -- to assess CPL
across four real-world datasets. We identify that many primary assumptions and
metrics in current approaches fall short of accurately characterising these
leakages. Moreover, current studies have been limited to a set of pure LDP
(i.e., {\delta = 0}) mechanisms. In response, we develop the first algorithmic
framework to theoretically quantify CPL for any general approximated LDP
(({\varepsilon},{\delta})-LDP) mechanism. We validate our theoretical results
against empirical statistical results and provide a theoretical explanation for
the observed statistical patterns. Finally, we propose two novel benchmarks to
validate correlation analysis algorithms and evaluate the utility vs CPL of LDP
mechanisms. Further, we demonstrate how these findings can be applied to
achieve an efficient privacy-utility trade-off in real-world data governance.

</details>


### [31] [DEFENDCLI: {Command-Line} Driven Attack Provenance Examination](https://arxiv.org/abs/2508.12553)
*Peilun Wu,Nan Sun,Nour Moustafa,Youyang Qu,Ming Ding*

Main category: cs.CR

TL;DR: DEFENDCLI enhances EDR precision by 1.6x-2.3x through command-line-level analysis and three-tier event differentiation in system process calls, suspicious CLI executions, and network connections, overcoming limitations in detecting obfuscation and low-frequency attacks.


<details>
  <summary>Details</summary>
Motivation: Current EDR systems using provenance graphs face challenges in interoperability, reliability, flexibility, and practicality. Existing methods struggle with detecting obfuscated attacks, correlating low-frequency events, and maintaining context-awareness for command-line activities.

Method: Introduces DEFENDCLI, a command-line-level detection system analyzing three levels: unusual system process calls, suspicious command-line executions, and infrequent external network connections. Utilizes attack provenance graphs with fine-grained detection granularity.

Result: 1.6x precision improvement over state-of-the-art (SOTA) on DARPA Engagement Series datasets. Real-world industrial testing shows 2.3x precision gains over SOTA and detects previously missed unknown attacks in commercial solutions.

Conclusion: DEFENDCLI addresses critical EDR limitations by introducing CLI-level analysis, enabling superior attack detection precision in complex environments through its multi-level differentiation approach, validated by both benchmark and real-world testing.

Abstract: Endpoint Detection and Response (EDR) solutions embrace the method of attack
provenance graph to discover unknown threats through system event correlation.
However, this method still faces some unsolved problems in the fields of
interoperability, reliability, flexibility, and practicability to deliver
actionable results. Our research highlights the limitations of current
solutions in detecting obfuscation, correlating attacks, identifying
low-frequency events, and ensuring robust context awareness in relation to
command-line activities. To address these challenges, we introduce DEFENDCLI,
an innovative system leveraging provenance graphs that, for the first time,
delves into command-line-level detection. By offering finer detection
granularity, it addresses a gap in modern EDR systems that has been overlooked
in previous research. Our solution improves the precision of the information
representation by evaluating differentiation across three levels: unusual
system process calls, suspicious command-line executions, and infrequent
external network connections. This multi-level approach enables EDR systems to
be more reliable in complex and dynamic environments. Our evaluation
demonstrates that DEFENDCLI improves precision by approximately 1.6x compared
to the state-of-the-art methods on the DARPA Engagement Series attack datasets.
Extensive real-time industrial testing across various attack scenarios further
validates its practical effectiveness. The results indicate that DEFENDCLI not
only detects previously unknown attack instances, which are missed by other
modern commercial solutions, but also achieves a 2.3x improvement in precision
over the state-of-the-art research work.

</details>


### [32] [Data-driven Trust Bootstrapping for Mobile Edge Computing-based Industrial IoT Services](https://arxiv.org/abs/2508.12560)
*Prabath Abeysekara,Hai Dong*

Main category: cs.CR

TL;DR: The paper proposes a context-aware, data-driven approach to address trust bootstrapping challenges in MEC-based IIoT systems, including prolonged service evaluation periods, unreliable peer recommendations, context parameter variability, and data sparsity. It validates the approach with real-world datasets showing improved trustworthiness assessment across MEC environments.


<details>
  <summary>Details</summary>
Motivation: Existing trust bootstrapping methods struggle with adapting to MEC-based IIoT systems due to limited service interaction windows, inconsistent peer recommendation access, and context-parameter variability causing unreliable trust evaluations across environments.

Method: A framework was developed that integrates context-aware analysis and cross-MEC environment knowledge sharing to mitigate data sparsity while accommodating diverse environmental contexts. The method was evaluated using two real-world datasets modified with context-dependent trust features.

Result: Experimental results demonstrated the effectiveness of the approach in reliably bootstrapping trustworthiness for lesser-known IoT services under MEC constraints, with successful handling of data sparsity and context-driven trust variability.

Conclusion: The proposed approach effectively enables robust trust evaluation in MEC-based IIoT systems by addressing interaction limitations, recommendation consistency, contextual variability, and data sparsity through knowledge sharing mechanisms.

Abstract: We propose a data-driven and context-aware approach to bootstrap
trustworthiness of homogeneous Internet of Things (IoT) services in Mobile Edge
Computing (MEC) based industrial IoT (IIoT) systems. The proposed approach
addresses key limitations in adapting existing trust bootstrapping approaches
into MEC-based IIoT systems. These key limitations include, the lack of
opportunity for a service consumer to interact with a lesser-known service over
a prolonged period of time to get a robust measure of its trustworthiness,
inability of service consumers to consistently interact with their peers to
receive reliable recommendations of the trustworthiness of a lesser-known
service as well as the impact of uneven context parameters in different MEC
environments causing uneven trust environments for trust evaluation. In
addition, the proposed approach also tackles the problem of data sparsity via
enabling knowledge sharing among different MEC environments within a given MEC
topology. To verify the effectiveness of the proposed approach, we carried out
a comprehensive evaluation on two real-world datasets suitably adjusted to
exhibit the context-dependent trust information accumulated in MEC environments
within a given MEC topology. The experimental results affirmed the
effectiveness of our approach and its suitability to bootstrap trustworthiness
of services in MEC-based IIoT systems.

</details>


### [33] [Cyber Risks to Next-Gen Brain-Computer Interfaces: Analysis and Recommendations](https://arxiv.org/abs/2508.12571)
*Tyler Schroder,Renee Sirbu,Sohee Park,Jessica Morley,Sam Street,Luciano Floridi*

Main category: cs.CR

TL;DR: This paper examines cybersecurity risks in brain-computer interfaces (BCIs) and provides technical/recommended safeguards for manufacturers and regulators to mitigate threats like data breaches and remote attacks, while developing a threat model to assess risk likelihood.


<details>
  <summary>Details</summary>
Motivation: BCIs enable personalized medicine but introduce novel attack vectors compromising patient safety (e.g., health/genetic data leaks, unauthorized movement control) and require balanced security guidance for stakeholders.

Method: Problem analysis of BCI-specific security risks through recommendations for technical controls (encryption, authentication, network segregation) and regulatory mandates, combined with a hypothetical average-case threat modeling framework.

Result: Generated implementation guidelines for BCI manufacturers to incorporate security features like non-surgical updates and encryption, along with a threat likelihood assessment prioritizing network-based compromise mitigation strategies.

Conclusion: Prioritizing network-centric security controls and mandatory regulatory standards is essential for BCI advancement, while the proposed threat model provides a structured approach to evaluating and addressing remote attack risks in medical cyber-physical systems.

Abstract: Brain-computer interfaces (BCIs) show enormous potential for advancing
personalized medicine. However, BCIs also introduce new avenues for
cyber-attacks or security compromises. In this article, we analyze the problem
and make recommendations for device manufacturers to better secure devices and
to help regulators understand where more guidance is needed to protect patient
safety and data confidentiality. Device manufacturers should implement the
prior suggestions in their BCI products. These recommendations help protect BCI
users from undue risks, including compromised personal health and genetic
information, unintended BCI-mediated movement, and many other cybersecurity
breaches. Regulators should mandate non-surgical device update methods, strong
authentication and authorization schemes for BCI software modifications,
encryption of data moving to and from the brain, and minimize network
connectivity where possible. We also design a hypothetical, average-case threat
model that identifies possible cybersecurity threats to BCI patients and
predicts the likeliness of risk for each category of threat. BCIs are at less
risk of physical compromise or attack, but are vulnerable to remote attack; we
focus on possible threats via network paths to BCIs and suggest technical
controls to limit network connections.

</details>


### [34] [Reducing False Positives with Active Behavioral Analysis for Cloud Security](https://arxiv.org/abs/2508.12584)
*Dikshant,Verma*

Main category: cs.CR

TL;DR: This paper proposes a validation-driven cloud security posture management approach using automated behavioral testing to reduce false positives by 93% on average through real-time exploitability evaluation without impacting cloud environments.


<details>
  <summary>Details</summary>
Motivation: Rule-based CSPM systems generate numerous false positives due to limited contextual understanding and reliance on static heuristics, wasting analyst time during manual validation of non-actionable alerts.

Method: The system employs lightweight automated probes constructed from open-source tools and penetration testing test cases to simulate adversarial attacks on misconfigured cloud assets, with controlled experiments conducted in a reproducible AWS environment.

Result: Achieved 93% average reduction in false positives across cloud misconfiguration and vulnerability alerts while maintaining low latency performance through real-time exploitability evaluation.

Conclusion: The framework demonstrates a scalable method to enhance CSPM accuracy and analyst productivity by prioritizing actionable insights over static rule dependencies, with modular architecture supporting multi-cloud environments beyond AWS.

Abstract: Rule-based cloud security posture management (CSPM) solutions are known to
produce a lot of false positives based on the limited contextual understanding
and dependence on static heuristics testing. This paper introduces a
validation-driven methodology that integrates active behavioral testing in
cloud security posture management solution(s) to evaluate the exploitability of
policy violations in real time. The proposed system employs lightweight and
automated probes, built from open-source tools, validation scripts, and
penetration testing test cases, to simulate adversarial attacks on
misconfigured or vulnerable cloud assets without any impact to the cloud
services or environment. For instance, cloud services may be flagged as
publicly exposed and vulnerable despite being protected by access control
layers, or secure policies, resulting in non-actionable alerts that consumes
analysts time during manual validation. Through controlled experimentation in a
reproducible AWS setup, we evaluated the reduction in false positive rates
across various misconfiguration and vulnerable alerts. Our findings indicate an
average reduction of 93\% in false positives. Furthermore, the framework
demonstrates low latency performance. These results demonstrate a scalable
method to improve detection accuracy and analyst productivity in large cloud
environments. While our evaluation focuses on AWS, the architecture is modular
and extensible to multi-cloud setups.

</details>


### [35] [UAV Individual Identification via Distilled RF Fingerprints-Based LLM in ISAC Networks](https://arxiv.org/abs/2508.12597)
*Haolin Zheng,Ning Gao,Donghong Cai,Shi Jin,Michail Matthaiou*

Main category: cs.CR

TL;DR: The paper proposes a dynamic knowledge distillation framework for UAV ID identification using a modified GPT-2 and Lite-HRNet, achieving 98.38% accuracy with low parameters and response time.


<details>
  <summary>Details</summary>
Motivation: Accurate UAV identification in complex outdoor environments is essential for security surveillance in low-altitude integrated sensing and communication networks, but existing models lack efficiency and effectiveness.

Method: A modified GPT-2 model forms the RFF-LLM framework, followed by dynamic KD with proximal policy optimization (PPO) to compress the model and transfer knowledge to Lite-HRNet.

Result: Experiments on the DRFF-R1 dataset show 98.38% ID identification accuracy, 0.15 million parameters, and 2.74 ms response time, outperforming benchmarks.

Conclusion: The proposed framework combines LLMs and dynamic KD for high-accuracy, low-overhead UAV ID identification in ISAC networks, demonstrating significant performance improvements.

Abstract: Unmanned aerial vehicle (UAV) individual (ID) identification is a critical
security surveillance strategy in low-altitude integrated sensing and
communication (ISAC) networks. In this paper, we propose a novel dynamic
knowledge distillation (KD)-enabled wireless radio frequency fingerprint large
language model (RFF-LLM) framework for UAV ID identification. First, we propose
an RFF-LLM framework based on the modified GPT-2 model to improve the
identification accuracy in complex outdoor environments. Then, considering the
parameter overhead of the RFF-LLM, we design a dynamic KD strategy to compress
the model. Specifically, the proximal policy optimization (PPO) algorithm is
employed to dynamically adjust the distillation temperature, overcoming the
local optimum dilemma inherent in static KD. As a next step, the knowledge of
the RFF-LLM is adequately transferred to the lightweight Lite-HRNet model.
Finally, our experiments are conducted based on the self-built drone RFF
dataset of Release one, namely DRFF-R1, by collecting the I/Q signals of 20
commercial UAVs in channel 149. The experiment results show that the proposed
framework achieves 98.38\% ID identification accuracy with merely 0.15 million
parameters and 2.74 ms response time, which outperforms the benchmarks.

</details>


### [36] [Consiglieres in the Shadow: Understanding the Use of Uncensored Large Language Models in Cybercrimes](https://arxiv.org/abs/2508.12622)
*Zilong Lin,Zichuan Li,Xiaojing Liao,XiaoFeng Wang*

Main category: cs.CR

TL;DR: This paper introduces the first systematic study to identify over 11,000 uncensored large language models (ULLMs) via graph-based deep learning on open-source LLM relationships, revealing their widespread use in malicious applications including harmful content generation and criminal code distribution.


<details>
  <summary>Details</summary>
Motivation: Prior research has identified ULLMs as critical backends for malicious services, yet no systematic method existed to distinguish them among thousands of open-source LLMs. This threat is amplified by their ability to evolve through model fine-tuning, merging, or compression while evading detection.

Method: Created a knowledge graph modeling interactions between LLMs and harmful data sources (e.g., datasets, merging techniques). Trained graph-based deep learning models on a small set of labeled ULLMs and uncensored datasets to identify relationships indicative of uncensored content.

Result: Discoveries include: 11,000+ ULLMs identified, 19 million downloads for one model, integration into apps for child pornography and erotic role-play, underground forums distributing ULLM construction scripts, and evidence of fine-tuning chains creating harmful capabilities.

Conclusion: ULLMs are being weaponized at scale by criminal actors, creating urgent risks for AI safety. Automated methods to detect these models are proving effective, but the research underscores the need for proactive mitigation strategies to prevent further exploitation.

Abstract: The advancement of AI technologies, particularly Large Language Models
(LLMs), has transformed computing while introducing new security and privacy
risks. Prior research shows that cybercriminals are increasingly leveraging
uncensored LLMs (ULLMs) as backends for malicious services. Understanding these
ULLMs has been hindered by the challenge of identifying them among the vast
number of open-source LLMs hosted on platforms like Hugging Face. In this
paper, we present the first systematic study of ULLMs, overcoming this
challenge by modeling relationships among open-source LLMs and between them and
related data, such as fine-tuning, merging, compressing models, and using or
generating datasets with harmful content. Representing these connections as a
knowledge graph, we applied graph-based deep learning to discover over 11,000
ULLMs from a small set of labeled examples and uncensored datasets.
  A closer analysis of these ULLMs reveals their alarming scale and usage. Some
have been downloaded over a million times, with one over 19 million installs.
These models -- created through fine-tuning, merging, or compression of other
models -- are capable of generating harmful content, including hate speech,
violence, erotic material, and malicious code. Evidence shows their integration
into hundreds of malicious applications offering services like erotic
role-play, child pornography, malicious code generation, and more. In addition,
underground forums reveal criminals sharing techniques and scripts to build
cheap alternatives to commercial malicious LLMs. These findings highlight the
widespread abuse of LLM technology and the urgent need for effective
countermeasures against this growing threat.

</details>


### [37] [MPOCryptoML: Multi-Pattern based Off-Chain Crypto Money Laundering Detection](https://arxiv.org/abs/2508.12641)
*Yasaman Samadi,Hai Dong,Xiaoyu Xia*

Main category: cs.CR

TL;DR: MPOCryptoML is a multi-pattern detection model for off-chain cryptocurrency money laundering, improving detection accuracy by addressing diverse laundering patterns through personalized algorithms and integrated evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: Existing graph neural network models for cryptocurrency laundering detection lack explicit design to handle diverse off-chain patterns, leading to under-detection and reduced accuracy due to unique transaction obfuscation techniques.

Method: Developed a multi-source Personalized PageRank algorithm for random patterns, two timestamp/weight analysis algorithms to detect structured patterns (fan-in/fan-out, bipartite, gather-scatter, stack), logistic regression to model pattern correlations, and an anomaly score function for risk evaluation.

Result: Demonstrated 9.13% higher precision, 10.16% higher recall, 7.63% higher F1-score, and 10.19% higher accuracy across Elliptic++, Ethereum fraud, and Wormhole datasets compared to existing methods.

Conclusion: MPOCryptoML effectively addresses multiple laundering patterns in high-volume crypto networks, achieving state-of-the-art performance while maintaining detection efficiency for systematized account risk ranking.

Abstract: Recent advancements in money laundering detection have demonstrated the
potential of using graph neural networks to capture laundering patterns
accurately. However, existing models are not explicitly designed to detect the
diverse patterns of off-chain cryptocurrency money laundering. Neglecting any
laundering pattern introduces critical detection gaps, as each pattern reflects
unique transactional structures that facilitate the obfuscation of illicit fund
origins and movements. Failure to account for these patterns may result in
under-detection or omission of specific laundering activities, diminishing
model accuracy and allowing schemes to bypass detection. To address this gap,
we propose the MPOCryptoML model to effectively detect multiple laundering
patterns in cryptocurrency transactions. MPOCryptoML includes the development
of a multi-source Personalized PageRank algorithm to identify random laundering
patterns. Additionally, we introduce two novel algorithms by analyzing the
timestamp and weight of transactions in high-volume financial networks to
detect various money laundering structures, including fan-in, fan-out,
bipartite, gather-scatter, and stack patterns. We further examine correlations
between these patterns using a logistic regression model. An anomaly score
function integrates results from each module to rank accounts by anomaly score,
systematically identifying high-risk accounts. Extensive experiments on public
datasets including Elliptic++, Ethereum fraud detection, and Wormhole
transaction datasets validate the efficacy and efficiency of MPOCryptoML.
Results show consistent performance gains, with improvements up to 9.13% in
precision, up to 10.16% in recall, up to 7.63% in F1-score, and up to 10.19% in
accuracy.

</details>


### [38] [Unlearning Comparator: A Visual Analytics System for Comparative Evaluation of Machine Unlearning Methods](https://arxiv.org/abs/2508.12730)
*Jaeung Lee,Suhyeon Yu,Yurim Jang,Simon S. Woo,Jaemin Jo*

Main category: cs.CR

TL;DR: This paper introduces Unlearning Comparator, a visual analytics system for systematically evaluating Machine Unlearning (MU) methods through model comparison and membership inference attack simulation.


<details>
  <summary>Details</summary>
Motivation: Researchers face challenges in analyzing MU methods due to reliance on aggregate metrics and ad-hoc evaluations, making it difficult to assess trade-offs in accuracy, efficiency, and privacy.

Method: The system supports (1) model comparison at class/instance/layer levels between unlearned models and retrained baselines, and (2) privacy evaluation via membership inference attack simulations to test data removal effectiveness.

Result: Case study evaluations demonstrate the system helps users understand model behavior changes after unlearning and gain actionable insights for improving MU techniques.

Conclusion: Unlearning Comparator addresses critical gaps in MU evaluation by enabling systematic analysis of accuracy, efficiency, and privacy through visual analytics and attack simulations.

Abstract: Machine Unlearning (MU) aims to remove target training data from a trained
model so that the removed data no longer influences the model's behavior,
fulfilling "right to be forgotten" obligations under data privacy laws. Yet, we
observe that researchers in this rapidly emerging field face challenges in
analyzing and understanding the behavior of different MU methods, especially in
terms of three fundamental principles in MU: accuracy, efficiency, and privacy.
Consequently, they often rely on aggregate metrics and ad-hoc evaluations,
making it difficult to accurately assess the trade-offs between methods. To
fill this gap, we introduce a visual analytics system, Unlearning Comparator,
designed to facilitate the systematic evaluation of MU methods. Our system
supports two important tasks in the evaluation process: model comparison and
attack simulation. First, it allows the user to compare the behaviors of two
models, such as a model generated by a certain method and a retrained baseline,
at class-, instance-, and layer-levels to better understand the changes made
after unlearning. Second, our system simulates membership inference attacks
(MIAs) to evaluate the privacy of a method, where an attacker attempts to
determine whether specific data samples were part of the original training set.
We evaluate our system through a case study visually analyzing prominent MU
methods and demonstrate that it helps the user not only understand model
behaviors but also gain insights that can inform the improvement of MU methods.

</details>


### [39] [Efficient and Verifiable Privacy-Preserving Convolutional Computation for CNN Inference with Untrusted Clouds](https://arxiv.org/abs/2508.12832)
*Jinyu Lu,Xinrong Sun,Yunting Tao,Tong Ji,Fanyu Kong,Guoqiang Yang*

Main category: cs.CR

TL;DR: This paper presents a novel verifiable privacy-preserving scheme for CNN convolutional layers, improving efficiency and enabling secure computation offloading to untrusted cloud servers with accuracy maintenance and results verification.


<details>
  <summary>Details</summary>
Motivation: Privacy leakage risks in MLaaS systems using CNNs due to untrusted cloud servers and inefficiencies in existing encryption methods like homomorphic encryption for convolution operations.

Method: A new encryption scheme optimized for CNN convolutional layers with verifiable results via a mechanism ensuring detection correctness probability of at least $1-rac{1}{|Z|}$.

Result: Achieved 26√ó-87√ó speedups compared to plaintext models across 10 datasets and various CNN architectures while preserving accuracy and enabling verification.

Conclusion: The proposed scheme effectively addresses privacy-utility-efficiency tradeoffs in MLaaS, providing a verifiable, high-speed solution for secure CNN computation offloading in resource-constrained scenarios.

Abstract: The widespread adoption of convolutional neural networks (CNNs) in
resource-constrained scenarios has driven the development of Machine Learning
as a Service (MLaaS) system. However, this approach is susceptible to privacy
leakage, as the data sent from the client to the untrusted cloud server often
contains sensitive information. Existing CNN privacy-preserving schemes, while
effective in ensuring data confidentiality through homomorphic encryption and
secret sharing, face efficiency bottlenecks, particularly in convolution
operations. In this paper, we propose a novel verifiable privacy-preserving
scheme tailored for CNN convolutional layers. Our scheme enables efficient
encryption and decryption, allowing resource-constrained clients to securely
offload computations to the untrusted cloud server. Additionally, we present a
verification mechanism capable of detecting the correctness of the results with
a success probability of at least $1-\frac{1}{\left|Z\right|}$. Extensive
experiments conducted on 10 datasets and various CNN models demonstrate that
our scheme achieves speedups ranging $26 \times$ ~ $\ 87\times$ compared to the
original plaintext model while maintaining accuracy.

</details>


### [40] [The covering radius of Butson Hadamard codes for the homogeneous metric](https://arxiv.org/abs/2508.12859)
*Xingxing Xu,Minjia Shi,Patrick Sole*

Main category: cs.CR

TL;DR: This paper studies covering radius bounds for Butson Hadamard codes under a homogeneous metric, using orthogonal arrays and bent sequences.


<details>
  <summary>Details</summary>
Motivation: The homogeneous metric (unique for Quasi Frobenius rings) enables advanced analysis of code performance, generalizing Hamming metric results and improving error correction efficiency.

Method: 1) Derives upper bound via orthogonal array argument
2) Proves lower bound using existence of bent sequences (Shi et al. 2022)
3) Compares bounds with Armario et al.'s Hamming metric results (2025)

Result: Established upper/lower bounds for covering radius of Butson Hadamard codes under homogeneous metric. Generalized 2025's Hamming bound as the lower limit.

Conclusion: The derived lower bound is a significant generalization of Hamming metric results, enhancing the understanding of Butson code efficiency in phase space applications.

Abstract: Butson matrices are complex Hadamard matrices with entries in the complex
roots of unity of given order. There is an interesting code in phase space
related to this matrix (Armario et al. 2023). We study the covering radius of
Butson Hadamard codes for the homogeneous metric, a metric defined uniquely, up
to scaling, for a commutative ring alphabet that is Quasi Frobenius. An upper
bound is derived by an orthogonal array argument. A lower bound relies on the
existence of bent sequences in the sense of (Shi et al. 2022). This latter
bound generalizes a bound of (Armario et al. 2025) for the Hamming metric.

</details>


### [41] [Supporting Socially Constrained Private Communications with SecureWhispers](https://arxiv.org/abs/2508.12870)
*Vinod Khandkar,Kieron Ivy Turk,Ehsan Toreini,Nishanth Sastry*

Main category: cs.CR

TL;DR: This paper introduces a device-to-device private communication method using physical phone shakes to generate shared secrets without network data transmission. It proposes three applications (message obfuscation, trust delegation, encrypted beacons) and presents an Android app implementation.


<details>
  <summary>Details</summary>
Motivation: Modern communication methods relying on internet infrastructure face limitations in accessibility and security, leaving users vulnerable in constrained public environments where trust in third-party services is unjustified.

Method: Devices generate shared secrets through coordinated shaking, extracting and conditioning physical randomness from motion sensors to produce 7.798 bits/byte of key material using no network communication.

Result: Demonstrated three privacy application protypes with successful Android message obfuscation implementation, plus empirical research on usability and integration challenges for mainstream adoption.

Conclusion: The shake-based cryptographic protocol establishes secure device pairing without infrastructure dependencies, offering practical solutions for privacy-constrained contexts while suggesting paths for broader implementation in safety-critical communication systems.

Abstract: Rapidly changing social norms and national, legal, and political conditions
socially constrain people from discussing sensitive topics such as sexuality or
religion. Such constrained, vulnerable minorities are often worried about
inadvertent information disclosure and may be unsure about the extent to which
their communications are being monitored in public or semi-public spaces like
workplaces or cafes. Personal devices extend trust to the digital domain,
making it desirable to have strictly private communication between trusted
devices. Currently, messaging services like WhatsApp provide alternative means
for exchanging sensitive private information, while personal safety apps such
as Noonlight enable private signaling. However, these rely on third-party
mechanisms for secure and private communication, which may not be accessible
for justifiable reasons, such as insecure internet access or companion device
connections. In these cases, it is challenging to achieve communication that is
strictly private between two devices instead of user accounts without any
dependency on third-party infrastructure. The goal of this paper is to support
private communications by setting up a shared secret between two or more
devices without sending any data on the network. We develop a method to create
a shared secret between phones by shaking them together. Each device extracts
the shared randomness from the shake, then conditions the randomness to 7.798
bits per byte of key material. This paper proposes three different applications
of this generated shared secret: message obfuscation, trust delegation, and
encrypted beacons. We have implemented the message obfuscation on Android as an
independent app that can be used for private communication with trusted
contacts. We also present research on the usability, design considerations, and
further integration of these tools in mainstream services.

</details>


### [42] [SecFSM: Knowledge Graph-Guided Verilog Code Generation for Secure Finite State Machines in Systems-on-Chip](https://arxiv.org/abs/2508.12910)
*Ziteng Hu,Yingjie Xia,Xiyuan Chen,Li Kuang*

Main category: cs.CR

TL;DR: The paper proposes SecFSM, a method using a security knowledge graph to guide LLMs in generating secure Verilog code for FSMs in SoCs. It achieves a 21/25 pass rate on security test cases via DeepSeek-R1 evaluation.


<details>
  <summary>Details</summary>
Motivation: Traditional manual Verilog coding for FSMs is time-consuming, while LLM-generated code has security vulnerabilities in critical applications. Current practices lack effective security assurance mechanisms for AI-generated control logic.

Method: 1) Constructs FSM Security Knowledge Graph (FSKG) with domain-specific security patterns. 2) Analyzes requirements to identify potential vulnerabilities. 3) Retrieves security knowledge from FSKG based on vulnerability list. 4) Creates security prompts to guide LLM code generation following these security constraints.

Result: Built comprehensive dataset combining academic/industrial cases. Achieved state-of-the-art security performance with 84% test case pass rate (21/25) using DeepSeek-R1 evaluation. Outperformed existing baseline methods in all measured security metrics.

Conclusion: SecFSM demonstrates effectiveness in addressing security vulnerabilities in LLM-generated FSM implementations by integrating security knowledge graphs with prompting techniques, establishing a new direction for secure hardware synthesis with AI.

Abstract: Finite State Machines (FSMs) play a critical role in implementing control
logic for Systems-on-Chip (SoC). Traditionally, FSMs are implemented by
hardware engineers through Verilog coding, which is often tedious and
time-consuming. Recently, with the remarkable progress of Large Language Models
(LLMs) in code generation, LLMs have been increasingly explored for automating
Verilog code generation. However, LLM-generated Verilog code often suffers from
security vulnerabilities, which is particularly concerning for
security-sensitive FSM implementations. To address this issue, we propose
SecFSM, a novel method that leverages a security-oriented knowledge graph to
guide LLMs in generating more secure Verilog code. Specifically, we first
construct a FSM Security Knowledge Graph (FSKG) as an external aid to LLMs.
Subsequently, we analyze users' requirements to identify vulnerabilities and
get a list of vulnerabilities in the requirements. Then, we retrieve knowledge
from FSKG based on the vulnerabilities list. Finally, we construct security
prompts based on the security knowledge for Verilog code generation. To
evaluate SecFSM, we build a dedicated dataset collected from academic datasets,
artificial datasets, papers, and industrial cases. Extensive experiments
demonstrate that SecFSM outperforms state-of-the-art baselines. In particular,
on a benchmark of 25 security test cases evaluated by DeepSeek-R1, SecFSM
achieves an outstanding pass rate of 21/25.

</details>


### [43] [Prescriptive Zero Trust- Assessing the impact of zero trust on cyber attack prevention](https://arxiv.org/abs/2508.12953)
*Samuel Aiello*

Main category: cs.CR

TL;DR: This paper proposes a data-driven methodology to quantify cybersecurity maturity aligned with Zero Trust Architecture (ZTA) principles, defining a four-tier model (Initial to Optimized) and key technical controls to assess organizational ZTA adoption.


<details>
  <summary>Details</summary>
Motivation: The proliferation of sophisticated cyber threats and the lack of clear, quantifiable guidelines for ZTA implementation motivate the need for a standardized framework to measure cybersecurity maturity and transformation progress.

Method: The research evaluates the integration depth of core ZTA components (identity verification, microsegmentation, data encryption, analytics, orchestration) against industry best practices to develop a maturity scale and prescriptive technical controls.

Result: A four-tier ZTA maturity model and technical control metrics were established, enabling organizations to evaluate their security posture across identity, segmentation, encryption, analytics, and orchestration domains.

Conclusion: The four-tier model provides a structured, capability-driven roadmap for ZTA implementation, helping organizations systematically enhance their cyber resilience against evolving threats through measurable progress tracking.

Abstract: Increasingly sophisticated and varied cyber threats necessitate ever
improving enterprise security postures. For many organizations today, those
postures have a foundation in the Zero Trust Architecture. This strategy sees
trust as something an enterprise must not give lightly or assume too broadly.
Understanding the ZTA and its numerous controls centered around the idea of not
trusting anything inside or outside the network without verification, will
allow organizations to comprehend and leverage this increasingly common
paradigm. The ZTA, unlike many other regulatory frameworks, is not tightly
defined. The research assesses the likelihood of quantifiable guidelines that
measure cybersecurity maturity for an enterprise organization in relation to
ZTA implementation. This is a new, data driven methodology for quantifying
cyber resilience enabled by the adoption of Zero Trust principles to
pragmatically address the critical need of organizations. It also looks at the
practical aspects ZTA has on capabilities in deterring cyberattacks on a
network. The outcomes of this research define a prescriptive set of key
technical controls across identity verification, microsegmentation, data
encryption, analytics, and orchestration that characterize the comprehensive
ZTA deployment. By evaluating the depth of integration for each control
component and aligning to industry best practices, the study's results help
assess an organization's ZTA maturity level on a scale from Initial to
Optimized adoption. The research's resultant four tier model demarcates phases
for an organization on its security transformation journey, with each tier
adding to the capability of the last.

</details>


### [44] [AuthenTree: A Scalable MPC-Based Distributed Trust Architecture for Chiplet-based Heterogeneous Systems](https://arxiv.org/abs/2508.13033)
*Ishraq Tashdid,Tasnuva Farheen,Sazadur Rahman*

Main category: cs.CR

TL;DR: AuthenTree is a distributed authentication framework using multi-party computation (MPC) to secure chiplet-based SiP systems without centralized trust or hardware. It achieves minimal overhead and fast authentication, improving chiplet security.


<details>
  <summary>Details</summary>
Motivation: Multi-vendor chiplet integration fragments the supply chain and introduces security risks (cloning, overproduction, substitution). Existing solutions depend on trusted integrators or centralized hardware, creating vulnerabilities via data exposure or single points of failure.

Method: AuthenTree employs a tree-based distributed authentication architecture using MPC, eliminating dedicated security hardware. Validation occurs via integrator chiplets without revealing raw signatures, distributing trust securely.

Result: AuthenTree demonstrates <0.5% power overhead, 0.48% area (7,000 sq-micrometers), and <1Œºs authentication latency. In some cases, it improves over prior art by 700√ó, validated across five SiP benchmarks.

Conclusion: AuthenTree establishes a scalable, robust, and efficient security framework for chiplet-based SiP systems, aligning with zero-trust principles while maintaining low overhead for AI/High Performance Computing applications.

Abstract: The rapid adoption of chiplet-based heterogeneous integration is reshaping
semiconductor design by enabling modular, scalable, and faster time-to-market
solutions for AI and high-performance computing. However, multi-vendor assembly
in post-fabrication environments fragments the supply chain and exposes SiP
systems to serious security threats, including cloning, overproduction, and
chiplet substitution. Existing authentication solutions depend on trusted
integrators or centralized security anchors, which can expose sensitive data or
create single points of failure. We introduce AuthenTree, a distributed
authentication framework that leverages multi-party computation (MPC) in a
scalable tree-based architecture, removing the need for dedicated security
hardware or centralized trust. AuthenTree enables secure chiplet validation
without revealing raw signatures, distributing trust across multiple integrator
chiplets. Our evaluation in five SiP benchmarks demonstrates that AuthenTree
imposes minimal overhead, with an area as low as 0.48% (7,000 sq-micrometers),
an overhead power under 0.5%, and an authentication latency below 1
microsecond, surpassing previous work in some cases by 700 times. These results
establish AuthenTree as an efficient, robust, and scalable solution for
next-generation chiplet-based security in zero-trust SiP environments.

</details>


### [45] [MAJIC: Markovian Adaptive Jailbreaking via Iterative Composition of Diverse Innovative Strategies](https://arxiv.org/abs/2508.13048)
*Weiwei Qi,Shuo Shao,Wei Gu,Tianhang Zheng,Puning Zhao,Zhan Qin,Kui Ren*

Main category: cs.CR

TL;DR: MAJIC is an adaptive jailbreaking framework for black-box LLMs that uses a Markov chain-based strategy selection from a refined pool of disguise methods, achieving high success rates with minimal queries.


<details>
  <summary>Details</summary>
Motivation: Traditional jailbreaking methods for LLMs are limited by static, non-adaptive prompts or rigid strategy combinations, reducing their effectiveness against diverse models. The authors aim to enhance adaptability and generalization to overcome these limitations.

Method: MAJIC combines a curated 'Disguise Strategy Pool' with a Markovian framework, dynamically adapting transitions between strategies based on real-time attack feedback to optimize pathways for eliciting harmful content.

Result: MAJIC achieves over 90	extbackpercent attack success rate on GPT-4o and Gemini-2.0-flash with an average of fewer than 15 queries per attempt, outperforming existing black-box jailbreaking techniques significantly.

Conclusion: MAJIC's adaptive Markovian strategy composition improves both effectiveness and efficiency in jailbreaking LLMs, demonstrating superior generalization and practical attack performance against state-of-the-art models.

Abstract: Large Language Models (LLMs) have exhibited remarkable capabilities but
remain vulnerable to jailbreaking attacks, which can elicit harmful content
from the models by manipulating the input prompts. Existing black-box
jailbreaking techniques primarily rely on static prompts crafted with a single,
non-adaptive strategy, or employ rigid combinations of several underperforming
attack methods, which limits their adaptability and generalization. To address
these limitations, we propose MAJIC, a Markovian adaptive jailbreaking
framework that attacks black-box LLMs by iteratively combining diverse
innovative disguise strategies. MAJIC first establishes a ``Disguise Strategy
Pool'' by refining existing strategies and introducing several innovative
approaches. To further improve the attack performance and efficiency, MAJIC
formulate the sequential selection and fusion of strategies in the pool as a
Markov chain. Under this formulation, MAJIC initializes and employs a Markov
matrix to guide the strategy composition, where transition probabilities
between strategies are dynamically adapted based on attack outcomes, thereby
enabling MAJIC to learn and discover effective attack pathways tailored to the
target model. Our empirical results demonstrate that MAJIC significantly
outperforms existing jailbreak methods on prominent models such as GPT-4o and
Gemini-2.0-flash, achieving over 90\% attack success rate with fewer than 15
queries per attempt on average.

</details>


### [46] [VerilogLAVD: LLM-Aided Rule Generation for Vulnerability Detection in Verilog](https://arxiv.org/abs/2508.13092)
*Xiang Long,Yingjie Xia,Xiyuan Chen,Li Kuang*

Main category: cs.CR

TL;DR: VerilogLAVD is an LLM-aided approach for Verilog vulnerability detection that uses a structured graph representation (VeriPG) to improve accuracy over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current early-stage hardware vulnerability detection requires specialized security expertise and existing LLM-based solutions produce inconsistent results by failing to capture Verilog code structure.

Method: We introduce VeriPG, a unified graph representation merging AST syntactic features with control-flow and data-dependency semantic graphs. LLMs generate detection rules from CWE descriptions to guide graph traversal for vulnerability identification.

Result: VerilogLAVD achieves 0.54 F1-score on 77 Verilog designs across 12 CWE types, surpassing LLM-only baselines by +31% and LLM with external knowledge baselines by +27% in F1-score improvements.

Conclusion: This work demonstrates that integrating LLM-generated rules with structured graph representations (VeriPG) enables more effective and explainable hardware vulnerability detection in Verilog code than prior approaches.

Abstract: Timely detection of hardware vulnerabilities during the early design stage is
critical for reducing remediation costs. Existing early detection techniques
often require specialized security expertise, limiting their usability. Recent
efforts have explored the use of large language models (LLMs) for Verilog
vulnerability detection. However, LLMs struggle to capture the structure in
Verilog code, resulting in inconsistent detection results. To this end, we
propose VerilogLAVD, the first LLM-aided graph traversal rule generation
approach for Verilog vulnerability detection. Our approach introduces the
Verilog Property Graph (VeriPG), a unified representation of Verilog code. It
combines syntactic features extracted from the abstract syntax tree (AST) with
semantic information derived from control flow and data dependency graphs. We
leverage LLMs to generate VeriPG-based detection rules from Common Weakness
Enumeration (CWE) descriptions. These rules guide the rule executor that
traversal VeriPG for potential vulnerabilities. To evaluate VerilogLAVD, we
build a dataset collected from open-source repositories and synthesized data.
In our empirical evaluation on 77 Verilog designs encompassing 12 CWE types,
VerilogLAVD achieves an F1-score of 0.54. Compared to the LLM-only and LLM with
external knowledge baselines, VerilogLAVD improves F1-score by 0.31 and 0.27,
respectively.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [47] [Benchmark Dataset Generation and Evaluation for Excel Formula Repair with LLMs](https://arxiv.org/abs/2508.11715)
*Ananya Singha,Harshita Sahijwani,Walt Williams,Emmanuel Aboah Boateng,Nick Hausman,Miguel Di Luca,Keegan Choudhury,Chaya Binet,Vu Le,Tianwei Chen,Oryan Rokeah Chen,Sulaiman Vesal,Sadid Hasan*

Main category: cs.SE

TL;DR: This paper introduces a novel benchmark dataset and context-aware baseline technique to address the lack of training and evaluation data for Excel formula repair using large language models (LLMs). The dataset includes 618 high-quality samples of runtime errors.


<details>
  <summary>Details</summary>
Motivation: Excel's complexity leads to runtime errors for novice users, and while LLMs can explain errors, automating semantic corrections remains challenging due to the absence of high-quality, comprehensive datasets for training and evaluation.

Method: The authors developed a data generation pipeline using curated seed samples from online forums, integrated few-shot prompting, LLM-as-a-Judge validation framework, and execution-based checks to ensure data quality. They also proposed a context-aware baseline technique that leverages both faulty formulas and spreadsheet context with LLMs.

Result: Generated a 618-sample benchmark dataset covering common runtime errors. Evaluated LLMs (GPT-4o, GPT-4.1, Phi-3, Mistral) using execution-based metrics. Manual annotation confirmed dataset quality, and error/function distribution insights were provided.

Conclusion: The proposed scalable methodology effectively creates high-quality evaluation benchmarks for Excel formula repair and can be adapted to similar code repair tasks in other underserved programming languages.

Abstract: Excel is a pervasive yet often complex tool, particularly for novice users,
where runtime errors arising from logical mistakes or misinterpretations of
functions pose a significant challenge. While large language models (LLMs)
offer promising assistance by explaining formula errors, the automated
correction of these semantic runtime errors remains an open problem. A primary
challenge to advancing models for such scenarios is the severe lack of
high-quality, comprehensive datasets for training and rigorous evaluation. This
paper addresses this gap by introducing a novel approach for constructing a
benchmark dataset specifically designed for Excel formula repair. We propose a
data generation pipeline, which leverages a small set of curated seed samples
from online forums to synthetically expand the dataset. Our pipeline integrates
few-shot prompting with LLMs and employs a robust \textit{LLM-as-a-Judge}
validation framework, combined with execution-based checks to ensure the
correctness and semantic fidelity of the generated data. This process produced
a benchmark dataset of 618 high-quality samples, covering common runtime
errors. Furthermore, we propose a context-aware baseline technique for Excel
formula repair that utilizes LLMs to leverage both the faulty formula, and
relevant spreadsheet context. We evaluate the performance of various LLMs
(GPT-4o, GPT-4.1, Phi-3, Mistral) on our newly generated benchmark using
execution-based metrics. Our analysis demonstrates the dataset's quality
through manual annotation and provides insights into error and function
distributions. The proposed generation methodology is highly scalable and can
be readily adapted to create evaluation benchmarks for similar code repair
tasks in other low-resource programming languages.

</details>


### [48] [WIP: Leveraging LLMs for Enforcing Design Principles in Student Code: Analysis of Prompting Strategies and RAG](https://arxiv.org/abs/2508.11717)
*Dhruv Kolhatkar,Soubhagya Akkena,Edward F. Gehringer*

Main category: cs.SE

TL;DR: This paper investigates integrating Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) to develop an automated code-review tool for student software projects, focusing on evaluating adherence to object-oriented design principles like SOLID and DRY. Preliminary results show improved code quality potential with plans for model refinement and expanded principle support.


<details>
  <summary>Details</summary>
Motivation: Traditional manual code-review processes for teaching software design principles are labor-intensive and often lack scalability to effectively teach and assess best practices in large classroom settings.

Method: The approach combines LLMs with RAG to analyze student code against object-oriented design principles. The study evaluates different prompting strategies and integrates RAG for context-aware feedback.

Result: Preliminary implementation demonstrates promising improvements in student code quality related to design principle adherence.

Conclusion: The integration of LLMs and RAG shows potential for creating scalable, automated code-review tools. Future work will focus on enhancing model accuracy and expanding support for additional software design principles.

Abstract: This work-in-progress research-to-practice paper explores the integration of
Large Language Models (LLMs) into the code-review process for open-source
software projects developed in computer science and software engineering
courses. The focus is on developing an automated feedback tool that evaluates
student code for adherence to key object-oriented design principles, addressing
the need for more effective and scalable methods to teach software design best
practices. The innovative practice involves leveraging LLMs and
Retrieval-Augmented Generation (RAG) to create an automated feedback system
that assesses student code for principles like SOLID, DRY, and design patterns.
It analyzes the effectiveness of various prompting strategies and the RAG
integration. Preliminary findings show promising improvements in code quality.
Future work will aim to improve model accuracy and expand support for
additional design principles.

</details>


### [49] [Rethinking Autonomy: Preventing Failures in AI-Driven Software Engineering](https://arxiv.org/abs/2508.11824)
*Satyam Kumar Navneet,Joydeep Chandra*

Main category: cs.SE

TL;DR: This paper introduces the SAFE-AI Framework to address risks in LLM-assisted code generation, proposing safety, auditability, feedback, and explainability mechanisms while aligning with AI governance regulations.


<details>
  <summary>Details</summary>
Motivation: LLMs in software engineering present risks like insecure code generation, hallucinations, and lack of governance, exemplified by incidents such as the Replit database deletion, necessitating robust safety and accountability measures.

Method: Analyzes challenges in LLM code generation via vulnerability inheritance, overtrust, and misinterpretation; proposes SAFE-AI with guardrails, sandboxing, runtime verification, human-in-the-loop systems, and a new AI behavior taxonomy (suggestive/generative/autonomous/destructive).

Result: Provides a comprehensive taxonomy for AI behaviors in code generation, outlines open problems (e.g., standardized benchmark gaps), and suggests future directions like hybrid verification and proactive governance tools.

Conclusion: Establishes a roadmap for responsible AI integration in software engineering through technical framework components and regulatory alignment, emphasizing safe, transparent development practices aligned with the EU AI Act and AIDA.

Abstract: The integration of Large Language Models (LLMs) into software engineering has
revolutionized code generation, enabling unprecedented productivity through
promptware and autonomous AI agents. However, this transformation introduces
significant risks, including insecure code generation, hallucinated outputs,
irreversible actions, and a lack of transparency and accountability. Incidents
like the Replit database deletion underscore the urgent need for robust safety
and governance mechanisms. This paper comprehensively analyzes the inherent
challenges of LLM-assisted code generation, such as vulnerability inheritance,
overtrust, misinterpretation, and the absence of standardized validation and
rollback protocols. To address these, we propose the SAFE-AI Framework, a
holistic approach emphasizing Safety, Auditability, Feedback, and
Explainability. The framework integrates guardrails, sandboxing, runtime
verification, risk-aware logging, human-in-the-loop systems, and explainable AI
techniques to mitigate risks while fostering trust and compliance. We introduce
a novel taxonomy of AI behaviors categorizing suggestive, generative,
autonomous, and destructive actions to guide risk assessment and oversight.
Additionally, we identify open problems, including the lack of standardized
benchmarks for code specific hallucinations and autonomy levels, and propose
future research directions for hybrid verification, semantic guardrails, and
proactive governance tools. Through detailed comparisons of autonomy control,
prompt engineering, explainability, and governance frameworks, this paper
provides a roadmap for responsible AI integration in software engineering,
aligning with emerging regulations like the EU AI Act and Canada's AIDA to
ensure safe, transparent, and accountable AI-driven development.

</details>


### [50] [AI-Augmented CI/CD Pipelines: From Code Commit to Production with Autonomous Decisions](https://arxiv.org/abs/2508.11867)
*Mohammad Baqar,Saba Naqvi,Rajat Khanda*

Main category: cs.SE

TL;DR: The paper introduces AI-Augmented CI/CD Pipelines that use large language models (LLMs) and autonomous agents as policy-bounded co-pilots/decision makers to address latency and operational toil caused by human interventions in modern rapid software delivery. It outlines a reference architecture, decision taxonomy, trust-tier framework, evaluation methodology integrating DORA metrics and AI-specific indicators, and an industrial case study.


<details>
  <summary>Details</summary>
Motivation: Modern software delivery requiring faster deployments is hindered by human decision points related to flaky tests, rollback strategies, feature flags, and canary promotions, which create latency and operational toil despite mature CI/CD tooling.

Method: Proposes AI-augmented pipelines with agentic decision points governed by trust-tier frameworks and policy-as-code guardrails, using LLMs to act as progressive co-pilots and decision makers. Presents an evaluation methodology combining DORA metrics and AI-specific indicators.

Result: Five key contributions: (1) Reference architecture for agentic CI/CD integration, (2) Decision taxonomy with policy guardrails, (3) Trust-tier framework for staged autonomy, (4) Evaluation methodology combining traditional DevOps metrics with AI-specific indicators, (5) Industrial case study demonstrating migration of React 19 microservice to AI-augmented pipeline.

Conclusion: Discusses ethical considerations, verification, auditability, and validity threats related to AI autonomy in production systems, while charting a roadmap toward verifiable autonomy in software delivery pipelines.

Abstract: Modern software delivery has accelerated from quarterly releases to multiple
deployments per day. While CI/CD tooling has matured, human decision points
interpreting flaky tests, choosing rollback strategies, tuning feature flags,
and deciding when to promote a canary remain major sources of latency and
operational toil. We propose AI-Augmented CI/CD Pipelines, where large language
models (LLMs) and autonomous agents act as policy-bounded co-pilots and
progressively as decision makers. We contribute: (1) a reference architecture
for embedding agentic decision points into CI/CD, (2) a decision taxonomy and
policy-as-code guardrail pattern, (3) a trust-tier framework for staged
autonomy, (4) an evaluation methodology using DevOps Research and Assessment (
DORA) metrics and AI-specific indicators, and (5) a detailed industrial-style
case study migrating a React 19 microservice to an AI-augmented pipeline. We
discuss ethics, verification, auditability, and threats to validity, and chart
a roadmap for verifiable autonomy in production delivery systems.

</details>


### [51] [Clean Code, Better Models: Enhancing LLM Performance with Smell-Cleaned Dataset](https://arxiv.org/abs/2508.11958)
*Zhipeng Xue,Xiaoting Zhang,Zhipeng Gao,Xing Hu,Shan Gao,Xin Xia,Shanping Li*

Main category: cs.SE

TL;DR: This paper systematically analyzes code smells in benchmark datasets used for training Large Language Models (LLMs) and proposes SmellCC, an LLM-based tool to clean code smells, demonstrating how curated datasets improve LLM-generated code quality and downstream tasks like code completion and search.


<details>
  <summary>Details</summary>
Motivation: Most LLM research focuses on output quality (e.g., correctness), but ignores the impact of code smells in training datasets on software maintainability, readability, and LLM performance.

Method: 1) Developed SmellCC for code smell refactoring. 2) Conducted functional testing on 50 CodeSearchNet-Python repositories. 3) Fine-tuned LLMs with cleaned datasets. 4) Evaluated effects on code completion and code search tasks.

Result: Code smells were prevalent in LLM inputs and outputs; SmellCC improved refactoring correctness; models trained on cleaned datasets generated higher-quality code and showed performance gains in downstream tasks.

Conclusion: Code smells in training datasets significantly affect LLMs and downstream code tasks. SmellCC offers a practical solution for dataset curation, and findings provide actionable guidelines for researchers and practitioners to prioritize code quality in data preparation.

Abstract: The Large Language Models (LLMs) have demonstrated great potential in
code-related tasks. However, most research focuses on improving the output
quality of LLMs (e.g., correctness), and less attention has been paid to the
LLM input (e.g., the training code quality). Given that code smells are widely
existed in practice and can negatively impact software maintainability and
readability, this study takes the first systematic research to assess and
improve dataset quality in terms of code smells. In this work, we first conduct
a preliminary study to explore the presence of code smells in a popular
benchmark dataset (i.e., CodeSearchNet-Python}) and evaluate the output of
several popular LLMs (i.e., DeepSeek-Coder, CodeLlama, and MagiCoder),
revealing that code smell issues extensively exist in LLM's input (e.g.,
benchmark dataset) and output (e.g., generated code). We then conduct our
systematic research by taking three main steps: Firstly, we propose an
LLM-based code smell cleaning tool, named SmellCC, which automatically
refactors and removes code smells. To evaluate the correctness of the code
refactoring, we construct a test set of 50 repositories sourced from the
CodeSearchNet-Python benchmark for functional testing. Then we apply our
curated smell-cleaned dataset to fine-tune two LLMs (i.e., DeepSeek-V2 and
Qwen-Coder) to explore their potential for generating high-quality code.
Thirdly, we investigate the impact of code smells on two downstream tasks: code
completion and code search. Lastly, we derive several actionable implications
for software engineering researchers and industry practitioners from our
findings.

</details>


### [52] [How Much Can a Behavior-Preserving Changeset Be Decomposed into Refactoring Operations?](https://arxiv.org/abs/2508.11993)
*Kota Someya,Lei Chen,Michael J. Decker,Shinpei Hayashi*

Main category: cs.SE

TL;DR: The paper evaluates the effectiveness of decomposition techniques for behavior-preserving modifications using a dataset of functionally-equivalent method pairs, revealing significant coverage improvements with expanded refactoring operations but also highlighting unresolved gaps.


<details>
  <summary>Details</summary>
Motivation: Existing methods for separating behavior-preserving and behavior-altering code changes lack validation on how effectively behavior-preserving modifications can be decomposed into refactoring operations. Understanding and improving this decomposition is critical for tools that require accurate differentiation of code changes.

Method: The authors use a dataset of functionally-equivalent method pairs to quantitatively assess decomposition into refactoring operations. They first apply an existing refactoring detector and then compare its coverage with a 67-operation expanded refactoring set, analyzing unexplained differences to identify improvement opportunities.

Result: Only 33.9% of behavior-preserving changes were detected as refactorings by existing tools, but adding 67 new operations raised this coverage by over 128%. Further analysis pinpointed specific areas where detection remains incomplete, suggesting systematic enhancements may be feasible.

Conclusion: This initial validation demonstrates that expanding functionally-equivalent refactoring operations significantly improves coverage of behavior-preserving changes, while the existence of unexplained differences implies ongoing research potential to refine detection accuracy and address current limitations.

Abstract: Developers sometimes mix behavior-preserving modifications, such as
refactorings, with behavior-altering modifications, such as feature additions.
Several approaches have been proposed to support understanding such
modifications by separating them into those two parts. Such refactoring-aware
approaches are expected to be particularly effective when the
behavior-preserving parts can be decomposed into a sequence of more primitive
behavior-preserving operations, such as refactorings, but this has not been
explored. In this paper, as an initial validation, we quantify how much of the
behavior-preserving modifications can be decomposed into refactoring operations
using a dataset of functionally-equivalent method pairs. As a result, when
using an existing refactoring detector, only 33.9% of the changes could be
identified as refactoring operations. In contrast, when including 67 newly
defined functionally-equivalent operations, the coverage increased by over
128%. Further investigation into the remaining unexplained differences was
conducted, suggesting improvement opportunities.

</details>


### [53] [LinkAnchor: An Autonomous LLM-Based Agent for Issue-to-Commit Link Recovery](https://arxiv.org/abs/2508.12232)
*Arshia Akhavan,Alireza Hosseinpour,Abbas Heydarnoori,Mehdi Keshani*

Main category: cs.SE

TL;DR: LinkAnchor is an autonomous LLM-based agent for issue-to-commit link recovery that uses a lazy-access architecture to efficiently retrieve relevant context and identify target commits, achieving 60-262% improvement in Hit@1 scores over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based approaches for issue-to-commit link recovery face two major challenges: limited context windows of LLMs prevent handling of extensive data sources, and pairwise methods are impractical for large repositories with many commits. This paper aims to address these limitations to improve software traceability and project management.

Method: LinkAnchor employs a lazy-access architecture to dynamically retrieve only the most relevant contextual data (commits, issue comments, code files) without exceeding token limits, and automatically identifies target commits instead of exhaustively scoring every pair.

Result: LinkAnchor outperformed state-of-the-art methods by 60-262% in Hit@1 scores across case study projects, demonstrating significant improvements in link recovery accuracy. The tool is released publicly for GitHub and Jira-compatible platforms.

Conclusion: LinkAnchor offers a scalable and efficient solution for issue-to-commit traceability. The open-source tool, tested on GitHub and Jira, provides practical benefits for real-world software repositories while setting a new benchmark for this task.

Abstract: Issue-to-commit link recovery plays an important role in software
traceability and improves project management. However, it remains a challenging
task. A study on GitHub shows that only 42.2% of the issues are correctly
linked to their commits. This highlights the potential for further development
and research in this area. Existing studies have employed various AI/ML-based
approaches, and with the recent development of large language models,
researchers have leveraged LLMs to tackle this problem. These approaches suffer
from two main issues. First, LLMs are constrained by limited context windows
and cannot ingest all of the available data sources, such as long commit
histories, extensive issue comments, and large code repositories. Second, most
methods operate on individual issue-commit pairs; that is, given a single
issue-commit pair, they determine whether the commit resolves the issue. This
quickly becomes impractical in real-world repositories containing tens of
thousands of commits. To address these limitations, we present LinkAnchor, the
first autonomous LLM-based agent designed for issue-to-commit link recovery.
The lazy-access architecture of LinkAnchor enables the underlying LLM to access
the rich context of software, spanning commits, issue comments, and code files,
without exceeding the token limit by dynamically retrieving only the most
relevant contextual data. Additionally, LinkAnchor is able to automatically
pinpoint the target commit rather than exhaustively scoring every possible
candidate. Our evaluations show that LinkAnchor outperforms state-of-the-art
issue-to-commit link recovery approaches by 60-262% in Hit@1 score across all
our case study projects. We also publicly release LinkAnchor as a ready-to-use
tool, along with our replication package. LinkAnchor is designed and tested for
GitHub and Jira, and is easily extendable to other platforms.

</details>


### [54] ["My productivity is boosted, but ..." Demystifying Users' Perception on AI Coding Assistants](https://arxiv.org/abs/2508.12285)
*Yunbo Lyu,Zhou Yang,Jieke Shi,Jianming Chang,Yue Liu,David Lo*

Main category: cs.SE

TL;DR: This study examines developer valuations and criticisms of AI coding assistants through manual analysis of user reviews, revealing unmet needs and suggesting practical improvements for real-world tools.


<details>
  <summary>Details</summary>
Motivation: With AI coding assistants becoming prevalent, understanding developers' authentic perceptions and requirements is critical for improving tool design and addressing real-world software development challenges.

Method: Manually analyzed user reviews from 32 AI coding assistants (selected from 1,085 VS Code Marketplace extensions) to create taxonomy of user concerns and annotate attitudes toward specific features and tool performance.

Result: Identified 1,085 AI coding assistants (90% released in last two years) and distilled nuanced insights about developer demands for context-aware, customizable, and resource-efficient AI interactions across a comprehensive taxonomy of feedback.

Conclusion: Developers seek AI coding assistants that provide intelligent suggestions combined with contextual understanding and customization options, leading to five practical implications for enhancing tool effectiveness in practical software engineering workflows.

Abstract: This paper aims to explore fundamental questions in the era when AI coding
assistants like GitHub Copilot are widely adopted: what do developers truly
value and criticize in AI coding assistants, and what does this reveal about
their needs and expectations in real-world software development? Unlike
previous studies that conduct observational research in controlled and
simulated environments, we analyze extensive, first-hand user reviews of AI
coding assistants, which capture developers' authentic perspectives and
experiences drawn directly from their actual day-to-day work contexts. We
identify 1,085 AI coding assistants from the Visual Studio Code Marketplace.
Although they only account for 1.64% of all extensions, we observe a surge in
these assistants: over 90% of them are released within the past two years. We
then manually analyze the user reviews sampled from 32 AI coding assistants
that have sufficient installations and reviews to construct a comprehensive
taxonomy of user concerns and feedback about these assistants. We manually
annotate each review's attitude when mentioning certain aspects of coding
assistants, yielding nuanced insights into user satisfaction and
dissatisfaction regarding specific features, concerns, and overall tool
performance. Built on top of the findings-including how users demand not just
intelligent suggestions but also context-aware, customizable, and
resource-efficient interactions-we propose five practical implications and
suggestions to guide the enhancement of AI coding assistants that satisfy user
needs.

</details>


### [55] [From Fomo3D to Lottery DAPP: Analysis of Ethereum-Based Gambling Applications](https://arxiv.org/abs/2508.12303)
*Xu Long,Yishun Wang,Xiaoqi Li*

Main category: cs.SE

TL;DR: This paper analyzes Ethereum-based gambling DApps as a disruptive force in online lottery systems using smart contracts for process automation and decentralization. It examines technical principles, implementation challenges, and future potential of these platforms.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore how blockchain technology can create fairer lottery systems through decentralization and transparency, eliminating single-entity control and reducing operational costs via smart contract automation.

Method: The analysis includes conceptual explanation, technical dissection of an existing DApp (covering implementation and vulnerabilities), discussion of development requirements, and evaluation of current limitations through technical principles and case studies.

Result: Identified key benefits of decentralization and automation in lottery processes, mapped vulnerabilities in existing implementations, and demonstrated that Ethereum's growing ecosystem reduces development complexity while maintaining process integrity through smart contracts.

Conclusion: Blockchain-based lottery DApps have transformative potential in the online gambling industry due to inherent advantages, though adoption depends on addressing current technical limitations as the Ethereum platform evolves.

Abstract: As blockchain technology advances, Ethereum based gambling decentralized
applications (DApps) represent a new paradigm in online gambling. This paper
examines the concepts, principles, implementation, and prospects of Ethereum
based gambling DApps. First, we outline the concept and operational principles
of gambling DApps. These DApps are blockchain based online lottery platforms.
They utilize smart contracts to manage the entire lottery process, including
issuance, betting, drawing, and prize distribution. Being decentralized,
lottery DApps operate without central oversight, unlike traditional lotteries.
This ensures fairness and eliminates control by any single entity. Automated
smart contract execution further reduces management costs, increases
profitability, and enhances game transparency and credibility. Next, we analyze
an existing Ethereum based gambling DApp, detailing its technical principles,
implementation, operational status, vulnerabilities, and potential solutions.
We then elaborate on the implementation of lottery DApps. Smart contracts
automate the entire lottery process including betting, drawing, and prize
distribution. Although developing lottery DApps requires technical expertise,
the expanding Ethereum ecosystem provides growing tools and frameworks,
lowering development barriers. Finally, we discuss current limitations and
prospects of lottery DApps. As blockchain technology and smart contracts
evolve, lottery DApps are positioned to significantly transform the online
lottery industry. Advantages like decentralization, automation, and
transparency will likely drive broader future adoption.

</details>


### [56] [Towards the Coordination and Verification of Heterogeneous Systems with Data and Time](https://arxiv.org/abs/2508.12325)
*Tim Kr√§uter,Adrian Rutle,Yngve Lamo,Harald K√∂nig,Francisco Dur√°n*

Main category: cs.SE

TL;DR: The paper presents a non-intrusive coordination framework using rewriting logic (Maude) to perform formal analysis of heterogeneous systems with real-time requirements, validated on a road-rail crossing system.


<details>
  <summary>Details</summary>
Motivation: Modern software systems often integrate heterogeneous components requiring seamless coordination to meet real-time requirements, yet verifying such systems remains challenging due to their complexity and diversity.

Method: The framework includes (1) a linguistic extension implemented as a central broker mediating data exchange, (2) a domain-specific language (DSL) for integration of heterogeneous systems, and (3) reified abstract rule templates as language adapters enabling non-intrusive communication. Rewriting logic is used for implementation.

Result: The framework successfully verified correctness properties of a heterogeneous road-rail crossing system, demonstrating its applicability for analyzing complex, time-sensitive software architectures.

Conclusion: The framework provides an effective approach to formal verification of heterogeneous systems through non-intrusive coordination mechanisms realized in rewriting logic.

Abstract: Modern software systems are often realized by coordinating multiple
heterogeneous parts, each responsible for specific tasks. These parts must work
together seamlessly to satisfy the overall system requirements. To verify such
complex systems, we have developed a non-intrusive coordination framework
capable of performing formal analysis of heterogeneous parts that exchange data
and include real-time capabilities. The framework utilizes a linguistic
extension, which is implemented as a central broker and a domain-specific
language for the integration of heterogeneous languages and coordination of
parts. Moreover, abstract rule templates are reified as language adapters for
non-intrusive communications with the broker. The framework is implemented
using rewriting logic (Maude), and its applicability is demonstrated by
verifying certain correctness properties of a heterogeneous road-rail crossing
system.

</details>


### [57] [Uncovering Systematic Failures of LLMs in Verifying Code Against Natural Language Specifications](https://arxiv.org/abs/2508.12358)
*Haolin Jin,Huaming Chen*

Main category: cs.SE

TL;DR: This paper reveals that LLMs systematically fail to evaluate code alignment with natural language requirements, leading to frequent misclassification of correct code, and proposes improved prompting strategies to address these reliability issues.


<details>
  <summary>Details</summary>
Motivation: Despite the widespread use of LLMs in code review, their reliability in assessing code compliance with natural language specifications remains unexplored, prompting the need for this investigation into systematic failures and mitigation strategies.

Method: The study uses widely used benchmarks with unified prompts to evaluate LLMs' code correctness judgments. It employs prompt engineering techniques involving explanations and corrections to analyze misjudgments and then derives root causes and improved strategies.

Result: LLMs misclassify correct code as non-compliant or defective, and complex prompting increases error rates. The research identifies underlying causes and demonstrates two prompting strategies that effectively reduce these misjudgments, highlighting previously unrecognized limitations.

Conclusion: The findings challenge the reliability of LLMs in task-oriented code review scenarios, demonstrate how existing techniques exacerbate issues, and provide novel methods and practical guidance to mitigate systematic failures while enabling better integration of LLMs in development workflows.

Abstract: Large language models (LLMs) have become essential tools in software
development, widely used for requirements engineering, code generation and
review tasks. Software engineers often rely on LLMs to assess whether system
code implementation satisfy task requirements, thereby enhancing code
robustness and accuracy. However, it remains unclear whether LLMs can reliably
determine whether the code complies fully with the given task descriptions,
which is usually natural language specifications. In this paper, we uncover a
systematic failure of LLMs in evaluating whether code aligns with natural
language requirements. Specifically, with widely used benchmarks, we employ
unified prompts to judge code correctness. Our results reveal that LLMs
frequently misclassify correct code implementations as either ``not satisfying
requirements'' or containing potential defects. Surprisingly, more complex
prompting, especially when leveraging prompt engineering techniques involving
explanations and proposed corrections, leads to higher misjudgment rate, which
highlights the critical reliability issues in using LLMs as code review
assistants. We further analyze the root causes of these misjudgments, and
propose two improved prompting strategies for mitigation. For the first time,
our findings reveals unrecognized limitations in LLMs to match code with
requirements. We also offer novel insights and practical guidance for effective
use of LLMs in automated code review and task-oriented agent scenarios.

</details>


### [58] [Feature Request Analysis and Processing: Tasks, Techniques, and Trends](https://arxiv.org/abs/2508.12436)
*Feifei Niu,Chuanyi Li,Haosheng Zuo,Jionghan Wu,Xin Xia*

Main category: cs.SE

TL;DR: This paper systematically reviews 131 studies on feature requests in software products, identifying key challenges and opportunities to guide future research.


<details>
  <summary>Details</summary>
Motivation: Feature requests reflect user demands and product competitiveness, yet the fragmented research landscape necessitates a systematic synthesis to pinpoint critical challenges and opportunities for advancement.

Method: The study follows a predefined process and search protocol, employing descriptive statistics and qualitative analysis on 131 primary studies. It classifies them by topics and requirements engineering activities, while evaluating available tools and datasets.

Result: Identified challenges include ensuring request quality, improving specification/validation, and developing language model benchmarks. Opportunities lie in advancing feature request analysis and fostering future research directions.

Conclusion: This systematic review provides a structured overview of feature request research, highlighting unresolved challenges and opportunities. It aims to catalyze improved practices and guide high-quality benchmark development for emerging AI-driven methodologies.

Abstract: Feature requests are proposed by users to request new features or
enhancements of existing features of software products, which represent users'
wishes and demands. Satisfying users' demands can benefit the product from both
competitiveness and user satisfaction. Feature requests have seen a rise in
interest in the past few years and the amount of research has been growing.
However, the diversity in the research topics suggests the need for their
collective analysis to identify the challenges and opportunities so as to
promote new advances in the future. In this work, following a defined process
and a search protocol, we provide a systematic overview of the research area by
searching and categorizing relevant studies. We select and analyze 131 primary
studies using descriptive statistics and qualitative analysis methods. We
classify the studies into different topics and group them from the perspective
of requirements engineering activities. We investigate open tools as well as
datasets for future research. In addition, we identify several key challenges
and opportunities, such as: (1) ensuring the quality of feature requests, (2)
improving their specification and validation, and (3) developing high-quality
benchmarks for large language model-driven tasks.

</details>


### [59] [XAMT: Cross-Framework API Matching for Testing Deep Learning Libraries](https://arxiv.org/abs/2508.12546)
*Bin Duan,Ruican Dong,Naipeng Dong,Dan Dongseong Kim,Guowei Yang*

Main category: cs.SE

TL;DR: XAMT is a cross-framework fuzzing method that detects consistent backend bugs in deep learning libraries by matching and comparing functionally equivalent APIs across frameworks, identifying 17 bugs in PyTorch, TensorFlow, and others.


<details>
  <summary>Details</summary>
Motivation: Existing intra-framework testing methods fail to detect bugs that manifest identically across hardware backends. Deep learning libraries require robust cross-backend verification to catch consistent but critical implementation errors in APIs.

Method: XAMT uses similarity-based API matching (names, descriptions, parameters) followed by variance-guided differential testing to compare behavior across frameworks like PyTorch, TensorFlow, Keras, Chainer, and JAX.

Result: XAMT matched 839 APIs into 238 groups across five frameworks and discovered 17 bugs (12 confirmed), with 136 already patched, demonstrating effectiveness in uncovering bugs undetectable by intra-framework testing.

Conclusion: XAMT provides a novel complementary approach to API testing by leveraging cross-framework comparisons, addressing a critical gap in detecting backend-consistent bugs while offering practical validation through extensive bug detection.

Abstract: Deep learning powers critical applications such as autonomous driving,
healthcare, and finance, where the correctness of underlying libraries is
essential. Bugs in widely used deep learning APIs can propagate to downstream
systems, causing serious consequences. While existing fuzzing techniques detect
bugs through intra-framework testing across hardware backends (CPU vs. GPU),
they may miss bugs that manifest identically across backends and thus escape
detection under these strategies. To address this problem, we propose XAMT, a
cross-framework fuzzing method that tests deep learning libraries by matching
and comparing functionally equivalent APIs across different frameworks. XAMT
matches APIs using similarity-based rules based on names, descriptions, and
parameter structures. It then aligns inputs and applies variance-guided
differential testing to detect bugs. We evaluated XAMT on five popular
frameworks, including PyTorch, TensorFlow, Keras, Chainer, and JAX. XAMT
matched 839 APIs and identified 238 matched API groups, and detected 17 bugs,
12 of which have been confirmed. Our results show that XAMT uncovers bugs
undetectable by intra-framework testing, especially those that manifest
consistently across backends. XAMT offers a complementary approach to existing
methods and offers a new perspective on the testing of deep learning libraries.

</details>


### [60] [Strengthening Programming Comprehension in Large Language Models through Code Generation](https://arxiv.org/abs/2508.12620)
*Xiaoning Ren,Qiang Hu,Wei Ma,Yan Li,Yao Zhang,Lingxiao Jiang,Yinxing Xue*

Main category: cs.SE

TL;DR: This paper introduces a counterfactual code augmentation framework with concept-aware tuning to enhance large language models' (LLMs) understanding of programming concepts like data/control flow, addressing their fragile performance on deeper code reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle to grasp fundamental programming concepts (e.g., data flow, control flow) during software development tasks, limiting their practical adoption despite large-scale training and instruction tuning. This shallow understanding creates performance instability when code requires deeper reasoning. 

Method: A novel framework combining counterfactual code augmentation with concept-aware tuning. It generates diverse code scenarios through counterfactual generation and strengthens conceptual understanding by tying instruction tuning to specific code concepts.

Result: Comprehensive evaluations across multiple LLMs and code benchmarks demonstrate significant improvements in performance metrics when the framework is applied. Model robustness on concept-dependent reasoning tasks increases substantially.

Conclusion: The counterfactual code augmentation framework with concept-aware tuning successfully improves LLMs' conceptual understanding of programming fundamentals. This approach offers a promising path to enhance LLM reliability in real-world software engineering applications.

Abstract: Large language models (LLMs) have recently shown impressive results on
diverse code-related tasks, benefiting from large-scale training and
instruction tuning. However, studies reveal that their grasp of fundamental
programming concepts, such as data flow and control flow, remains shallow,
leading to fragile performance when code requires deeper reasoning. This
limitation restricts the practical adoption of LLMs in real-world software
development. To address this issue, this work introduces a counterfactual code
augmentation framework combined with concept-aware tuning, designed to guide
LLMs toward stronger conceptual understanding. Comprehensive evaluation across
multiple models and benchmarks demonstrates the effectiveness of the proposed
approach.

</details>


### [61] [ChangePrism: Visualizing the Essence of Code Changes](https://arxiv.org/abs/2508.12649)
*Lei Chen,Michele Lanza,Shinpei Hayashi*

Main category: cs.SE

TL;DR: ChangePrism is a tool that provides visualization of code changes in repositories to enhance understanding during software maintenance.


<details>
  <summary>Details</summary>
Motivation: Reviewing code diffs is cumbersome and inefficient for comprehending comprehensive changes in commits, necessitating a better approach to differentiate significant changes.

Method: The tool uses two components: extraction from git history to retrieve code changes and visualization with general and detailed views to represent modifications across commits.

Result: The paper demonstrates a novel visualization approach through ChangePrism to improve code comprehension by addressing limitations of traditional diff-based reviews.

Conclusion: ChangePrism offers a structured way to visualize code changes, aiding developers in understanding both broad patterns and specific details in software evolution.

Abstract: Understanding the changes made by developers when they submit a pull request
and/or perform a commit on a repository is a crucial activity in software
maintenance and evolution. The common way to review changes relies on examining
code diffs, where textual differences between two file versions are highlighted
in red and green to indicate additions and deletions of lines. This can be
cumbersome for developers, making it difficult to obtain a comprehensive
overview of all changes in a commit. Moreover, certain types of code changes
can be particularly significant and may warrant differentiation from standard
modifications to enhance code comprehension. We present a novel visualization
approach supported by a tool named ChangePrism, which provides a way to better
understand code changes. The tool comprises two components: extraction, which
retrieves code changes and relevant information from the git history, and
visualization, which offers both general and detailed views of code changes in
commits. The general view provides an overview of different types of code
changes across commits, while the detailed view displays the exact changes in
the source code for each commit.

</details>


### [62] [RUM: Rule+LLM-Based Comprehensive Assessment on Testing Skills](https://arxiv.org/abs/2508.12922)
*Yue Wang,Zhenyu Chen,Yuan Zhao,Chunrong Fang,Ziyuan Wang,Song Huang*

Main category: cs.SE

TL;DR: This paper proposes RUM, an automated testing skill assessment system combining rules and large language models (LLMs) to address the limitations of the META method in evaluating subjective aspects like test cases and reports. RUM significantly improves assessment efficiency (80.77%) and reduces costs (97.38%) while maintaining accuracy, and offers new insights for software testing education and quality assurance.


<details>
  <summary>Details</summary>
Motivation: The META method primarily assesses test scripts through objective indicators but lacks automated evaluation of subjective aspects (test cases/reports). This creates inefficiencies in manual evaluation for large-scale software testing education assessments.

Method: RUM integrates rule-based processing for objective metrics with LLM-powered subjective analysis of test case documents, test scripts, and test reports through a comprehensive assessment approach. The method was tested in the National College Student Software Testing Contest.

Result: RUM demonstrated 80.77% improved assessment efficiency and 97.38% cost reduction versus manual methods, achieving similar accuracy/consistency. In the contest application, it enhanced scalability and provided richer evidence for personalized teaching decisions.

Conclusion: RUM addresses the limitations of existing testing skill assessment methods by combining rules and LLMs. It advances both assessment efficiency and educational decision-making quality in software testing education, offering new directions for test process optimization and quality assurance practices.

Abstract: Over the past eight years, the META method has served as a multidimensional
testing skill assessment system in the National College Student Contest on
Software Testing, successfully assessing over 100,000 students' testing skills.
However, META is primarily limited to the objective assessment of test scripts,
lacking the ability to automatically assess subjective aspects such as test
case and test report. To address this limitation, this paper proposes RUM, a
comprehensive assessment approach that combines rules and large language models
(LLMs). RUM achieves a comprehensive assessment by rapidly processing objective
indicators through rules while utilizing LLMs for in-depth subjective analysis
of test case documents, test scripts, and test reports. The experimental
results show that compared to traditional manual testing skill assessment, RUM
improves assessment efficiency by 80.77\% and reduces costs by 97.38\%, while
maintaining high accuracy and consistency of assessment. By applying RUM on the
contest on software testing, we find that it not only enhances the efficiency
and scalability of skill assessment in software testing education, but also
provides teachers with more comprehensive and objective evidence for student
ability assessment, facilitating personalized teaching and learning. This study
offers new insights into the assessment of testing skills, which are expected
to promote further development in test process optimization and software
quality assurance.

</details>


### [63] [Investigating VR Accessibility Reviews for Users with Disabilities: A Qualitative Analysis](https://arxiv.org/abs/2508.13051)
*Yi Wang,Chetan Arora,Xiao Liu,Thuong Hoang,ZHengxin Zhang,Henry Been Lirn Duh,John Grundy*

Main category: cs.SE

TL;DR: This paper analyzes accessibility reviews for VR apps to identify limitations and benefits for users with disabilities. It found 1,076 (0.078%) accessibility-related reviews across 100 VR applications, categorizing 16 disability types into six categories and highlighting under-supported issues.


<details>
  <summary>Details</summary>
Motivation: To fill the gap in comprehensive investigation into VR accessibility limitations, as no systematic analysis of user-reported accessibility issues in VR applications exists.

Method: The study collected 1,367,419 reviews from Meta and Steam for VR apps (top 40, 20 most popular, and 40 lowest-rated), applied selection criteria to identify accessibility-related reviews, categorized applications by genre, and classified disabilities and their reported causes.

Result: Identified 1,076 accessibility reviews in 100 VR apps, with Action apps receiving the most accessibility reports. 16 disability types (e.g., mobility, vision) were categorized under six groups, and most reported issues lack support.

Conclusion: VR accessibility issues are predominantly under-supported based on user reports, suggesting a need for improved accessibility features across VR applications and platforms.

Abstract: Accessibility reviews provide valuable insights into both the limitations and
benefits experienced by users with disabilities when using virtual reality (VR)
applications. However, a comprehensive investigation into VR accessibility for
users with disabilities is still lacking. To fill this gap, this study analyzes
user reviews from the Meta and Steam stores of VR apps, focusing on the
reported issues affecting users with disabilities. We applied selection
criteria to 1,367,419 reviews from the top 40, the 20 most popular, and the 40
lowest-rated VR applications on both platforms. In total, 1,076 (0.078%) VR
accessibility reviews referenced various disabilities across 100 VR
applications. These applications were categorized into Action, Sports, Social,
Puzzle, Horror, and Simulation, with Action receiving the highest number of
accessibility related-reviews. We identified 16 different types of disabilities
across six categories. Furthermore, we examined the causes of accessibility
issues as reported by users with disabilities. Overall, VR accessibility
reviews were predominantly under-supported.

</details>


### [64] [Influencia de fatores organizacionais e sociais na etapa de levantamento de requisitos](https://arxiv.org/abs/2508.13134)
*Glauber da Rocha Balthazar,Marcia Ito*

Main category: cs.SE

TL;DR: This paper reviews studies on non-technical factors (emotions, organizational environment, social context) in software requirements gathering, highlighting their impact and underrepresentation in existing literature.


<details>
  <summary>Details</summary>
Motivation: The study addresses a gap in Requirements Engineering research, emphasizing the need to understand humanistic relationships and behaviors to improve requirements gathering processes.

Method: The authors conducted a survey analyzing existing studies that focus on non-technical aspects of requirements gathering, synthesizing insights about social and organizational influences.

Result: The survey identifies key non-technical challenges and factors, such as emotional dynamics and social interactions, that significantly affect requirements gathering effectiveness.

Conclusion: Integrating non-technical considerations into Requirements Engineering can enhance project outcomes, urging further research and application of these insights.

Abstract: The most critical and fragile stage of a software development project is
requirements gathering. Because of this, Requirements Engineering has been
evolving its techniques to minimize the challenges faced by Requirements
Analysts. However, few studies consider the humanistic relationships and
behaviors of those involved in this stage. This article presents a survey of
some studies conducted at this stage that consider non-technical factors such
as emotions, organizational environment, and social context.

</details>
