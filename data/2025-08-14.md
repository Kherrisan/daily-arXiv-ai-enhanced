<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 12]
- [cs.SE](#cs.SE) [Total: 10]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models: A Unified and Accurate Approach](https://arxiv.org/abs/2508.09201)
*Shuang Liang,Zhihao Xu,Jialing Tao,Hui Xue,Xiting Wang*

Main category: cs.CR

TL;DR: This paper introduces LoD, an unsupervised framework that effectively detects jailbreak attacks in Large Vision-Language Models (LVLMs) through anomaly detection using multi-modal safety representations and a pattern auto-encoder.


<details>
  <summary>Details</summary>
Motivation: LVLMs remain unsafe to jailbreak attacks despite alignment efforts. Existing detection methods utilize internal representations but rely on heuristic rules with suboptimal performance due to unprincipled objectives.

Method: LoD formulates jailbreak detection as anomaly detection. It uses MSCAV to capture safety-related representations across modalities and a Safety Pattern Auto-Encoder to model safe input distributions, identifying anomalies via reconstruction errors. The auto-encoder is trained exclusively on safe samples without attack labels.

Result: LoD achieves state-of-the-art performance across multiple LVLMs and benchmarks, attaining an average AUROC of 0.9951 and demonstrating up to 38.89% improvement in minimum AUROC over existing methods.

Conclusion: The proposed framework provides an accurate and unified approach to detecting jailbreak attacks by leveraging multi-modal safety patterns, showing significant advantages over heuristic-based baselines and demonstrating practical effectiveness.

Abstract: Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)
remain vulnerable to jailbreak attacks, posing serious safety risks. Although
recent detection works have shifted to internal representations due to their
rich cross-modal information, most methods rely on heuristic rules rather than
principled objectives, resulting in suboptimal performance. To address these
limitations, we propose Learning to Detect (LoD), a novel unsupervised
framework that formulates jailbreak detection as anomaly detection. LoD
introduces two key components: Multi-modal Safety Concept Activation Vectors
(MSCAV), which capture layer-wise safety-related representations across
modalities, and the Safety Pattern Auto-Encoder, which models the distribution
of MSCAV derived from safe inputs and detects anomalies via reconstruction
errors. By training the auto-encoder (AE) solely on safe samples without attack
labels, LoD naturally identifies jailbreak inputs as distributional anomalies,
enabling accurate and unified detection of jailbreak attacks. Comprehensive
experiments on three different LVLMs and five benchmarks demonstrate that LoD
achieves state-of-the-art performance, with an average AUROC of 0.9951 and an
improvement of up to 38.89% in the minimum AUROC over the strongest baselines.

</details>


### [2] [VeriPHY: Physical Layer Signal Authentication for Wireless Communication in 5G Environments](https://arxiv.org/abs/2508.09213)
*Clifton Paul Robinson,Salvatore D'Oro,Tommaso Melodia*

Main category: cs.CR

TL;DR: VeriPHY is a deep learning-based physical layer authentication method for 5G networks using steganographic I/Q signatures with high accuracy and stealthiness


<details>
  <summary>Details</summary>
Motivation: Addresses the need for secure, efficient wireless authentication without traditional cryptography, leveraging deep learning to enhance accuracy and reliability in 5G environments

Method: Generates time-varying Gaussian Mixture Model-based signatures embedded via steganography in I/Q signals, utilizes DNNs for authentication with a stealth generation mode ensuring signature undetectability

Result: Achieved 93-100% signature identification accuracy (0.5% - 6% false positives) with 28ms inference latency during 20ms signature updates; stealth mode maintains 93%+ detection accuracy while preserving 5G signal indistinguishability

Conclusion: VeriPHY demonstrates a novel PLA approach for 5G with strong security guarantees through unique device fingerprinting and near-real-time authentication capabilities without compromising communication integrity

Abstract: Physical layer authentication (PLA) uses inherent characteristics of the
communication medium to provide secure and efficient authentication in wireless
networks, bypassing the need for traditional cryptographic methods. With
advancements in deep learning, PLA has become a widely adopted technique for
its accuracy and reliability. In this paper, we introduce VeriPHY, a novel deep
learning-based PLA solution for 5G networks, which enables unique device
identification by embedding signatures within wireless I/Q transmissions using
steganography. VeriPHY continuously generates pseudo-random signatures by
sampling from Gaussian Mixture Models whose distribution is carefully varied to
ensure signature uniqueness and stealthiness over time, and then embeds the
newly generated signatures over I/Q samples transmitted by users to the 5G gNB.
Utilizing deep neural networks, VeriPHY identifies and authenticates users
based on these embedded signatures. VeriPHY achieves high precision,
identifying unique signatures between 93% and 100% with low false positive
rates and an inference time of 28 ms when signatures are updated every 20 ms.
Additionally, we also demonstrate a stealth generation mode where signatures
are generated in a way that makes them virtually indistinguishable from
unaltered 5G signals while maintaining over 93% detection accuracy.

</details>


### [3] [Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs](https://arxiv.org/abs/2508.09288)
*Aayush Gupta*

Main category: cs.CR

TL;DR: This paper proposes Contextual Integrity Verification (CIV), a lightweight, no-fine-tuning-required security architecture that cryptographically enforces token-level trust boundaries in LLMs, achieving 0% attack success rate on prompt injection benchmarks with minimal performance impact.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) are vulnerable to prompt injection and jailbreak attacks that bypass existing heuristic guardrails, requiring a robust, generalizable defense mechanism to ensure secure deployment in real-world applications.

Method: CIV implements source-trust lattice enforcement via pre-softmax hard attention masks (with optional FFN/residual gating) that cryptographically sign and verify provenance labels for each token, ensuring lower-trust tokens cannot interfere with higher-trust representations.

Result: On Elite-Attack and SoK-246 benchmarks, CIV achieves 0% attack success while retaining 93.1% token-level similarity to baselines and showing no perplexity degradation on benign tasks, though non-optimized implementations face latency overhead.

Conclusion: CIV provides deterministic, model-agnostic non-interference guarantees as a drop-in security patch for frozen models like Llama-3-8B and Mistral-7B, supported by a released reference implementation and reproducible research artifacts (corpus, harness).

Abstract: Large language models (LLMs) remain acutely vulnerable to prompt injection
and related jailbreak attacks; heuristic guardrails (rules, filters, LLM
judges) are routinely bypassed. We present Contextual Integrity Verification
(CIV), an inference-time security architecture that attaches cryptographically
signed provenance labels to every token and enforces a source-trust lattice
inside the transformer via a pre-softmax hard attention mask (with optional
FFN/residual gating). CIV provides deterministic, per-token non-interference
guarantees on frozen models: lower-trust tokens cannot influence higher-trust
representations. On benchmarks derived from recent taxonomies of
prompt-injection vectors (Elite-Attack + SoK-246), CIV attains 0% attack
success rate under the stated threat model while preserving 93.1% token-level
similarity and showing no degradation in model perplexity on benign tasks; we
note a latency overhead attributable to a non-optimized data path. Because CIV
is a lightweight patch -- no fine-tuning required -- we demonstrate drop-in
protection for Llama-3-8B and Mistral-7B. We release a reference
implementation, an automated certification harness, and the Elite-Attack corpus
to support reproducible research.

</details>


### [4] [Security Analysis of ChatGPT: Threats and Privacy Risks](https://arxiv.org/abs/2508.09426)
*Yushan Xiang,Zhongwen Li,Xiaoqi Li*

Main category: cs.CR

TL;DR: The paper analyzes security threats and privacy risks of ChatGPT, examines vulnerabilities and ethical issues, and explores attack/defense simulation scenarios for vulnerability detection and security tool generation.


<details>
  <summary>Details</summary>
Motivation: Rising security/privacy concerns as ChatGPT's application scope expands and malicious attacks increase, necessitating systematic vulnerability analysis and ethical consideration.

Method: 1) Systematic analysis of vulnerabilities and causes in security/privacy dimensions 2) Simulation of attack scenarios from attacker's perspective 3) Exploration of defender-side solutions for vulnerability detection and security tool generation.

Result: 1) Identification of various vulnerabilities leading to security threats and privacy risks 2) Evidence of ChatGPT's role in generating security tools through simulation experiments 3) Validation of ethical risks and attack/defense effectiveness.

Conclusion: While acknowledging ChatGPT's capabilities, the paper emphasizes urgent security/privacy risk mitigation, proposes dual-perspective simulation approaches for threat analysis and defense system development, and guides secure deployment of conversational AI.

Abstract: As artificial intelligence technology continues to advance, chatbots are
becoming increasingly powerful. Among them, ChatGPT, launched by OpenAI, has
garnered widespread attention globally due to its powerful natural language
processing capabilities based on the GPT model, which enables it to engage in
natural conversations with users, understand various forms of linguistic
expressions, and generate useful information and suggestions. However, as its
application scope expands, user demand grows, and malicious attacks related to
it become increasingly frequent, the security threats and privacy risks faced
by ChatGPT are gradually coming to the forefront. In this paper, the security
of ChatGPT is mainly studied from two aspects, security threats and privacy
risks. The article systematically analyzes various types of vulnerabilities
involved in the above two types of problems and their causes. Briefly, we
discuss the controversies that ChatGPT may cause at the ethical and moral
levels. In addition, this paper reproduces several network attack and defense
test scenarios by simulating the attacker's perspective and methodology.
Simultaneously, it explores the feasibility of using ChatGPT for security
vulnerability detection and security tool generation from the defender's
perspective.

</details>


### [5] [Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference](https://arxiv.org/abs/2508.09442)
*Zhifan Luo,Shuo Shao,Su Zhang,Lijing Zhou,Yuke Hu,Chenxu Zhao,Zhihao Liu,Zhan Qin*

Main category: cs.CR

TL;DR: KV-Caches in LLMs pose privacy risks that can be exploited to reconstruct user inputs. The paper introduces three attack methods and proposes KV-Cloak, a lightweight defense with minimal performance overhead.


<details>
  <summary>Details</summary>
Motivation: KV-Caches accelerate LLM inference but introduce unexplored privacy vulnerabilities; this work addresses the critical need for secure and efficient mitigation strategies.

Method: Describes three attacks: Inversion, Collision, and Injection to exploit KV-Cache leakage, then introduces KV-Cloak using reversible matrix-based obfuscation and operator fusion to neutralize them.

Result: KV-Cloak effectively prevents attacks by degrading reconstruction quality to random noise while maintaining model accuracy and incurring negligible performance overhead.

Conclusion: KV-Cache privacy risks are significant but mitigable with KV-Cloak, a practical defense that balances security, performance, and accuracy for trustworthy LLM deployment.

Abstract: The Key-Value (KV) cache, which stores intermediate attention computations
(Key and Value pairs) to avoid redundant calculations, is a fundamental
mechanism for accelerating Large Language Model (LLM) inference. However, this
efficiency optimization introduces significant yet underexplored privacy risks.
This paper provides the first comprehensive analysis of these vulnerabilities,
demonstrating that an attacker can reconstruct sensitive user inputs directly
from the KV-cache. We design and implement three distinct attack vectors: a
direct Inversion Attack, a more broadly applicable and potent Collision Attack,
and a semantic-based Injection Attack. These methods demonstrate the
practicality and severity of KV-cache privacy leakage issues. To mitigate this,
we propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.
KV-Cloak uses a reversible matrix-based obfuscation scheme, combined with
operator fusion, to secure the KV-cache. Our extensive experiments show that
KV-Cloak effectively thwarts all proposed attacks, reducing reconstruction
quality to random noise. Crucially, it achieves this robust security with
virtually no degradation in model accuracy and minimal performance overhead,
offering a practical solution for trustworthy LLM deployment.

</details>


### [6] [Demystifying the Role of Rule-based Detection in AI Systems for Windows Malware Detection](https://arxiv.org/abs/2508.09652)
*Andrea Ponte,Luca Demetrio,Luca Oneto,Ivan Tesfai Ogbu,Battista Biggio,Fabio Roli*

Main category: cs.CR

TL;DR: This paper explores how integrating signature-based detection into an AI training pipeline improves robustness against adversarial examples and temporal drift but introduces a fixed false positive lower bound due to suboptimal rule selection.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the missed opportunities in current AI-based malware detection systems where signature-based detection and machine learning are developed in isolation, leading to suboptimal data complexity reduction and vulnerability to adversarial examples.

Method: The authors compare two approaches: (1) a model trained on a comprehensive dataset and (2) an AI system where the machine learning component is trained only on samples not already flagged by signature-based detection rules.

Result: Models trained with signature-based filtering showed enhanced robustness against adversarial examples and temporal data drift but exhibited a fixed lower bound on false positives caused by suboptimal selection of detection rules.

Conclusion: The paper highlights limitations in rule selection and advocates for integrating dynamic analysis into future AI-based malware detection systems to further strengthen resilience against evasion attacks.

Abstract: Malware detection increasingly relies on AI systems that integrate
signature-based detection with machine learning. However, these components are
typically developed and combined in isolation, missing opportunities to reduce
data complexity and strengthen defenses against adversarial EXEmples, carefully
crafted programs designed to evade detection. Hence, in this work we
investigate the influence that signature-based detection exerts on model
training, when they are included inside the training pipeline. Specifically, we
compare models trained on a comprehensive dataset with an AI system whose
machine learning component is trained solely on samples not already flagged by
signatures. Our results demonstrate improved robustness to both adversarial
EXEmples and temporal data drift, although this comes at the cost of a fixed
lower bound on false positives, driven by suboptimal rule selection. We
conclude by discussing these limitations and outlining how future research
could extend AI-based malware detection to include dynamic analysis, thereby
further enhancing system resilience.

</details>


### [7] [Social-Sensor Identity Cloning Detection Using Weakly Supervised Deep Forest and Cryptographic Authentication](https://arxiv.org/abs/2508.09665)
*Ahmed Alharbi,Hai Dong,Xun Yi*

Main category: cs.CR

TL;DR: A novel method for detecting identity cloning in social-sensor clouds using a weakly supervised deep forest model and cryptography-based authentication, outperforming current state-of-the-art approaches.


<details>
  <summary>Details</summary>
Motivation: Existing identity cloning detection methods in social-sensor clouds have unsatisfactory performance, no solutions for duplicated account detection, and lack large-scale evaluations on real-world datasets.

Method: 1) Weakly supervised deep forest model to detect similar identities using non-privacy-sensitive user profile features; 2) Cryptography-based authentication protocol to verify if similar identities originate from the same provider.

Result: The technique demonstrated superior performance and feasibility compared to state-of-the-art methods through extensive experiments on a large real-world dataset.

Conclusion: The proposed method addresses critical gaps in existing approaches by combining machine learning and cryptographic verification, enabling effective identity cloning detection in social-sensor cloud environments.

Abstract: Recent years have witnessed a rising trend in social-sensor cloud identity
cloning incidents. However, existing approaches suffer from unsatisfactory
performance, a lack of solutions for detecting duplicated accounts, and a lack
of large-scale evaluations on real-world datasets. We introduce a novel method
for detecting identity cloning in social-sensor cloud service providers. Our
proposed technique consists of two primary components: 1) a similar identity
detection method and 2) a cryptography-based authentication protocol.
Initially, we developed a weakly supervised deep forest model to identify
similar identities using non-privacy-sensitive user profile features provided
by the service. Subsequently, we designed a cryptography-based authentication
protocol to verify whether similar identities were generated by the same
provider. Our extensive experiments on a large real-world dataset demonstrate
the feasibility and superior performance of our technique compared to current
state-of-the-art identity clone detection methods.

</details>


### [8] [Succinct Oblivious Tensor Evaluation and Applications: Adaptively-Secure Laconic Function Evaluation and Trapdoor Hashing for All Circuits](https://arxiv.org/abs/2508.09673)
*Damiano Abram,Giulio Malavolta,Lawrence Roy*

Main category: cs.CR

TL;DR: This paper introduces succinct oblivious tensor evaluation (OTE) based on the standard LWE problem, enabling multiple cryptographic primitives with improved communication efficiency and adaptive security. It features an optimal OTE construction and a novel notion of adaptive lattice encodings for general functions.


<details>
  <summary>Details</summary>
Motivation: Existing cryptographic protocols for secure computation often have communication costs dependent on input dimension, and prior adaptively secure schemes relied on non-standard assumptions. This work addresses these limitations by providing OTE with communication independent of vector dimension and adaptive security reducible to standard LWE.

Method: 1. Constructed OTE protocols with optimal complexity using standard LWE assumptions
2. Introduced adaptive lattice encodings as a technical innovation
3. Developed cryptographic primitives via reductions from OTE and lattice encodings
4. Achieved additive secret sharing of tensor products with two simultaneous messages of size independent of input dimension

Result: 1. First adaptively secure laconic function evaluation from standard LWE
2. Trapdoor hash function for all functions with LWE security
3. Optimal succinct homomorphic secret sharing for all functions
4. Rate-1/2 laconic oblivious transfer for batch messages
5. Improved communication complexity for depth-D functions: m+ℓ+D·poly(λ)

Conclusion: The paper establishes OTE as a foundational cryptographic primitive, achieving optimal efficiency and adaptive security from standard LWE. The results represent significant progress in secure computation, particularly improving upon prior work (FOCS 2018) by removing reliance on non-standard assumptions and enabling practical implementations with concrete efficiency bounds.

Abstract: We propose the notion of succinct oblivious tensor evaluation (OTE), where
two parties compute an additive secret sharing of a tensor product of two
vectors $\mathbf{x} \otimes \mathbf{y}$, exchanging two simultaneous messages.
Crucially, the size of both messages and of the CRS is independent of the
dimension of $\mathbf{x}$.
  We present a construction of OTE with optimal complexity from the standard
learning with errors (LWE) problem. Then we show how this new technical tool
enables a host of cryptographic primitives, all with security reducible to LWE,
such as:
  * Adaptively secure laconic function evaluation for depth-$D$ functions
$f:\{0, 1\}^m\rightarrow\{0, 1\}^\ell$ with communication $m+\ell+D\cdot
\mathrm{poly}(\lambda)$.
  * A trapdoor hash function for all functions.
  * An (optimally) succinct homomorphic secret sharing for all functions.
  * A rate-$1/2$ laconic oblivious transfer for batch messages, which is best
possible.
  In particular, we obtain the first laconic function evaluation scheme that is
adaptively secure from the standard LWE assumption, improving upon Quach, Wee,
and Wichs (FOCS 2018).
  As a key technical ingredient, we introduce a new notion of \emph{adaptive
lattice encodings}, which may be of independent interest.

</details>


### [9] [Enhance the machine learning algorithm performance in phishing detection with keyword features](https://arxiv.org/abs/2508.09765)
*Zijiang Yang*

Main category: cs.CR

TL;DR: A novel URL feature selection method improves phishing detection accuracy by 30% (up to 99.68%) using keyword features without third-party data.


<details>
  <summary>Details</summary>
Motivation: The increasing prevalence of phishing attacks necessitates better detection methods to prevent information leakage and financial losses, as traditional machine learning approaches have limited effectiveness.

Method: The paper introduces a keyword feature engineering approach that complements traditional URL features across multiple machine learning algorithms, focusing on dataset size adaptation and eliminating third-party data dependencies.

Result: Achieved 30% classification error reduction for large datasets and even greater improvement for small datasets, with the best algorithm reaching 99.68% accuracy through this method.

Conclusion: The proposed feature selection technique provides consistent error reduction and remarkable accuracy gains, particularly for small training datasets, while maintaining URL-only analysis.

Abstract: Recently, we can observe a significant increase of the phishing attacks in
the Internet. In a typical phishing attack, the attacker sets up a malicious
website that looks similar to the legitimate website in order to obtain the
end-users' information. This may cause the leakage of the sensitive information
and the financial loss for the end-users. To avoid such attacks, the early
detection of these websites' URLs is vital and necessary. Previous researchers
have proposed many machine learning algorithms to distinguish the phishing URLs
from the legitimate ones. In this paper, we would like to enhance these machine
learning algorithms from the perspective of feature selection. We propose a
novel method to incorporate the keyword features with the traditional features.
This method is applied on multiple traditional machine learning algorithms and
the experimental results have shown this method is useful and effective. On
average, this method can reduce the classification error by 30% for the large
dataset. Moreover, its enhancement is more significant for the small dataset.
In addition, this method extracts the information from the URL and does not
rely on the additional information provided by the third-part service. The best
result for the machine learning algorithm using our proposed method has
achieved the accuracy of 99.68%.

</details>


### [10] [Perfect message authentication codes are robust to small deviations from uniform key distributions](https://arxiv.org/abs/2508.09783)
*Boris Ryabko*

Main category: cs.CR

TL;DR: This paper derives a formula for security loss in perfect message authentication codes when key distributions deviate from uniformity, showing their robustness to small deviations.


<details>
  <summary>Details</summary>
Motivation: Understanding how real-world key distribution deviations affect the theoretical security guarantees of information-theoretic message authentication codes remains crucial for practical implementations.

Method: The analysis derives a mathematical expression relating security degradation to the statistical distance between actual and uniform key distributions using information-theoretic principles.

Result: The authors show that security loss can be quantified by statistical distance metrics, confirming that perfect MACs maintain robustness against minor distribution deviations.

Conclusion: Perfect message authentication codes demonstrate resilience to small non-uniformities in key distribution without significant loss of information-theoretic security guarantees.

Abstract: We investigate the impact of (possible) deviations of the probability
distribution of key values from a uniform distribution for the
information-theoretic strong, or perfect, message authentication code. We found
a simple expression for the decrease in security as a function of the
statistical distance between the real key probability distribution and the
uniform one. In a sense, a perfect message authentication code is robust to
small deviations from a uniform key distribution.

</details>


### [11] [Explainable Ensemble Learning for Graph-Based Malware Detection](https://arxiv.org/abs/2508.09801)
*Hossein Shokouhinejad,Roozbeh Razavi-Far,Griffin Higgins,Ali A Ghorbani*

Main category: cs.CR

TL;DR: A stacking ensemble framework using GNNs for improved and interpretable malware detection.


<details>
  <summary>Details</summary>
Motivation: Single-model approaches limit generalization and lack interpretability in security-critical malware detection tasks.

Method: The framework dynamically extracts CFGs from PE files with two-step embeddings, uses diverse GNNs for complementary features, and an attention-based MLP meta-learner to aggregate predictions while enabling ensemble-aware explanations via edge-importance fusion.

Result: Enhanced classification performance with actionable explanations of malware behavior patterns.

Conclusion: The ensemble method addresses robustness and interpretability challenges while maintaining detection accuracy.

Abstract: Malware detection in modern computing environments demands models that are
not only accurate but also interpretable and robust to evasive techniques.
Graph neural networks (GNNs) have shown promise in this domain by modeling rich
structural dependencies in graph-based program representations such as control
flow graphs (CFGs). However, single-model approaches may suffer from limited
generalization and lack interpretability, especially in high-stakes security
applications. In this paper, we propose a novel stacking ensemble framework for
graph-based malware detection and explanation. Our method dynamically extracts
CFGs from portable executable (PE) files and encodes their basic blocks through
a two-step embedding strategy. A set of diverse GNN base learners, each with a
distinct message-passing mechanism, is used to capture complementary behavioral
features. Their prediction outputs are aggregated by a meta-learner implemented
as an attention-based multilayer perceptron, which both classifies malware
instances and quantifies the contribution of each base model. To enhance
explainability, we introduce an ensemble-aware post-hoc explanation technique
that leverages edge-level importance scores generated by a GNN explainer and
fuses them using the learned attention weights. This produces interpretable,
model-agnostic explanations aligned with the final ensemble decision.
Experimental results demonstrate that our framework improves classification
performance while providing insightful interpretations of malware behavior.

</details>


### [12] [On the Consistency and Performance of the Iterative Bayesian Update](https://arxiv.org/abs/2508.09980)
*Ehab ElSalamouny,Catuscia Palamidessi*

Main category: cs.CR

TL;DR: This paper proves the consistency of the Iterative Bayesian Update (IBU) estimator as a maximum likelihood method, demonstrates its superior performance with geometric/Laplace/exponential mechanisms through experiments, and proposes an extension for infinite sensitive data alphabets.


<details>
  <summary>Details</summary>
Motivation: Estimating sensitive user data distributions while preserving privacy is critical across domains, but prior works lack rigorous consistency proofs for the IBU estimator or provide incorrect ones, despite its widespread use.

Method: The authors exploit the relationship between IBU and maximum likelihood estimation to analytically prove its consistency as sample size increases. They conduct experiments comparing IBU to existing methods (INV, RAPPOR) using different local differential privacy mechanisms.

Result: IBU shows significant performance improvements over other methods with geometric, Laplace, and exponential mechanisms in experiments. It performs comparably to alternatives for k-RR and RAPPOR mechanisms. A new technique enables IBU to handle infinite sensitivity alphabets.

Conclusion: The paper establishes IBU's theoretical consistency, validates its practical superiority for certain privacy mechanisms, and extends its applicability to infinite domains, making it a robust choice across diverse privacy-preserving estimation scenarios.

Abstract: For many social, scientific, and commercial purposes, it is often important
to estimate the distribution of the users' data regarding a sensitive
attribute, e.g., their ages, locations, etc. To allow this estimation while
protecting the users' privacy, every user applies a local privacy protection
mechanism that releases a noisy (sanitized) version of their original datum to
the data collector; then the original distribution is estimated using one of
the known methods, such as the matrix inversion (INV), RAPPOR's estimator, and
the iterative Bayesian update (IBU). Unlike the other estimators, the
consistency of IBU, i.e., the convergence of its estimate to the real
distribution as the amount of noisy data grows, has been either ignored or
incorrectly proved in the literature. In this article, we use the fact that IBU
is a maximum likelihood estimator to prove that IBU is consistent. We also
show, through experiments on real datasets, that IBU significantly outperforms
the other methods when the users' data are sanitized by geometric, Laplace, and
exponential mechanisms, whereas it is comparable to the other methods in the
case of the k-RR and RAPPOR mechanisms. Finally, we consider the case when the
alphabet of the sensitive data is infinite, and we show a technique that allows
IBU to operate in this case too.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [13] [Teaching Code Refactoring Using LLMs](https://arxiv.org/abs/2508.09332)
*Anshul Khairnar,Aarya Rajoju,Edward F. Gehringer*

Main category: cs.SE

TL;DR: This paper proposes using LLMs to provide real-time, context-aware feedback for code refactoring in software engineering education, demonstrating improved student understanding and maintainability in a live OSS project through structured prompting during Spring 2025.


<details>
  <summary>Details</summary>
Motivation: Traditional software engineering teaching methods (manual code reviews and static analysis tools) provide insufficient, inconsistent feedback when teaching complex refactoring concepts. Students struggle to connect theory with real-world codebases.

Method: Integrated LLM-assisted refactoring into course projects via structured prompts to identify code smells (e.g., long methods, low cohesion). Evaluated through student surveys and code quality metrics in a Spring 2025 live OSS environment.

Result: Preliminary findings indicate LLMs significantly improve practical comprehension of refactoring techniques while preserving code functionality. Students reported enhanced ability to apply maintainability principles.

Conclusion: LLMs serve as effective context-aware teaching tools for code refactoring, bridging theoretical knowledge and hands-on practice in complex OSS projects. Suggests potential for broader adoption in software engineering curricula with further code quality analysis.

Abstract: This Innovative Practice full paper explores how Large Language Models (LLMs)
can enhance the teaching of code refactoring in software engineering courses
through real-time, context-aware feedback. Refactoring improves code quality
but is difficult to teach, especially with complex, real-world codebases.
Traditional methods like code reviews and static analysis tools offer limited,
inconsistent feedback. Our approach integrates LLM-assisted refactoring into a
course project using structured prompts to help students identify and address
code smells such as long methods and low cohesion. Implemented in Spring 2025
in a long-lived OSS project, the intervention is evaluated through student
feedback and planned analysis of code quality improvements. Findings suggest
that LLMs can bridge theoretical and practical learning, supporting a deeper
understanding of maintainability and refactoring principles.

</details>


### [14] [Plug it and Play on Logs: A Configuration-Free Statistic-Based Log Parser](https://arxiv.org/abs/2508.09366)
*Qiaolin Qin,Xingfang Wu,Heng Li,Ettore Merlo*

Main category: cs.SE

TL;DR: PIPLUP is a novel statistic-based log parser that challenges the belief that semantic-based parsers are superior by achieving competitive accuracy without requiring external knowledge or infrastructure.


<details>
  <summary>Details</summary>
Motivation: Prior statistic-based log parsers lack accuracy and generalizability, and the assumption that semantic-based parsers (using pretrained models) are inherently more effective needed empirical validation.

Method: PIPLUP eliminates position assumptions for constant tokens and employs data-insensitive parameters, enabling ready adaptability to new log files. Evaluations were conducted on a large open-source log dataset.

Result: PIPLUP outperforms Drain and variants while matching the best semantic-based parser (LUNAR) in accuracy. It maintains low computational costs (~10x faster than state-of-the-art) and operates without GPUs or external APIs.

Conclusion: Statistical methods remain practical for real-world log parsing, particularly in cost-sensitive or privacy-constrained environments, challenging the dominance of API-dependent semantic approaches.

Abstract: Log parsing is an essential task in log analysis, and many tools have been
designed to accomplish it. Existing log parsers can be categorized into
statistic-based and semantic-based approaches. In comparison to semantic-based
parsers, existing statistic-based parsers tend to be more efficient, require
lower computational costs, and be more privacy-preserving thanks to on-premise
deployment, but often fall short in their accuracy (e.g., grouping or parsing
accuracy) and generalizability. Therefore, it became a common belief that
statistic-based parsers cannot be as effective as semantic-based parsers since
the latter could take advantage of external knowledge supported by pretrained
language models. Our work, however, challenges this belief with a novel
statistic-based parser, PIPLUP. PIPLUP eliminates the pre-assumption of the
position of constant tokens for log grouping and relies on data-insensitive
parameters to overcome the generalizability challenge, allowing "plug and play"
on given log files. According to our experiments on an open-sourced large log
dataset, PIPLUP shows promising accuracy and generalizability with the
data-insensitive default parameter set. PIPLUP not only outperforms the
state-of-the-art statistic-based log parsers, Drain and its variants, but also
obtains a competitive performance compared to the best unsupervised
semantic-based log parser (i.e., LUNAR). Further, PIPLUP exhibits low time
consumption without GPU acceleration and external API usage; our simple,
efficient, and effective approach makes it more practical in real-world
adoptions, especially when costs and privacy are of major concerns.

</details>


### [15] [Your Coding Intent is Secretly in the Context and You Should Deliberately Infer It Before Completion](https://arxiv.org/abs/2508.09537)
*Yanzhou Li,Tianlin Li,Yiran Zhang,Shangqing Liu,Aishan Liu,Yang Liu*

Main category: cs.SE

TL;DR: This paper introduces a three-stage framework to improve Function Completion in Code by LLMs Without Docstrings. The method includes intent inference from preceding code, interactive refinement of inferred intent, and code generation. Experiments show significant performance boosts.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based code generation relies on explicit instructions (like docstrings) which are often absent in real repositories, leading to performance loss. None of prior methods address intent recovery from code-only context effectively.

Method: 1. Intent Inference: Reasoning-based prompting to extract function intent from preceding code.
2. Interactive Refinement: Generate 3-5 intent candidates for developer selection/editing.
3. Code Generation: Use refined intent with 7 different LLMs including Codex 20.05 and StarCoder. 
Annotated dataset of 40k code snippets with reasoning traces is introduced.

Result: Achieved +20% relative improvements on DevEval & ComplexCodeEval benchmarks with multiple LLMs. Interactive refinement added extra 7-10% gains when used. Framework works best with 7-shot prompting.

Conclusion: The framework significantly improves code generation from implicit context rather than explicit instructions. Interactive refinement stage adds measurable value beyond intent inference alone. Approach is practical for real codebases without requiring manual docstring annotations.

Abstract: Large Language Models (LLMs) are increasingly used for function completion in
repository-scale codebases. Prior studies demonstrate that when explicit
instructions--such as docstrings--are provided, these models can generate
highly accurate implementations. However, in real-world repositories, such
annotations are frequently absent, and performance drops substantially without
them. To address this gap, we frame the task as a three-stage process. The
first stage focuses on intent inference, where the model analyzes the code
preceding the target function to uncover cues about the desired functionality.
Such preceding context often encodes subtle but critical information, and we
design a reasoning-based prompting framework to guide the LLM through
step-by-step extraction and synthesis of these signals before any code is
generated. The second stage introduces an optional interactive refinement
mechanism to handle cases where preceding context alone is insufficient for
intent recovery. In this stage, the model proposes a small set of candidate
intentions, enabling the developer to select or edit them so that the inferred
intent closely matches the actual requirement. Finally, in the third stage, the
LLM generates the target function conditioned on the finalized intent. To
support this pipeline, we curate a dataset of 40,000 examples annotated with
intermediate reasoning traces and corresponding docstrings. Extensive
experiments on DevEval and ComplexCodeEval show that our approach consistently
boosts multiple LLMs, achieving over 20\% relative gains in both
reference-based and execution-based metrics, with the interactive refinement
stage delivering additional improvements beyond these gains.

</details>


### [16] [ReqInOne: A Large Language Model-Based Agent for Software Requirements Specification Generation](https://arxiv.org/abs/2508.09648)
*Taohong Zhu,Lucas C. Cordeiro,Youcheng Sun*

Main category: cs.SE

TL;DR: ReqInOne automates software requirements specification (SRS) using a modular LLM-based approach with tailored prompts, achieving better accuracy and structure than existing methods.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenges of manual SRS document drafting (ambiguity, inefficiency) and limitations of prior automated approaches (manual analysis, LLM hallucinations, low controllability).

Method: The system decomposes SRS generation into three tasks (summary, extraction, classification) using modular prompt templates for each step to enhance LLM output quality and consistency.

Result: Empirical evaluation against GPT-4-based baselines and entry-level engineers shows ReqInOne produces more accurate structured SRS documents, with its classification component matching/benchmarking against state-of-the-art models.

Conclusion: The modular architecture that aligns with human workflow significantly improves both the accuracy and reliability of automated SRS generation compared to monolithic LLM approaches.

Abstract: Software Requirements Specification (SRS) is one of the most important
documents in software projects, but writing it manually is time-consuming and
often leads to ambiguity. Existing automated methods rely heavily on manual
analysis, while recent Large Language Model (LLM)-based approaches suffer from
hallucinations and limited controllability. In this paper, we propose ReqInOne,
an LLM-based agent that follows the common steps taken by human requirements
engineers when writing an SRS to convert natural language into a structured
SRS. ReqInOne adopts a modular architecture by decomposing SRS generation into
three tasks: summary, requirement extraction, and requirement classification,
each supported by tailored prompt templates to improve the quality and
consistency of LLM outputs.
  We evaluate ReqInOne using GPT-4o, LLaMA 3, and DeepSeek-R1, and compare the
generated SRSs against those produced by the holistic GPT-4-based method from
prior work as well as by entry-level requirements engineers. Expert evaluations
show that ReqInOne produces more accurate and well-structured SRS documents.
The performance advantage of ReqInOne benefits from its modular design, and
experimental results further demonstrate that its requirement classification
component achieves comparable or even better results than the state-of-the-art
requirement classification model.

</details>


### [17] [DeputyDev -- AI Powered Developer Assistant: Breaking the Code Review Logjam through Contextual AI to Boost Developer Productivity](https://arxiv.org/abs/2508.09676)
*Vishal Khare,Vijay Saini,Deepak Sharma,Anand Kumar,Ankit Rana,Anshul Yadav*

Main category: cs.SE

TL;DR: The paper presents DeputyDev, an AI-powered code review assistant that addresses inefficiencies in software development by reducing pull request processing times through automated reviews. A 200-engineer A/B experiment showed a 23.09% reduction in average PR review duration and 40.13% reduction per line, with successful enterprise-scale deployment at TATA 1mg and external SaaS adoption.


<details>
  <summary>Details</summary>
Motivation: Software code reviews at TATA 1mg exhibited severe inefficiencies with 73-hour average PR pickup and 82-hour review times (6.2-day total closure), while UC Irvine studies highlighted 23-minute average focus losses from interruptions during reviews. These issues demand automated solutions that maintain contextual understanding to optimize development workflows and code quality.

Method: The authors developed DeputyDev's contextual review engine using telemetry data analysis and implemented a double-controlled A/B experiment across 200+ engineers. They applied statistical analysis to measure review duration impacts and incorporated outlier-exclusion safeguards before full organizational deployment, subsequently expanding it as a SaaS offering.

Result: DeputyDev achieved statistically significant results: 23.09% reduction in average PR review duration and 40.13% reduction in per-line code review time. The solution successfully reduced development cycle friction, with implementation across 1mg's engineering team and adoption by external companies through its SaaS platform.

Conclusion: DeputyDev demonstrates that AI-assisted code reviews can substantially improve development workflow efficiency while maintaining code quality. Its dual implementation as an internal tool and external SaaS solution validates both the practical effectiveness of automated review systems and their broader scalability potential in enterprise environments.

Abstract: This study investigates the implementation and efficacy of DeputyDev, an
AI-powered code review assistant developed to address inefficiencies in the
software development process. The process of code review is highly inefficient
for several reasons, such as it being a time-consuming process, inconsistent
feedback, and review quality not being at par most of the time. Using our
telemetry data, we observed that at TATA 1mg, pull request (PR) processing
exhibits significant inefficiencies, with average pick-up and review times of
73 and 82 hours, respectively, resulting in a 6.2 day closure cycle. The review
cycle was marked by prolonged iterative communication between the reviewing and
submitting parties. Research from the University of California, Irvine
indicates that interruptions can lead to an average of 23 minutes of lost
focus, critically affecting code quality and timely delivery. To address these
challenges, we developed DeputyDev's PR review capabilities by providing
automated, contextual code reviews. We conducted a rigorous double-controlled
A/B experiment involving over 200 engineers to evaluate DeputyDev's impact on
review times. The results demonstrated a statistically significant reduction in
both average per PR (23.09%) and average per-line-of-code (40.13%) review
durations. After implementing safeguards to exclude outliers, DeputyDev has
been effectively rolled out across the entire organisation. Additionally, it
has been made available to external companies as a Software-as-a-Service (SaaS)
solution, currently supporting the daily work of numerous engineering
professionals. This study explores the implementation and effectiveness of
AI-assisted code reviews in improving development workflow timelines and code.

</details>


### [18] [Inclusive Employment Pathways: Career Success Factors for Autistic Individuals in Software Engineering](https://arxiv.org/abs/2508.09680)
*Orvila Sarker,Mona Jamshaid,M. Ali Babar*

Main category: cs.SE

TL;DR: The study synthesizes success factors for autistic inclusion in Software Engineering (SE) roles, identifying four thematic categories (education, career training, work environment, tools) from 30 reviewed studies to provide evidence-based recommendations for stakeholders.


<details>
  <summary>Details</summary>
Motivation: Autistic individuals demonstrate strengths in ICT sectors but face barriers due to non-inclusive environments and practices. Existing progress in Diversity, Equity, and Inclusion (DEI) lacks a comprehensive pathways analysis from education to workplace inclusion.

Method: Systematic review of 30 studies, clustering 18 identified success factors into four thematically organized categories (Software Engineering Education, Career and Employment Training, Work Environment, Tools and Assistive Technologies).

Result: Identified 18 success factors grouped into four categories with actionable strategies for inclusive meeting practices, structured work environments, role clarity, and tailored accommodations for effective autistic inclusion in SE.

Conclusion: The research provides a structured pathway for stakeholders to optimize inclusion of autistic individuals in SE by addressing educational, workplace, and tool-based requirements, aligning with neurodiversity principles and refuting deficit-based perspectives.

Abstract: Research has highlighted the valuable contributions of autistic individuals
in the Information and Communication Technology (ICT) sector, particularly in
areas such as software development, testing, and cybersecurity. Their strengths
in information processing, attention to detail, innovative thinking, and
commitment to high-quality outcomes in the ICT domain are well-documented.
However, despite their potential, autistic individuals often face barriers in
Software Engineering (SE) roles due to a lack of personalised tools, complex
work environments, non-inclusive recruitment practices, limited co-worker
support, challenging social dynamics and so on. Motivated by the ethical
framework of the neurodiversity movement and the success of pioneering
initiatives like the Dandelion program, corporate Diversity, Equity, and
Inclusion (DEI) in the ICT sector has increasingly focused on autistic talent.
This movement fundamentally reframes challenges not as individual deficits but
as failures of environments designed for a neurotypical majority. Despite this
progress, there is no synthesis of knowledge reporting the full pathway from
software engineering education through to sustainable workplace inclusion. To
address this, we conducted a Systematic Review of 30 studies and identified 18
success factors grouped into four thematic categories: (1) Software Engineering
Education, (2) Career and Employment Training, (3) Work Environment, and (4)
Tools and Assistive Technologies. Our findings offer evidence-based
recommendations for educational institutions, employers, organisations, and
tool developers to enhance the inclusion of autistic individuals in SE. These
include strategies for inclusive meeting and collaboration practices,
accessible and structured work environments, clear role and responsibility
definitions, and the provision of tailored workplace accommodations.

</details>


### [19] [LibRec: Benchmarking Retrieval-Augmented LLMs for Library Migration Recommendations](https://arxiv.org/abs/2508.09791)
*Junxiao Han,Yarong Wang,Xiaodong Gu,Cuiyun Gao,Yao Wan,Song Han,David Lo,Shuiguang Deng*

Main category: cs.SE

TL;DR: LibRec integrates LLMs with RAG techniques to automate alternative library recommendations, using in-context learning from commit messages for intent extraction, validated via LibEval benchmark with 2,888 migration cases and multi-LLM evaluations.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for automated library migration recommendations to aid developers in identifying efficient alternatives when existing libraries become obsolete or problematic, leveraging LLMs for intent inference from commit history.

Method: LibRec combines retrieval-augmented generation with in-context learning to extract migration intents from commit messages, while LibEval provides a benchmark with 2,888 source-target library pairs and intent types from 2,324 Python repositories.

Result: Evaluations show LibRec's effectiveness with ten LLMs, ablation studies confirm component contributions, prompt strategies are analyzed, intent type performance is assessed, and failure cases are detailed to highlight limitations.

Conclusion: LibRec demonstrates robust capability for library migration recommendation through LLM integration and intent-driven RAG, validated by LibEval and multi-faceted analysis, offering insights into effective prompting and system components.

Abstract: In this paper, we propose LibRec, a novel framework that integrates the
capabilities of LLMs with retrieval-augmented generation(RAG) techniques to
automate the recommendation of alternative libraries. The framework further
employs in-context learning to extract migration intents from commit messages
to enhance the accuracy of its recommendations. To evaluate the effectiveness
of LibRec, we introduce LibEval, a benchmark designed to assess the performance
in the library migration recommendation task. LibEval comprises 2,888 migration
records associated with 2,368 libraries extracted from 2,324 Python
repositories. Each migration record captures source-target library pairs, along
with their corresponding migration intents and intent types. Based on LibEval,
we evaluated the effectiveness of ten popular LLMs within our framework,
conducted an ablation study to examine the contributions of key components
within our framework, explored the impact of various prompt strategies on the
framework's performance, assessed its effectiveness across various intent
types, and performed detailed failure case analyses.

</details>


### [20] [Fast and Accurate Heuristics for Bus-Factor Estimation](https://arxiv.org/abs/2508.09828)
*Sebastiano Antonio Piccolo*

Main category: cs.SE

TL;DR: This paper introduces two novel heuristics (Minimum and Maximum Coverage) based on bipartite graphs to approximate the NP-Hard bus-factor calculation in software projects, demonstrating improved accuracy and scalability over existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate bus-factor computation is NP-Hard and impractical for large systems, necessitating scalable approximation methods to identify critical contributors and knowledge risks in software projects.

Method: 1. Model software projects as bipartite graphs with developers and tasks
2. Propose iterative graph peeling heuristics (Minimum/Maximum Coverage) for two bus-factor formalizations
3. Compare against existing degree-based heuristics through empirical evaluation

Result: 1. Heuristics outperform degree-based methods on 1,000 synthetic power-law graphs
2. Achieve minutes-level processing on graphs with millions of nodes/edges
3. Maintain accuracy and robustness across varying developer-task graph structures

Conclusion: The proposed heuristics provide reliable, scalable bus-factor estimation for large software systems while addressing limitations of degree-based approaches. Open-source implementation enables practical adoption and further research in knowledge risk analysis.

Abstract: The bus-factor is a critical risk indicator that quantifies how many key
contributors a project can afford to lose before core knowledge or
functionality is compromised. Despite its practical importance, accurately
computing the bus-factor is NP-Hard under established formalizations, making
scalable analysis infeasible for large software systems.
  In this paper, we model software projects as bipartite graphs of developers
and tasks and propose two novel approximation heuristics, Minimum Coverage and
Maximum Coverage, based on iterative graph peeling, for two influential
bus-factor formalizations. Our methods significantly outperform the widely
adopted degree-based heuristic, which we show can yield severely inflated
estimates.
  We conduct a comprehensive empirical evaluation on over $1\,000$ synthetic
power-law graphs and demonstrate that our heuristics provide tighter estimates
while scaling to graphs with millions of nodes and edges in minutes. Our
results reveal that the proposed heuristics are not only more accurate but also
robust to structural variations in developer-task assignment graph. We release
our implementation as open-source software to support future research and
practical adoption.

</details>


### [21] [Exploring the Potential of Large Language Models in Fine-Grained Review Comment Classification](https://arxiv.org/abs/2508.09832)
*Linh Nguyen,Chunhua Liu,Hong Yi Lin,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: This paper explores usingLarge Language Models (LLMs) to classify code review comments, showing LLMs outperform state-of-the-art deep learning models by providing accurate and balanced classification across 17 categories, including low-frequency issues, with minimal training data requirements.


<details>
  <summary>Details</summary>
Motivation: Previous automated code review comment classification methods rely on supervised machine learning requiring extensive manual annotation, which is labor-intensive. State-of-the-art approaches struggle with low-training-example categories that are often critical for effectiveness.

Method: The authors assess LLM performance across 17 code review comment categories through empirical evaluation to demonstrate their ability to handle classification tasks without dependence on highly distributed small training data.

Result: LLMs achieved higher accuracy than state-of-the-art deep learning models, particularly excelling in the five most useful categories with low training examples. They provided balanced classification performance across high- and low-frequency categories.

Conclusion: LLMs offer a scalable solution for code review analytics by reducing reliance on manual annotation and addressing the limitations of existing models in handling low-frequency but important comments, potentially improving code review effectiveness.

Abstract: Code review is a crucial practice in software development. As code review
nowadays is lightweight, various issues can be identified, and sometimes, they
can be trivial. Research has investigated automated approaches to classify
review comments to gauge the effectiveness of code reviews. However, previous
studies have primarily relied on supervised machine learning, which requires
extensive manual annotation to train the models effectively. To address this
limitation, we explore the potential of using Large Language Models (LLMs) to
classify code review comments. We assess the performance of LLMs to classify 17
categories of code review comments. Our results show that LLMs can classify
code review comments, outperforming the state-of-the-art approach using a
trained deep learning model. In particular, LLMs achieve better accuracy in
classifying the five most useful categories, which the state-of-the-art
approach struggles with due to low training examples. Rather than relying
solely on a specific small training data distribution, our results show that
LLMs provide balanced performance across high- and low-frequency categories.
These results suggest that the LLMs could offer a scalable solution for code
review analytics to improve the effectiveness of the code review process.

</details>


### [22] [An Empirical Study of CGO Usage in Go Projects -- Distribution, Purposes, Patterns and Critical Issues](https://arxiv.org/abs/2508.09875)
*Jinbao Chen,Boyao Ding,Yu Zhang,Qingwei Li,Fugen Tang*

Main category: cs.SE

TL;DR: The paper explores CGO, an emerging FFI in Go, identifying its unique risks through an empirical study of 920 projects and proposing improvements to the Go toolchain.


<details>
  <summary>Details</summary>
Motivation: Existing FFI research focuses on Python/Java, neglecting CGO in Go where unique risks exist. This study aims to reveal CGO's distribution, patterns, purposes, and critical issues to enhance development safety.

Method: Conducted empirical analysis of CGO across 920 Go projects using CGOAnalyzer, a tool for identifying and quantifying CGO-related features.

Result: 11.3% of projects use CGO with 15 observed patterns. 19 issue types (including runtime crash risks from pointer checks) identified. A temporary pointer check mitigation and a proposed toolchain improvement were developed.

Conclusion: Findings provide actionable insights for developers to reduce CGO risks while aiding Go's toolchain evolution. The accepted proposal offers a path toward resolving compilation limitations.

Abstract: Multilingual software development integrates multiple languages into a single
application, with the Foreign Function Interface (FFI) enabling seamless
interaction. While FFI boosts efficiency and extensibility, it also introduces
risks. Existing studies focus on FFIs in languages like Python and Java,
neglecting CGO, the emerging FFI in Go, which poses unique risks.
  To address these concerns, we conduct an empirical study of CGO usage across
920 open-source Go projects. Our study aims to reveal the distribution,
patterns, purposes, and critical issues associated with CGO, offering insights
for developers and the Go team. We develop CGOAnalyzer, a tool to efficiently
identify and quantify CGO-related features. Our findings reveal that: (1) 11.3%
of analyzed Go projects utilize CGO, with usage concentrated in a subset of
projects; (2) CGO serves 4 primary purposes, including system-level
interactions and performance optimizations, with 15 distinct usage patterns
observed; (3) 19 types of CGO-related issues exist, including one critical
issue involving unnecessary pointer checks that pose risks of runtime crashes
due to limitations in the current Go compilation toolchain; (4) a temporary
solution reduces unnecessary pointer checks, mitigating crash risks, and (5) we
submitted a proposal to improve the Go toolchain for a permanent fix, which has
been grouped within an accepted proposal for future resolution. Our findings
provide valuable insights for developers and the Go team, enhancing development
efficiency and reliability while improving the robustness of the Go toolchain.

</details>
