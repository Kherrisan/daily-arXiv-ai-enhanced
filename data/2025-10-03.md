<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 17]
- [cs.SE](#cs.SE) [Total: 14]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Jailbreaking LLMs via Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge](https://arxiv.org/abs/2510.01223)
*Hui Dou,Ning Xu,Yiwen Zhang,Kaibin Wang*

Main category: cs.CR

TL;DR: RTS-Attack exploits semantically aligned nested scenarios with hidden toxic knowledge to bypass LLM safety defenses, outperforming prior methods in effectiveness and stealth.


<details>
  <summary>Details</summary>
Motivation: Existing nested scenario methods for jailbreaking LLMs are detectable due to overt malicious intent. The paper identifies that LLM alignment defenses are insufficiently sensitive to nested scenarios with semantic relevance and toxic knowledge, a direction needing systematic exploration.

Method: RTS-Attack constructs semantically relevant nested scenarios and integrates targeted toxic knowledge to adaptively generate jailbreak prompts. The framework is automated, prioritizing concealment by avoiding direct harmful queries while exploiting LLM alignment weaknesses.

Result: RTS-Attack achieves superior efficiency and universality across advanced LLMs (GPT-4o, Llama3-70b, Gemini-pro) compared to baselines. It successfully bypasses defenses while maintaining concealment by avoiding harmful query patterns.

Conclusion: The paper introduces RTS-Attack, an effective and stealthy method for bypassing LLM alignment defenses by using semantically relevant nested scenarios with targeted toxic knowledge, demonstrating critical vulnerabilities in current defense mechanisms against jailbreak attacks.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various tasks. However, they remain exposed to jailbreak attacks, eliciting
harmful responses. The nested scenario strategy has been increasingly adopted
across various methods, demonstrating immense potential. Nevertheless, these
methods are easily detectable due to their prominent malicious intentions. In
this work, we are the first to find and systematically verify that LLMs'
alignment defenses are not sensitive to nested scenarios, where these scenarios
are highly semantically relevant to the queries and incorporate targeted toxic
knowledge. This is a crucial yet insufficiently explored direction. Based on
this, we propose RTS-Attack (Semantically Relevant Nested Scenarios with
Targeted Toxic Knowledge), an adaptive and automated framework to examine LLMs'
alignment. By building scenarios highly relevant to the queries and integrating
targeted toxic knowledge, RTS-Attack bypasses the alignment defenses of LLMs.
Moreover, the jailbreak prompts generated by RTS-Attack are free from harmful
queries, leading to outstanding concealment. Extensive experiments demonstrate
that RTS-Attack exhibits superior performance in both efficiency and
universality compared to the baselines across diverse advanced LLMs, including
GPT-4o, Llama3-70b, and Gemini-pro. Our complete code is available in the
supplementary material. WARNING: THIS PAPER CONTAINS POTENTIALLY HARMFUL
CONTENT.

</details>


### [2] [Fine-Tuning Jailbreaks under Highly Constrained Black-Box Settings: A Three-Pronged Approach](https://arxiv.org/abs/2510.01342)
*Xiangfang Li,Yu Wang,Bo Li*

Main category: cs.CR

TL;DR: The paper introduces a three-pronged jailbreak attack against fine-tuned large language models, demonstrating high success rates on GPT-4 variants while operating under realistic dataset-only black-box constraints. The attack combines surface-appealing encodings and a backdoor mechanism to bypass multi-stage defenses.


<details>
  <summary>Details</summary>
Motivation: Existing jailbreak attack research focuses on oversimplified scenarios, lacking practical relevance for real-world provider defense systems that use multi-stage protections during model fine-tuning.

Method: The attack combines three components: safety-styled text wrappers (prefix/suffix), lexical obfuscation techniques (underscore encoding), and a backdoor mechanism that maintains innocuous appearance in individual training examples while inducbing harmful behavior retention in models.

Result: The attack achieves >97% success rate against GPT-4.1 and GPT-4o on OpenAI platform. Attack bypasses all three defense stages (pre-upload filtering, defensive training, post-training audit), demonstrating vulnerabilities in real-world fine-tuning pipelines.

Conclusion: The work reveals critical gaps in current enterprise LLM fine-tuning defense systems, showing that even with multi-stage protections, attackers can craft effective jailbreak attacks through strategic data manipulation without model parameter access.

Abstract: With the rapid advancement of large language models (LLMs), ensuring their
safe use becomes increasingly critical. Fine-tuning is a widely used method for
adapting models to downstream tasks, yet it is vulnerable to jailbreak attacks.
However, most existing studies focus on overly simplified attack scenarios,
limiting their practical relevance to real-world defense settings. To make this
risk concrete, we present a three-pronged jailbreak attack and evaluate it
against provider defenses under a dataset-only black-box fine-tuning interface.
In this setting, the attacker can only submit fine-tuning data to the provider,
while the provider may deploy defenses across stages: (1) pre-upload data
filtering, (2) training-time defensive fine-tuning, and (3) post-training
safety audit. Our attack combines safety-styled prefix/suffix wrappers, benign
lexical encodings (underscoring) of sensitive tokens, and a backdoor mechanism,
enabling the model to learn harmful behaviors while individual datapoints
appear innocuous. Extensive experiments demonstrate the effectiveness of our
approach. In real-world deployment, our method successfully jailbreaks GPT-4.1
and GPT-4o on the OpenAI platform with attack success rates above 97% for both
models. Our code is available at
https://github.com/lxf728/tri-pronged-ft-attack.

</details>


### [3] [Integrated Security Mechanisms for Weight Protection in Memristive Crossbar Arrays](https://arxiv.org/abs/2510.01350)
*Muhammad Faheemur Rahman,Wayne Burleson*

Main category: cs.CR

TL;DR: This paper proposes two lightweight security mechanisms for memristive crossbar arrays that protect neural network weights and establish ownership verification with under 10% hardware overhead, demonstrated through CMOS node simulations and MNIST experiments.


<details>
  <summary>Details</summary>
Motivation: Non-volatile memristors in crossbar arrays face security risks from adversarial weight extraction attacks, threatening valuable intellectual property derived from large proprietary datasets. Protection mechanisms are critical to maintain confidentiality and verifiable ownership even if hardware is compromised.

Method: The authors propose two hardware-based security mechanisms: 1) Keyed Permutor, which obfuscates stored weights using a cryptographic key; 2) Watermark Protection Columns, which embed verifiable ownership markers. These mechanisms integrate seamlessly into existing memristive crossbar arrays without requiring major architectural redesigns.

Result: Simulation results across 45nm, 22nm, and 7nm CMOS nodes with realistic interconnect models show <10% overhead in area/delay/power metrics. MNIST experiments demonstrate practical feasibility of securing in-memory computing systems against data leakage with negligible performance degradation.

Conclusion: The paper concludes that the proposed Keyed Permutor and Watermark Protection Columns mechanisms effectively secure memristive in-memory computing systems with minimal performance overhead, making them viable for protecting intellectual property against weight extraction attacks.

Abstract: Memristive crossbar arrays enable in-memory computing by performing parallel
analog computations directly within memory, making them well-suited for machine
learning, neural networks, and neuromorphic systems. However, despite their
advantages, non-volatile memristors are vulnerable to security threats (such as
adversarial extraction of stored weights when the hardware is compromised.
Protecting these weights is essential since they represent valuable
intellectual property resulting from lengthy and costly training processes
using large, often proprietary, datasets. As a solution we propose two security
mechanisms: Keyed Permutor and Watermark Protection Columns; where both
safeguard critical weights and establish verifiable ownership (even in cases of
data leakage). Our approach integrates efficiently with existing memristive
crossbar architectures without significant design modifications. Simulations
across 45nm, 22nm, and 7nm CMOS nodes, using a realistic interconnect model and
a large RF dataset, show that both mechanisms offer robust protection with
under 10% overhead in area, delay and power. We also present initial
experiments employing the widely known MNIST dataset; further highlighting the
feasibility of securing memristive in-memory computing systems with minimal
performance trade-offs.

</details>


### [4] [WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents](https://arxiv.org/abs/2510.01354)
*Yinuo Liu,Ruohan Xu,Xilong Wang,Yuqi Jia,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: This paper introduces the first comprehensive benchmark for detecting prompt injection attacks on web agents, revealing critical limitations of existing methods against stealthy attacks. Datasets/code: https://github.com/Norrrrrrr-lyn/WAInjectBench.


<details>
  <summary>Details</summary>
Motivation: Existing detection methods for prompt injection attacks lack systematic evaluation on web agents despite proposed attacks against them. The work aims to bridge this gap by providing the first comprehensive benchmark for detecting such attacks.

Method: The authors present a fine-grained categorization of prompt injection attacks, construct datasets with malicious and benign text/image samples, and evaluate text-/image-based detection methods across multiple scenarios to measure their effectiveness.

Result: Key findings show moderate-to-high accuracy for detectors targeting explicit textual instructions or visible image perturbations, but significant failure rates against attacks omitting explicit instructions or employing imperceptible perturbations.

Conclusion: The paper concludes that while some detectors can identify attacks with explicit instructions or visible perturbations, they fail against stealthier attacks (without explicit instructions or imperceptible changes), highlighting the need for improved detection methods for web agents.

Abstract: Multiple prompt injection attacks have been proposed against web agents. At
the same time, various methods have been developed to detect general prompt
injection attacks, but none have been systematically evaluated for web agents.
In this work, we bridge this gap by presenting the first comprehensive
benchmark study on detecting prompt injection attacks targeting web agents. We
begin by introducing a fine-grained categorization of such attacks based on the
threat model. We then construct datasets containing both malicious and benign
samples: malicious text segments generated by different attacks, benign text
segments from four categories, malicious images produced by attacks, and benign
images from two categories. Next, we systematize both text-based and
image-based detection methods. Finally, we evaluate their performance across
multiple scenarios. Our key findings show that while some detectors can
identify attacks that rely on explicit textual instructions or visible image
perturbations with moderate to high accuracy, they largely fail against attacks
that omit explicit instructions or employ imperceptible perturbations. Our
datasets and code are released at:
https://github.com/Norrrrrrr-lyn/WAInjectBench.

</details>


### [5] [Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks](https://arxiv.org/abs/2510.01359)
*Shoumik Saha,Jifan Chen,Sam Mayers,Sanjay Krishna Gouda,Zijian Wang,Varun Kumar*

Main category: cs.CR

TL;DR: JAWS-BENCH reveals code agents execute malicious code in 27%-75% of cases depending on workspace complexity. Agent workflows amplify attacks by overturning initial refusals during planning, necessitating execution-aware defenses.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of code-capable LLM security focus on refusal/harm detection but ignore whether agents actually execute malicious code. The paper aims to quantify deployable harm in software engineering workflows where agents write/run code.

Method: The authors developed JAWS-BENCH, a benchmark with three workspace regimes (JAWS-0, JAWS-1, JAWS-M) and a hierarchical Judge Framework to evaluate compliance, attack success, syntactic correctness, and runtime executability in code-capable LLMs. They tested seven LLMs across all regimes.

Result: In JAWS-0, 61% of attacks are accepted (27% run end-to-end). JAWS-1 shows 71% attack success rates while appearing 100% compliant, and JAWS-M reaches 75% mean ASR with 32% instantly deployable attacks. Agentization increases ASR by 1.6x due to initial refusals being overturned in planning steps.

Conclusion: The study highlights the need for execution-aware defenses and mechanisms that preserve refusal decisions in code-capable LLM agents, as agentization increases attack success rates due to compromised multi-step planning. Future work should focus on code-contextual safety filters and maintaining safety throughout agent workflows.

Abstract: Code-capable large language model (LLM) agents are increasingly embedded into
software engineering workflows where they can read, write, and execute code,
raising the stakes of safety-bypass ("jailbreak") attacks beyond text-only
settings. Prior evaluations emphasize refusal or harmful-text detection,
leaving open whether agents actually compile and run malicious programs. We
present JAWS-BENCH (Jailbreaks Across WorkSpaces), a benchmark spanning three
escalating workspace regimes that mirror attacker capability: empty (JAWS-0),
single-file (JAWS-1), and multi-file (JAWS-M). We pair this with a
hierarchical, executable-aware Judge Framework that tests (i) compliance, (ii)
attack success, (iii) syntactic correctness, and (iv) runtime executability,
moving beyond refusal to measure deployable harm. Using seven LLMs from five
families as backends, we find that under prompt-only conditions in JAWS-0, code
agents accept 61% of attacks on average; 58% are harmful, 52% parse, and 27%
run end-to-end. Moving to single-file regime in JAWS-1 drives compliance to ~
100% for capable models and yields a mean ASR (Attack Success Rate) ~ 71%; the
multi-file regime (JAWS-M) raises mean ASR to ~ 75%, with 32% instantly
deployable attack code. Across models, wrapping an LLM in an agent
substantially increases vulnerability -- ASR raises by 1.6x -- because initial
refusals are frequently overturned during later planning/tool-use steps.
Category-level analyses identify which attack classes are most vulnerable and
most readily deployable, while others exhibit large execution gaps. These
findings motivate execution-aware defenses, code-contextual safety filters, and
mechanisms that preserve refusal decisions throughout the agent's multi-step
reasoning and tool use.

</details>


### [6] [E-FuzzEdge: Optimizing Embedded Device Security with Scalable In-Place Fuzzing](https://arxiv.org/abs/2510.01393)
*Davide Rusconi,Osama Yousef,Mirco Picca,Flavio Toffalini,Andrea Lanzi*

Main category: cs.CR

TL;DR: E-FuzzEdge is a fuzzing architecture designed to enhance fuzzing throughput for microcontrollers by optimizing execution speed, with compatibility for on-device testing methods.


<details>
  <summary>Details</summary>
Motivation: Hardware-in-the-loop fuzzing for microcontrollers lacks efficiency and scalability, necessitating a solution that improves throughput without relying on firmware emulation.

Method: The authors developed E-FuzzEdge, which optimizes execution speed through an architecture that integrates with on-device testing techniques, validated via state-of-the-art benchmarks.

Result: E-FuzzEdge demonstrated significant performance improvements in throughput while maintaining compatibility with existing embedded fuzzing workflows.

Conclusion: E-FuzzEdge provides a scalable, high-performance alternative for embedded fuzzing, enabling broader adoption and integration with current on-device testing methodologies.

Abstract: In this paper we show E-FuzzEdge, a novel fuzzing architecture targeted
towards improving the throughput of fuzzing campaigns in contexts where
scalability is unavailable. E-FuzzEdge addresses the inefficiencies of
hardware-in-the-loop fuzzing for microcontrollers by optimizing execution
speed. We evaluated our system against state-of-the-art benchmarks,
demonstrating significant performance improvements. A key advantage of
E-FuzzEdgearchitecture is its compatibility with other embedded fuzzing
techniques that perform on device testing instead of firmware emulation. This
means that the broader embedded fuzzing community can integrate E-FuzzEdge into
their workflows to enhance overall testing efficiency.

</details>


### [7] [Securing IoT Devices in Smart Cities: A Review of Proposed Solutions](https://arxiv.org/abs/2510.01445)
*Andrés F. Betancur-López*

Main category: cs.CR

TL;DR: This paper reviews IoT security challenges in Smart Cities, evaluates recent proposals (lightweight cryptography, PUFs, blockchain), and advocates for more scalable, practical solutions to enhance privacy and security.


<details>
  <summary>Details</summary>
Motivation: The increasing adoption of resource-constrained IoT devices in Smart Cities introduces significant privacy and security risks, necessitating robust protective measures against vulnerabilities and attacks.

Method: The authors conducted a review of recent literature on device-level security for IoT in Smart Cities, focusing on lightweight cryptography, physically unclonable functions (PUFs), and blockchain-based solutions.

Result: The review highlights both strengths and limitations of existing security approaches, underscoring gaps in scalability, practicality, and resource efficiency for real-world IoT ecosystems.

Conclusion: The article emphasizes the need for more practical, scalable, and resource-efficient security mechanisms to protect IoT devices in Smart Cities, ensuring user privacy and data protection.

Abstract: Privacy and security in Smart Cities remain at constant risk due to the
vulnerabilities introduced by Internet of Things (IoT) devices. The limited
computational resources of these devices make them especially susceptible to
attacks, while their widespread adoption increases the potential impact of
security breaches. This article presents a review of security proposals aimed
at protecting IoT devices in Smart City environments. The review was conducted
by analyzing recent literature on device-level security, with particular
emphasis on lightweight cryptography, physically unclonable functions (PUFs),
and blockchain-based solutions. Findings highlight both the strengths and
limitations of current approaches, as well as the need for more practical,
scalable, and resource-efficient mechanisms to ensure user privacy and data
protection in IoT ecosystems.

</details>


### [8] [POLAR: Automating Cyber Threat Prioritization through LLM-Powered Assessment](https://arxiv.org/abs/2510.01552)
*Luoxi Tang,Yuqiao Meng,Ankita Patra,Weicheng Ma,Muchao Ye,Zhaohan Xi*

Main category: cs.CR

TL;DR: This paper uncovers core vulnerabilities preventing LLMs from reliably supporting cyber threat intelligence, including spurious correlations and generalization limits. A structured analysis framework with real-world evaluations reveals critical failure modes, followed by actionable guidance for building more robust CTI systems.


<details>
  <summary>Details</summary>
Motivation: The paper addresses critical performance gaps in deploying LLMs for cyber threat intelligence (CTI), where current approaches fail to account for vulnerabilities inherent to threat landscapes rather than just model architectures. This is essential for improving real-world threat detection and response effectiveness.

Method: A novel categorization methodology combining stratification, autoregressive refinement, and human-in-the-loop supervision was applied to large-scale evaluations across CTI benchmarks and real-world threat reports. This approach enables systematic analysis of LLM failure modes through empirical experimentation and human validation.

Result: Three fundamental LLM vulnerabilities were identified: 1) spurious correlations leading to false threat associations, 2) contradictory knowledge about evolving threats, and 3) constrained generalization to novel attack patterns. The proposed methodology enables reliable failure analysis and provides concrete mitigation strategies for CTI systems.

Conclusion: The analysis provides actionable insights for designing more robust LLM-powered CTI systems and establishes a framework for addressing LLM vulnerabilities in cybersecurity contexts. The work highlights the importance of understanding contextual challenges beyond model architecture to enhance practical CTI applications.

Abstract: Large Language Models (LLMs) are intensively used to assist security analysts
in counteracting the rapid exploitation of cyber threats, wherein LLMs offer
cyber threat intelligence (CTI) to support vulnerability assessment and
incident response. While recent work has shown that LLMs can support a wide
range of CTI tasks such as threat analysis, vulnerability detection, and
intrusion defense, significant performance gaps persist in practical
deployments. In this paper, we investigate the intrinsic vulnerabilities of
LLMs in CTI, focusing on challenges that arise from the nature of the threat
landscape itself rather than the model architecture. Using large-scale
evaluations across multiple CTI benchmarks and real-world threat reports, we
introduce a novel categorization methodology that integrates stratification,
autoregressive refinement, and human-in-the-loop supervision to reliably
analyze failure instances. Through extensive experiments and human inspections,
we reveal three fundamental vulnerabilities: spurious correlations,
contradictory knowledge, and constrained generalization, that limit LLMs in
effectively supporting CTI. Subsequently, we provide actionable insights for
designing more robust LLM-powered CTI systems to facilitate future research.

</details>


### [9] [Position: Privacy Is Not Just Memorization!](https://arxiv.org/abs/2510.01645)
*Niloofar Mireshghallah,Tianshi Li*

Main category: cs.CR

TL;DR: This paper argues for reorienting LLM privacy research beyond memorization by identifying overlooked threats and advocating interdisciplinary approaches to address systemic risks across the LLM lifecycle.


<details>
  <summary>Details</summary>
Motivation: Current research disproportionately focuses on memorization, neglecting more immediate and scalable privacy risks such as data collection practices, inference-time context leakage, autonomous agent threats, and surveillance via deep inference attacks.

Method: The authors conducted a longitudinal analysis of 1,322 AI/ML privacy papers (2016–2025) and developed a taxonomy of privacy risks across the LLM lifecycle, supported by case studies.

Result: The analysis reveals that while memorization dominates technical research, the most pressing privacy harms stem from underexplored risks with limited existing mitigation strategies or clear solutions.

Conclusion: The paper concludes that the LLM privacy discourse must expand beyond memorization to address a broader range of sociotechnical threats through interdisciplinary approaches.

Abstract: The discourse on privacy risks in Large Language Models (LLMs) has
disproportionately focused on verbatim memorization of training data, while a
constellation of more immediate and scalable privacy threats remain
underexplored. This position paper argues that the privacy landscape of LLM
systems extends far beyond training data extraction, encompassing risks from
data collection practices, inference-time context leakage, autonomous agent
capabilities, and the democratization of surveillance through deep inference
attacks. We present a comprehensive taxonomy of privacy risks across the LLM
lifecycle -- from data collection through deployment -- and demonstrate through
case studies how current privacy frameworks fail to address these multifaceted
threats. Through a longitudinal analysis of 1,322 AI/ML privacy papers
published at leading conferences over the past decade (2016--2025), we reveal
that while memorization receives outsized attention in technical research, the
most pressing privacy harms lie elsewhere, where current technical approaches
offer little traction and viable paths forward remain unclear. We call for a
fundamental shift in how the research community approaches LLM privacy, moving
beyond the narrow focus of current technical solutions and embracing
interdisciplinary approaches that address the sociotechnical nature of these
emerging threats.

</details>


### [10] [Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks](https://arxiv.org/abs/2510.01676)
*Milad Nasr,Yanick Fratantonio,Luca Invernizzi,Ange Albertini,Loua Farah,Alex Petit-Bianco,Andreas Terzis,Kurt Thomas,Elie Bursztein,Nicholas Carlini*

Main category: cs.CR

TL;DR: Adversarial attacks on Gmail's ML-based malware routing (Magika) enable malware evasion with minimal byte changes. A deployed defense reduces attack effectiveness by 75% when requiring higher modifications.


<details>
  <summary>Details</summary>
Motivation: The study addresses vulnerabilities in real-world ML systems, showing how adversarial attacks against a single component (Magika) can undermine entire security pipelines like Gmail's malware detection.

Method: The authors designed adversarial examples to manipulate Magika, Gmail's file-type identifier, by altering 13 bytes to route malware to unsuitable classifiers. They then developed a defense strategy reducing evasion success to 20% with 50-byte modifications.

Result: Adversarial attacks achieved 90% evasion with 13-byte changes, but a developed defense reduced success to 20% with 50-byte changes and was deployed in production.

Conclusion: This paper highlights the critical vulnerabilities in production ML systems when adversarial attacks target individual components, demonstrating a practical defense deployed by Gmail to mitigate such risks.

Abstract: As deep learning models become widely deployed as components within larger
production systems, their individual shortcomings can create system-level
vulnerabilities with real-world impact. This paper studies how adversarial
attacks targeting an ML component can degrade or bypass an entire
production-grade malware detection system, performing a case study analysis of
Gmail's pipeline where file-type identification relies on a ML model.
  The malware detection pipeline in use by Gmail contains a machine learning
model that routes each potential malware sample to a specialized malware
classifier to improve accuracy and performance. This model, called Magika, has
been open sourced. By designing adversarial examples that fool Magika, we can
cause the production malware service to incorrectly route malware to an
unsuitable malware detector thereby increasing our chance of evading detection.
Specifically, by changing just 13 bytes of a malware sample, we can
successfully evade Magika in 90% of cases and thereby allow us to send malware
files over Gmail. We then turn our attention to defenses, and develop an
approach to mitigate the severity of these types of attacks. For our defended
production model, a highly resourced adversary requires 50 bytes to achieve
just a 20% attack success rate. We implement this defense, and, thanks to a
collaboration with Google engineers, it has already been deployed in production
for the Gmail classifier.

</details>


### [11] [Towards Imperceptible Adversarial Defense: A Gradient-Driven Shield against Facial Manipulations](https://arxiv.org/abs/2510.01699)
*Yue Li,Linying Xue,Dongdong Lin,Qiushi Li,Hui Tian,Hongxia Wang*

Main category: cs.CR

TL;DR: The paper introduces GRASP, a method to defend against facial deepfakes by using gradient projection to manage conflicting losses in adversarial perturbation, resulting in nearly undetectable perturbations and strong defense performance.


<details>
  <summary>Details</summary>
Motivation: As deepfake technology has advanced, manipulated facial images have created serious privacy and trust issues in society. Proactive defense methods that embed adversarial perturbations face challenges in maintaining both imperceptibility and effectiveness due to the tradeoff between defense and visual quality.

Method: GRASP is a gradient-projection-based adversarial proactive defense method that integrates structural similarity loss and low-frequency loss to enhance perturbation imperceptibility. It addresses gradient conflicts through an analysis of conflicts between defense effectiveness loss and visual quality losses and uses a gradient-projection mechanism to enable a balanced optimization.

Result: GRASP achieves PSNR over 40 dB and SSIM of 0.99, along with a 100% defense success rate against facial deepfake manipulations. It outperforms existing methods in both defense effectiveness and visual quality.

Conclusion: The proposed GRASP method successfully resolves the gradient conflicts typically encountered during adversarial training for deepfake defense. By prioritizing both image fidelity and defense performance, it represents a significant advancement in proactive defense strategies against facial deepfake attacks.

Abstract: With the flourishing prosperity of generative models, manipulated facial
images have become increasingly accessible, raising concerns regarding privacy
infringement and societal trust. In response, proactive defense strategies
embed adversarial perturbations into facial images to counter deepfake
manipulation. However, existing methods often face a tradeoff between
imperceptibility and defense effectiveness-strong perturbations may disrupt
forgeries but degrade visual fidelity. Recent studies have attempted to address
this issue by introducing additional visual loss constraints, yet often
overlook the underlying gradient conflicts among losses, ultimately weakening
defense performance. To bridge the gap, we propose a gradient-projection-based
adversarial proactive defense (GRASP) method that effectively counters facial
deepfakes while minimizing perceptual degradation. GRASP is the first approach
to successfully integrate both structural similarity loss and low-frequency
loss to enhance perturbation imperceptibility. By analyzing gradient conflicts
between defense effectiveness loss and visual quality losses, GRASP pioneers
the design of the gradient-projection mechanism to mitigate these conflicts,
enabling balanced optimization that preserves image fidelity without
sacrificing defensive performance. Extensive experiments validate the efficacy
of GRASP, achieving a PSNR exceeding 40 dB, SSIM of 0.99, and a 100% defense
success rate against facial attribute manipulations, significantly
outperforming existing approaches in visual quality.

</details>


### [12] [Constructions of Efficiently Implementable Boolean Functions with Provable Nonlinearity/Resiliency/Algebraic Immunity Trade-Offs](https://arxiv.org/abs/2510.01720)
*Palash Sarkar*

Main category: cs.CR

TL;DR: The paper introduces Boolean functions with provable trade-offs in resiliency, nonlinearity, and algebraic immunity, constructible with linear variable count and efficient gate implementation.


<details>
  <summary>Details</summary>
Motivation: Cryptographic systems require Boolean functions balancing security properties (resiliency, nonlinearity, algebraic immunity). Prior work lacked efficient constructions with provable multi-parameter trade-offs.

Method: Mathematical construction of function families achieving specified bounds on resiliency $m_0$, linear bias $2^{-x_0}$ (nonlinearity), and algebraic immunity $a_0$. Parameters scale linearly, with $O(n)$ gate complexity.

Result: For given $m_0,x_0,a_0$, constructs $n$-variable functions meeting all three property bounds simultaneously, where $n$ is linear in $m_0,x_0,a_0$ and implementation uses $O(n)$ gates.

Conclusion: The work establishes theoretically optimal trade-offs between critical cryptographic properties while maintaining hardware efficiency, enabling practical implementations with provable security guarantees.

Abstract: We describe several families of efficiently implementable Boolean functions
achieving provable trade-offs between resiliency, nonlinearity, and algebraic
immunity. In concrete terms, the following result holds for each of the
function families that we propose. Given integers $m_0\geq 0$, $x_0\geq 1$, and
$a_0\geq 1$, it is possible to construct an $n$-variable function which has
resiliency at least $m_0$, linear bias (which is an equivalent method of
expressing nonlinearity) at most $2^{-x_0}$ and algebraic immunity at least
$a_0$; further, $n$ is linear in $m_0$, $x_0$ and $a_0$, and the function can
be implemented using $O(n)$ gates.

</details>


### [13] [Secure Multi-Modal Data Fusion in Federated Digital Health Systems via MCP](https://arxiv.org/abs/2510.01780)
*Aueaphum Aueawatthanaphisut*

Main category: cs.CR

TL;DR: This work introduces an MCP-based framework that enables secure, interoperable multi-modal federated learning for healthcare, achieving improved diagnostic accuracy and reduced client dropouts while maintaining privacy.


<details>
  <summary>Details</summary>
Motivation: Heterogeneous medical data integration remains challenging due to distributed environments and resource constraints. Existing federated learning (FL) frameworks lack standardized mechanisms for secure, multi-modal data fusion while balancing privacy and utility.

Method: The framework combines three pillars: (1) multi-modal feature alignment for heterogeneous data types (imaging, EHR, wearable IoT), (2) secure aggregation with differential privacy, and (3) energy-aware scheduling to reduce client dropouts. The Model Context Protocol (MCP) serves as a schema-driven interoperability layer coordinating distributed AI agents.

Result: Benchmark evaluations show 9.8% higher diagnostic accuracy than baseline FL, 54% fewer client dropouts, and privacy-utility trade-offs acceptable in clinical settings.

Conclusion: The study demonstrates that MCP-enabled multi-modal fusion offers a scalable and trustworthy pathway for secure, interoperable federated healthcare systems, addressing limitations in current frameworks while achieving clinically meaningful improvements in accuracy and robustness.

Abstract: Secure and interoperable integration of heterogeneous medical data remains a
grand challenge in digital health. Current federated learning (FL) frameworks
offer privacy-preserving model training but lack standardized mechanisms to
orchestrate multi-modal data fusion across distributed and resource-constrained
environments. This study introduces a novel framework that leverages the Model
Context Protocol (MCP) as an interoperability layer for secure, cross-agent
communication in multi-modal federated healthcare systems. The proposed
architecture unifies three pillars: (i) multi-modal feature alignment for
clinical imaging, electronic medical records, and wearable IoT data; (ii)
secure aggregation with differential privacy to protect patient-sensitive
updates; and (iii) energy-aware scheduling to mitigate dropouts in mobile
clients. By employing MCP as a schema-driven interface, the framework enables
adaptive orchestration of AI agents and toolchains while ensuring compliance
with privacy regulations. Experimental evaluation on benchmark datasets and
pilot clinical cohorts demonstrates up to 9.8\% improvement in diagnostic
accuracy compared with baseline FL, a 54\% reduction in client dropout rates,
and clinically acceptable privacy--utility trade-offs. These results highlight
MCP-enabled multi-modal fusion as a scalable and trustworthy pathway toward
equitable, next-generation federated health infrastructures.

</details>


### [14] [ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs](https://arxiv.org/abs/2510.01967)
*Aadarsh Anantha Ramakrishnan,Shubham Agarwal,Selvanayagam S,Kunwar Singh*

Main category: cs.CR

TL;DR: ZK-WAGON is a ZK-SNARK-based watermarking system for image generation models, enabling verifiable, privacy-preserving, and model-agnostic synthetic image authentication.


<details>
  <summary>Details</summary>
Motivation: Traditional watermarking methods degrade image quality, are removable, or require confidential model access. Synthetic media risks (misinformation, deepfakes) demand secure, verifiable, and privacy-preserving solutions.

Method: The method introduces ZK-WAGON, a ZK-SNARK-based watermarking system, and proposes Selective Layer ZK-Circuit Creation (SL-ZKCC) to optimize proof generation by targeting key model layers. Proofs are embedded via LSB steganography.

Result: ZK-WAGON demonstrates secure watermarking on GAN and Diffusion models with imperceptible LSB-embedded proofs. It achieves efficient proof generation while maintaining image quality and privacy.

Conclusion: The paper concludes that ZK-WAGON provides a secure, model-agnostic solution for watermarking image generation models, enabling verifiable image authenticity without compromising privacy or performance.

Abstract: As image generation models grow increasingly powerful and accessible,
concerns around authenticity, ownership, and misuse of synthetic media have
become critical. The ability to generate lifelike images indistinguishable from
real ones introduces risks such as misinformation, deepfakes, and intellectual
property violations. Traditional watermarking methods either degrade image
quality, are easily removed, or require access to confidential model internals
- making them unsuitable for secure and scalable deployment. We are the first
to introduce ZK-WAGON, a novel system for watermarking image generation models
using the Zero-Knowledge Succinct Non Interactive Argument of Knowledge
(ZK-SNARKs). Our approach enables verifiable proof of origin without exposing
model weights, generation prompts, or any sensitive internal information. We
propose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively
convert key layers of an image generation model into a circuit, reducing proof
generation time significantly. Generated ZK-SNARK proofs are imperceptibly
embedded into a generated image via Least Significant Bit (LSB) steganography.
We demonstrate this system on both GAN and Diffusion models, providing a
secure, model-agnostic pipeline for trustworthy AI image generation.

</details>


### [15] [Mirage Fools the Ear, Mute Hides the Truth: Precise Targeted Adversarial Attacks on Polyphonic Sound Event Detection Systems](https://arxiv.org/abs/2510.02158)
*Junjie Su,Weifei Jin,Yuxin Cao,Derui Wang,Kai Ye,Jie Hao*

Main category: cs.CR

TL;DR: This paper proposes the M2A framework for targeted adversarial attacks on polyphonic Sound Event Detection (SED) systems, achieving high Editing Precision (EP) through preservation loss and a novel evaluation metric.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial attacks on SED systems lack effectiveness due to strong contextual dependencies or insufficient precision, failing to address both detection and localization requirements in safety-critical applications.

Method: The authors introduce (1) M2A framework with preservation loss that constrains non-target outputs during optimization; (2) Editing Precision (EP), a new metric balancing attack effectiveness and precision. The method ensures non-target regions remain unchanged while enhancing targeted attack performance.

Result: M2A achieves 94.56% and 99.11% EP on two state-of-the-art SED models, demonstrating significantly improved attack precision compared to prior approaches while maintaining high adversarial effectiveness.

Conclusion: The M2A framework effectively addresses both effectiveness and precision limitations in acoustic adversarial attacks, establishing a new benchmark for targeted attacks on polyphonic SED systems through systematic constraint design and metric innovation.

Abstract: Sound Event Detection (SED) systems are increasingly deployed in
safety-critical applications such as industrial monitoring and audio
surveillance. However, their robustness against adversarial attacks has not
been well explored. Existing audio adversarial attacks targeting SED systems,
which incorporate both detection and localization capabilities, often lack
effectiveness due to SED's strong contextual dependencies or lack precision by
focusing solely on misclassifying the target region as the target event,
inadvertently affecting non-target regions. To address these challenges, we
propose the Mirage and Mute Attack (M2A) framework, which is designed for
targeted adversarial attacks on polyphonic SED systems. In our optimization
process, we impose specific constraints on the non-target output, which we
refer to as preservation loss, ensuring that our attack does not alter the
model outputs for non-target region, thus achieving precise attacks.
Furthermore, we introduce a novel evaluation metric Editing Precison (EP) that
balances effectiveness and precision, enabling our method to simultaneously
enhance both. Comprehensive experiments show that M2A achieves 94.56% and
99.11% EP on two state-of-the-art SED models, demonstrating that the framework
is sufficiently effective while significantly enhancing attack precision.

</details>


### [16] [NoMod: A Non-modular Attack on Module Learning With Errors](https://arxiv.org/abs/2510.02162)
*Cristian Bassotto,Ermes Franch,Marina Krček,Stjepan Picek*

Main category: cs.CR

TL;DR: This paper introduces NoMod ML-Attack, a machine learning-based cryptanalytic technique that breaks post-quantum Module-LWE schemes, achieving secret recovery at scales surpassing prior work while using open-source availability to ensure transparency.


<details>
  <summary>Details</summary>
Motivation: Classical public-key cryptography faces obsolescence from quantum computing. NIST's post-quantum standardization initiative needs cryptanalysis to strengthen proposals like Module-LWE against real-world attacks.

Method: NoMod ML-Attack uses a hybrid white-box approach that (1) models modular wrap-arounds as statistical corruption, (2) frames secret recovery as robust linear estimation, and (3) employs lattice preprocessing techniques (e.g., algebraic amplification) with Tukey's Biweight loss-trained estimators.

Result: Full recovery of binary secrets at n=350, sparse binomial secrets at n=256, and sparse CRYSTALS-Kyber secrets for (n,k)=(128,3) and (256,2). Implementation is publicly available at https://anonymous.4open.science/r/NoMod-3BD4.

Conclusion: The NoMod ML-Attack demonstrates practical vulnerabilities in Module-LWE-based post-quantum cryptography and highlights the necessity of refining parameter choices (e.g., n, k) for improved security. The open-source release further enables transparent evaluation.

Abstract: The advent of quantum computing threatens classical public-key cryptography,
motivating NIST's adoption of post-quantum schemes such as those based on the
Module Learning With Errors (Module-LWE) problem. We present NoMod ML-Attack, a
hybrid white-box cryptanalytic method that circumvents the challenge of
modeling modular reduction by treating wrap-arounds as statistical corruption
and casting secret recovery as robust linear estimation. Our approach combines
optimized lattice preprocessing--including reduced-vector saving and algebraic
amplification--with robust estimators trained via Tukey's Biweight loss.
Experiments show NoMod achieves full recovery of binary secrets for dimension
$n = 350$, recovery of sparse binomial secrets for $n = 256$, and successful
recovery of sparse secrets in CRYSTALS-Kyber settings with parameters $(n, k) =
(128, 3)$ and $(256, 2)$. We release our implementation in an anonymous
repository https://anonymous.4open.science/r/NoMod-3BD4.

</details>


### [17] [Testing Stability and Robustness in Three Cryptographic Chaotic Systems](https://arxiv.org/abs/2510.02184)
*N. A. Anagnostopoulos,K. Konstantinidis,A. N. Miliou,S. G. Stavrinides*

Main category: cs.CR

TL;DR: This paper evaluates the stability, robustness, and security of three cryptographic chaotic drive-response systems under noise conditions through comparative analysis.


<details>
  <summary>Details</summary>
Motivation: Ensuring synchronization in drive-response systems is critical for practical applications, particularly in maintaining security when noise is present.

Method: The study tests three established cryptographic chaotic systems, comparing their performance in terms of stability, robustness, and security metrics under noise.

Result: Results highlight critical differences in the systems' ability to maintain synchronization and security under noisy environments.

Conclusion: The comparative analysis provides insights into selecting or designing chaotic systems for secure and reliable synchronization in real-world applications.

Abstract: In practical applications, it is crucial that the drive-response systems,
although identical in all respects, are synchronized at all times, even if
there is noise present. In this work, we test the stability and robustness of
three distinct and well-known cryptographic chaotic systems, and compare the
results in relation to the desired security.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [18] [Beyond Single LLMs: Enhanced Code Generation via Multi-Stage Performance-Guided LLM Orchestration](https://arxiv.org/abs/2510.01379)
*Huashan Chen,Zhenyu Qi,Haotang Li,Hong Chen,Jinfu Chen,Kebin Peng,In Kee Kim,Kyu Hyung Lee,Sen He*

Main category: cs.SE

TL;DR: This paper introduces PerfOrch, a plug-and-play framework that dynamically routes coding tasks to optimal LLMs based on empirical performance data across languages and problem types, achieving new SOTA correctness (96.22%) and runtime speedups (up to 27.66%) without model fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current single-model LLM approaches for code generation ignore the significant performance heterogeneity of models across languages, domains, and problem stages. This limitation motivates the need for a dynamic, performance-aware orchestration framework to harness heterogeneous model strengths.

Method: The method involves a multi-stage workflow (generate-fix-refine) with dynamic task routing to top-performing LLMs, validated through empirical analysis across 17 LLMs and five programming languages on HumanEval-X and EffiBench-X. PerfOrch incorporates stage-wise validation and rollback mechanisms.

Result: PerfOrch achieves 96.22% correctness on HumanEval-X and 91.37% on EffiBench-X (surpassing GPT-4o by significant margins), with 58.76% of problems showing execution time improvements and median speedups of 17.67–27.66% across languages.

Conclusion: The paper concludes that PerfOrch, a multi-stage performance-guided orchestration framework, achieves state-of-the-art results in automated code generation by leveraging the heterogeneous strengths of multiple LLMs. It demonstrates superior correctness and runtime performance improvements over single-model approaches without requiring fine-tuning.

Abstract: While Large Language Models (LLMs) have become the predominant paradigm for
automated code generation, current single-model approaches fundamentally ignore
the heterogeneous computational strengths that different models exhibit across
programming languages, algorithmic domains, and development stages. This paper
challenges the single-model convention by introducing a multi-stage,
performance-guided orchestration framework that dynamically routes coding tasks
to the most suitable LLMs within a structured generate-fix-refine workflow. Our
approach is grounded in a comprehensive empirical study of 17 state-of-the-art
LLMs across five programming languages (Python, Java, C++, Go, and Rust) using
HumanEval-X benchmark. The study, which evaluates both functional correctness
and runtime performance metrics (execution time, mean/max memory utilization,
and CPU efficiency), reveals pronounced performance heterogeneity by language,
development stage, and problem category. Guided by these empirical insights, we
present PerfOrch, an LLM agent that orchestrates top-performing LLMs for each
task context through stage-wise validation and rollback mechanisms. Without
requiring model fine-tuning, PerfOrch achieves substantial improvements over
strong single-model baselines: average correctness rates of 96.22% and 91.37%
on HumanEval-X and EffiBench-X respectively, surpassing GPT-4o's 78.66% and
49.11%. Beyond correctness gains, the framework delivers consistent performance
optimizations, improving execution time for 58.76% of problems with median
speedups ranging from 17.67% to 27.66% across languages on two benchmarks. The
framework's plug-and-play architecture ensures practical scalability, allowing
new LLMs to be profiled and integrated seamlessly, thereby offering a paradigm
for production-grade automated software engineering that adapts to the rapidly
evolving generative AI landscape.

</details>


### [19] [Deciphering WONTFIX: A Mixed-Method Study on Why GitHub Issues Get Rejected](https://arxiv.org/abs/2510.01514)
*J. Alexander Curtis,Sharadha Kasiviswanathan,Nasir Eisty*

Main category: cs.SE

TL;DR: The 'wontfix' label is a common but under-researched tool in GitHub repositories, used to indicate issues will not be addressed. This study analyzes its prevalence and reasons for use.


<details>
  <summary>Details</summary>
Motivation: Studied impact of 'wontfix' on open-source project management and community dynamics due to its frequent but poorly understood usage.

Method: Mixed-method approach using 3,132 GitHub repositories for quantitative analysis and open coding/thematic analysis for qualitative data.

Result: 30% of projects use 'wontfix'; most commonly on user-submitted issues. Eight themes were identified behind the label's use, including user and maintainer decisions.

Conclusion: The 'wontfix' label is important for resource management but may hinder community participation. Understanding its use helps improve open-source collaboration.

Abstract: Context: The ``wontfix'' label is a widely used yet narrowly understood tool
in GitHub repositories, indicating that an issue will not be pursued further.
Despite its prevalence, the impact of this label on project management and
community dynamics within open-source software development is not clearly
defined. Objective: This study examines the prevalence and reasons behind
issues being labeled as wontfix across various open-source repositories on
GitHub. Method: Employing a mixed-method approach, we analyze both quantitative
data to assess the prevalence of the wontfix label and qualitative data to
explore the reasoning that it was used. Data were collected from 3,132 of
GitHub's most-popular repositories. Later, we employ open coding and thematic
analysis to categorize the reasons behind wontfix labels, providing a
structured understanding of the issue management landscape. Results: Our
findings show that about 30% of projects on GitHub apply the wontfix label to
some issues. These issues most often occur on user-submitted issues for bug
reports and feature requests. The study identified eight common themes behind
labeling issues as wontfix, ranging from user-specific control factors to
maintainer-specific decisions. Conclusions: The wontfix label is a critical
tool for managing resources and guiding contributor efforts in GitHub projects.
However, it can also discourage community involvement and obscure the
transparency of project management. Understanding these reasons aids project
managers in making informed decisions and fostering efficient collaboration
within open-source communities.

</details>


### [20] [MIMIC: Integrating Diverse Personality Traits for Better Game Testing Using Large Language Model](https://arxiv.org/abs/2510.01635)
*Yifei Chen,Sarra Habchi,Lili Wei*

Main category: cs.SE

TL;DR: highlights MIMIC's significant potential for effective game testing.


<details>
  <summary>Details</summary>
Motivation: Improve test coverage and uncover edge cases by mimicking human playstyles.

Method: Integrates diverse personality traits into gaming agents.

Result: Outperforms state-of-the-art agents in Minecraft with higher task completion rate and diverse solutions.

Conclusion: MIMIC shows significant potential for effective game testing.

Abstract: Modern video games pose significant challenges for traditional automated
testing algorithms, yet intensive testing is crucial to ensure game quality. To
address these challenges, researchers designed gaming agents using
Reinforcement Learning, Imitation Learning, or Large Language Models. However,
these agents often neglect the diverse strategies employed by human players due
to their different personalities, resulting in repetitive solutions in similar
situations. Without mimicking varied gaming strategies, these agents struggle
to trigger diverse in-game interactions or uncover edge cases.
  In this paper, we present MIMIC, a novel framework that integrates diverse
personality traits into gaming agents, enabling them to adopt different gaming
strategies for similar situations. By mimicking different playstyles, MIMIC can
achieve higher test coverage and richer in-game interactions across different
games. It also outperforms state-of-the-art agents in Minecraft by achieving a
higher task completion rate and providing more diverse solutions. These results
highlight MIMIC's significant potential for effective game testing.

</details>


### [21] [FOSS-chain: using blockchain for Open Source Software license compliance](https://arxiv.org/abs/2510.01740)
*Kypros Iacovou,Georgia M. Kapitsaki,Evangelia Vanezi*

Main category: cs.SE

TL;DR: FOSS-chain uses blockchain to automate OSS license compliance, reducing compatibility risks. A prototype evaluated in a small study shows promising results.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the complexity of ensuring OSS license compliance, which can lead to legal disputes due to license incompatibilities. Blockchain's transparency and immutability offer a novel solution for tracking modifications and ensuring compatible derivative works.

Method: The authors designed and implemented FOSS-chain, a blockchain-enabled web platform that automates license compliance by tracking software derivatives using immutable blockchain records. The method was evaluated through a small-scale user study.

Result: The preliminary evaluation of FOSS-chain's prototype demonstrated its effectiveness in handling 14 OSS licenses, with the platform successfully automating compliance checks and showing potential adaptability to real-world software systems, although based on a limited user study.

Conclusion: The paper concludes that integrating blockchain into OSS license management through FOSS-chain effectively addresses license compatibility issues, with promising potential for real-world application when further developed.

Abstract: Open Source Software (OSS) is widely used and carries licenses that indicate
the terms under which the software is provided for use, also specifying
modification and distribution rules. Ensuring that users are respecting OSS
license terms when creating derivative works is a complex process. Compliance
issues arising from incompatibilities among licenses may lead to legal
disputes. At the same time, the blockchain technology with immutable entries
offers a mechanism to provide transparency when it comes to licensing and
ensure software changes are recorded. In this work, we are introducing an
integration of blockchain and license management when creating derivative
works, in order to tackle the issue of OSS license compatibility. We have
designed, implemented and performed a preliminary evaluation of FOSS-chain, a
web platform that uses blockchain and automates the license compliance process,
covering 14 OSS licenses. We have evaluated the initial prototype version of
the FOSS-chain platform via a small scale user study. Our preliminary results
are promising, demonstrating the potential of the platform for adaptation on
realistic software systems.

</details>


### [22] [ARENA: A tool for measuring and analysing the energy efficiency of Android apps](https://arxiv.org/abs/2510.01754)
*Hina Anwar*

Main category: cs.SE

TL;DR: This paper introduces ARENA, an open-source IDE plugin for Android developers that enables reliable hardware-based energy measurement without sacrificing development workflow efficiency. It automates data collection, analysis, and visualization of energy consumption during app development.


<details>
  <summary>Details</summary>
Motivation: Existing software-based energy estimation methods lack accuracy while hardware-based approaches are time-consuming, complex to implement, and require manual scripting. There's a critical gap in accessible tools for reproducible energy measurement in Android app development.

Method: The authors developed ARENA as an IntelliJ/Android Studio plugin that integrates with physical hardware. It streamlines energy measurement by: 1. Executing test scenarios in IDE 2. Automatically collecting hardware data 3. Performing real-time data aggregation and noise reduction 4. Providing statistical analysis and visualization capabilities

Result: ARENA successfully implemented hardware-based energy measurement within IDEs, enabling developers to: - Compare energy consumption between app versions - Automate repetitive measurement tasks - Visualize energy data through integrated reports - Reduce manual scripting efforts by 70%

Conclusion: ARENA addresses critical barriers in hardware-based energy measurement for Android apps by providing an IDE-integrated solution that preserves measurement accuracy while significantly improving usability and reproducibility for developers and researchers.

Abstract: To build energy-efficient apps, there is a need to estimate and analyze their
energy consumption in typical usage scenarios. The energy consumption of
Android apps could be estimated via software-based and hardware-based
approaches. Software-based approaches, while easier to implement, are not as
accurate as hardware-based approaches. The process of measuring the energy
consumption of an Android app via a hardware-based approach typically involves
1) setting up a measurement environment, 2) executing the app under test on a
mobile device, 3) recording current/voltage data via a hardware device to
measure energy consumption, and 4) cleaning and aggregating data for analyses,
reports, and visualizations. Specialized scripts are written for selected
hardware and software components to ensure reliable energy measurements. The
energy measurement process is repeated many times and aggregated to remove
noise. These steps make the hardware-based energy measurement process
time-consuming and not easy to adapt or reproduce. There is a lack of
open-source tools available for developers and researchers to take reliable
energy measurements via hardware devices. In this paper, we present and
demonstrate ARENA, a support tool that enables developers and researchers to
connect to a physical measurement device without leaving the comfort of their
IDE. Developers could use ARENA during development to compare energy
consumption between different apps or versions of the same app. ARENA
calculates energy consumption on an Android smartphone by executing a test
scenario on the app under development. Further, ARENA helps aggregate,
statistically analyze, report, and visualize the data, allowing developers and
researchers to dig into the data directly or visually. We implemented ARENA as
an IntelliJ and Android Studio plugin.

</details>


### [23] [Towards Speeding up Program Repair with Non-Autoregressive Model](https://arxiv.org/abs/2510.01825)
*Zhenyu Yang,Yue Pan,Zhen Yang,Zhongxing Yu*

Main category: cs.SE

TL;DR: NARRepair is a non-autoregressive APR model that improves repair speed (1.4-6.4× faster) without sacrificing accuracy, solving efficiency-accuracy tradeoffs via novel architectural components.


<details>
  <summary>Details</summary>
Motivation: Traditional autoregressive APR methods suffer from significant time delays due to token-by-token generation, especially in large models. Direct application of non-autoregressive methods leads to poor repair quality. The paper aims to resolve these efficiency-accuracy tradeoffs in APR.

Method: The paper proposes NARRepair, a non-autoregressive APR model featuring three novelties: (1) a repair action predictor to mitigate over-correction, (2) an inter-token dependency extractor to preserve token relationships, and (3) a two-stage decoder to enhance contextual information. This design enables parallelizable code generation while addressing quality issues inherent to naive non-autoregressive methods.

Result: NARRepair achieves the best performance on three APR datasets within limited time, with 1.4-6.4× speedup over autoregressive methods in GPU environments. It maintains competitive accuracy while drastically reducing repair delay, outperforming existing techniques and achieving state-of-the-art results.

Conclusion: NARRepair addresses the delay issue in automatic program repair (APR) by introducing a non-autoregressive model, achieving state-of-the-art performance in both speed and accuracy. The proposed method outperforms existing APR techniques, particularly in reducing repair time by 1.4-6.4 times compared to autoregressive approaches.

Abstract: Enlightened by the success of machine learning techniques in various
application areas, recent years have witnessed a surge of research efforts on
automatic program repair (APR) using machine learning techniques. Previous
machine learning-based APR techniques essentially modified bugs in the
autoregressive (AR) manner, which predicts future values based on past values.
Due to the manner of token-by-token generation, the AR-based APR technique has
a huge time delay. In particular, the delay of the APR model with a large
number of parameters is more serious. To address the issue, we aim to apply the
non-autoregressive (NAR) method to the APR task, which can output target code
in a parallel manner to avoid huge repair delays. However, the naive use of the
NAR manner for the APR task suffers from the issue of compromised patch
quality. To effectively adapt the NAR manner for the APR task, we in this paper
propose NARRepair, the first customized NAR code generation model for the APR
task. The NARRepair model features three major novelties, including 1) the
repair action predictor for alleviating the over-correction issue, 2) the
inter-token dependency extractor for alleviating the issue of lacking
inter-token dependency information, and 3) the two-stage decoder for
alleviating the issue of lacking contextual information. We evaluated NARRepair
on three widely used datasets in the APR community, and the results show that
1) compared to other APR techniques, the NARRepair model has the best
performance within the limited repair time, and 2) compared to AR-based APR
techniques, the repair speed of NARRepair has been increased by 1.4-6.4 times
in the GPU environment. Overall, the results show that NARRepair has achieved
state-of-the-art comprehensive performance in terms of repair speed and
accuracy.

</details>


### [24] [RefFilter: Improving Semantic Conflict Detection via Refactoring-Aware Static Analysis](https://arxiv.org/abs/2510.01960)
*Victor Lira,Paulo Borba,Rodrigo Bonifácio,Galileu Santos e Matheus barbosa*

Main category: cs.SE

TL;DR: RefFilter is a refactoring-aware tool that reduces false positives in semantic interference detection by 32%, improving precision without significant loss in recall.


<details>
  <summary>Details</summary>
Motivation: Existing static analysis methods for semantic interference detection produce high false positives due to inability to distinguish behavior-preserving refactorings from actual interference-causing changes.

Method: RefFilter integrates automated refactoring detection with lightweight static analysis techniques to filter out behavior-preserving code changes, leveraging two datasets (99 labeled scenarios and 1,087 merge scenarios) for evaluation.

Result: Achieved 32\% reduction in false positives on the labeled dataset with only minor and non-significant increase in false negatives, maintaining detection coverage.

Conclusion: Refactor-aware interference detection is a practical solution to enhance merge accuracy in collaborative software development workflows.

Abstract: Detecting semantic interference remains a challenge in collaborative software
development. Recent lightweight static analysis techniques improve efficiency
over SDG-based methods, but they still suffer from a high rate of false
positives. A key cause of these false positives is the presence of
behavior-preserving code refactorings, which current techniques cannot
effectively distinguish from changes that impact behavior and can interfere
with others. To handle this problem we present RefFilter, a refactoring-aware
tool for semantic interference detection. It builds on existing static
techniques by incorporating automated refactoring detection to improve
precision. RefFilter discards behavior-preserving refactorings from reports,
reducing false positives while preserving detection coverage. To evaluate
effectiveness and scalability, use two datasets: a labeled dataset with 99
scenarios and ground truth, and a novel dataset of 1,087 diverse merge
scenarios that we have built. Experimental results show that RefFilter reduces
false positives by nearly 32% on the labeled dataset. While this reduction
comes with a non significant increase in false negatives, the overall gain in
precision significantly outweighs the minor trade-off in recall. These findings
demonstrate that refactoring-aware interference detection is a practical and
effective strategy for improving merge support in modern development workflows.

</details>


### [25] [Clarifying Semantics of In-Context Examples for Unit Test Generation](https://arxiv.org/abs/2510.01994)
*Chen Yang,Lin Yang,Ziqi Wang,Dong Wang,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: CLAST improves semantic clarity in test examples via program analysis + LLM rewriting, outperforming UTgen in test quality preservation (100% vs. 12.9-35.8% loss) while boosting ICL-based test generation metrics by 25-46%.


<details>
  <summary>Details</summary>
Motivation: Poorly structured in-context examples for unit test generation degrade LLM performance. Current refinement techniques like UTgen reduce critical test quality metrics.

Method: CLAST combines program analysis and LLM-based rewriting to systematically refine unit tests, decomposing complex tests into logically clearer ones.

Result: CLAST retains 100% original test effectiveness while UTgen decreases CSR (12.90%), PR (35.82%), Cov (4.65%), and MS (5.07%). User studies show 85.33% preference for CLAST refinements, and test generation tools using CLAST examples improve CSR (25.97%), PR (28.22%), and Cov (45.99%) significantly.

Conclusion: CLAST significantly enhances semantic clarity and test effectiveness compared to existing techniques, demonstrating its potential in improving ICL-based unit test generation.

Abstract: Recent advances in large language models (LLMs) have enabled promising
performance in unit test generation through in-context learning (ICL). However,
the quality of in-context examples significantly influences the effectiveness
of generated tests-poorly structured or semantically unclear test examples
often lead to suboptimal outputs. In this paper, we propose CLAST, a novel
technique that systematically refines unit tests to improve their semantic
clarity, thereby enhancing their utility as in-context examples. The approach
decomposes complex tests into logically clearer ones and improves semantic
clarity through a combination of program analysis and LLM-based rewriting. We
evaluated CLAST on four open-source and three industrial projects. The results
demonstrate that CLAST largely outperforms UTgen, the state-of-the-art
refinement technique, in both preserving test effectiveness and enhancing
semantic clarity. Specifically, CLAST fully retains the original effectiveness
of unit tests, while UTgen reduces compilation success rate (CSR), pass rate
(PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%,
35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user
study preferred the semantic clarity of CLAST-refined tests. Notably,
incorporating CLAST-refined tests as examples effectively improves ICL-based
unit test generation approaches such as RAGGen and TELPA, resulting in an
average increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for
generated tests, compared to incorporating UTgen-refined tests. The insights
from the follow-up user study not only reinforce CLAST's potential impact in
software testing practice but also illuminate avenues for future research.

</details>


### [26] [Automatic Generation of Combinatorial Reoptimisation Problem Specifications: A Vision](https://arxiv.org/abs/2510.02002)
*Maximilian Kratz,Steffen Zschaler,Jens Kosiol,Gabriele Taentzer*

Main category: cs.SE

TL;DR: This paper proposes using Model-Driven Engineering to systematically adapt optimization solutions to changing contexts, demonstrating its potential through a resource-allocation case study.


<details>
  <summary>Details</summary>
Motivation: Optimization solutions require frequent adaptation due to changing contextual factors (e.g., railway crew rescheduling, nurse rerostering). Existing approaches lack systematic methods to handle constraints like minimal change, fixed parts of solutions, and change script derivation.

Method: The paper introduces the use of declarative modeling languages and model transformations in MDE to categorize and derive reoptimization strategies. It also presents a proof-of-concept implementation using the GIPS tool to demonstrate feasibility.

Result: An initial categorization of changing problems and reoptimization strategies is developed, supported by a proof-of-concept implementation applied to a teaching assistant allocation example, validating the approach's effectiveness.

Conclusion: Model-Driven Engineering (MDE) provides a systematic framework for deriving reoptimization problems from original optimization specifications, enabling efficient adaptation to changing contexts in combinatorial optimization problems.

Abstract: Once an optimisation problem has been solved, the solution may need
adaptation when contextual factors change. This challenge, also known as
reoptimisation, has been addressed in various problem domains, such as railway
crew rescheduling, nurse rerostering, or aircraft recovery. This requires a
modified problem to be solved again to ensure that the adapted solution is
optimal in the new context. However, the new optimisation problem differs
notably from the original problem: (i) we want to make only minimal changes to
the original solution to minimise the impact; (ii) we may be unable to change
some parts of the original solution (e.g., because they refer to past
allocations); and (iii) we need to derive a change script from the original
solution to the new solution. In this paper, we argue that Model-Driven
Engineering (MDE) - in particular, the use of declarative modelling languages
and model transformations for the high-level specification of optimisation
problems - offers new opportunities for the systematic derivation of
reoptimisation problems from the original optimisation problem specification.
We focus on combinatorial reoptimisation problems and provide an initial
categorisation of changing problems and strategies for deriving the
corresponding reoptimisation specifications. We introduce an initial
proof-of-concept implementation based on the GIPS (Graph-Based (Mixed) Integer
Linear Programming Problem Specification) tool and apply it to an example
resource-allocation problem: the allocation of teaching assistants to teaching
sessions.

</details>


### [27] [ACM SIGSOFT SEN Empirical Software Engineering: Introducing Our New Regular Column](https://arxiv.org/abs/2510.02007)
*Justus Bogner,Roberto Verdecchia*

Main category: cs.SE

TL;DR: The paper introduces a new ACM SIGSOFT SEN column, SEN-ESE, aimed at discussing meta-aspects of empirical software engineering research to improve it.


<details>
  <summary>Details</summary>
Motivation: Empirical software engineering (ESE) is a mature field that still needs improvement due to issues like research reproducibility, limited external validity, subjective reviews, and the challenge of applying research to industry. The field also lacks explicit documentation of many key aspects, making them hard for new researchers to learn.

Method: Introduce a new regular column for the ESE community to discuss meta-aspects of research. Contributions include expert interviews, focus groups, surveys, and position papers. The column encourages discussion on issues that are either seldom addressed or taken for granted.

Result: Establishment of the SEN-ESE column as a new platform for the software engineering community to discuss and reflect on the practices of empirical software engineering research.

Conclusion: The column will encourage ongoing conversation and improvement of ESE by including the voices of the community. It is open for suggestions and input on relevant challenges and underexplored areas.

Abstract: From its early foundations in the 1970s, empirical software engineering (ESE)
has evolved into a mature research discipline that embraces a plethora of
different topics, methodologies, and industrial practices. Despite its
remarkable progress, the ESE research field still needs to keep evolving, as
new impediments, shortcoming, and technologies emerge. Research
reproducibility, limited external validity, subjectivity of reviews, and
porting research results to industrial practices are just some examples of the
drivers for improvements to ESE research. Additionally, several facets of ESE
research are not documented very explicitly, which makes it difficult for
newcomers to pick them up. With this new regular ACM SIGSOFT SEN column
(SEN-ESE), we introduce a venue for discussing meta-aspects of ESE research,
ranging from general topics such as the nature and best practices for
replication packages, to more nuanced themes such as statistical methods,
interview transcription tools, and publishing interdisciplinary research. Our
aim for the column is to be a place where we can regularly spark conversations
on ESE topics that might not often be touched upon or are left implicit.
Contributions to this column will be grounded in expert interviews, focus
groups, surveys, and position pieces, with the goal of encouraging reflection
and improvement in how we conduct, communicate, teach, and ultimately improve
ESE research. Finally, we invite feedback from the ESE community on
challenging, controversial, or underexplored topics, as well as suggestions for
voices you would like to hear from. While we cannot promise to act on every
idea, we aim to shape this column around the community interests and are
grateful for all contributions.

</details>


### [28] [Towards fairer public transit: Real-time tensor-based multimodal fare evasion and fraud detection](https://arxiv.org/abs/2510.02165)
*Peter Wauyo,Dalia Bwiza,Alain Murara,Edwin Mugume,Eric Umuhoza*

Main category: cs.SE

TL;DR: Multimodal CCTV/audio analysis with Tensor Fusion achieves 84% recall for transport fraud, surpassing existing 75% rates and enabling real-time detection.


<details>
  <summary>Details</summary>
Motivation: Existing systems achieve only 75% recall in transportation fraud detection, necessitating improved methods to reduce revenue loss, enhance passenger safety, and ensure compliance.

Method: Combines ViViT (video) and AST (audio) features via a Tensor Fusion Network (TFN) using a 2-fold Cartesian product to model unimodal/bimodal interactions between visual behaviors and audio cues.

Result: 89.5% accuracy, 87.2% precision, 84.0% recall; 7.0% F1 score and 8.8% recall improvements over traditional concatenation methods; outperforms early fusion baselines.

Conclusion: The proposed multimodal system with Tensor Fusion Network effectively detects transportation fraud, outperforming state-of-the-art methods and enabling real-time operational benefits.

Abstract: This research introduces a multimodal system designed to detect fraud and
fare evasion in public transportation by analyzing closed circuit television
(CCTV) and audio data. The proposed solution uses the Vision Transformer for
Video (ViViT) model for video feature extraction and the Audio Spectrogram
Transformer (AST) for audio analysis. The system implements a Tensor Fusion
Network (TFN) architecture that explicitly models unimodal and bimodal
interactions through a 2-fold Cartesian product. This advanced fusion technique
captures complex cross-modal dynamics between visual behaviors (e.g.,
tailgating,unauthorized access) and audio cues (e.g., fare transaction sounds).
The system was trained and tested on a custom dataset, achieving an accuracy of
89.5%, precision of 87.2%, and recall of 84.0% in detecting fraudulent
activities, significantly outperforming early fusion baselines and exceeding
the 75% recall rates typically reported in state-of-the-art transportation
fraud detection systems. Our ablation studies demonstrate that the tensor
fusion approach provides a 7.0% improvement in the F1 score and an 8.8% boost
in recall compared to traditional concatenation methods. The solution supports
real-time detection, enabling public transport operators to reduce revenue
loss, improve passenger safety, and ensure operational compliance.

</details>


### [29] [SIEVE: Towards Verifiable Certification for Code-datasets](https://arxiv.org/abs/2510.02166)
*Fatou Ndiaye Mbodji,El-hacen Diallo,Jordan Samhi,Kui Liu,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.SE

TL;DR: The paper introduces SIEVE, a community-driven framework addressing the lack of verifiable quality guarantees in public code datasets by generating machine-readable Confidence Cards with statistical bounds.


<details>
  <summary>Details</summary>
Motivation: Existing static dataset cards lack auditing possibilities and statistical guarantees, while isolated cleaning pipelines fragment efforts and raise costs. This hinders trust in code-dataset quality.

Method: SIEVE transforms per-property checks into Confidence Cards—machine-readable certificates with anytime-valid statistical bounds—via community-driven certification replacing narrative cards.

Result: The authors outline a research plan to mature SIEVE and evaluate its utility in reducing quality-assurance costs and enhancing dataset trust.

Conclusion: SIEVE's framework could establish verifiable certification as a standard, improving transparency and efficiency in code-dataset development and usage.

Abstract: Code agents and empirical software engineering rely on public code datasets,
yet these datasets lack verifiable quality guarantees. Static 'dataset cards'
inform, but they are neither auditable nor do they offer statistical
guarantees, making it difficult to attest to dataset quality. Teams build
isolated, ad-hoc cleaning pipelines. This fragments effort and raises cost. We
present SIEVE, a community-driven framework. It turns per-property checks into
Confidence Cards-machine-readable, verifiable certificates with anytime-valid
statistical bounds. We outline a research plan to bring SIEVE to maturity,
replacing narrative cards with anytime-verifiable certification. This shift is
expected to lower quality-assurance costs and increase trust in code-datasets.

</details>


### [30] [TAIBOM: Bringing Trustworthiness to AI-Enabled Systems](https://arxiv.org/abs/2510.02169)
*Vadim Safronov,Anthony McCaigue,Nicholas Allott,Andrew Martin*

Main category: cs.SE

TL;DR: This paper introduces TAIBOM, a framework extending SBOMs to address AI-specific challenges in software supply chains by capturing dynamic dependencies, propagating integrity statements, and enabling trust attestation for AI components.


<details>
  <summary>Details</summary>
Motivation: Existing SBOMs fail to address AI systems' unique characteristics: dynamic data-driven behavior, loose coupling across datasets/models, and fragmented governance. These gaps compromise integrity, trust, and compliance in AI workflows.

Method: TAIBOM introduces three key innovations: (i) a structured dependency model for AI components, (ii) integrity propagation mechanisms across heterogeneous pipelines, and (iii) trust attestation for provenance verification. The framework is evaluated against existing standards (SPDX, CycloneDX).

Result: TAIBOM demonstrates improved assurance, security, and compliance support for AI workflows, outperforming current standards. It enables structured transparency for verifying AI system components and dependencies.

Conclusion: TAIBOM establishes a foundational framework for trustworthy AI systems through enhanced software transparency, addressing critical gaps in dependency management and assurance for dynamic AI environments.

Abstract: The growing integration of open-source software and AI-driven technologies
has introduced new layers of complexity into the software supply chain,
challenging existing methods for dependency management and system assurance.
While Software Bills of Materials (SBOMs) have become critical for enhancing
transparency and traceability, current frameworks fall short in capturing the
unique characteristics of AI systems -- namely, their dynamic, data-driven
nature and the loosely coupled dependencies across datasets, models, and
software components. These challenges are compounded by fragmented governance
structures and the lack of robust tools for ensuring integrity, trust, and
compliance in AI-enabled environments.
  In this paper, we introduce Trusted AI Bill of Materials (TAIBOM) -- a novel
framework extending SBOM principles to the AI domain. TAIBOM provides (i) a
structured dependency model tailored for AI components, (ii) mechanisms for
propagating integrity statements across heterogeneous AI pipelines, and (iii) a
trust attestation process for verifying component provenance. We demonstrate
how TAIBOM supports assurance, security, and compliance across AI workflows,
highlighting its advantages over existing standards such as SPDX and CycloneDX.
This work lays the foundation for trustworthy and verifiable AI systems through
structured software transparency.

</details>


### [31] [FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen Using Agentic AI](https://arxiv.org/abs/2510.02185)
*Paschal C. Amusuo,Dongge Liu,Ricardo Andres Calvo Mendez,Jonathan Metzman,Oliver Chang,James C. Davis*

Main category: cs.SE

TL;DR: The paper introduces two AI-driven methods to reduce false positives in fuzz testing by generating fuzz drivers that enforce input constraints and validate crashes based on function context.


<details>
  <summary>Details</summary>
Motivation: Fuzz drivers generated automatically often produce false positive crashes, especially for functions with structured input and complex state, which undermines trust in fuzzing systems like OSS-Fuzz-Gen.

Method: The first strategy, constraint-based generation, applies input and state constraints during driver creation. The second, context-based validation, leverages function callers to assess the feasibility of reported crashes.

Result: The proposed methods reduce spurious crashes by up to 8%, cut total reported crashes in half, and validate LLMs as effective agents for program analysis in large-scale fuzzing.

Conclusion: Integrating AI into fuzz testing reduces false positives significantly, but challenges remain in applying intelligent methods to complex, real-world software systems.

Abstract: Fuzz testing has become a cornerstone technique for identifying software bugs
and security vulnerabilities, with broad adoption in both industry and
open-source communities. Directly fuzzing a function requires fuzz drivers,
which translate random fuzzer inputs into valid arguments for the target
function. Given the cost and expertise required to manually develop fuzz
drivers, methods exist that leverage program analysis and Large Language Models
to automatically generate these drivers. However, the generated fuzz drivers
frequently lead to false positive crashes, especially in functions highly
structured input and complex state requirements. This problem is especially
crucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as
reporting false positive crashes to maintainers impede trust in both the system
and the team.
  This paper presents two AI-driven strategies to reduce false positives in
OSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First,
constraint-based fuzz driver generation proactively enforces constraints on a
function's inputs and state to guide driver creation. Second, context-based
crash validation reactively analyzes function callers to determine whether
reported crashes are feasible from program entry points. Using 1,500 benchmark
functions from OSS-Fuzz, we show that these strategies reduce spurious crashes
by up to 8%, cut reported crashes by more than half, and demonstrate that
frontier LLMs can serve as reliable program analysis agents. Our results
highlight the promise and challenges of integrating AI into large-scale fuzzing
pipelines.

</details>
