{"id": "2509.21367", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21367", "abs": "https://arxiv.org/abs/2509.21367", "authors": ["Yu-Kai Shih", "You-Kai Kang"], "title": "Design and Implementation of a Secure RAG-Enhanced AI Chatbot for Smart Tourism Customer Service: Defending Against Prompt Injection Attacks -- A Case Study of Hsinchu, Taiwan", "comment": "12 pages, 7 figures, 5 tables", "summary": "As smart tourism evolves, AI-powered chatbots have become indispensable for\ndelivering personalized, real-time assistance to travelers while promoting\nsustainability and efficiency. However, these systems are increasingly\nvulnerable to prompt injection attacks, where adversaries manipulate inputs to\nelicit unintended behaviors such as leaking sensitive information or generating\nharmful content. This paper presents a case study on the design and\nimplementation of a secure retrieval-augmented generation (RAG) chatbot for\nHsinchu smart tourism services. The system integrates RAG with API function\ncalls, multi-layered linguistic analysis, and guardrails against injections,\nachieving high contextual awareness and security. Key features include a tiered\nresponse strategy, RAG-driven knowledge grounding, and intent decomposition\nacross lexical, semantic, and pragmatic levels. Defense mechanisms include\nsystem norms, gatekeepers for intent judgment, and reverse RAG text to\nprioritize verified data. We also benchmark a GPT-5 variant (released\n2025-08-07) to assess inherent robustness. Evaluations with 674 adversarial\nprompts and 223 benign queries show over 95% accuracy on benign tasks and\nsubstantial detection of injection attacks. GPT-5 blocked about 85% of attacks,\nshowing progress yet highlighting the need for layered defenses. Findings\nemphasize contributions to sustainable tourism, multilingual accessibility, and\nethical AI deployment. This work offers a practical framework for deploying\nsecure chatbots in smart tourism and contributes to resilient, trustworthy AI\napplications."}
{"id": "2509.21389", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21389", "abs": "https://arxiv.org/abs/2509.21389", "authors": ["Devashish Chaudhary", "Sutharshan Rajasegarar", "Shiva Raj Pokhrel"], "title": "Towards Adapting Federated & Quantum Machine Learning for Network Intrusion Detection: A Survey", "comment": "34 pages, 16 figures, IEEE Communication Surveys and Tutorials", "summary": "This survey explores the integration of Federated Learning (FL) with Network\nIntrusion Detection Systems (NIDS), with particular emphasis on deep learning\nand quantum machine learning approaches. FL enables collaborative model\ntraining across distributed devices while preserving data privacy-a critical\nrequirement in network security contexts where sensitive traffic data cannot be\ncentralized. Our comprehensive analysis systematically examines the full\nspectrum of FL architectures, deployment strategies, communication protocols,\nand aggregation methods specifically tailored for intrusion detection. We\nprovide an in-depth investigation of privacy-preserving techniques, model\ncompression approaches, and attack-specific federated solutions for threats\nincluding DDoS, MITM, and botnet attacks. The survey further delivers a\npioneering exploration of Quantum FL (QFL), discussing quantum feature\nencoding, quantum machine learning algorithms, and quantum-specific aggregation\nmethods that promise exponential speedups for complex pattern recognition in\nnetwork traffic. Through rigorous comparative analysis of classical and quantum\napproaches, identification of research gaps, and evaluation of real-world\ndeployments, we outline a concrete roadmap for industrial adoption and future\nresearch directions. This work serves as an authoritative reference for\nresearchers and practitioners seeking to enhance privacy, efficiency, and\nrobustness of federated intrusion detection systems in increasingly complex\nnetwork environments, while preparing for the quantum-enhanced cybersecurity\nlandscape of tomorrow."}
{"id": "2509.21392", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.21392", "abs": "https://arxiv.org/abs/2509.21392", "authors": ["Wenxuan Wang", "Chenglei Wang", "Xuelin Qian"], "title": "Dynamic Dual-level Defense Routing for Continual Adversarial Training", "comment": null, "summary": "As adversarial attacks continue to evolve, defense models face the risk of\nrecurrent vulnerabilities, underscoring the importance of continuous\nadversarial training (CAT). Existing CAT approaches typically balance decision\nboundaries by either data replay or optimization strategy to constrain shared\nmodel parameters. However, due to the diverse and aggressive nature of\nadversarial examples, these methods suffer from catastrophic forgetting of\nprevious defense knowledge after continual learning. In this paper, we propose\na novel framework, called Dual-level Defense Routing or DDeR, that can\nautonomously select appropriate routers to integrate specific defense experts,\nthereby adapting to evolving adversarial attacks. Concretely, the first-level\ndefense routing comprises multiple defense experts and routers, with each\nrouter dynamically selecting and combining suitable experts to process attacked\nfeatures. Routers are independently incremented as continuous adversarial\ntraining progresses, and their selections are guided by an Adversarial Sentinel\nNetwork (ASN) in the second-level defense routing. To compensate for the\ninability to test due to the independence of routers, we further present a\nPseudo-task Substitution Training (PST) strategy, which leverages\ndistributional discrepancy in data to facilitate inter-router communication\nwithout storing historical data. Extensive experiments demonstrate that DDeR\nachieves superior continuous defense performance and classification accuracy\ncompared to existing methods."}
{"id": "2509.21400", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.21400", "abs": "https://arxiv.org/abs/2509.21400", "authors": ["Xiyu Zeng", "Siyuan Liang", "Liming Lu", "Haotian Zhu", "Enguang Liu", "Jisheng Dang", "Yongbin Zhou", "Shuchao Pang"], "title": "SafeSteer: Adaptive Subspace Steering for Efficient Jailbreak Defense in Vision-Language Models", "comment": null, "summary": "As the capabilities of Vision Language Models (VLMs) continue to improve,\nthey are increasingly targeted by jailbreak attacks. Existing defense methods\nface two major limitations: (1) they struggle to ensure safety without\ncompromising the model's utility; and (2) many defense mechanisms significantly\nreduce the model's inference efficiency. To address these challenges, we\npropose SafeSteer, a lightweight, inference-time steering framework that\neffectively defends against diverse jailbreak attacks without modifying model\nweights. At the core of SafeSteer is the innovative use of Singular Value\nDecomposition to construct a low-dimensional \"safety subspace.\" By projecting\nand reconstructing the raw steering vector into this subspace during inference,\nSafeSteer adaptively removes harmful generation signals while preserving the\nmodel's ability to handle benign inputs. The entire process is executed in a\nsingle inference pass, introducing negligible overhead. Extensive experiments\nshow that SafeSteer reduces the attack success rate by over 60% and improves\naccuracy on normal tasks by 1-2%, without introducing significant inference\nlatency. These results demonstrate that robust and practical jailbreak defense\ncan be achieved through simple, efficient inference-time control."}
{"id": "2509.21427", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21427", "abs": "https://arxiv.org/abs/2509.21427", "authors": ["Ying Wang", "Wenjun Mao", "Chong Wang", "Zhenhao Zhou", "Yicheng Zhou", "Wenyun Zhao", "Yiling Lou", "Xin Peng"], "title": "Extracting Conceptual Knowledge to Locate Software Issues", "comment": null, "summary": "Issue localization, which identifies faulty code elements such as files or\nfunctions, is critical for effective bug fixing. While recent LLM-based and\nLLM-agent-based approaches improve accuracy, they struggle in large-scale\nrepositories due to concern mixing, where relevant logic is buried in large\nfunctions, and concern scattering, where related logic is dispersed across\nfiles.\n  To address these challenges, we propose RepoLens, a novel approach that\nabstracts and leverages conceptual knowledge from code repositories. RepoLens\ndecomposes fine-grained functionalities and recomposes them into high-level\nconcerns, semantically coherent clusters of functionalities that guide LLMs. It\noperates in two stages: an offline stage that extracts and enriches conceptual\nknowledge into a repository-wide knowledge base, and an online stage that\nretrieves issue-specific terms, clusters and ranks concerns by relevance, and\nintegrates them into localization workflows via minimally intrusive prompt\nenhancements. We evaluate RepoLens on SWE-Lancer-Loc, a benchmark of 216 tasks\nderived from SWE-Lancer. RepoLens consistently improves three state-of-the-art\ntools, namely AgentLess, OpenHands, and mini-SWE-agent, achieving average gains\nof over 22% in Hit@k and 46% in Recall@k for file- and function-level\nlocalization. It generalizes across models (GPT-4o, GPT-4o-mini, GPT-4.1) with\nHit@1 and Recall@10 gains up to 504% and 376%, respectively. Ablation studies\nand manual evaluation confirm the effectiveness and reliability of the\nconstructed concerns."}
{"id": "2509.21475", "categories": ["cs.CR", "cs.CE", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.21475", "abs": "https://arxiv.org/abs/2509.21475", "authors": ["Sen Yang", "Burak Ã–z", "Fei Wu", "Fan Zhang"], "title": "Designing Ethereum's Geographical (De)Centralization Beyond the Atlantic", "comment": null, "summary": "Decentralization has a geographic dimension that conventional metrics such as\nstake distribution overlook. Where validators run affects resilience to\nregional shocks (outages, disasters, government intervention) and fairness in\nreward access. Yet in permissionless systems, locations cannot be mandated, but\nthey emerge from incentives. Today, Ethereum's validators cluster along the\nAtlantic (EU and U.S. East Coast), where latency is structurally favorable.\nThis raises a key question: when some regions already enjoy latency advantages,\nhow does protocol design shape validator incentives and the geography of\n(de)centralization? We develop a latency-calibrated agent-based model and\ncompare two Ethereum block-building paradigms: a Single-Source Paradigm (SSP),\nakin to MEV-Boost, where proposers fetch full blocks from a relay that also\npropagates them; and a Multi-Source Paradigm (MSP), where proposers aggregate\nvalue from multiple sources and broadcast the block themselves. Simulations\nshow that SSP concentrates around relay placement but more slowly, since\nproximity mainly affects propagation, and the marginal value of time is\nrelatively uniform across regions. MSP centralizes faster: aggregating across\nsources makes marginal value location-dependent, amplifying payoff dispersion\nand migration toward latency minima. Source placement and consensus settings\ncan dampen or intensify these effects, though once validators are already\nclustered, the impact of source placement on decentralization is marginal. In\nmost cases, North America consistently emerges as the focal hub. These findings\nshow that protocol design materially shapes validator geography and offer\nlevers for promoting geographical decentralization."}
{"id": "2509.21533", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21533", "abs": "https://arxiv.org/abs/2509.21533", "authors": ["Shalini Chakraborty", "Sebastian Baltes"], "title": "Lost in Transition: The Struggle of Women Returning to Software Engineering Research after Career Breaks", "comment": "3 pages, published in the Proceedings of the 18th International\n  Conference on Cooperative and Human Aspects of Software Engineering (CHASE\n  2025)", "summary": "The IT industry provides supportive pathways such as returnship programs,\ncoding boot camps, and buddy systems for women re-entering their job after a\ncareer break. Academia, however, offers limited opportunities to motivate women\nto return. We propose a diverse multicultural research project investigating\nthe challenges faced by women with software engineering (SE) backgrounds\nre-entering academia or related research roles after a career break. Career\ndisruptions due to pregnancy, immigration status, or lack of flexible work\noptions can significantly impact women's career progress, creating barriers for\nreturning as lecturers, professors, or senior researchers. Although many\ncompanies promote gender diversity policies, such measures are less prominent\nand often under-recognized within academic institutions. Our goal is to explore\nthe specific challenges women encounter when re-entering academic roles\ncompared to industry roles; to understand the institutional perspective,\nincluding a comparative analysis of existing policies and opportunities in\ndifferent countries for women to return to the field; and finally, to provide\nrecommendations that support transparent hiring practices. The research project\nwill be carried out in multiple universities and in multiple countries to\ncapture the diverse challenges and policies that vary by location."}
{"id": "2509.21497", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21497", "abs": "https://arxiv.org/abs/2509.21497", "authors": ["Alexandru IoniÅ£Äƒ", "Andreea IoniÅ£Äƒ"], "title": "Functional Encryption in Secure Neural Network Training: Data Leakage and Practical Mitigations", "comment": "Accepted at RAID 2025. (c) IEEE", "summary": "With the increased interest in artificial intelligence, Machine Learning as a\nService provides the infrastructure in the Cloud for easy training, testing,\nand deploying models. However, these systems have a major privacy issue:\nuploading sensitive data to the Cloud, especially during training. Therefore,\nachieving secure Neural Network training has been on many researchers' minds\nlately. More and more solutions for this problem are built around a main\npillar: Functional Encryption (FE). Although these approaches are very\ninteresting and offer a new perspective on ML training over encrypted data,\nsome vulnerabilities do not seem to be taken into consideration. In our paper,\nwe present an attack on neural networks that uses FE for secure training over\nencrypted data. Our approach uses linear programming to reconstruct the\noriginal input, unveiling the previous security promises. To address the\nattack, we propose two solutions for secure training and inference that involve\nthe client during the computation phase. One approach ensures security without\nrelying on encryption, while the other uses function-hiding inner-product\ntechniques."}
{"id": "2509.21816", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21816", "abs": "https://arxiv.org/abs/2509.21816", "authors": ["Yuhang Xie", "Jian Mu", "Xiaojun Ma", "Chaoyun Zhang", "Lu Wang", "Mengyu Zhou", "Mugeng Liu", "Si Qin", "Qingwei Lin", "Saravan Rajmohan", "Shi Han", "Dongmei Zhang"], "title": "No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials", "comment": null, "summary": "Excel is one of the most widely used productivity tools across domains,\noffering rich functionality but also overwhelming users with its complexity.\nThis creates a persistent demand for tutorials to support effective usage.\nHowever, existing tutorials are manually authored by experts, require frequent\nupdates after each software release, and incur substantial labor costs. Prior\nwork has not achieved fully automated tutorial generation, since existing\nmethods still depend on handcrafted operation sequences or example materials.\nIn this paper, we present the first framework for automatically generating\nExcel tutorials directly from natural language task descriptions. Our framework\nfirst instantiates the task. Then a central component of this framework,\nExecution Agent, plans and executes the solution in Excel, and collects the\nintermediate artifacts required for tutorial construction. These artifacts are\nthen transformed into both structured Excel documents and video demonstrations.\nTo build a comprehensive tutorial corpus, we collected 1,559 task descriptions\nfrom real-world scenarios. In addition, we designed a systematic evaluation\nframework that integrates assessments from both large language models (LLMs)\nand human reviewers. Experimental results show that our framework improves task\nexecution success rates by 8.5% over state-of-the-art baselines. Moreover, the\ngenerated tutorials demonstrate superior readability and instructional\neffectiveness, often approaching or surpassing expert-authored materials.\nImportantly, the automated pipeline eliminates manual labor and reduces time\ncosts to 1/20 of expert authoring, making scalable and high-quality tutorial\ngeneration practical for the first time."}
{"id": "2509.21586", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.21586", "abs": "https://arxiv.org/abs/2509.21586", "authors": ["Moritz Grundei", "Aayush Rajasekaran", "Kishori Konwar", "Muriel Medard"], "title": "From Indexing to Coding: A New Paradigm for Data Availability Sampling", "comment": null, "summary": "The data availability problem is a central challenge in blockchain systems\nand lies at the core of the accessibility and scalability issues faced by\nplatforms such as Ethereum. Modern solutions employ several approaches, with\ndata availability sampling (DAS) being the most self-sufficient and\nminimalistic in its security assumptions. Existing DAS methods typically form\ncryptographic commitments on codewords of fixed-rate erasure codes, which\nrestrict light nodes to sampling from a predetermined set of coded symbols.\n  In this paper, we introduce a new approach to DAS that modularizes the coding\nand commitment process by committing to the uncoded data while performing\nsampling through on-the-fly coding. The resulting samples are significantly\nmore expressive, enabling light nodes to obtain, in concrete implementations,\nup to multiple orders of magnitude stronger assurances of data availability\nthan from sampling pre-committed symbols from a fixed-rate redundancy code as\ndone in established DAS schemes using Reed Solomon or low density parity check\ncodes. We present a concrete protocol that realizes this paradigm using random\nlinear network coding (RLNC)."}
{"id": "2509.21881", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21881", "abs": "https://arxiv.org/abs/2509.21881", "authors": ["Chaman Wijesiriwardana", "Prasad Wimalaratne"], "title": "Software Engineering Data Analytics: A Framework Based on a Multi-Layered Abstraction Mechanism", "comment": null, "summary": "This paper presents a concept of a domain-specific framework for software\nanalytics by enabling querying, modeling, and integration of heterogeneous\nsoftware repositories. The framework adheres to a multi-layered abstraction\nmechanism that consists of domain-specific operators. We showcased the\npotential of this approach by employing a case study."}
{"id": "2509.21590", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.21590", "abs": "https://arxiv.org/abs/2509.21590", "authors": ["Ben Rosenzweig", "Valentino Dalla Valle", "Giovanni Apruzzese", "Aurore Fass"], "title": "It's not Easy: Applying Supervised Machine Learning to Detect Malicious Extensions in the Chrome Web Store", "comment": "Accepted to ACM Transactions on the Web", "summary": "Google Chrome is the most popular Web browser. Users can customize it with\nextensions that enhance their browsing experience. The most well-known\nmarketplace of such extensions is the Chrome Web Store (CWS). Developers can\nupload their extensions on the CWS, but such extensions are made available to\nusers only after a vetting process carried out by Google itself. Unfortunately,\nsome malicious extensions bypass such checks, putting the security and privacy\nof downstream browser extension users at risk.\n  Here, we scrutinize the extent to which automated mechanisms reliant on\nsupervised machine learning (ML) can be used to detect malicious extensions on\nthe CWS. To this end, we first collect 7,140 malicious extensions published in\n2017--2023. We combine this dataset with 63,598 benign extensions published or\nupdated on the CWS before 2023, and we develop three supervised-ML-based\nclassifiers. We show that, in a \"lab setting\", our classifiers work well (e.g.,\n98% accuracy). Then, we collect a more recent set of 35,462 extensions from the\nCWS, published or last updated in 2023, with unknown ground truth. We were\neventually able to identify 68 malicious extensions that bypassed the vetting\nprocess of the CWS. However, our classifiers also reported >1k likely malicious\nextensions. Based on this finding (further supported with empirical evidence),\nwe elucidate, for the first time, a strong concept drift effect on browser\nextensions. We also show that commercial detectors (e.g., VirusTotal) work\npoorly to detect known malicious extensions. Altogether, our results highlight\nthat detecting malicious browser extensions is a fundamentally hard problem.\nThis requires additional work both by the research community and by Google\nitself -- potentially by revising their approaches. In the meantime, we\ninformed Google of our discoveries, and we release our artifacts."}
{"id": "2509.21891", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21891", "abs": "https://arxiv.org/abs/2509.21891", "authors": ["Yangtian Zi", "Zixuan Wu", "Aleksander Boruch-Gruszecki", "Jonathan Bell", "Arjun Guha"], "title": "AgentPack: A Dataset of Code Changes, Co-Authored by Agents and Humans", "comment": null, "summary": "Fine-tuning large language models for code editing has typically relied on\nmining commits and pull requests. The working hypothesis has been that commit\nmessages describe human intent in natural language, and patches to code\ndescribe the changes that implement that intent. However, much of the\npreviously collected data is noisy: commit messages are terse, human-written\ncommits commingle several unrelated edits, and many commits come from simple,\nrule-based bots.\n  The recent adoption of software engineering agents changes this landscape.\nCode changes co-authored by humans and agents tend to be more narrowly scoped\nand focused on clearer goals. Their commit messages, generated by LLMs,\narticulate intent and rationale in much greater detail. Moreover, when these\nchanges land in public repositories, they are implicitly filtered by humans:\nmaintainers discard low-quality commits to their projects.\n  We present AgentPack, a corpus of 1.3M code edits co-authored by Claude Code,\nOpenAI Codex, and Cursor Agent across public GitHub projects up to mid-August\n2025. We describe the identification and curation pipeline, quantify adoption\ntrends of these agents, and analyze the structural properties of the edits.\nFinally, we show that models fine-tuned on AgentPack can outperform models\ntrained on prior human-only commit corpora, highlighting the potential of using\npublic data from software engineering agents to train future code-editing\nmodels."}
{"id": "2509.21601", "categories": ["cs.CR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.21601", "abs": "https://arxiv.org/abs/2509.21601", "authors": ["Jason Anderson"], "title": "World's First Authenticated Satellite Pseudorange from Orbit", "comment": "Pending publication:\n  https://www.ion.org/gnss/abstracts.cfm?paperID=16052", "summary": "Cryptographic Ranging Authentication is here! We present initial results on\nthe Pulsar authenticated ranging service broadcast from space with Pulsar-0\nutilizing a recording taken at Xona headquarters in Burlingame, CA. No\nassumptions pertaining to the ownership or leakage of encryption keys are\nrequired. This work discusses the Pulsar watermark design and security\nanalysis. We derive the Pulsar watermark's probabilities of missed detection\nand false alarm, and we discuss the required receiver processing needed to\nutilize the Pulsar watermark. We present validation results of the Pulsar\nwatermark utilizing the transmissions from orbit. Lastly, we provide results\nthat demonstrate the spoofing detection efficacy with a spoofing scenario that\nincorporates the authentic transmissions from orbit. Because we make no\nassumption about the leakage of symmetric encryption keys, this work provides\nmathematical justification of the watermark's security, and our July 2025\ntransmissions from orbit, we claim the world's first authenticated satellite\npseudorange from orbit."}
{"id": "2509.21945", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21945", "abs": "https://arxiv.org/abs/2509.21945", "authors": ["Pengzhou Chen", "Hongyuan Liang", "Tao Chen"], "title": "Unveiling Many Faces of Surrogate Models for Configuration Tuning: A Fitness Landscape Analysis Perspective", "comment": "This paper is under review", "summary": "To efficiently tune configuration for better system performance (e.g.,\nlatency), many tuners have leveraged a surrogate model to expedite the process\ninstead of solely relying on the profoundly expensive system measurement. As\nsuch, it is naturally believed that we need more accurate models. However, the\nfact of accuracy can lie-a somewhat surprising finding from prior work-has left\nus many unanswered questions regarding what role the surrogate model plays in\nconfiguration tuning. This paper provides the very first systematic exploration\nand discussion, together with a resolution proposal, to disclose the many faces\nof surrogate models for configuration tuning, through the novel perspective of\nfitness landscape analysis. We present a theory as an alternative to accuracy\nfor assessing the model usefulness in tuning, based on which we conduct an\nextensive empirical study involving up to 27,000 cases. Drawing on the above,\nwe propose Model4Tune, an automated predictive tool that estimates which\nmodel-tuner pairs are the best for an unforeseen system without expensive tuner\nprofiling. Our results suggest that Moldel4Tune, as one of the first of its\nkind, performs significantly better than random guessing in 79%-82% of the\ncases. Our results not only shed light on the possible future research\ndirections but also offer a practical resolution that can assist practitioners\nin evaluating the most useful model for configuration tuning."}
{"id": "2509.21634", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.21634", "abs": "https://arxiv.org/abs/2509.21634", "authors": ["Prakhar Sharma", "Haohuang Wen", "Vinod Yegneswaran", "Ashish Gehani", "Phillip Porras", "Zhiqiang Lin"], "title": "MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G Open RANs", "comment": null, "summary": "The evolution toward 6G networks is being accelerated by the Open Radio\nAccess Network (O-RAN) paradigm -- an open, interoperable architecture that\nenables intelligent, modular applications across public telecom and private\nenterprise domains. While this openness creates unprecedented opportunities for\ninnovation, it also expands the attack surface, demanding resilient, low-cost,\nand autonomous security solutions. Legacy defenses remain largely reactive,\nlabor-intensive, and inadequate for the scale and complexity of next-generation\nsystems. Current O-RAN applications focus mainly on network optimization or\npassive threat detection, with limited capability for closed-loop, automated\nresponse.\n  To address this critical gap, we present an agentic AI framework for fully\nautomated, end-to-end threat mitigation in 6G O-RAN environments. MobiLLM\norchestrates security workflows through a modular multi-agent system powered by\nLarge Language Models (LLMs). The framework features a Threat Analysis Agent\nfor real-time data triage, a Threat Classification Agent that uses\nRetrieval-Augmented Generation (RAG) to map anomalies to specific\ncountermeasures, and a Threat Response Agent that safely operationalizes\nmitigation actions via O-RAN control interfaces. Grounded in trusted knowledge\nbases such as the MITRE FiGHT framework and 3GPP specifications, and equipped\nwith robust safety guardrails, MobiLLM provides a blueprint for trustworthy\nAI-driven network security. Initial evaluations demonstrate that MobiLLM can\neffectively identify and orchestrate complex mitigation strategies,\nsignificantly reducing response latency and showcasing the feasibility of\nautonomous security operations in 6G."}
{"id": "2509.22097", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22097", "abs": "https://arxiv.org/abs/2509.22097", "authors": ["Junkai Chen", "Huihui Huang", "Yunbo Lyu", "Junwen An", "Jieke Shi", "Chengran Yang", "Ting Zhang", "Haoye Tian", "Yikun Li", "Zhenhao Li", "Xin Zhou", "Xing Hu", "David Lo"], "title": "SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios", "comment": null, "summary": "Large language model (LLM) powered code agents are rapidly transforming\nsoftware engineering by automating tasks such as testing, debugging, and\nrepairing, yet the security risks of their generated code have become a\ncritical concern. Existing benchmarks have offered valuable insights but remain\ninsufficient: they often overlook the genuine context in which vulnerabilities\nwere introduced or adopt narrow evaluation protocols that fail to capture\neither functional correctness or newly introduced vulnerabilities. We therefore\nintroduce SecureAgentBench, a benchmark of 105 coding tasks designed to\nrigorously evaluate code agents' capabilities in secure code generation. Each\ntask includes (i) realistic task settings that require multi-file edits in\nlarge repositories, (ii) aligned contexts based on real-world open-source\nvulnerabilities with precisely identified introduction points, and (iii)\ncomprehensive evaluation that combines functionality testing, vulnerability\nchecking through proof-of-concept exploits, and detection of newly introduced\nvulnerabilities using static analysis. We evaluate three representative agents\n(SWE-agent, OpenHands, and Aider) with three state-of-the-art LLMs (Claude 3.7\nSonnet, GPT-4.1, and DeepSeek-V3.1). Results show that (i) current agents\nstruggle to produce secure code, as even the best-performing one, SWE-agent\nsupported by DeepSeek-V3.1, achieves merely 15.2% correct-and-secure solutions,\n(ii) some agents produce functionally correct code but still introduce\nvulnerabilities, including new ones not previously recorded, and (iii) adding\nexplicit security instructions for agents does not significantly improve secure\ncoding, underscoring the need for further research. These findings establish\nSecureAgentBench as a rigorous benchmark for secure code generation and a step\ntoward more reliable software development with LLMs."}
{"id": "2509.21712", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21712", "abs": "https://arxiv.org/abs/2509.21712", "authors": ["Bingcan Guo", "Eryue Xu", "Zhiping Zhang", "Tianshi Li"], "title": "Not My Agent, Not My Boundary? Elicitation of Personal Privacy Boundaries in AI-Delegated Information Sharing", "comment": null, "summary": "Aligning AI systems with human privacy preferences requires understanding\nindividuals' nuanced disclosure behaviors beyond general norms. Yet eliciting\nsuch boundaries remains challenging due to the context-dependent nature of\nprivacy decisions and the complex trade-offs involved. We present an AI-powered\nelicitation approach that probes individuals' privacy boundaries through a\ndiscriminative task. We conducted a between-subjects study that systematically\nvaried communication roles and delegation conditions, resulting in 1,681\nboundary specifications from 169 participants for 61 scenarios. We examined how\nthese contextual factors and individual differences influence the boundary\nspecification. Quantitative results show that communication roles influence\nindividuals' acceptance of detailed and identifiable disclosure, AI delegation\nand individuals' need for privacy heighten sensitivity to disclosed\nidentifiers, and AI delegation results in less consensus across individuals.\nOur findings highlight the importance of situating privacy preference\nelicitation within real-world data flows. We advocate using nuanced privacy\nboundaries as an alignment goal for future AI systems."}
{"id": "2509.22114", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22114", "abs": "https://arxiv.org/abs/2509.22114", "authors": ["Hanzhuo Tan", "Weihao Li", "Xiaolong Tian", "Siyi Wang", "Jiaming Liu", "Jing Li", "Yuqun Zhang"], "title": "SK2Decompile: LLM-based Two-Phase Binary Decompilation from Skeleton to Skin", "comment": null, "summary": "Large Language Models (LLMs) have emerged as a promising approach for binary\ndecompilation. However, the existing LLM-based decompilers still are somewhat\nlimited in effectively presenting a program's source-level structure with its\noriginal identifiers. To mitigate this, we introduce SK2Decompile, a novel\ntwo-phase approach to decompile from the skeleton (semantic structure) to the\nskin (identifier) of programs. Specifically, we first apply a Structure\nRecovery model to translate a program's binary code to an Intermediate\nRepresentation (IR) as deriving the program's \"skeleton\", i.e., preserving\ncontrol flow and data structures while obfuscating all identifiers with generic\nplaceholders. We also apply reinforcement learning to reward the model for\nproducing program structures that adhere to the syntactic and semantic rules\nexpected by compilers. Second, we apply an Identifier Naming model to produce\nmeaningful identifiers which reflect actual program semantics as deriving the\nprogram's \"skin\". We train the Identifier Naming model with a separate\nreinforcement learning objective that rewards the semantic similarity between\nits predictions and the reference code. Such a two-phase decompilation process\nfacilitates advancing the correctness and readability of decompilation\nindependently. Our evaluations indicate that SK2Decompile, significantly\noutperforms the SOTA baselines, achieving 21.6% average re-executability rate\ngain over GPT-5-mini on the HumanEval dataset and 29.4% average R2I improvement\nover Idioms on the GitHub2025 benchmark."}
{"id": "2509.21761", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21761", "abs": "https://arxiv.org/abs/2509.21761", "authors": ["Miao Yu", "Zhenhong Zhou", "Moayad Aloqaily", "Kun Wang", "Biwei Huang", "Stephen Wang", "Yueming Jin", "Qingsong Wen"], "title": "Backdoor Attribution: Elucidating and Controlling Backdoor in Language Models", "comment": null, "summary": "Fine-tuned Large Language Models (LLMs) are vulnerable to backdoor attacks\nthrough data poisoning, yet the internal mechanisms governing these attacks\nremain a black box. Previous research on interpretability for LLM safety tends\nto focus on alignment, jailbreak, and hallucination, but overlooks backdoor\nmechanisms, making it difficult to understand and fully eliminate the backdoor\nthreat. In this paper, aiming to bridge this gap, we explore the interpretable\nmechanisms of LLM backdoors through Backdoor Attribution (BkdAttr), a\ntripartite causal analysis framework. We first introduce the Backdoor Probe\nthat proves the existence of learnable backdoor features encoded within the\nrepresentations. Building on this insight, we further develop Backdoor\nAttention Head Attribution (BAHA), efficiently pinpointing the specific\nattention heads responsible for processing these features. Our primary\nexperiments reveals these heads are relatively sparse; ablating a minimal\n\\textbf{$\\sim$ 3%} of total heads is sufficient to reduce the Attack Success\nRate (ASR) by \\textbf{over 90%}. More importantly, we further employ these\nfindings to construct the Backdoor Vector derived from these attributed heads\nas a master controller for the backdoor. Through only \\textbf{1-point}\nintervention on \\textbf{single} representation, the vector can either boost ASR\nup to \\textbf{$\\sim$ 100% ($\\uparrow$)} on clean inputs, or completely\nneutralize backdoor, suppressing ASR down to \\textbf{$\\sim$ 0% ($\\downarrow$)}\non triggered inputs. In conclusion, our work pioneers the exploration of\nmechanistic interpretability in LLM backdoors, demonstrating a powerful method\nfor backdoor control and revealing actionable insights for the community."}
{"id": "2509.22170", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22170", "abs": "https://arxiv.org/abs/2509.22170", "authors": ["Chengjia Wang", "Lanling Tang", "Ming Yuan", "Jiongchi Yu", "Xiaofei Xie", "Jiajun Bu"], "title": "Leveraging LLM Agents for Automated Video Game Testing", "comment": "17 pages", "summary": "Testing MMORPGs (Massively Multiplayer Online Role-Playing Games) is a\ncritical yet labor-intensive task in game development due to their complexity\nand frequent updating nature. Traditional automated game testing approaches\nstruggle to achieve high state coverage and efficiency in these rich,\nopen-ended environments, while existing LLM-based game-playing approaches are\nlimited to shallow reasoning ability in understanding complex game state-action\nspaces and long-complex tasks. To address these challenges, we propose TITAN,\nan effective LLM-driven agent framework for intelligent MMORPG testing. TITAN\nincorporates four key components to: (1) perceive and abstract high-dimensional\ngame states, (2) proactively optimize and prioritize available actions, (3)\nenable long-horizon reasoning with action trace memory and reflective\nself-correction, and (4) employ LLM-based oracles to detect potential\nfunctional and logic bugs with diagnostic reports.\n  We implement the prototype of TITAN and evaluate it on two large-scale\ncommercial MMORPGs spanning both PC and mobile platforms. In our experiments,\nTITAN achieves significantly higher task completion rates (95%) and bug\ndetection performance compared to existing automated game testing approaches.\nAn ablation study further demonstrates that each core component of TITAN\ncontributes substantially to its overall performance. Notably, TITAN detects\nfour previously unknown bugs that prior testing approaches fail to identify. We\nprovide an in-depth discussion of these results, which offer guidance for new\navenues of advancing intelligent, general-purpose testing systems. Moreover,\nTITAN has been deployed in eight real-world game QA pipelines, underscoring its\npractical impact as an LLM-driven game testing framework."}
{"id": "2509.21768", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.21768", "abs": "https://arxiv.org/abs/2509.21768", "authors": ["Jiawei Zhao", "Yuang Qi", "Weiming Zhang", "Nenghai Yu", "Kejiang Chen"], "title": "PSRT: Accelerating LRM-based Guard Models via Prefilled Safe Reasoning Traces", "comment": null, "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable performance on\ntasks such as mathematics and code generation. Motivated by these strengths,\nrecent work has empirically demonstrated the effectiveness of LRMs as guard\nmodels in improving harmful query detection. However, LRMs typically generate\nlong reasoning traces during inference, causing substantial computational\noverhead. In this paper, we introduce PSRT, a method that replaces the model's\nreasoning process with a Prefilled Safe Reasoning Trace, thereby significantly\nreducing the inference cost of LRMs. Concretely, PSRT prefills \"safe reasoning\nvirtual tokens\" from a constructed dataset and learns over their continuous\nembeddings. With the aid of indicator tokens, PSRT enables harmful-query\ndetection in a single forward pass while preserving the classification\neffectiveness of LRMs. We evaluate PSRT on 7 models, 13 datasets, and 8\njailbreak methods. In terms of efficiency, PSRT completely removes the overhead\nof generating reasoning tokens during inference. In terms of classification\nperformance, PSRT achieves nearly identical accuracy, with only a minor average\nF1 drop of 0.015 across 7 models and 5 datasets."}
{"id": "2509.22202", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22202", "abs": "https://arxiv.org/abs/2509.22202", "authors": ["Lukas Twist", "Jie M. Zhang", "Mark Harman", "Helen Yannakoudakis"], "title": "Library Hallucinations in LLMs: Risk Analysis Grounded in Developer Queries", "comment": "23 pages, 5 tables", "summary": "Large language models (LLMs) are increasingly used to generate code, yet they\ncontinue to hallucinate, often inventing non-existent libraries. Such library\nhallucinations are not just benign errors: they can mislead developers, break\nbuilds, and expose systems to supply chain threats such as slopsquatting.\nDespite increasing awareness of these risks, little is known about how\nreal-world prompt variations affect hallucination rates. Therefore, we present\nthe first systematic study of how user-level prompt variations impact library\nhallucinations in LLM-generated code. We evaluate six diverse LLMs across two\nhallucination types: library name hallucinations (invalid imports) and library\nmember hallucinations (invalid calls from valid libraries). We investigate how\nrealistic user language extracted from developer forums and how user errors of\nvarying degrees (one- or multi-character misspellings and completely fake\nnames/members) affect LLM hallucination rates. Our findings reveal systemic\nvulnerabilities: one-character misspellings in library names trigger\nhallucinations in up to 26% of tasks, fake library names are accepted in up to\n99% of tasks, and time-related prompts lead to hallucinations in up to 84% of\ntasks. Prompt engineering shows promise for mitigating hallucinations, but\nremains inconsistent and LLM-dependent. Our results underscore the fragility of\nLLMs to natural prompt variation and highlight the urgent need for safeguards\nagainst library-related hallucinations and their potential exploitation."}
{"id": "2509.21772", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.21772", "abs": "https://arxiv.org/abs/2509.21772", "authors": ["Daiki Chiba", "Hiroki Nakano", "Takashi Koide"], "title": "PhishLumos: An Adaptive Multi-Agent System for Proactive Phishing Campaign Mitigation", "comment": null, "summary": "Phishing attacks are a significant societal threat, disproportionately\nharming vulnerable populations and eroding trust in essential digital services.\nCurrent defenses are often reactive, failing against modern evasive tactics\nlike cloaking that conceal malicious content. To address this, we introduce\nPhishLumos, an adaptive multi-agent system that proactively mitigates entire\nattack campaigns. It confronts a core cybersecurity imbalance: attackers can\neasily scale operations, while defense remains an intensive expert task.\nInstead of being blocked by evasion, PhishLumos treats it as a critical signal\nto investigate the underlying infrastructure. Its Large Language Model\n(LLM)-powered agents uncover shared hosting, certificates, and domain\nregistration patterns. On real-world data, our system identified 100% of\ncampaigns in the median case, over a week before their confirmation by\ncybersecurity experts. PhishLumos demonstrates a practical shift from reactive\nURL blocking to proactive campaign mitigation, protecting users before they are\nharmed and making the digital world safer for all."}
{"id": "2509.22320", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22320", "abs": "https://arxiv.org/abs/2509.22320", "authors": ["Vincenzo De Martino", "Mohammad Amin Zadenoori", "Xavier Franch", "Alessio Ferrari"], "title": "Green Prompt Engineering: Investigating the Energy Impact of Prompt Design in Software Engineering", "comment": null, "summary": "Language Models are increasingly applied in software engineering, yet their\ninference raises growing environmental concerns. Prior work has examined\nhardware choices and prompt length, but little attention has been paid to\nlinguistic complexity as a sustainability factor. This paper introduces Green\nPrompt Engineering, framing linguistic complexity as a design dimension that\ncan influence energy consumption and performance. We conduct an empirical study\non requirement classification using open-source Small Language Models, varying\nthe readability of prompts. Our results reveal that readability affects\nenvironmental sustainability and performance, exposing trade-offs between them.\nFor practitioners, simpler prompts can reduce energy costs without a\nsignificant F1-score loss; for researchers, it opens a path toward guidelines\nand studies on sustainable prompt design within the Green AI agenda."}
{"id": "2509.21786", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.21786", "abs": "https://arxiv.org/abs/2509.21786", "authors": ["Junjie Song", "Jinguang Han", "Man Ho Au", "Rupeng Yang", "Chao Sun"], "title": "Lattice-Based Dynamic $k$-times Anonymous Authentication", "comment": null, "summary": "With the development of Internet, privacy has become a close concern of\nusers. Anonymous authentication plays an important role in privacy-preserving\nsystems. $k$-times anonymous authentication ($k$-TAA) scheme allows members of\na group to be authenticated anonymously by application providers up to $k$\ntimes. Considering quantum computing attacks, lattice-based $k$-TAA was\nintroduced. However, existing schemes do not support dynamically granting and\nrevoking users. In this paper, we construct the first lattice-based dynamic\n$k$-TAA, which offers limited times anonymous authentication, dynamic member\nmanagement, and post-quantum security. We present a concrete construction, and\nreduce its security to standard complexity assumptions. Notably, compared with\nexisting lattice-based $k$-TAA, our scheme is efficient in terms of\ncommunication cost."}
{"id": "2509.22337", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22337", "abs": "https://arxiv.org/abs/2509.22337", "authors": ["Haoyu Feng", "Xin Zhang"], "title": "GPU-Accelerated Loopy Belief Propagation for Program Analysis", "comment": null, "summary": "Loopy Belief Propagation (LBP) is a widely used approximate inference\nalgorithm in probabilistic graphical models, with applications in computer\nvision, error correction codes, protein folding, program analysis, etc.\nHowever, LBP faces significant computational challenges when applied to\nlarge-scale program analysis. While GPU (Graphics Processing Unit) parallel\ncomputing provides a promising solution, existing approaches lack support for\nflexible update strategies and have yet to integrate logical constraints with\nGPU acceleration, leading to suboptimal practical performance.\n  This paper presents a GPU-accelerated LBP algorithm for program analysis. To\nsupport the diverse update strategies required by users, we propose a unified\nrepresentation for specifying arbitrary user-defined update strategies, along\nwith a dependency analysis algorithm. Furthermore, building on previous work\nthat leverages the local structure of Horn clauses to simplify message passing,\nwe group messages to minimize warp divergence and better utilize GPU resources.\nExperimental results on datarace analysis over eight real-world Java programs\nshow that our approach achieves an average speedup of $2.14\\times$ over the\nstate-of-the-art sequential approach and $5.56\\times$ over the state-of-the-art\nGPU-based approach, while maintaining high accuracy."}
{"id": "2509.21821", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.21821", "abs": "https://arxiv.org/abs/2509.21821", "authors": ["Xinyu Hu", "Zhiwei Fu", "Shaocong Xie", "Steven H. H. Ding", "Philippe Charland"], "title": "SoK: Potentials and Challenges of Large Language Models for Reverse Engineering", "comment": null, "summary": "Reverse Engineering (RE) is central to software security, enabling tasks such\nas vulnerability discovery and malware analysis, but it remains labor-intensive\nand requires substantial expertise. Earlier advances in deep learning start to\nautomate parts of RE, particularly for malware detection and vulnerability\nclassification. More recently, a rapidly growing body of work has applied Large\nLanguage Models (LLMs) to similar purposes. Their role compared to prior\nmachine learning remains unclear, since some efforts simply adapt existing\npipelines with minimal change while others seek to exploit broader reasoning\nand generative abilities. These differences, combined with varied problem\ndefinitions, methods, and evaluation practices, limit comparability,\nreproducibility, and cumulative progress. This paper systematizes the field by\nreviewing 44 research papers, including peer-reviewed publications and\npreprints, and 18 additional open-source projects that apply LLMs in RE. We\npropose a taxonomy that organizes existing work by objective, target, method,\nevaluation strategy, and data scale. Our analysis identifies strengths and\nlimitations, highlights reproducibility and evaluation gaps, and examines\nemerging risks. We conclude with open challenges and future research directions\nthat aim to guide more coherent and security-relevant applications of LLMs in\nRE."}
{"id": "2509.22379", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22379", "abs": "https://arxiv.org/abs/2509.22379", "authors": ["Stefano Carlo Lambertenghi", "Mirena Flores Valdez", "Andrea Stocco"], "title": "A Multi-Modality Evaluation of the Reality Gap in Autonomous Driving Systems", "comment": "In proceedings of the 40th IEEE/ACM International Conference on\n  Automated Software Engineering (ASE '25)", "summary": "Simulation-based testing is a cornerstone of Autonomous Driving System (ADS)\ndevelopment, offering safe and scalable evaluation across diverse driving\nscenarios. However, discrepancies between simulated and real-world behavior,\nknown as the reality gap, challenge the transferability of test results to\ndeployed systems. In this paper, we present a comprehensive empirical study\ncomparing four representative testing modalities: Software-in-the-Loop (SiL),\nVehicle-in-the-Loop (ViL), Mixed-Reality (MR), and full real-world testing.\nUsing a small-scale physical vehicle equipped with real sensors (camera and\nLiDAR) and its digital twin, we implement each setup and evaluate two ADS\narchitectures (modular and end-to-end) across diverse indoor driving scenarios\ninvolving real obstacles, road topologies, and indoor environments. We\nsystematically assess the impact of each testing modality along three\ndimensions of the reality gap: actuation, perception, and behavioral fidelity.\nOur results show that while SiL and ViL setups simplify critical aspects of\nreal-world dynamics and sensing, MR testing improves perceptual realism without\ncompromising safety or control. Importantly, we identify the conditions under\nwhich failures do not transfer across testing modalities and isolate the\nunderlying dimensions of the gap responsible for these discrepancies. Our\nfindings offer actionable insights into the respective strengths and\nlimitations of each modality and outline a path toward more robust and\ntransferable validation of autonomous driving systems."}
{"id": "2509.21831", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.21831", "abs": "https://arxiv.org/abs/2509.21831", "authors": ["Hesam Sarkhosh", "Uzma Maroof", "Diogo Barradas"], "title": "The Dark Art of Financial Disguise in Web3: Money Laundering Schemes and Countermeasures", "comment": "Accepted manuscript to APWG eCrime 2025", "summary": "The rise of Web3 and Decentralized Finance (DeFi) has enabled borderless\naccess to financial services empowered by smart contracts and blockchain\ntechnology. However, the ecosystem's trustless, permissionless, and borderless\nnature presents substantial regulatory challenges. The absence of centralized\noversight and the technical complexity create fertile ground for financial\ncrimes. Among these, money laundering is particularly concerning, as in the\nevent of successful scams, code exploits, and market manipulations, it\nfacilitates covert movement of illicit gains. Beyond this, there is a growing\nconcern that cryptocurrencies can be leveraged to launder proceeds from drug\ntrafficking, or to transfer funds linked to terrorism financing.\n  This survey aims to outline a taxonomy of high-level strategies and\nunderlying mechanisms exploited to facilitate money laundering in Web3. We\nexamine how criminals leverage the pseudonymous nature of Web3, alongside weak\nregulatory frameworks, to obscure illicit financial activities. Our study seeks\nto bridge existing knowledge gaps on laundering schemes, identify open\nchallenges in the detection and prevention of such activities, and propose\nfuture research directions to foster a more transparent Web3 financial\necosystem -- offering valuable insights for researchers, policymakers, and\nindustry practitioners."}
{"id": "2509.22420", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.22420", "abs": "https://arxiv.org/abs/2509.22420", "authors": ["Ziyi Zhang", "Devjeet Roy", "Venera Arnaoudova"], "title": "Context-Specific Instruction: A Longitudinal Study on Debugging Skill Acquisition and Retention for Novice Programmers", "comment": "31 pages (25 pages for the paper, rest pages are references and\n  appendix). 4 tables, 7 figures", "summary": "Bug localization is a critical skill, yet novices often lack systematic\napproaches. Prior work tested abstract guidelines and general concrete steps;\nthe impact of context-specific instruction is unclear. We ran an eight-week\nlongitudinal study with four conditions: no instruction (G1), abstract\nguidelines (G2), concrete steps (G3), and our context-specific instruction that\npairs concrete bug-localization steps with problem-specific details (G4).\nForty-four undergraduates participated; 41 completed all five sessions (S1-S5).\nEach session included 2-3 debugging tasks to identify the minimal code element\ncontaining a seeded logical fault. We measured correctness (binary), time to\ncompletion, self-perceived scores (stress, difficulty, satisfaction, and\nstrategy adherence). G4 achieved higher correctness and shorter time to\ncompletion: it reached 80% correctness after one session (vs. 20-44% for other\ngroups) and maintained 80% after three weeks, outperforming all groups (p <\n0.05); its time to completion stabilized at 13-15 minutes in S1, whereas other\ngroups took 2-3 sessions to stabilize at 22-27 minutes. Qualitative responses\nshowed lower stress and higher satisfaction in G4, with participants\ninternalizing strategies via contextual examples. We conclude that\ncontext-specific instruction yields faster skill acquisition and stronger\nretention than abstract guidelines or context-agnostic steps. Even 1-2 sessions\nproduced significant gains, while extended practice optimized and stabilized\nperformance. Integrating contextual examples with abstract principles may\nbridge theory-practice gaps in bug-localization education and provide a more\nequitable path for novices."}
{"id": "2509.21843", "categories": ["cs.CR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21843", "abs": "https://arxiv.org/abs/2509.21843", "authors": ["Jingkai Guo", "Chaitali Chakrabarti", "Deliang Fan"], "title": "SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models", "comment": "10 pages, 4 figures, 5 tables, 2 equations. Topics: Bit-flip attacks,\n  adversarial attacks, large language models (LLMs)", "summary": "Model integrity of Large language models (LLMs) has become a pressing\nsecurity concern with their massive online deployment. Prior Bit-Flip Attacks\n(BFAs) -- a class of popular AI weight memory fault-injection techniques -- can\nseverely compromise Deep Neural Networks (DNNs): as few as tens of bit flips\ncan degrade accuracy toward random guessing. Recent studies extend BFAs to LLMs\nand reveal that, despite the intuition of better robustness from modularity and\nredundancy, only a handful of adversarial bit flips can also cause LLMs'\ncatastrophic accuracy degradation. However, existing BFA methods typically\nfocus on either integer or floating-point models separately, limiting attack\nflexibility. Moreover, in floating-point models, random bit flips often cause\nperturbed parameters to extreme values (e.g., flipping in exponent bit), making\nit not stealthy and leading to numerical runtime error (e.g., invalid tensor\nvalues (NaN/Inf)). In this work, for the first time, we propose SBFA (Sneaky\nBit-Flip Attack), which collapses LLM performance with only one single bit flip\nwhile keeping perturbed values within benign layer-wise weight distribution. It\nis achieved through iterative searching and ranking through our defined\nparameter sensitivity metric, ImpactScore, which combines gradient sensitivity\nand perturbation range constrained by the benign layer-wise weight\ndistribution. A novel lightweight SKIP searching algorithm is also proposed to\ngreatly reduce searching complexity, which leads to successful SBFA searching\ntaking only tens of minutes for SOTA LLMs. Across Qwen, LLaMA, and Gemma\nmodels, with only one single bit flip, SBFA successfully degrades accuracy to\nbelow random levels on MMLU and SST-2 in both BF16 and INT8 data formats.\nRemarkably, flipping a single bit out of billions of parameters reveals a\nsevere security concern of SOTA LLM models."}
{"id": "2509.22431", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22431", "abs": "https://arxiv.org/abs/2509.22431", "authors": ["Zhengyu Chen", "Zhaoyi Meng", "Wenxiang Zhao", "Wansen Wang", "Haoyang Zhao", "Jiahao Zhan", "Jie Cui", "Hong Zhong"], "title": "TreeMind: Automatically Reproducing Android Bug Reports via LLM-empowered Monte Carlo Tree Search", "comment": null, "summary": "Automatically reproducing Android app crashes from textual bug reports is\nchallenging, particularly when the reports are incomplete and the modern UI\nexhibits high combinatorial complexity. Existing approaches based on\nreinforcement learning or large language models (LLMs) exhibit limitations in\nsuch scenarios. They struggle to infer unobserved steps and reconstruct the\nunderlying user action sequences to navigate the vast UI interaction space,\nprimarily due to limited goal-directed reasoning and planning. We present\nTreeMind, a novel technique that integrates LLMs with a customized Monte Carlo\nTree Search (MCTS) algorithm to achieve strategic UI exploration in bug\nreproduction. To the best of our knowledge, this is the first work to combine\nexternal decision-making with LLM semantic reasoning for reliable bug\nreproduction. We formulate the reproduction task as a target-driven search\nproblem, leveraging MCTS as the core planning mechanism to iteratively refine\naction sequences. To enhance MCTS with semantic reasoning, we introduce two\nLLM-guided agents with distinct roles: Expander generates top-k promising\nactions based on the current UI state and exploration history, while Simulator\nestimates the likelihood that each action leads toward successful reproduction.\nBy incorporating multi-modal UI inputs and advanced prompting techniques,\nTreeMind conducts feedback-aware navigation that identifies missing but\nessential user actions and incrementally reconstructs the reproduction paths.\nWe evaluate TreeMind on a dataset of 93 real-world Android bug reports from\nthree widely-used benchmarks. Experimental results show that it significantly\noutperforms four state-of-the-art baselines in reproduction success rate. A\nreal-world case study indicates that integrating LLM reasoning with MCTS-based\nplanning is a compelling direction for automated bug reproduction."}
{"id": "2509.21884", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21884", "abs": "https://arxiv.org/abs/2509.21884", "authors": ["Bochuan Cao", "Changjiang Li", "Yuanpu Cao", "Yameng Ge", "Ting Wang", "Jinghui Chen"], "title": "You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors", "comment": "29 pages, 10 tables, 6figures, accepted by CCS 25", "summary": "Large language models (LLMs) have been widely adopted across various\napplications, leveraging customized system prompts for diverse tasks. Facing\npotential system prompt leakage risks, model developers have implemented\nstrategies to prevent leakage, primarily by disabling LLMs from repeating their\ncontext when encountering known attack patterns. However, it remains vulnerable\nto new and unforeseen prompt-leaking techniques. In this paper, we first\nintroduce a simple yet effective prompt leaking attack to reveal such risks.\nOur attack is capable of extracting system prompts from various LLM-based\napplication, even from SOTA LLM models such as GPT-4o or Claude 3.5 Sonnet. Our\nfindings further inspire us to search for a fundamental solution to the\nproblems by having no system prompt in the context. To this end, we propose\nSysVec, a novel method that encodes system prompts as internal representation\nvectors rather than raw text. By doing so, SysVec minimizes the risk of\nunauthorized disclosure while preserving the LLM's core language capabilities.\nRemarkably, this approach not only enhances security but also improves the\nmodel's general instruction-following abilities. Experimental results\ndemonstrate that SysVec effectively mitigates prompt leakage attacks, preserves\nthe LLM's functional integrity, and helps alleviate the forgetting issue in\nlong-context scenarios."}
{"id": "2509.22530", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22530", "abs": "https://arxiv.org/abs/2509.22530", "authors": ["Baijun Cheng", "Kailong Wang", "Ling Shi", "Haoyu Wang", "Peng Di", "Yao Guo", "Ding Li", "Xiangqun Chen"], "title": "Boosting Pointer Analysis With Large Language Model-Enhanced Allocation Function Detection", "comment": null, "summary": "Pointer analysis is foundational for many static analysis tasks, yet its\neffectiveness is often hindered by imprecise modeling of heap allocations,\nparticularly in C/C++ programs where user-defined allocation functions (AFs)\nare pervasive. Existing approaches largely overlook these custom allocators,\nleading to coarse aliasing and reduced analysis precision. In this paper, we\npresent AFD, a novel technique that enhances pointer analysis by automatically\nidentifying and modeling custom allocation functions. AFD employs a hybrid\napproach: it uses value-flow analysis to detect straightforward wrappers and\nleverages Large Language Models (LLMs) to reason about more complex allocation\npatterns with side effects. This targeted enhancement enables precise modeling\nof heap objects at each call site, achieving context-sensitivity-like benefits\nwithout the associated overhead. We evaluate AFD on 15 real-world C projects,\nidentifying over 600 custom AFs. Integrating AFD into a baseline pointer\nanalysis yields a 26x increase in modeled heap objects and a 39% reduction in\nalias set sizes, with only 1.4x runtime overhead. Furthermore, our enhanced\nanalysis improves indirect call resolution and uncovers 17 previously\nundetected memory bugs. These results demonstrate that precise modeling of\ncustom allocation functions offers a scalable and practical path to improving\npointer analysis in large software systems."}
{"id": "2509.22022", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22022", "abs": "https://arxiv.org/abs/2509.22022", "authors": ["Marc Damie", "Florian Hahn", "Andreas Peter", "Jan Ramon"], "title": "Eliminating Exponential Key Growth in PRG-Based Distributed Point Functions", "comment": "Accepted in DPM 2025", "summary": "Distributed Point Functions (DPFs) enable sharing secret point functions\nacross multiple parties, supporting privacy-preserving technologies such as\nPrivate Information Retrieval, and anonymous communications. While 2-party\nPRG-based schemes with logarithmic key sizes have been known for a decade,\nextending these solutions to multi-party settings has proven challenging. In\nparticular, PRG-based multi-party DPFs have historically struggled with\npracticality due to key sizes growing exponentially with the number of parties\nand the field size.\n  Our work addresses this efficiency bottleneck by optimizing the PRG-based\nmulti-party DPF scheme of Boyle et al. (EUROCRYPT'15). By leveraging the\nhonest-majority assumption, we eliminate the exponential factor present in this\nscheme. Our construction is the first PRG-based multi-party DPF scheme with\npractical key sizes, and provides key up to 3x smaller than the best known\nmulti-party DPF. This work demonstrates that with careful optimization,\nPRG-based multi-party DPFs can achieve practical performances, and even obtain\ntop performances."}
{"id": "2509.22040", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22040", "abs": "https://arxiv.org/abs/2509.22040", "authors": ["Yue Liu", "Yanjie Zhao", "Yunbo Lyu", "Ting Zhang", "Haoyu Wang", "David Lo"], "title": "\"Your AI, My Shell\": Demystifying Prompt Injection Attacks on Agentic AI Coding Editors", "comment": null, "summary": "Agentic AI coding editors driven by large language models have recently\nbecome more popular due to their ability to improve developer productivity\nduring software development. Modern editors such as Cursor are designed not\njust for code completion, but also with more system privileges for complex\ncoding tasks (e.g., run commands in the terminal, access development\nenvironments, and interact with external systems). While this brings us closer\nto the \"fully automated programming\" dream, it also raises new security\nconcerns. In this study, we present the first empirical analysis of prompt\ninjection attacks targeting these high-privilege agentic AI coding editors. We\nshow how attackers can remotely exploit these systems by poisoning external\ndevelopment resources with malicious instructions, effectively hijacking AI\nagents to run malicious commands, turning \"your AI\" into \"attacker's shell\". To\nperform this analysis, we implement AIShellJack, an automated testing framework\nfor assessing prompt injection vulnerabilities in agentic AI coding editors.\nAIShellJack contains 314 unique attack payloads that cover 70 techniques from\nthe MITRE ATT&CK framework. Using AIShellJack, we conduct a large-scale\nevaluation on GitHub Copilot and Cursor, and our evaluation results show that\nattack success rates can reach as high as 84% for executing malicious commands.\nMoreover, these attacks are proven effective across a wide range of objectives,\nranging from initial access and system discovery to credential theft and data\nexfiltration."}
{"id": "2509.22027", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22027", "abs": "https://arxiv.org/abs/2509.22027", "authors": ["Mingkai Li", "Hang Ye", "Joseph Devietti", "Suman Jana", "Tanvir Ahmed Khan"], "title": "NanoTag: Systems Support for Efficient Byte-Granular Overflow Detection on ARM MTE", "comment": null, "summary": "Memory safety bugs, such as buffer overflows and use-after-frees, are the\nleading causes of software safety issues in production. Software-based\napproaches, e.g., Address Sanitizer (ASAN), can detect such bugs with high\nprecision, but with prohibitively high overhead. ARM's Memory Tagging Extension\n(MTE) offers a promising alternative to detect these bugs in hardware with a\nmuch lower overhead. However, in this paper, we perform a thorough\ninvestigation of Google Pixel 8, the first production implementation of ARM\nMTE, and show that MTE can only achieve coarse precision in bug detection\ncompared with software-based approaches such as ASAN, mainly due to its 16-byte\ntag granularity. To address this issue, we present NanoTag, a system to detect\nmemory safety bugs in unmodified binaries at byte granularity with ARM MTE.\nNanoTag detects intra-granule buffer overflows by setting up a tripwire for tag\ngranules that may require intra-granule overflow detection. The memory access\nto the tripwire causes additional overflow detection in the software while\nusing MTE's hardware to detect bugs for the rest of the accesses. We implement\nNanoTag based on the Scudo Hardened Allocator, the default memory allocator on\nAndroid since Android 11. Our evaluation results across popular benchmarks and\nreal-world case studies show that NanoTag detects nearly as many memory safety\nbugs as ASAN while incurring similar run-time overhead to Scudo Hardened\nAllocator in MTE SYNC mode."}
{"id": "2509.22040", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22040", "abs": "https://arxiv.org/abs/2509.22040", "authors": ["Yue Liu", "Yanjie Zhao", "Yunbo Lyu", "Ting Zhang", "Haoyu Wang", "David Lo"], "title": "\"Your AI, My Shell\": Demystifying Prompt Injection Attacks on Agentic AI Coding Editors", "comment": null, "summary": "Agentic AI coding editors driven by large language models have recently\nbecome more popular due to their ability to improve developer productivity\nduring software development. Modern editors such as Cursor are designed not\njust for code completion, but also with more system privileges for complex\ncoding tasks (e.g., run commands in the terminal, access development\nenvironments, and interact with external systems). While this brings us closer\nto the \"fully automated programming\" dream, it also raises new security\nconcerns. In this study, we present the first empirical analysis of prompt\ninjection attacks targeting these high-privilege agentic AI coding editors. We\nshow how attackers can remotely exploit these systems by poisoning external\ndevelopment resources with malicious instructions, effectively hijacking AI\nagents to run malicious commands, turning \"your AI\" into \"attacker's shell\". To\nperform this analysis, we implement AIShellJack, an automated testing framework\nfor assessing prompt injection vulnerabilities in agentic AI coding editors.\nAIShellJack contains 314 unique attack payloads that cover 70 techniques from\nthe MITRE ATT&CK framework. Using AIShellJack, we conduct a large-scale\nevaluation on GitHub Copilot and Cursor, and our evaluation results show that\nattack success rates can reach as high as 84% for executing malicious commands.\nMoreover, these attacks are proven effective across a wide range of objectives,\nranging from initial access and system discovery to credential theft and data\nexfiltration."}
{"id": "2509.22126", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22126", "abs": "https://arxiv.org/abs/2509.22126", "authors": ["Enoal Gesny", "Eva Giboulot", "Teddy Furon", "Vivien Chappelier"], "title": "Guidance Watermarking for Diffusion Models", "comment": null, "summary": "This paper introduces a novel watermarking method for diffusion models. It is\nbased on guiding the diffusion process using the gradient computed from any\noff-the-shelf watermark decoder. The gradient computation encompasses different\nimage augmentations, increasing robustness to attacks against which the decoder\nwas not originally robust, without retraining or fine-tuning. Our method\neffectively convert any \\textit{post-hoc} watermarking scheme into an\nin-generation embedding along the diffusion process. We show that this approach\nis complementary to watermarking techniques modifying the variational\nautoencoder at the end of the diffusion process. We validate the methods on\ndifferent diffusion models and detectors. The watermarking guidance does not\nsignificantly alter the generated image for a given seed and prompt, preserving\nboth the diversity and quality of generation."}
{"id": "2509.22143", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22143", "abs": "https://arxiv.org/abs/2509.22143", "authors": ["Johnnatan Messias", "Christof Ferreira Torres"], "title": "The Express Lane to Spam and Centralization: An Empirical Analysis of Arbitrum's Timeboost", "comment": null, "summary": "DeFi applications are vulnerable to MEV, where specialized actors profit by\nreordering or inserting transactions. To mitigate latency races and internalize\nMEV revenue, Arbitrum introduced Timeboost, an auction-based transaction\nsequencing mechanism that grants short-term priority access to an express lane.\nIn this paper we present the first large-scale empirical study of Timeboost,\nanalyzing over 11.5 million express lane transactions and 151 thousand auctions\nbetween April and July 2025. Our results reveal five main findings. First,\nexpress lane control is highly centralized, with two entities winning more than\n90% of auctions. Second, while express lane access provides earlier inclusion,\nprofitable MEV opportunities cluster at the end of blocks, limiting the value\nof priority access. Third, approximately 22% of time-boosted transactions are\nreverted, indicating that the Timeboost does not effectively mitigate spam.\nFourth, secondary markets for reselling express lane rights have collapsed due\nto poor execution reliability and unsustainable economics. Finally, auction\ncompetition declined over time, leading to steadily reduced revenue for the\nArbitrum DAO. Taken together, these findings show that Timeboost fails to\ndeliver on its stated goals of fairness, decentralization, and spam reduction.\nInstead, it reinforces centralization and narrows adoption, highlighting the\nlimitations of auction-based ordering as a mechanism for fair transaction\nsequencing in rollups."}
{"id": "2509.22154", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22154", "abs": "https://arxiv.org/abs/2509.22154", "authors": ["Zhou Xu", "Guyue Li", "Zhe Peng", "Aiqun Hu"], "title": "Collusion-Driven Impersonation Attack on Channel-Resistant RF Fingerprinting", "comment": null, "summary": "Radio frequency fingerprint (RFF) is a promising device identification\ntechnology, with recent research shifting from robustness to security due to\ngrowing concerns over vulnerabilities. To date, while the security of RFF\nagainst basic spoofing such as MAC address tampering has been validated, its\nresilience to advanced mimicry remains unknown. To address this gap, we propose\na collusion-driven impersonation attack that achieves RF-level mimicry,\nsuccessfully breaking RFF identification systems across diverse environments.\nSpecifically, the attacker synchronizes with a colluding receiver to match the\ncentralized logarithmic power spectrum (CLPS) of the legitimate transmitter;\nonce the colluder deems the CLPS identical, the victim receiver will also\naccept the forged fingerprint, completing RF-level spoofing. Given that the\ndistribution of CLPS features is relatively concentrated and has a clear\nunderlying structure, we design a spoofed signal generation network that\nintegrates a variational autoencoder (VAE) with a multi-objective loss function\nto enhance the similarity and deceptive capability of the generated samples. We\ncarry out extensive simulations, validating cross-channel attacks in\nenvironments that incorporate standard channel variations including additive\nwhite Gaussian noise (AWGN), multipath fading, and Doppler shift. The results\nindicate that the proposed attack scheme essentially maintains a success rate\nof over 95% under different channel conditions, revealing the effectiveness of\nthis attack."}
{"id": "2509.22213", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22213", "abs": "https://arxiv.org/abs/2509.22213", "authors": ["Ossi RÃ¤isÃ¤", "Antti Koskela", "Antti Honkela"], "title": "Accuracy-First RÃ©nyi Differential Privacy and Post-Processing Immunity", "comment": null, "summary": "The accuracy-first perspective of differential privacy addresses an important\nshortcoming by allowing a data analyst to adaptively adjust the quantitative\nprivacy bound instead of sticking to a predetermined bound. Existing works on\nthe accuracy-first perspective have neglected an important property of\ndifferential privacy known as post-processing immunity, which ensures that an\nadversary is not able to weaken the privacy guarantee by post-processing. We\naddress this gap by determining which existing definitions in the\naccuracy-first perspective have post-processing immunity, and which do not. The\nonly definition with post-processing immunity, pure ex-post privacy, lacks\nuseful tools for practical problems, such as an ex-post analogue of the\nGaussian mechanism, and an algorithm to check if accuracy on separate private\nvalidation set is high enough. To address this, we propose a new definition\nbased on R\\'enyi differential privacy that has post-processing immunity, and we\ndevelop basic theory and tools needed for practical applications. We\ndemonstrate the practicality of our theory with an application to synthetic\ndata generation, where our algorithm successfully adjusts the privacy bound\nuntil an accuracy threshold is met on a private validation dataset."}
{"id": "2509.22215", "categories": ["cs.CR", "cs.FL", "D.2.4; F.3.1; K.6.5"], "pdf": "https://arxiv.org/pdf/2509.22215", "abs": "https://arxiv.org/abs/2509.22215", "authors": ["Stefan Marksteiner", "Mikael SjÃ¶din", "Marjan Sirjani"], "title": "Learn, Check, Test -- Security Testing Using Automata Learning and Model Checking", "comment": "19 pages, 5 figures, 2 tables, preprint submitted to Elsevier\n  Computers & Security - Original abstract shortened to comply to the arXiv\n  requirements", "summary": "Cyber-physical systems are part of industrial systems and critical\ninfrastructure. Therefore, they should be examined in a comprehensive manner to\nverify their correctness and security. At the same time, the complexity of such\nsystems demands such examinations to be systematic and, if possible, automated\nfor efficiency and accuracy. A method that can be useful in this context is\nmodel checking. However, this requires a model that faithfully represents the\nbehavior of the examined system. Obtaining such a model is not trivial, as many\nof these systems can be examined only in black box settings due to, e.g., long\nsupply chains or secrecy. We therefore utilize active black box learning\ntechniques to infer behavioral models in the form of Mealy machines of such\nsystems and translate them into a form that can be evaluated using a model\nchecker. To this end, we will investigate a cyber-physical systems as a black\nbox using its external communication interface. We first annotate the model\nwith propositions by mapping context information from the respective protocol\nto the model using Context-based Proposition Maps (CPMs). We gain annotated\nMealy machines that resemble Kripke structures. We then formally define a\ntemplate, to transfer the structures model checker-compatible format. We\nfurther define generic security properties based on basic security\nrequirements. Due to the used CPMs, we can instantiate these properties with a\nmeaningful context to check a specific protocol, which makes the approach\nflexible and scalable. The gained model can be easily altered to introduce\nnon-deterministic behavior (like timeouts) or faults and examined if the\nproperties still. Lastly, we demonstrate the versatility of the approach by\nproviding case studies of different communication protocols (NFC and UDS),\nchecked with the same tool chain and the same security properties."}
{"id": "2509.22256", "categories": ["cs.CR", "cs.AI", "cs.OS"], "pdf": "https://arxiv.org/pdf/2509.22256", "abs": "https://arxiv.org/abs/2509.22256", "authors": ["Haochen Gong", "Chenxiao Li", "Rui Chang", "Wenbo Shen"], "title": "Secure and Efficient Access Control for Computer-Use Agents via Context Space", "comment": null, "summary": "Large language model (LLM)-based computer-use agents represent a convergence\nof AI and OS capabilities, enabling natural language to control system- and\napplication-level functions. However, due to LLMs' inherent uncertainty issues,\ngranting agents control over computers poses significant security risks. When\nagent actions deviate from user intentions, they can cause irreversible\nconsequences. Existing mitigation approaches, such as user confirmation and\nLLM-based dynamic action validation, still suffer from limitations in\nusability, security, and performance. To address these challenges, we propose\nCSAgent, a system-level, static policy-based access control framework for\ncomputer-use agents. To bridge the gap between static policy and dynamic\ncontext and user intent, CSAgent introduces intent- and context-aware policies,\nand provides an automated toolchain to assist developers in constructing and\nrefining them. CSAgent enforces these policies through an optimized OS service,\nensuring that agent actions can only be executed under specific user intents\nand contexts. CSAgent supports protecting agents that control computers through\ndiverse interfaces, including API, CLI, and GUI. We implement and evaluate\nCSAgent, which successfully defends against more than 99.36% of attacks while\nintroducing only 6.83% performance overhead."}
{"id": "2509.22280", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22280", "abs": "https://arxiv.org/abs/2509.22280", "authors": ["Gustavo SÃ¡nchez", "Ghada Elbez", "Veit Hagenmeyer"], "title": "A Global Analysis of Cyber Threats to the Energy Sector: \"Currents of Conflict\" from a Geopolitical Perspective", "comment": "THIS IS A POSTPRINT OF A PEER-REVIEWED ARTICLE, PLEASE CITE IT IF\n  USING THIS WORK: Gustavo Sanchez, Ghada Elbez, and Veit Hagenmeyer. \"A Global\n  Analysis of Cyber Threats to the Energy Sector:\"Currents of Conflict\" from a\n  geopolitical perspective.\" atp magazin 67.9 (2025): 56-66.\n  https://doi.org/10.17560/atp.v67i9.2797", "summary": "The escalating frequency and sophistication of cyber threats increased the\nneed for their comprehensive understanding. This paper explores the\nintersection of geopolitical dynamics, cyber threat intelligence analysis, and\nadvanced detection technologies, with a focus on the energy domain. We leverage\ngenerative artificial intelligence to extract and structure information from\nraw cyber threat descriptions, enabling enhanced analysis. By conducting a\ngeopolitical comparison of threat actor origins and target regions across\nmultiple databases, we provide insights into trends within the general threat\nlandscape. Additionally, we evaluate the effectiveness of cybersecurity tools\n-- with particular emphasis on learning-based techniques -- in detecting\nindicators of compromise for energy-targeted attacks. This analysis yields new\ninsights, providing actionable information to researchers, policy makers, and\ncybersecurity professionals."}
{"id": "2509.22428", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.22428", "abs": "https://arxiv.org/abs/2509.22428", "authors": ["Leonhard Grosse", "Sara Saeidian", "Mikael Skoglund", "Tobias J. Oechtering"], "title": "Privacy Mechanism Design based on Empirical Distributions", "comment": "accepted to IEEE CSF 2026", "summary": "Pointwise maximal leakage (PML) is a per-outcome privacy measure based on\nthreat models from quantitative information flow. Privacy guarantees with PML\nrely on knowledge about the distribution that generated the private data. In\nthis work, we propose a framework for PML privacy assessment and mechanism\ndesign with empirical estimates of this data-generating distribution. By\nextending the PML framework to consider sets of data-generating distributions,\nwe arrive at bounds on the worst-case leakage within a given set. We use these\nbounds alongside large-deviation bounds from the literature to provide a method\nfor obtaining distribution-independent $(\\varepsilon,\\delta)$-PML guarantees\nwhen the data-generating distribution is estimated from available data samples.\nWe provide an optimal binary mechanism, and show that mechanism design with\nthis type of uncertainty about the data-generating distribution reduces to a\nlinearly constrained convex program. Further, we show that optimal mechanisms\ndesigned for a distribution estimate can be used. Finally, we apply these tools\nto leakage assessment of the Laplace mechanism and the Gaussian mechanism for\nbinary private data, and numerically show that the presented approach to\nmechanism design can yield significant utility increase compared to local\ndifferential privacy, while retaining similar privacy guarantees."}
{"id": "2509.22097", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22097", "abs": "https://arxiv.org/abs/2509.22097", "authors": ["Junkai Chen", "Huihui Huang", "Yunbo Lyu", "Junwen An", "Jieke Shi", "Chengran Yang", "Ting Zhang", "Haoye Tian", "Yikun Li", "Zhenhao Li", "Xin Zhou", "Xing Hu", "David Lo"], "title": "SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios", "comment": null, "summary": "Large language model (LLM) powered code agents are rapidly transforming\nsoftware engineering by automating tasks such as testing, debugging, and\nrepairing, yet the security risks of their generated code have become a\ncritical concern. Existing benchmarks have offered valuable insights but remain\ninsufficient: they often overlook the genuine context in which vulnerabilities\nwere introduced or adopt narrow evaluation protocols that fail to capture\neither functional correctness or newly introduced vulnerabilities. We therefore\nintroduce SecureAgentBench, a benchmark of 105 coding tasks designed to\nrigorously evaluate code agents' capabilities in secure code generation. Each\ntask includes (i) realistic task settings that require multi-file edits in\nlarge repositories, (ii) aligned contexts based on real-world open-source\nvulnerabilities with precisely identified introduction points, and (iii)\ncomprehensive evaluation that combines functionality testing, vulnerability\nchecking through proof-of-concept exploits, and detection of newly introduced\nvulnerabilities using static analysis. We evaluate three representative agents\n(SWE-agent, OpenHands, and Aider) with three state-of-the-art LLMs (Claude 3.7\nSonnet, GPT-4.1, and DeepSeek-V3.1). Results show that (i) current agents\nstruggle to produce secure code, as even the best-performing one, SWE-agent\nsupported by DeepSeek-V3.1, achieves merely 15.2% correct-and-secure solutions,\n(ii) some agents produce functionally correct code but still introduce\nvulnerabilities, including new ones not previously recorded, and (iii) adding\nexplicit security instructions for agents does not significantly improve secure\ncoding, underscoring the need for further research. These findings establish\nSecureAgentBench as a rigorous benchmark for secure code generation and a step\ntoward more reliable software development with LLMs."}
