{"id": "2510.13822", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.13822", "abs": "https://arxiv.org/abs/2510.13822", "authors": ["Bartosz Burgiel"], "title": "Noisy Networks, Nosy Neighbors: Inferring Privacy Invasive Information from Encrypted Wireless Traffic", "comment": "80 pages, 49 figures, bachelor thesis at the data privacy and\n  security chair of the leipzig university", "summary": "This thesis explores the extent to which passive observation of wireless\ntraffic in a smart home environment can be used to infer privacy-invasive\ninformation about its inhabitants. Using a setup that mimics the capabilities\nof a nosy neighbor in an adjacent flat, we analyze raw 802.11 packets and\nBluetooth Low Energy advertisemets. From this data, we identify devices, infer\ntheir activity states and approximate their location using RSSI-based\ntrilateration. Despite the encrypted nature of the data, we demonstrate that it\nis possible to detect active periods of multimedia devices, infer common\nactivities such as sleeping, working and consuming media, and even approximate\nthe layout of the neighbor's apartment. Our results show that privacy risks in\nsmart homes extend beyond traditional data breaches: a nosy neighbor behind the\nwall can gain privacy-invasive insights into the lives of their neighbors\npurely from encrypted network traffic.", "AI": {"tldr": "This paper shows encrypted smart home network traffic can reveal private behaviors and spatial layouts, exposing new privacy risks from passive observation by neighbors.", "motivation": "To demonstrate how encrypted wireless traffic in smart homes can be exploited to infer sensitive information about inhabitants' behaviors and living spaces, even without data breaches.", "method": "The study uses a setup mimicking a nosy neighbor's capabilities, analyzing 802.11 packets and Bluetooth Low Energy advertisements to identify devices, infer activity states, and approximate locations via RSSI-based trilateration.", "result": "The research successfully detected multimedia device activity periods, inferred daily activities (sleeping, working, media consumption), and approximated the layout of a neighbor's apartment using solely encrypted network traffic.", "conclusion": "Privacy risks in smart homes extend beyond traditional data breaches, as passive observation by a nosy neighbor can reveal privacy-invasive insights through encrypted network traffic."}}
{"id": "2510.13824", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.13824", "abs": "https://arxiv.org/abs/2510.13824", "authors": ["Wai Ming Chan", "Remi Chou", "Taejoon Kim"], "title": "Multi-Layer Secret Sharing for Cross-Layer Attack Defense in 5G Networks: a COTS UE Demonstration", "comment": null, "summary": "This demo presents the first implementation of multi-layer secret sharing on\ncommercial-off-the-shelf (COTS) 5G user equipment (UE), operating without\ninfrastructure modifications or pre-shared keys. Our XOR-based approach\ndistributes secret shares across network operators and distributed relays,\nensuring perfect recovery and data confidentiality even if one network operator\nand one relay are simultaneously lost (e.g., under denial of service (DoS) or\nunanticipated attacks).", "AI": {"tldr": "The paper introduces the first implementation of multi-layer secret sharing on COTS 5G UE, ensuring perfect recovery and confidentiality even if one network operator and one relay are lost.", "motivation": "The paper may be motivated by the need for secure data transmission in 5G networks without the complexities of infrastructure modification or pre-shared keys.", "method": "The method is based on an XOR-based approach to distribute secret shares across different network operators and distributed relays. This allows for data confidentiality and perfect recovery despite potential loss of either a network operator or a relay, allowing for a robust communication system that can withstand DoS or unexpected attacks.", "result": "The authors have successfully demonstrated the first real-world implementation of multi-layer secret sharing on COTS 5G user equipment. The approach ensures data confidentiality and allows perfect recovery even if both a network operator and a relay are simultaneously lost, which is a major concern in 5G communication security.", "conclusion": "The authors have outlined a novel approach for secure, reliable multi-layer secret sharing for COTS 5G devices. This method does away with the traditional overheads of infrastructure change or prior key sharing and achieves data confidentiality while allowing perfect secret recovery under conditions of inconsiderable system component loss."}}
{"id": "2510.13825", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13825", "abs": "https://arxiv.org/abs/2510.13825", "authors": ["Eugene Neelou", "Ivan Novikov", "Max Moroz", "Om Narayan", "Tiffany Saade", "Mika Ayenson", "Ilya Kabanov", "Jen Ozmen", "Edward Lee", "Vineeth Sai Narajala", "Emmanuel Guilherme Junior", "Ken Huang", "Huseyin Gulsin", "Jason Ross", "Marat Vyshegorodtsev", "Adelin Travers", "Idan Habler", "Rahul Jadav"], "title": "A2AS: Agentic AI Runtime Security and Self-Defense", "comment": null, "summary": "The A2AS framework is introduced as a security layer for AI agents and\nLLM-powered applications, similar to how HTTPS secures HTTP. A2AS enforces\ncertified behavior, activates model self-defense, and ensures context window\nintegrity. It defines security boundaries, authenticates prompts, applies\nsecurity rules and custom policies, and controls agentic behavior, enabling a\ndefense-in-depth strategy. The A2AS framework avoids latency overhead, external\ndependencies, architectural changes, model retraining, and operational\ncomplexity. The BASIC security model is introduced as the A2AS foundation: (B)\nBehavior certificates enable behavior enforcement, (A) Authenticated prompts\nenable context window integrity, (S) Security boundaries enable untrusted input\nisolation, (I) In-context defenses enable secure model reasoning, (C) Codified\npolicies enable application-specific rules. This first paper in the series\nintroduces the BASIC security model and the A2AS framework, exploring their\npotential toward establishing the A2AS industry standard.", "AI": {"tldr": "A2AS: HTTPS-like security for AI/LLMs, using the BASIC model to certify behavior, authenticate contexts, and enforce policies, enabling secure, scalable deployment without latency or complexity.", "motivation": "The work addresses the need for a standardized security layer (like HTTPS for HTTP) to protect AI/LLM systems from adversarial attacks, ensure context integrity, and enforce secure behavior while avoiding model retraining and external dependencies.", "method": "A2AS employs the BASIC model (Behavior certificates, Authenticated prompts, Security boundaries, In-context defenses, Codified policies) to enforce certified behavior, authenticate context, isolate risks, and apply application-specific rules without introducing latency, architectural changes, or operational overhead.", "result": "The BASIC model and A2AS framework are formally defined, demonstrating their potential to establish a secure, scalable, and interoperable approach to AI and LLM security, positioning A2AS as a candidate industry standard.", "conclusion": "The paper introduces the A2AS framework and BASIC security model as a foundational step toward establishing an industry standard for securing AI agents and LLM-powered applications, achieving defense-in-depth without compromising efficiency."}}
{"id": "2510.14005", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14005", "abs": "https://arxiv.org/abs/2510.14005", "authors": ["Wei Zou", "Yupei Liu", "Yanting Wang", "Ying Chen", "Neil Gong", "Jinyuan Jia"], "title": "PIShield: Detecting Prompt Injection Attacks via Intrinsic LLM Features", "comment": "The code is available at https://github.com/weizou52/PIShield", "summary": "LLM-integrated applications are vulnerable to prompt injection attacks, where\nan attacker contaminates the input to inject malicious prompts, causing the LLM\nto follow the attacker's intent instead of the original user's. Existing prompt\ninjection detection methods often have sub-optimal performance and/or high\ncomputational overhead. In this work, we propose PIShield, a detection method\nthat is both effective and efficient. Our key observation is that the internal\nrepresentation of the final token in a prompt-extracted from a specific layer\nof the LLM, which we term the injection-critical layer-captures distinguishing\nfeatures between clean and contaminated prompts. Leveraging this insight, we\ntrain a simple linear classifier on these internal representations using a\nlabeled set of clean and contaminated prompts. We compare PIShield against 11\nbaselines across 5 diverse benchmark datasets and 8 prompt injection attacks.\nThe results demonstrate that PIShield is both highly effective and efficient,\nsubstantially outperforming existing methods. Additionally, we show that\nPIShield resists strong adaptive attacks.", "AI": {"tldr": "PIShield detects prompt injections by analyzing LLM internal features from a critical layer, offering a computationally efficient yet powerful solution to secure LLM applications.", "motivation": "Existing prompt injection detection methods suffer from suboptimal accuracy or high computational costs. Applications relying on LLMs require robust yet efficient solutions to prevent security breaches via poisoned inputs.", "method": "Trains a linear classifier on internal representations of the final token from a pre-specified 'injection-critical' layer within LLMs, using labeled clean/contaminated prompt datasets to detect manipulations.", "result": "PIShield outperforms 11 baselines across 5 datasets and 8 attack types, demonstrating high effectiveness, efficiency, and resistance to sophisticated adaptive attacks.", "conclusion": "PIShield effectively addresses the limitations of existing prompt injection detection methods by leveraging internal representations from the injection-critical layer, achieving superior performance and efficiency while resisting adaptive attacks."}}
{"id": "2510.13857", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13857", "abs": "https://arxiv.org/abs/2510.13857", "authors": ["Qiang Xu", "Xiangyu Wen", "Changran Xu", "Zeju Li", "Jianyuan Zhong"], "title": "From Craft to Constitution: A Governance-First Paradigm for Principled Agent Engineering", "comment": null, "summary": "The advent of powerful Large Language Models (LLMs) has ushered in an ``Age\nof the Agent,'' enabling autonomous systems to tackle complex goals. However,\nthe transition from prototype to production is hindered by a pervasive ``crisis\nof craft,'' resulting in agents that are brittle, unpredictable, and ultimately\nuntrustworthy in mission-critical applications. This paper argues this crisis\nstems from a fundamental paradigm mismatch -- attempting to command inherently\nprobabilistic processors with the deterministic mental models of traditional\nsoftware engineering. To solve this crisis, we introduce a governance-first\nparadigm for principled agent engineering, embodied in a formal architecture we\ncall ArbiterOS.", "AI": {"tldr": "This paper addresses the 'crisis of craft' in deploying Large Language Model (LLM)-based agents into mission-critical applications by proposing a governance-first paradigm called ArbiterOS, which bridges the gap between LLMs probabilistic nature and deterministic software engineering approaches.", "motivation": "The transition from LLM-based agents from prototypes to reliable production systems is hindered by brittleness and unpredictability caused by a fundamental paradigm mismatch between probabilistic LLMs and deterministic software engineering mental models.", "method": "The authors introduce a formal architecture called ArbiterOS grounded in a governance-first paradigm, emphasizing principled agent engineering to address the inherent challenges of managing LLMs probabilistic behavior.", "result": "ArbiterOS provides a structured framework to mitigate the crisis of craft by aligning agent development with the probabilistic nature of LLMs, although specific quantitative results are not detailed in the abstract.", "conclusion": "The paper underscores the need to move beyond deterministic software engineering paradigms and adopt governance-driven approaches like ArbiterOS to ensure trustworthy, mission-critical LLM-based agents."}}
{"id": "2510.14066", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.14066", "abs": "https://arxiv.org/abs/2510.14066", "authors": ["Rajendra Upadhyay", "Al Nahian Bin Emran", "Rajendra Paudyal", "Lisa Donnan", "Duminda Wijesekera"], "title": "Quantitative Analysis of UAV Intrusion Mitigation for Border Security in 5G with LEO Backhaul Impairments", "comment": null, "summary": "Uncooperative unmanned aerial vehicles (UAVs) pose emerging threats to\ncritical infrastructure and border protection by operating as rogue user\nequipment (UE) within cellular networks, consuming resources, creating\ninterference, and potentially violating restricted airspaces. This paper\npresents minimal features of the operating space, yet an end-to-end simulation\nframework to analyze detect-to-mitigate latency of such intrusions in a hybrid\nterrestrial-non-terrestrial (LEO satellite) 5G system. The system model\nincludes terrestrial gNBs, satellite backhaul (with stochastic outages), and a\ndetection logic (triggered by handover instability and signal quality\nvariance). A lockdown mechanism is invoked upon detection, with optional local\nfallback to cap mitigation delays. Monte Carlo sweeps across UAV altitudes,\nspeeds, and satellite outage rates yield several insights. First, satellite\nbackhaul outages can cause arbitrarily long mitigation delays, yet, to meet\nfallback deadlines, they need to be effectively bounded. Second, while handover\ninstability was hypothesized, our results show that extra handovers have a\nnegligible effect within the range of parameters we considered. The main\nbenefit of resilience from fallback comes from the delay in limiting\nmitigation. Third, patrol UEs experience negligible collateral impact, with\nhandover rates close to terrestrial baselines. Stress scenarios further\nhighlight that fallback is indispensable in preventing extreme control-plane\nand physical security vulnerabilities: Without fallback, prolonged outages in\nthe satellite backhaul delay lockdown commands, allowing rogue UAVs to linger\ninside restricted corridors for several seconds longer. These results\nunderscore the importance of complementing non-terrestrial links with local\ncontrol to ensure robust and timely response against uncooperative UAV\nintrusions.", "AI": {"tldr": "This paper analyzes the detection-to-mitigation latency in a hybrid terrestrial-LEO satellite 5G system for rogue UAVs by modeling a simulation framework with satellite backhaul outages and fallback mechanisms.", "motivation": "Uncooperative UAVs exploit cellular networks to disrupt infrastructure and border security. Hybrid 5G systems with satellite backhaul introduce risks due to stochastic outages and mitigation delays, necessitating strategies to ensure timely responses.", "method": "An end-to-end simulation framework models terrestrial gNBs, LEO satellite backhaul with stochastic outages, and a detection-mitigation pipeline. Detection logic uses handover instability and signal variance; mitigation includes a lockdown with optional local fallback. Monte Carlo simulations evaluate UAV altitudes, speeds, and outage rates.", "result": "Satellite outages cause long delays unless bounded by fallbacks; handover instability has negligible impact. Fallback mechanisms reduce mitigation delays, allowing patrol UEs to maintain terrestrial-like handover rates. Without fallback, outages prolong UAV presence in restricted areas, amplifying security risks.", "conclusion": "Local fallback mechanisms are critical for robust mitigation of rogue UAVs in hybrid terrestrial-terrestrial 5G systems. Complementing non-terrestrial links with local control ensures timely responses to uncooperative UAV intrusions."}}
{"id": "2510.13859", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13859", "abs": "https://arxiv.org/abs/2510.13859", "authors": ["Ruchit Rawal", "Jeffrey Yang Fan Chiang", "Chihao Shen", "Jeffery Siyuan Tian", "Aastha Mahajan", "Tom Goldstein", "Yizheng Chen"], "title": "Benchmarking Correctness and Security in Multi-Turn Code Generation", "comment": null, "summary": "AI coding assistants powered by large language models (LLMs) have transformed\nsoftware development, significantly boosting productivity. While existing\nbenchmarks evaluate the correctness and security of LLM-generated code, they\nare typically limited to single-turn tasks that do not reflect the iterative\nnature of real-world development. We introduce MT-Sec, the first benchmark to\nsystematically evaluate both correctness and security in multi-turn coding\nscenarios. We construct this using a synthetic data pipeline that transforms\nexisting single-turn tasks into semantically aligned multi-turn interaction\nsequences, allowing reuse of original test suites while modeling the complexity\nof real-world coding processes. We evaluate 32 open- and closed-source models,\nand three agent-scaffolding on MT-Sec and observe a consistent 20-27% drop in\n\"correct and secure\" outputs from single-turn to multi-turn settings -- even\namong state-of-the-art models. Beyond full-program generation, we also evaluate\nmodels on multi-turn code-diff generation -- an unexplored yet practically\nrelevant setting -- and find that models perform worse here, with increased\nrates of functionally incorrect and insecure outputs. Finally, we find that\nwhile agent scaffoldings boost single-turn code generation performance, they\nare not quite as effective in multi-turn evaluations. Together, these findings\nhighlight the need for benchmarks that jointly evaluate correctness and\nsecurity in multi-turn, real-world coding workflows.", "AI": {"tldr": "The paper introduces MT-Sec, a new benchmark for evaluating correctness and security in multi-turn coding scenarios, revealing a significant drop in model performance compared to single-turn settings across 32 models and different agent scaffolds.", "motivation": "Existing benchmarks for LLM-generated code focus on correctness and security in single-turn tasks but do not account for the iterative nature of real development.", "method": "MT-Sec is built using a synthetic data pipeline to transform single-turn tasks into multi-turn sequences. It evaluates 32 open- and closed-source models with three agent scaffolding configurations across both full-program and code-diff generations.", "result": "Models show a 20-27% decline in correct and secure outputs in multi-turn settings, especially in code-diff generation, with higher rates of errors. Agent scaffolding helps in single-turn tasks but less so in multi-turn ones.", "conclusion": "Multi-turn coding evaluation shows critical performance gaps in LLMs, emphasizing the need for benchmarking real-world workflows involving correctness and security."}}
{"id": "2510.14086", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14086", "abs": "https://arxiv.org/abs/2510.14086", "authors": ["Matthew Finlayson", "Xiang Ren", "Swabha Swayamdipta"], "title": "Every Language Model Has a Forgery-Resistant Signature", "comment": null, "summary": "The ubiquity of closed-weight language models with public-facing APIs has\ngenerated interest in forensic methods, both for extracting hidden model\ndetails (e.g., parameters) and for identifying models by their outputs. One\nsuccessful approach to these goals has been to exploit the geometric\nconstraints imposed by the language model architecture and parameters. In this\nwork, we show that a lesser-known geometric constraint--namely, that language\nmodel outputs lie on the surface of a high-dimensional ellipse--functions as a\nsignature for the model and can be used to identify the source model of a given\noutput. This ellipse signature has unique properties that distinguish it from\nexisting model-output association methods like language model fingerprints. In\nparticular, the signature is hard to forge: without direct access to model\nparameters, it is practically infeasible to produce log-probabilities\n(logprobs) on the ellipse. Secondly, the signature is naturally occurring,\nsince all language models have these elliptical constraints. Thirdly, the\nsignature is self-contained, in that it is detectable without access to the\nmodel inputs or the full weights. Finally, the signature is compact and\nredundant, as it is independently detectable in each logprob output from the\nmodel. We evaluate a novel technique for extracting the ellipse from small\nmodels and discuss the practical hurdles that make it infeasible for\nproduction-scale models. Finally, we use ellipse signatures to propose a\nprotocol for language model output verification, analogous to cryptographic\nsymmetric-key message authentication systems.", "AI": {"tldr": "This paper introduces a novel forensic method for identifying language models via their geometric 'ellipse signature' output pattern, offering advantages over existing techniques.", "motivation": "Current model-output analysis methods lack robustness and security; the proposed ellipse signature addresses these gaps with natural, unforgeable properties.", "method": "Explores high-dimensional elliptical constraints inherently produced by all language models, develops extraction techniques for small models, and proposes a cryptographic-analogous verification protocol.", "result": "Demonstrates ellipse signature extraction feasibility on small models, highlights practical challenges for production-scale models, and validates the signature's unique properties.", "conclusion": "Ellipse signatures provide a secure, natural model identification mechanism with potential for output verification systems, though scalability challenges require further research."}}
{"id": "2510.13914", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.13914", "abs": "https://arxiv.org/abs/2510.13914", "authors": ["Janghan Yoon", "Jaegwan Cho", "Junhyeok Kim", "Jiwan Chung", "Jaehyun Jeon", "Youngjae Yu"], "title": "A11YN: aligning LLMs for accessible web UI code generation", "comment": null, "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nin generating functional and aesthetic web interfaces directly from\ninstructions. However, these models often replicate accessibility flaws from\ntheir training data, resulting in interfaces that exclude users with diverse\nneeds and contexts. To address this gap, we introduce A11yn, the first method\nthat aligns code-generating LLMs to reliably produce accessibility-compliant\nweb UIs. A11yn optimizes a novel reward function that penalizes violations of\nthe Web Content Accessibility Guidelines (WCAG), with penalties scaled to the\nseverity of each violation as identified by an accessibility testing engine. To\nsupport training, we construct UIReq-6.8K, a dataset of 6,800 diverse\ninstructions for web UI generation. For evaluation, we introduce RealUIReq-300,\na benchmark of 300 real-world web UI requests grounded and manually curated\nfrom public web pages, spanning a broad range of use cases. Empirical results\nshow that A11yn significantly outperforms strong baselines, lowering the\nInaccessibility Rate by 60% over the base model while preserving semantic\nfidelity and visual quality of generated UIs. These findings demonstrate that\naccessibility can be systematically optimized within LLMs, showing the\nfeasibility of aligning code generation for accessibility.", "AI": {"tldr": "Researchers develop A11yn, the first method to align LLMs with accessibility standards, achieving a 60% reduction in accessibility flaws using a WCAG violation penalty reward function and specialized datasets.", "motivation": "Current LLMs replicate accessibility flaws from training data, excluding users with diverse needs. There is a pressing need to align code generation with accessibility standards.", "method": "A11yn optimizes a reward function penalizing WCAG violations based on severity, supported by two datasets (UIReq-6.8K for training and RealUIReq-300 as a benchmark) for training and evaluation.", "result": "A11yn reduces the Inaccessibility Rate by 60% compared to baseline models while maintaining UI semantic fidelity and visual quality.", "conclusion": "The study demonstrates that accessibility in code-generating LLMs can be systematically optimized through structured alignment with accessibility guidelines, achieving significant improvements without compromising UI quality."}}
{"id": "2510.14171", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.14171", "abs": "https://arxiv.org/abs/2510.14171", "authors": ["Jack Vanlyssel"], "title": "Power Grid Cybersecurity: Policy Analysis White Paper", "comment": null, "summary": "The U.S. power grid underpins national security, public safety, and economic\nstability, but faces growing cyber risks from vulnerabilities in industrial\ncontrol systems, remote access, and poor cyber hygiene. Despite its critical\nimportance, current policy remains fragmented and reactive. This paper proposes\na dual policy approach to strengthen grid cybersecurity: enhanced information\nsharing between government and private utilities to improve threat detection\nand response, and standardized cyber hygiene practices to reduce common attack\nvectors. For long-term resilience, a Unified National Cybersecurity Framework\nis recommended to align existing NERC, IEC, IEEE, and NIST standards, eliminate\nregulatory overlap, and adapt to evolving threats. Together, these policies\noffer both immediate and sustainable improvements in safeguarding the nation's\nmost vital infrastructure.", "AI": {"tldr": "This paper proposes a dual-policy strategy with information sharing, standardized cyber hygiene, and a unified framework to strengthen U.S. power grid cybersecurity and address fragmented, reactive policies.", "motivation": "The U.S. power grid faces growing cyber risks from industrial control vulnerabilities, remote access weaknesses, and poor cyber hygiene. Current policies are fragmented and reactive, failing to ensure adequate protection for critical infrastructure supporting national security and economic stability.", "method": "The paper recommends (1) enhanced government-utility information sharing for threat detection/response and (2) standardized cyber hygiene practices to reduce attack vectors, paired with a (3) Unified National Cybersecurity Framework to harmonize standards like NERC, IEC, IEEE, and NIST.", "result": "The proposed policies align standards, eliminate regulatory overlap, improve threat detection, and create long-term resilience by adapting to evolving threats through unified frameworks.", "conclusion": "The dual policy approach and Unified National Cybersecurity Framework proposed in the paper provide both immediate and sustainable improvements for safeguarding the U.S. power grid against evolving cyber threats."}}
{"id": "2510.13992", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13992", "abs": "https://arxiv.org/abs/2510.13992", "authors": ["Quoc Hung Le", "Thanh Le-Cong", "Bach Le", "Bowen Xu"], "title": "Signature in Code Backdoor Detection, how far are we?", "comment": "20 pages, 3 figures", "summary": "As Large Language Models (LLMs) become increasingly integrated into software\ndevelopment workflows, they also become prime targets for adversarial attacks.\nAmong these, backdoor attacks are a significant threat, allowing attackers to\nmanipulate model outputs through hidden triggers embedded in training data.\nDetecting such backdoors remains a challenge, and one promising approach is the\nuse of Spectral Signature defense methods that identify poisoned data by\nanalyzing feature representations through eigenvectors. While some prior works\nhave explored Spectral Signatures for backdoor detection in neural networks,\nrecent studies suggest that these methods may not be optimally effective for\ncode models. In this paper, we revisit the applicability of Spectral\nSignature-based defenses in the context of backdoor attacks on code models. We\nsystematically evaluate their effectiveness under various attack scenarios and\ndefense configurations, analyzing their strengths and limitations. We found\nthat the widely used setting of Spectral Signature in code backdoor detection\nis often suboptimal. Hence, we explored the impact of different settings of the\nkey factors. We discovered a new proxy metric that can more accurately estimate\nthe actual performance of Spectral Signature without model retraining after the\ndefense.", "AI": {"tldr": "This paper reevaluates Spectral Signatures for code model backdoor detection, identifies suboptimal practices, and proposes a new metric to assess defense effectiveness without model retraining.", "motivation": "Recent studies indicate Spectral Signatures are inadequately effective for code models, necessitating revisiting their applicability in backdoor attack scenarios where poisoned data manipulation is a critical threat.", "method": "The authors systematically evaluated Spectral Signature-based defenses under diverse attack scenarios and configurations, analyzing their effectiveness through key factor settings.", "result": "Identified suboptimal usage of Spectral Signatures in code backdoor detection, discovered a new proxy metric for accurate performance estimation without requiring post-defense model retraining.", "conclusion": "The paper concludes that existing Spectral Signature settings are often suboptimal for code model backdoor detection, but introduces a novel proxy metric to estimate defense performance without model retraining."}}
{"id": "2510.14185", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.14185", "abs": "https://arxiv.org/abs/2510.14185", "authors": ["Jack Vanlyssel"], "title": "Securing U.S. Critical Infrastructure: Lessons from Stuxnet and the Ukraine Power Grid Attacks", "comment": null, "summary": "Industrial Control Systems (ICS) underpin the United States' critical\ninfrastructure, managing essential services such as power, water, and\ntransportation that are vital to national security and public safety. However,\nincreasing digital integration has exposed these systems to escalating cyber\nthreats. Historical attacks like Stuxnet and the Ukraine power grid incident\nrevealed exploitable weaknesses-poor network segmentation, outdated software,\nweak authentication, and inadequate monitoring-that persist in many U.S. ICS\nenvironments today. This paper analyzes these landmark attacks to identify\nrecurring vulnerabilities and assess their relevance to current U.S.\ninfrastructure. It argues that without immediate reforms, similar exploits\ncould lead to catastrophic disruptions and national security crises. To address\nthese risks, the paper proposes policy measures focused on implementing\nzero-trust architecture and improved network segmentation to enhance system\nresilience. These recommendations aim to guide policymakers and industry\nleaders in securing the nation's most critical operational technologies against\nfuture cyber threats.", "AI": {"tldr": "This paper examines historical ICS cyber attacks to highlight ongoing vulnerabilities in U.S. critical infrastructure and recommends zero-trust policies and network segmentation to improve security.", "motivation": "The motivation is to address the persistent cyber vulnerabilities in U.S. Industrial Control Systems, which are crucial for national security but remain exposed due to outdated practices and integration of digital technologies.", "method": "The paper analyzes landmark cyber attacks (e.g., Stuxnet, Ukraine power grid) to identify recurring vulnerabilities and assess their relevance to current ICS environments.", "result": "The analysis reveals that issues such as poor network segmentation, outdated software, weak authentication, and inadequate monitoring are still present in many U.S. ICS systems, making them susceptible to similar historical attacks.", "conclusion": "The paper concludes that without immediate policy reforms, including the adoption of zero-trust architecture and better network segmentation, the U.S. risks facing similar catastrophic cyber incidents, and urges swift action to secure critical operational infrastructures."}}
{"id": "2510.14036", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14036", "abs": "https://arxiv.org/abs/2510.14036", "authors": ["Qiushi Wu", "Yue Xiao", "Dhilung Kirat", "Kevin Eykholt", "Jiyong Jang", "Douglas Lee Schales"], "title": "One Bug, Hundreds Behind: LLMs for Large-Scale Bug Discovery", "comment": null, "summary": "Fixing bugs in large programs is a challenging task that demands substantial\ntime and effort. Once a bug is found, it is reported to the project\nmaintainers, who work with the reporter to fix it and eventually close the\nissue. However, across the program, there are often similar code segments,\nwhich may also contain the bug, but were missed during discovery. Finding and\nfixing each recurring bug instance individually is labor intensive. Even more\nconcerning, bug reports can inadvertently widen the attack surface as they\nprovide attackers with an exploitable pattern that may be unresolved in other\nparts of the program.\n  In this paper, we explore these Recurring Pattern Bugs (RPBs) that appear\nrepeatedly across various code segments of a program or even in different\nprograms, stemming from a same root cause, but are unresolved. Our\ninvestigation reveals that RPBs are widespread and can significantly compromise\nthe security of software programs. This paper introduces BugStone, a program\nanalysis system empowered by LLVM and a Large Language Model (LLM). The key\nobservation is that many RPBs have one patched instance, which can be leveraged\nto identify a consistent error pattern, such as a specific API misuse. By\nexamining the entire program for this pattern, it is possible to identify\nsimilar sections of code that may be vulnerable. Starting with 135 unique RPBs,\nBugStone identified more than 22K new potential issues in the Linux kernel.\nManual analysis of 400 of these findings confirmed that 246 were valid. We also\ncreated a dataset from over 1.9K security bugs reported by 23 recent top-tier\nconference works. We manually annotate the dataset, identify 80 recurring\npatterns and 850 corresponding fixes. Even with a cost-efficient model choice,\nBugStone achieved 92.2% precision and 79.1% pairwise accuracy on the dataset.", "AI": {"tldr": "This paper explores Recurring Pattern Bugs (RPBs) in large programs, proposing BugStone\u2014an LLVM and LLM-powered system\u2014to automatically detect and address them. Large-scale evaluation shows it identifies 22K+ potential issues in the Linux kernel with 92.2% precision, highlighting a significant vulnerability pattern in software security.", "motivation": "Manual bug fixing is error-prone and time-consuming. Unfixed RPBs widen attack surfaces as attackers exploit recurring patterns. Existing practices miss similar bugs in other code segments, compromising software security at scale.", "method": "BugStone analyzes code for patterns from patched RPB instances (e.g., API misuse). It uses LLVM for code analysis and an LLM to refine patterns. The system scans entire codebases, identifies similar vulnerabilities, and validates findings via manual analysis of 1.9K security bug datasets and 22K+ Linux kernel issues.", "result": "BugStone found 246 valid issues from 400 analyzed in Linux kernel. It achieved 92.2% precision and 79.1% pairwise accuracy on a manually annotated dataset of 1,900 bugs. The system identified 80 recurring patterns and 850 fixes across top-tier security research.", "conclusion": "BugStone demonstrates automated, cost-effective detection of RPBs, significantly improving software security. Its approach reduces manual labor and mitigates risks from exploitable patterns, with the annotated dataset enabling future research on recurring vulnerabilities."}}
{"id": "2510.14198", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.14198", "abs": "https://arxiv.org/abs/2510.14198", "authors": ["Morium Akter Munny", "Mahbub Alam", "Sonjoy Kumar Paul", "Daniel Timko", "Muhammad Lutfor Rahman", "Nitesh Saxena"], "title": "Infrastructure Patterns in Toll Scam Domains: A Comprehensive Analysis of Cybercriminal Registration and Hosting Strategies", "comment": "This paper has been accepted for presentation at eCrime 2025", "summary": "Toll scams involve criminals registering fake domains that pretend to be\nlegitimate transportation agencies to trick users into making fraudulent\npayments. Although these scams are rapidly increasing and causing significant\nharm, they have not been extensively studied. We present the first large-scale\nanalysis of toll scam domains, using a newly created dataset of 67,907\nconfirmed scam domains mostly registered in 2025. Our study reveals that\nattackers exploit permissive registrars and less common top-level domains, with\n86.9% of domains concentrated in just five non-mainstream TLDs and 72.9%\nregistered via a single provider. We also discover specific registration\npatterns, including short bursts of activity that suggest automated,\ncoordinated attacks, with over half of domains registered in the first quarter\nof 2025. This extreme temporal clustering reflects highly synchronized campaign\nlaunches. Additionally, we build a simple predictive model using only domain\nregistration data to predict which scam domains are likely to be suspended -- a\nproxy for confirmed abuse -- achieving 80.4% accuracy, and 92.3% sensitivity.\nOur analysis reveals attacker strategies for evading detection -- such as\nexploiting obscure TLDs, permissive registrars, and coordinated registration\nbursts -- which can inform more targeted interventions by registrars, hosting\nproviders, and security platforms. However, our results suggest that\nregistration metadata alone may be insufficient, and incorporating features\nfrom domain URLs and webpage content could further improve detection.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.14115", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14115", "abs": "https://arxiv.org/abs/2510.14115", "authors": ["Philipp Bauerfeind", "Amir Salarpour", "David Fernandez", "Pedram MohajerAnsari", "Johannes Reschke", "Mert D. Pes\u00e9"], "title": "David vs. Goliath: A comparative study of different-sized LLMs for code generation in the domain of automotive scenario generation", "comment": null, "summary": "Scenario simulation is central to testing autonomous driving systems. Scenic,\na domain-specific language (DSL) for CARLA, enables precise and reproducible\nscenarios, but NL-to-Scenic generation with large language models (LLMs)\nsuffers from scarce data, limited reproducibility, and inconsistent metrics. We\nintroduce NL2Scenic, an open dataset and framework with 146 NL/Scenic pairs, a\ndifficulty-stratified 30-case test split, an Example Retriever, and 14\nprompting variants (ZS, FS, CoT, SP, MoT). We evaluate 13 models: four\nproprietary (GPT-4o, GPT-5, Claude-Sonnet-4, Gemini-2.5-pro) and nine\nopen-source code models (Qwen2.5Coder 0.5B-32B; CodeLlama 7B/13B/34B), using\ntext metrics (BLEU, ChrF, EDIT-SIM, CrystalBLEU) and execution metrics\n(compilation and generation), and compare them with an expert study (n=11).\nEDIT-SIM correlates best with human judgments; we also propose EDIT-COMP (F1 of\nEDIT-SIM and compilation) as a robust dataset-level proxy that improves ranking\nfidelity. GPT-4o performs best overall, while Qwen2.5Coder-14B reaches about 88\npercent of its expert score on local hardware. Retrieval-augmented prompting,\nFew-Shot with Example Retriever (FSER), consistently boosts smaller models, and\nscaling shows diminishing returns beyond mid-size, with Qwen2.5Coder\noutperforming CodeLlama at comparable scales. NL2Scenic and EDIT-COMP offer a\nstandardized, reproducible basis for evaluating Scenic code generation and\nindicate that mid-size open-source models are practical, cost-effective options\nfor autonomous-driving scenario programming.", "AI": {"tldr": "NL2Scenic introduces a standardized framework for NL-to-Scenic code generation in autonomous driving, showing mid-size open-source models like Qwen2.5Coder are practical, cost-effective alternatives to larger models.", "motivation": "NL-to-Scenic generation with LLMs suffers from scarce data, limited reproducibility, and inconsistent metrics, requiring standardized evaluation for autonomous driving scenario simulations.", "method": "Introduced NL2Scenic, an open dataset and framework with 146 NL/Scenic pairs, difficulty-stratified test split, Example Retriever, and 14 prompting variants (ZS, FS, CoT, SP, MoT). Evaluated 13 models using text and execution metrics, plus expert judgment (n=11). Proposed EDIT-COMP (combined F1 of EDIT-SIM and compilation) as a robust proxy.", "result": "GPT-4o performs best; Qwen2.5Coder-14B reaches 88% of expert scores on local hardware. Retrieval-augmented prompting (FSER) improves smaller models. Open-source models outperform CodeLlama at comparable scales. EDIT-SIM correlates best with human judgment, while EDIT-COMP improves ranking fidelity.", "conclusion": "NL2Scenic and EDIT-COMP provide a reproducible basis for evaluating Scenic code generation, highlighting mid-size open-source models as practical, cost-effective options for autonomous-driving scenario programming."}}
{"id": "2510.14218", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.14218", "abs": "https://arxiv.org/abs/2510.14218", "authors": ["Chaoyue Huang", "Gejian Zhao", "Hanzhou Wu", "Zhihua Xia", "Asad Malik"], "title": "An Information Asymmetry Game for Trigger-based DNN Model Watermarking", "comment": null, "summary": "As a valuable digital product, deep neural networks (DNNs) face increasingly\nsevere threats to the intellectual property, making it necessary to develop\neffective technical measures to protect them. Trigger-based watermarking\nmethods achieve copyright protection by embedding triggers into the host DNNs.\nHowever, the attacker may remove the watermark by pruning or fine-tuning. We\nmodel this interaction as a game under conditions of information asymmetry,\nnamely, the defender embeds a secret watermark with private knowledge, while\nthe attacker can only access the watermarked model and seek removal. We define\nstrategies, costs, and utilities for both players, derive the attacker's\noptimal pruning budget, and establish an exponential lower bound on the\naccuracy of watermark detection after attack. Experimental results demonstrate\nthe feasibility of the watermarked model, and indicate that sparse watermarking\ncan resist removal with negligible accuracy loss. This study highlights the\neffectiveness of game-theoretic analysis in guiding the design of robust\nwatermarking schemes for model copyright protection.", "AI": {"tldr": "This paper explores robust watermarking for deep neural networks under information asymmetry and provides a game-theoretic analysis to design schemes that can resist watermarks removal via pruning or fine-tuning.", "motivation": "The motivation lies in the urgent need for effective technical measures to protect the intellectual property of deep neural networks, given the increasing threats posed by attacks.", "method": "This paper models the watermarking and removal process as a game with an information asymmetry. It then defines the strategies, costs and utilities of both defender and attacker, derives the attacker's optimal pruning budget, and establishes bounds on watermark detection accuracy after attack.", "result": "Experiments show the feasibility of the watermarked model and demonstrate that sparse watermarking can resist removal attempts with minimal impact on model accuracy.", "conclusion": "The study emphasizes the effectiveness of game-theoretic approaches in guiding the development of robust watermarking for model copyright protection."}}
{"id": "2510.14279", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.14279", "abs": "https://arxiv.org/abs/2510.14279", "authors": ["Evangelos Lamprou", "Seong-Heon Jung", "Mayank Keoliya", "Lukas Lazarek", "Konstantinos Kallas", "Michael Greenberg", "Nikos Vasilakis"], "title": "Caruca: Effective and Efficient Specification Mining for Opaque Software Components", "comment": null, "summary": "A wealth of state-of-the-art systems demonstrate impressive improvements in\nperformance, security, and reliability on programs composed of opaque\ncomponents, such as Unix shell commands. To reason about commands, these\nsystems require partial specifications. However, creating such specifications\nis a manual, laborious, and error-prone process, limiting the practicality of\nthese systems. This paper presents Caruca, a system for automatic specification\nmining for opaque commands. To overcome the challenge of language diversity\nacross commands, Caruca first instruments a large language model to translate a\ncommand's user-facing documentation into a structured invocation syntax. Using\nthis representation, Caruca explores the space of syntactically valid command\ninvocations and execution environments. Caruca concretely executes each\ncommand-environment pair, interposing at the system-call and filesystem level\nto extract key command properties such as parallelizability and filesystem pre-\nand post-conditions. These properties can be exported in multiple specification\nformats and are immediately usable by existing systems. Applying Caruca across\n60 GNU Coreutils, POSIX, and third-party commands across several\nspecification-dependent systems shows that Caruca generates correct\nspecifications for all but one case, completely eliminating manual effort from\nthe process and currently powering the full specifications for a\nstate-of-the-art static analysis tool.", "AI": {"tldr": "Caruca automates specification mining for shell commands by translating docs into structured syntax, executing valid invocations, and extracting properties. It eliminates manual effort and works correctly for 98% of tested commands.", "motivation": "Manual creation of partial specifications for opaque commands is laborious, error-prone, and limits the practicality of specification-dependent systems. Automating this process improves scalability and reliability.", "method": "Caruca uses a large language model to translate command documentation into structured syntax, explores valid command invocations and environments, concretely executes them, and extracts properties via system-call and filesystem interposition to generate specifications.", "result": "Caruca successfully generated correct specifications for 59 out of 60 tested commands (GNU Coreutils, POSIX, third-party), fully automating the process and enabling immediate integration with existing tools like a state-of-the-art static analysis system.", "conclusion": "Caruca effectively automates specification mining for opaque commands, making specification-dependent systems more practical by eliminating manual effort and demonstrating correctness across most tested cases."}}
{"id": "2510.14233", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.14233", "abs": "https://arxiv.org/abs/2510.14233", "authors": ["Fanchao Meng", "Jiaping Gui", "Yunbo Li", "Yue Wu"], "title": "RHINO: Guided Reasoning for Mapping Network Logs to Adversarial Tactics and Techniques with Large Language Models", "comment": null, "summary": "Modern Network Intrusion Detection Systems generate vast volumes of low-level\nalerts, yet these outputs remain semantically fragmented, requiring\nlabor-intensive manual correlation with high-level adversarial behaviors.\nExisting solutions for automating this mapping-rule-based systems and machine\nlearning classifiers-suffer from critical limitations: rule-based approaches\nfail to adapt to novel attack variations, while machine learning methods lack\ncontextual awareness and treat tactic-technique mapping as a syntactic matching\nproblem rather than a reasoning task. Although Large Language Models have shown\npromise in cybersecurity tasks, preliminary experiments reveal that existing\nLLM-based methods frequently hallucinate technique names or produce\ndecontextualized mappings due to their single-step classification approach.\n  To address these challenges, we introduce RHINO, a novel framework that\ndecomposes LLM-based attack analysis into three interpretable phases mirroring\nhuman reasoning: (1) behavioral abstraction, where raw logs are translated into\ncontextualized narratives; (2) multi-role collaborative inference, generating\ncandidate techniques by evaluating behavioral evidence against MITRE ATT&CK\nknowledge; and (3) validation, cross-referencing predictions with official\nMITRE definitions to rectify hallucinations. RHINO bridges the semantic gap\nbetween low-level observations and adversarial intent while improving output\nreliability through structured reasoning.\n  We evaluate RHINO on three benchmarks across four backbone models. RHINO\nachieved high accuracy, with model performance ranging from 86.38% to 88.45%,\nresulting in relative gains from 24.25% to 76.50% across different models. Our\nresults demonstrate that RHINO significantly enhances the interpretability and\nscalability of threat analysis, offering a blueprint for deploying LLMs in\noperational security settings.", "AI": {"tldr": "RHINO is a framework enhancing LLM-based network intrusion analysis by decomposing attack detection into interpretable phases (behavioral abstraction, collaborative inference, validation), reducing hallucinations and improving accuracy (86.38%-88.45%) through structured reasoning aligned with MITRE ATT&CK.", "motivation": "Current intrusion detection systems generate semantically fragmented alerts requiring manual correlation. Rule-based methods lack adaptability, ML methods lack contextual reasoning, and LLMs hallucinate technique names due to single-step classification.", "method": "RHINO's three-phase approach: (1)application of LLM to transform raw logs into contextual narratives, (2)multiperspective technique generation using MITRE ATT&CK knowledge evaluation, and (3)predicate validation against official MITRE definitions to correct errors.", "result": "Achieved 86.38%-88.45\n% accuracy across four models with 24.25%-76.50\n% relative performance gains on three benchmarks, demonstrating improved reliability and MITRE alignment compared to existing methods.", "conclusion": "RHINO bridges the semantic gap between low-level observations and adversarial intent, establishing a scalable, interpretable framework for operationalizing LLMs in threat analysis while maintaining fidelity to established cybersecurity knowledge bases."}}
{"id": "2510.14292", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.14292", "abs": "https://arxiv.org/abs/2510.14292", "authors": ["Haolin Pan", "Hongbin Zhang", "Mingjie Xing", "Yanjun Wu"], "title": "A Hybrid, Knowledge-Guided Evolutionary Framework for Personalized Compiler Auto-Tuning", "comment": null, "summary": "Compiler pass auto-tuning is critical for enhancing software performance, yet\nfinding the optimal pass sequence for a specific program is an NP-hard problem.\nTraditional, general-purpose optimization flags like -O3 and -Oz adopt a\none-size-fits-all approach, often failing to unlock a program's full\nperformance potential. To address this challenge, we propose a novel Hybrid,\nKnowledge-Guided Evolutionary Framework. This framework intelligently guides\nonline, personalized optimization using knowledge extracted from a large-scale\noffline analysis phase. During the offline stage, we construct a comprehensive\ncompilation knowledge base composed of four key components: (1) Pass Behavioral\nVectors to quantitatively capture the effectiveness of each optimization; (2)\nPass Groups derived from clustering these vectors based on behavior similarity;\n(3) a Synergy Pass Graph to model beneficial sequential interactions; and (4) a\nlibrary of Prototype Pass Sequences evolved for distinct program types. In the\nonline stage, a bespoke genetic algorithm leverages this rich knowledge base\nthrough specially designed, knowledge-infused genetic operators. These\noperators transform the search by performing semantically-aware recombination\nand targeted, restorative mutations. On a suite of seven public datasets, our\nframework achieves an average of 11.0% additional LLVM IR instruction reduction\nover the highly-optimized opt -Oz baseline, demonstrating its state-of-the-art\ncapability in discovering personalized, high-performance optimization\nsequences.", "AI": {"tldr": "A Hybrid, Knowledge-Guided Evolutionary Framework improves compiler pass auto-tuning by combining offline knowledge extraction and online genetic algorithms, achieving 11.0% better LLVM IR instruction reduction than -Oz.", "motivation": "Traditional one-size-fits-all compiler optimizations (like -O3/-Oz) fail to unlock programs' full performance potential due to the NP-hard nature of finding optimal pass sequences.", "method": "The framework uses offline analysis to create a compilation knowledge base with Pass Behavioral Vectors, Pass Groups, Synergy Pass Graphs, and Prototype Sequences, then applies knowledge-infused genetic operators in an online stage for personalized optimization.", "result": "Achieved 11.0% average additional LLVM IR instruction reduction over the optimized -Oz baseline across seven datasets.", "conclusion": "The hybrid approach demonstrates state-of-the-art performance in discovering personalized, high-efficiency compiler optimization sequences."}}
{"id": "2510.14283", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14283", "abs": "https://arxiv.org/abs/2510.14283", "authors": ["Xinhao Deng", "Jingyou Chen", "Linxiao Yu", "Yixiang Zhang", "Zhongyi Gu", "Changhao Qiu", "Xiyuan Zhao", "Ke Xu", "Qi Li"], "title": "Beyond a Single Perspective: Towards a Realistic Evaluation of Website Fingerprinting Attacks", "comment": null, "summary": "Website Fingerprinting (WF) attacks exploit patterns in encrypted traffic to\ninfer the websites visited by users, posing a serious threat to anonymous\ncommunication systems. Although recent WF techniques achieve over 90% accuracy\nin controlled experimental settings, most studies remain confined to single\nscenarios, overlooking the complexity of real-world environments. This paper\npresents the first systematic and comprehensive evaluation of existing WF\nattacks under diverse realistic conditions, including defense mechanisms,\ntraffic drift, multi-tab browsing, early-stage detection, open-world settings,\nand few-shot scenarios. Experimental results show that many WF techniques with\nstrong performance in isolated settings degrade significantly when facing other\nconditions. Since real-world environments often combine multiple challenges,\ncurrent WF attacks are difficult to apply directly in practice. This study\nhighlights the limitations of WF attacks and introduces a multidimensional\nevaluation framework, offering critical insights for developing more robust and\npractical WF attacks.", "AI": {"tldr": "This paper evaluates Website Fingerprinting (WF) attacks across multiple realistic scenarios, showing that their performance drops in real-world conditions and introduces a multidimensional evaluation framework.", "motivation": "Existing WF attacks are highly accurate in controlled environments but lack practical evaluation in realistic, complex conditions. The paper aims to bridge this gap by testing WF techniques under various real-world factors like defense mechanisms, traffic drift, and multi-tab browsing.", "method": "The authors conducted a systematic evaluation of WF attacks under diverse realistic conditions, including defense mechanisms, traffic drift, multi-tab browsing, early-stage detection, open-world settings, and few-shot scenarios. They compared the performance of existing techniques across these scenarios.", "result": "The experimental results demonstrate that many high-performing WF techniques in isolated settings experience significant degradation when exposed to multiple real-world challenges. The study found that current attacks are inadequate in environments where different factors coexist.", "conclusion": "The paper concludes that existing WF attacks are limited in practical deployment due to their performance under combined real-world conditions. It introduces a multidimensional evaluation framework to better address these challenges and improve the robustness of future WF attacks."}}
{"id": "2510.14339", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.14339", "abs": "https://arxiv.org/abs/2510.14339", "authors": ["Jialu Zhang", "Jialiang Gu", "Wangmeiyu Zhang", "Jos\u00e9 Pablo Cambronero", "John Kolesar", "Ruzica Piskac", "Daming Li", "Hanyuan Shi"], "title": "A Systematic Study of Time Limit Exceeded Errors in Online Programming Assignments", "comment": null, "summary": "Online programming platforms such as Codeforces and LeetCode attract millions\nof users seeking to learn to program or refine their skills for industry\ninterviews. A major challenge for these users is the Time Limit Exceeded (TLE)\nerror, triggered when a program exceeds the execution time bound. Although\ndesigned as a performance safeguard, TLE errors are difficult to resolve: error\nmessages provide no diagnostic insight, platform support is minimal, and\nexisting debugging tools offer little help. As a result, many users abandon\ntheir submissions after repeated TLE failures.\n  This paper presents the first large-scale empirical study of TLE errors in\nonline programming. We manually analyzed 1000 Codeforces submissions with TLE\nerrors, classified their root causes, and traced how users attempted to fix\nthem. Our analysis shows that TLE errors often arise not only from inefficient\nalgorithms but also from infinite loops, improper data structure use, and\ninefficient I/O, challenging the conventional view that TLEs are purely\nperformance issues.\n  Guided by these findings, we introduce Nettle, the first automated repair\ntool specifically designed for TLE errors, and Nettle-Eval, the first framework\nfor evaluating TLE repairs. Integrating LLMs with targeted automated feedback\ngenerated by the compiler and test cases, Nettle produces small, correct code\nedits that eliminate TLEs while preserving functionality. Evaluated on the same\n1000 real-world cases, Nettle achieves a 98.5% fix rate, far exceeding the\nstrongest LLM baseline, and all of its repairs pass both Nettle-Eval and the\nplatform's official checker, confirming the reliability of our framework.", "AI": {"tldr": "Through a 1000-submission analysis of TLE errors, researchers identify algorithm inefficiency, infinite loops, and improper I/O as root causes. Their Nettle system combines LLMs with test feedback to automatically fix TLE errors at 98.5% success rate, validated by both custom and platform evaluators.", "motivation": "TLE errors in online programming platforms lack diagnostic support, forcing users to abandon submissions after repeated failures. Existing tools provide minimal assistance, necessitating automated solutions for reliable TLE error resolution.", "method": "The authors conducted a manual analysis of 1000 Codeforces submissions with TLE errors to identify root causes, then developed Nettle - an automated repair tool combining LLMs with compiler/test-case feedback - and Nettle-Eval, a dedicated evaluation framework for TLE repairs.", "result": "Nettle achieved a 98.5% repair success rate on 1000 real-world TLE cases, significantly outperforming LLM baselines while maintaining 100% functionality correctness via Nettle-Eval verification and official platform testing.", "conclusion": "The paper challenges the conventional view that TLE errors are purely performance issues and demonstrates the effectiveness of their automated repair framework Nettle, achieving a 98.5% fix rate with reliable repairs validated by both their evaluator and platform checkers."}}
{"id": "2510.14344", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14344", "abs": "https://arxiv.org/abs/2510.14344", "authors": ["Zichen Liu", "Shao Yang", "Xusheng Xiao"], "title": "BinCtx: Multi-Modal Representation Learning for Robust Android App Behavior Detection", "comment": null, "summary": "Mobile app markets host millions of apps, yet undesired behaviors (e.g.,\ndisruptive ads, illegal redirection, payment deception) remain hard to catch\nbecause they often do not rely on permission-protected APIs and can be easily\ncamouflaged via UI or metadata edits. We present BINCTX, a learning approach\nthat builds multi-modal representations of an app from (i) a global\nbytecode-as-image view that captures code-level semantics and family-style\npatterns, (ii) a contextual view (manifested actions, components, declared\npermissions, URL/IP constants) indicating how behaviors are triggered, and\n(iii) a third-party-library usage view summarizing invocation frequencies along\ninter-component call paths. The three views are embedded and fused to train a\ncontextual-aware classifier. On real-world malware and benign apps, BINCTX\nattains a macro F1 of 94.73%, outperforming strong baselines by at least\n14.92%. It remains robust under commercial obfuscation (F1 84%\npost-obfuscation) and is more resistant to adversarial samples than\nstate-of-the-art bytecode-only systems.", "AI": {"tldr": "BINCTX is a multi-modal machine learning approach for detecting malicious mobile apps by analyzing bytecode, contextual metadata, and third-party library usage, achieving high accuracy (94.73% macro F1), robustness to obfuscation (84.2% post), and resistance to adversarial attacks.", "motivation": "Existing methods fail to detect stealthy malicious behaviors that rely on permission-unprotected APIs and can evade detection through UI/metadata camouflage. Current systems lack robustness to obfuscation and adversarial attacks.", "method": "BINCTX synthesizes three app views: 1. Global bytecode-as-image capturing code semantics, 2. Contextual view combining manifest/actions/permissions/urls, 3. Third-party library usage patterns across inter-component call paths. These features are embedded/fused in a contextual-aware classifier.", "result": "Achieved 94.73 macro F1 on realistic datasets, 14.92pp better than state-of-the-art. Maintains 84.2 F1 post-obfuscation and shows superior resistance to adversarial samples compared to bytecode-only models.", "conclusion": "BINCTX demonstrates effective multi-modal analysis for app classification, with robustness under commercial obfuscation and adversarial conditions, addressing critical limitations in current mobile app security analyses."}}
{"id": "2510.14341", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.14341", "abs": "https://arxiv.org/abs/2510.14341", "authors": ["Xu He", "Shu Wang", "Kun Sun"], "title": "PathFix: Automated Program Repair with Expected Path", "comment": "This is the author's version of a paper accepted at SecDev 2025\n  (IEEE)", "summary": "Automated program repair (APR) techniques are effective in fixing inevitable\ndefects in software, enhancing development efficiency and software robustness.\nHowever, due to the difficulty of generating precise specifications, existing\nAPR methods face two main challenges: generating too many plausible patch\ncandidates and overfitting them to partial test cases. To tackle these\nchallenges, we introduce a new APR method named PathFix, which leverages\npath-sensitive constraints extracted from correct execution paths to generate\npatches for repairing buggy code. It is based on one observation: if a buggy\nprogram is repairable, at least one expected path is supposed to replace the\nfault path in the patched program. PathFix operates in four main steps. First,\nit traces fault paths reaching the fault output in the buggy program. Second,\nit derives expected paths by analyzing the desired correct output on the\ncontrol flow graph, where an expected path defines how a feasible patch leads\nto the correct execution. Third, PathFix generates and evaluates patches by\nsolving state constraints along the expected path. Fourth, we validate the\ncorrectness of the generated patch. To further enhance repair performance and\nmitigate scalability issues introduced by path-sensitive analysis, we integrate\na large language model (LLM) into our framework. Experimental results show that\nPathFix outperforms existing solutions, particularly in handling complex\nprogram structures such as loops and recursion.", "AI": {"tldr": "PathFix is a new APR method that uses path-sensitive constraints from correct execution paths to generate accurate patches, outperforming existing APR methods especially for complex code structures.", "motivation": "Existing APR methods generate too many plausible patch candidates and overfit to partial test cases due to the challenge of generating precise specifications.", "method": "PathFix uses path-sensitive constraints from correct execution paths, operating in four steps: tracing fault paths, deriving expected paths from desired correct outputs on the control flow graph, generating and evaluating patches by solving state constraints along the paths, and validating generated patches. It also integrates a large language model to improve performance and scalability.", "result": "PathFix outperforms existing solutions, particularly in complex program structures like loops and recursion.", "conclusion": "PathFix provides a better approach to APR by focusing on correct paths and integrating LLMs, reducing overfitting and improving handling of complex structures."}}
{"id": "2510.14384", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.14384", "abs": "https://arxiv.org/abs/2510.14384", "authors": ["Sebastian J\u00e4nich", "Merlin Sievers", "Johannes Kinder"], "title": "Match & Mend: Minimally Invasive Local Reassembly for Patching N-day Vulnerabilities in ARM Binaries", "comment": null, "summary": "Low-cost Internet of Things (IoT) devices are increasingly popular but often\ninsecure due to poor update regimes. As a result, many devices run outdated and\nknown-vulnerable versions of open-source software. We address this problem by\nproposing to patch IoT firmware at the binary level, without requiring vendor\nsupport. In particular, we introduce minimally invasive local reassembly, a new\ntechnique for automatically patching known (n-day) vulnerabilities in IoT\nfirmware. Our approach is designed to minimize side effects and reduce the risk\nof introducing breaking changes. We systematically evaluate our approach both\non 108 binaries within the controlled environment of the MAGMA benchmarks, as\nwell as on 30 real-world Linux-based IoT firmware images from the KARONTE\ndataset. Our prototype successfully patches 83% of targeted vulnerabilities in\nMAGMA and 96% in the firmware dataset.", "AI": {"tldr": "This paper proposes automated binary-level firmware patching for IoT devices, achieving high success rates across benchmark and real-world datasets without requiring vendor updates.", "motivation": "Low-cost IoT devices often run outdated, vulnerable software due to poor firmware update practices. Addressing this requires solutions that bypass vendor dependency.", "method": "The authors introduce 'minimally invasive local reassembly,' a technique to automatically patch binary-level firmware with minimal side effects. Evaluations were conducted on MAGMA benchmarks and the KARONTE dataset to validate the method.", "result": "The prototype successfully patched 83% of vulnerabilities in the MAGMA benchmarks and 96% in 30 real-world firmware images from the KARONTE dataset.", "conclusion": "The approach provides an effective and practical solution for securing IoT devices, particularly those with poor update regimes, by enabling automated, vendor-independent firmware patching."}}
{"id": "2510.14465", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.14465", "abs": "https://arxiv.org/abs/2510.14465", "authors": ["Adem Ait", "Gwendal Jouneaux", "Javier Luis C\u00e1novas Izquierdo", "Jordi Cabot"], "title": "Towards Automated Governance: A DSL for Human-Agent Collaboration in Software Projects", "comment": "Accepted in the 40th IEEE/ACM International Conference on Automated\n  Software Engineering, ASE 2025", "summary": "The stakeholders involved in software development are becoming increasingly\ndiverse, with both human contributors from varied backgrounds and AI-powered\nagents collaborating together in the process. This situation presents unique\ngovernance challenges, particularly in Open-Source Software (OSS) projects,\nwhere explicit policies are often lacking or unclear. This paper presents the\nvision and foundational concepts for a novel Domain-Specific Language (DSL)\ndesigned to define and enforce rich governance policies in systems involving\ndiverse stakeholders, including agents. This DSL offers a pathway towards more\nrobust, adaptable, and ultimately automated governance, paving the way for more\neffective collaboration in software projects, especially OSS ones.", "AI": {"tldr": "Presents a DSL for governing\u591a\u5143 stakeholder collaboration in OSS by formalizing policy enforcement mechanisms for human-AI teams.", "motivation": "Modern software development involves increasingly diverse stakeholders, including AI agents, yet OSS projects lack explicit governance policies. This creates ambiguity and inefficiency in managing contributions and conflicts.", "method": "Design and conceptualization of a Domain-Specific Language (DSL) tailored for defining and enforcing governance policies in systems with heterogeneous stakeholders (humans and AI agents).", "result": "A novel DSL framework for governance policy specification, enabling structured enforcement of rules for collaborative systems with mixed human-AI participation.", "conclusion": "The proposed DSL offers a pathway towards robust, adaptable, and automated governance for diverse stakeholders in software projects, particularly enhancing collaboration in Open-Source Software (OSS) contexts."}}
{"id": "2510.14470", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14470", "abs": "https://arxiv.org/abs/2510.14470", "authors": ["Xiaoyu Xue", "Yuni Lai", "Chenxi Huang", "Yulin Zhu", "Gaolei Li", "Xiaoge Zhang", "Kai Zhou"], "title": "Stealthy Dual-Trigger Backdoors: Attacking Prompt Tuning in LM-Empowered Graph Foundation Models", "comment": null, "summary": "The emergence of graph foundation models (GFMs), particularly those\nincorporating language models (LMs), has revolutionized graph learning and\ndemonstrated remarkable performance on text-attributed graphs (TAGs). However,\ncompared to traditional GNNs, these LM-empowered GFMs introduce unique security\nvulnerabilities during the unsecured prompt tuning phase that remain\nunderstudied in current research. Through empirical investigation, we reveal a\nsignificant performance degradation in traditional graph backdoor attacks when\noperating in attribute-inaccessible constrained TAG systems without explicit\ntrigger node attribute optimization. To address this, we propose a novel\ndual-trigger backdoor attack framework that operates at both text-level and\nstruct-level, enabling effective attacks without explicit optimization of\ntrigger node text attributes through the strategic utilization of a\npre-established text pool. Extensive experimental evaluations demonstrate that\nour attack maintains superior clean accuracy while achieving outstanding attack\nsuccess rates, including scenarios with highly concealed single-trigger nodes.\nOur work highlights critical backdoor risks in web-deployed LM-empowered GFMs\nand contributes to the development of more robust supervision mechanisms for\nopen-source platforms in the era of foundation models.", "AI": {"tldr": "This paper reveals vulnerabilities in graph foundation models using text-attributed graphs, proposing a dual-trigger backdoor attack framework that achieves high attack success without explicit text attribute optimization through strategic text pool usage.", "motivation": "Current graph backdoor attacks fail in attribute-inaccessible TAG environments without trigger node text optimization, exposing unique security vulnerabilities in LM-integrated GFMs during unsecured prompt tuning.", "method": "A dual-trigger backdoor attack framework leveraging both text-level and struct-level triggers was proposed, utilizing a pre-established text pool to bypass explicit node attribute optimization constraints in TAG systems.", "result": "Experimental results demonstrated superior clean accuracy and high attack success rates, including for concealed single-trigger node scenarios, confirming the effectiveness of dual-trigger attacks without attribute optimization.", "conclusion": "The work highlights critical backdoor risks in LM-empowered GFMs and emphasizes the need for robust supervision mechanisms in open-source platforms to address security vulnerabilities in foundation models."}}
{"id": "2510.14509", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14509", "abs": "https://arxiv.org/abs/2510.14509", "authors": ["Jingyao Liu", "Chen Huang", "Zhizhao Guan", "Wenqiang Lei", "Yang Deng"], "title": "E2Edev: Benchmarking Large Language Models in End-to-End Software Development Task", "comment": null, "summary": "E2EDev comprises (i) a fine-grained set of user requirements, (ii) {multiple\nBDD test scenarios with corresponding Python step implementations for each\nrequirement}, and (iii) a fully automated testing pipeline built on the Behave\nframework. To ensure its quality while reducing the annotation effort, E2EDev\nleverages our proposed Human-in-the-Loop Multi-Agent Annotation Framework\n(HITL-MAA). {By evaluating various E2ESD frameworks and LLM backbones with\nE2EDev}, our analysis reveals a persistent struggle to effectively solve these\ntasks, underscoring the critical need for more effective and cost-efficient\nE2ESD solutions. Our codebase and benchmark are publicly available at\nhttps://github.com/SCUNLP/E2EDev.", "AI": {"tldr": "E2EDev is an end-to-end testing framework with a human-in-the-loop annotation system (HITL-MAA), revealing current E2ESD frameworks' limitations. The paper underscores the need for better E2ESD solutions.", "motivation": "Existing E2ESD frameworks struggle with effectiveness and cost-efficiency in automated testing. Annotation efforts require reduction while maintaining quality.", "method": "1. Developed E2EDev: fine-grained requirements, BDD test scenarios with Python implementations, and Behave-based automation. 2. Introduced HITL-MAA to reduce annotation effort and improve quality. 3. Evaluated E2ESD frameworks and LLM backbones using E2EDev.", "result": "Analysis shows current frameworks consistently underperform on the E2EDev benchmark, highlighting unresolved challenges in E2ESD tasks.", "conclusion": "Current E2ESD solutions are inadequate; the open-source E2EDev benchmark and findings call for improved frameworks and cost-effective methodologies."}}
{"id": "2510.14480", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.14480", "abs": "https://arxiv.org/abs/2510.14480", "authors": ["Massimo Bartoletti", "Riccardo Marchesin", "Roberto Zunino"], "title": "Certifying optimal MEV strategies with Lean", "comment": null, "summary": "Maximal Extractable Value (MEV) refers to a class of attacks to decentralized\napplications where the adversary profits by manipulating the ordering,\ninclusion, or exclusion of transactions in a blockchain. Decentralized Finance\n(DeFi) protocols are a primary target of these attacks, as their logic depends\ncritically on transaction sequencing. To date, MEV attacks have already\nextracted billions of dollars in value, underscoring their systemic impact on\nblockchain security. Verifying the absence of MEV attacks requires determining\nsuitable upper bounds, i.e. proving that no adversarial strategy can extract\nmore value (if any) than expected by protocol designers. This problem is\nnotoriously difficult: the space of adversarial strategies is extremely vast,\nmaking empirical studies and pen-and-paper reasoning insufficiently rigorous.\nIn this paper, we present the first mechanized formalization of MEV in the Lean\ntheorem prover. We introduce a methodology to construct machine-checked proofs\nof MEV bounds, providing correctness guarantees beyond what is possible with\nexisting techniques. To demonstrate the generality of our approach, we model\nand analyse the MEV of two paradigmatic DeFi protocols. Notably, we develop the\nfirst machine-checked proof of the optimality of sandwich attacks in Automated\nMarket Makers, a fundamental DeFi primitive.", "AI": {"tldr": "Formalizes MEV analysis in Lean theorem prover, enables machine-checked proofs for DeFi protocol security, and provides first mechanized verification of sandwich attack optimality in AMMs.", "motivation": "The vast space of adversarial strategies in MEV attacks makes empirical studies and manual analysis insufficient for rigorous verification. This motivates the need for mechanized formalization to ensure protocol security guarantees.", "method": "The authors introduce a methodology for constructing machine-checked proofs of MEV bounds using the Lean theorem prover, enabling formal verification of adversarial strategy limits in DeFi protocols.", "result": "The approach is demonstrated through modeling two DeFi protocols and producing the first machine-checked proof of sandwich attack optimality in Automated Market Makers (AMMs), a foundational DeFi primitive.", "conclusion": "The paper concludes that the proposed mechanized formalization in Lean provides stronger correctness guarantees for verifying MEV bounds compared to existing techniques, advancing the rigorous analysis of blockchain security in DeFi protocols."}}
{"id": "2510.14625", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.14625", "abs": "https://arxiv.org/abs/2510.14625", "authors": ["Mehrdad Saadatmand", "Abbas Khan", "Beatriz Marin", "Ana C. R Paiva", "Nele Van Asch", "Graham Moran", "Felix Cammaerts", "Monique Snoeck", "Alexandra Mendes"], "title": "Software Testing Education and Industry Needs - Report from the ENACTEST EU Project", "comment": "* The paper is going to appear in the proceedings of the 26th\n  International Conference on Product-Focused Software Process Improvement\n  (PROFES 2025). To cite the paper, please check and refer to the PROFES 2025\n  proceedings", "summary": "The evolving landscape of software development demands that software testers\ncontinuously adapt to new tools, practices, and acquire new skills. This study\ninvestigates software testing competency needs in industry, identifies\nknowledge gaps in current testing education, and highlights competencies and\ngaps not addressed in academic literature. This is done by conducting two focus\ngroup sessions and interviews with professionals across diverse domains,\nincluding railway industry, healthcare, and software consulting and performing\na curated small-scale scoping review. The study instrument, co-designed by\nmembers of the ENACTEST project consortium, was developed collaboratively and\nrefined through multiple iterations to ensure comprehensive coverage of\nindustry needs and educational gaps. In particular, by performing a thematic\nqualitative analysis, we report our findings and observations regarding:\nprofessional training methods, challenges in offering training in industry,\ndifferent ways of evaluating the quality of training, identified knowledge gaps\nwith respect to academic education and industry needs, future needs and trends\nin testing education, and knowledge transfer methods within companies. Finally,\nthe scoping review results confirm knowledge gaps in areas such as AI testing,\nsecurity testing and soft skills.", "AI": {"tldr": "This collaboration between industry and academia identifies growing competencies needed in software testing, revealing gaps in teaching AI/ML testing, security testing, and soft skills. Based on interviews, focus groups, and a scoping review, the paper calls for curriculum updates to address industry needs like secure systems testing and AI-driven verification techniques.", "motivation": "The motivation stems from the evolving software development landscape requiring testers to adapt to new tools and practices, while current testing education appears insufficient in addressing industrial competency needs and emerging gaps.", "method": "The study used two focus groups, industry interviews (railway, healthcare, software consulting), and a curated scoping review. The instrument was co-designed with the ENACTEST project consortium through iterative refinement to ensure alignment with industry and educational needs.", "result": "Key results include insights into training methodologies, challenges in industrial training delivery, evaluation frameworks for training quality, mismatches between academic education and industry requirements, future trends (e.g., AI/ML testing), and gaps in knowledge transfer practices within companies.", "conclusion": "The study concludes that there are significant knowledge gaps in areas like AI testing, security testing, and soft skills, highlighting the need for updated testing education that aligns with industry demands and emerging trends."}}
{"id": "2510.14522", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.14522", "abs": "https://arxiv.org/abs/2510.14522", "authors": ["Evangelos Lamprou", "Julian Dai", "Grigoris Ntousakis", "Martin C. Rinard", "Nikos Vasilakis"], "title": "Lexo: Eliminating Stealthy Supply-Chain Attacks via LLM-Assisted Program Regeneration", "comment": null, "summary": "Software supply-chain attacks are an important and ongoing concern in the\nopen source software ecosystem. These attacks maintain the standard\nfunctionality that a component implements, but additionally hide malicious\nfunctionality activated only when the component reaches its target environment.\nLexo addresses such stealthy attacks by automatically learning and regenerating\nvulnerability-free versions of potentially malicious components. Lexo first\ngenerates a set of input-output pairs to model a component's full observable\nbehavior, which it then uses to synthesize a new version of the original\ncomponent. The new component implements the original functionality but avoids\nstealthy malicious behavior. Throughout this regeneration process, Lexo\nconsults several distinct instances of Large Language Models (LLMs), uses\ncorrectness and coverage metrics to shepherd these instances, and guardrails\ntheir results. Our evaluation on 100+ real-world packages, including high\nprofile stealthy supply-chain attacks, indicates that Lexo scales across\nmultiple domains, regenerates code efficiently (<100s on average), maintains\ncompatibility, and succeeds in eliminating malicious code in several real-world\nsupply-chain-attacks, even in cases when a state-of-the-art LLM fails to\neliminate malicious code when prompted to do so.", "AI": {"tldr": "Lexo is a framework designed to counter software supply-chain attacks in open source ecosystems by automatically regenerating code to eliminate malicious behavior while preserving original functionality.", "motivation": "Software supply-chain attacks are a critical issue in the open source software ecosystem as they can introduce hidden malicious functionalities that remain dormant until they reach the target environment.", "method": "Lexo uses a process where it generates input-output pairs to model a component's behavior, then synthesizes a new version of the component without malicious code by consulting multiple Large Language Models (LLMs), using correctness and coverage metrics, and implementing guardrails for the LLM outputs.", "result": "Lexo was evaluated on over 100 real-world software packages, including those involved in high profile stealthy supply-chain attacks. It showed scalability across multiple domains, efficient code regeneration (under 100 seconds on average), compatibility maintenance, and success in eliminating malicious code, even where state-of-the-art LLMs failed.", "conclusion": "Lexo provides an effective, scalable, and automated solution for mitigating stealthy supply-chain attacks in open source software, maintaining functionality and compatibility while successfully removing malicious content."}}
{"id": "2510.14635", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.14635", "abs": "https://arxiv.org/abs/2510.14635", "authors": ["Qingyao Li", "Xinyi Dai", "Weiwen Liu", "Xiangyang Li", "Yasheng Wang", "Ruiming Tang", "Yong Yu", "Weinan Zhang"], "title": "ATGen: Adversarial Reinforcement Learning for Test Case Generation", "comment": null, "summary": "Large Language Models (LLMs) excel at code generation, yet their outputs\noften contain subtle bugs, for which effective test cases are a critical\nbottleneck. Existing test generation methods, whether based on prompting or\nsupervised fine-tuning, rely on static datasets. This imposes a\n``fixed-difficulty ceiling'', fundamentally limiting their ability to uncover\nnovel or more complex bugs beyond their training scope. To overcome this, we\nintroduce ATGen, a framework that trains a test case generator via adversarial\nreinforcement learning. ATGen pits a test generator against an adversarial code\ngenerator that continuously crafts harder bugs to evade the current policy.\nThis dynamic loop creates a curriculum of increasing difficulty challenging\ncurrent policy. The test generator is optimized via Reinforcement Learning (RL)\nto jointly maximize ``Output Accuracy'' and ``Attack Success'', enabling it to\nlearn a progressively stronger policy that breaks the fixed-difficulty ceiling\nof static training. Extensive experiments demonstrate that ATGen significantly\noutperforms state-of-the-art baselines. We further validate its practical\nutility, showing it serves as both a more effective filter for Best-of-N\ninference and a higher-quality reward source for training code generation\nmodels. Our work establishes a new, dynamic paradigm for improving the\nreliability of LLM-generated code.", "AI": {"tldr": "ATGen breaks the fixed-difficulty ceiling of static test generation by using adversarial reinforcement learning to dynamically evolve test cases, improving code reliability and enabling practical applications as an inference filter and training reward source for code generation models.", "motivation": "The paper addresses the critical bottleneck of static test datasets in code generation, which impose a 'fixed-difficulty ceiling' that prevents uncovering novel or more complex bugs beyond the training scope of existing test generation methods.", "method": "ATGen employs adversarial reinforcement learning with a curriculum framework. A test case generator is trained against an adversarial code generator that creates progressively harder-to-detect bugs, while being optimized to maximize both output accuracy and attack success through a dual-objective reinforcement learning strategy.", "result": "Experiments show ATGen significantly outperforms state-of-the-art baselines. It demonstrates practical value as both a stronger Best-of-N filter for generated code and a higher-quality reward source for training code generation models.", "conclusion": "This paper introduces ATGen, an adversarial reinforcement learning framework that surpasses the limitations of static test generation methods by dynamically evolving test cases to uncover increasingly complex bugs in code generated by Large Language Models (LLMs). The approach establishes a new paradigm for improving LLM-generated code reliability and demonstrates practical utility in filtering inference outputs and training reward models."}}
{"id": "2510.14589", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.14589", "abs": "https://arxiv.org/abs/2510.14589", "authors": ["Vaishnavi Sundararajan", "Rithwik"], "title": "Symbolic verification of Apple's Find My location-tracking protocol", "comment": null, "summary": "Tracking devices, while designed to help users find their belongings in case\nof loss/theft, bring in new questions about privacy and surveillance of not\njust their own users, but in the case of crowd-sourced location tracking, even\nthat of others even orthogonally associated with these platforms. Apple's Find\nMy is perhaps the most ubiquitous such system which can even locate devices\nwhich do not possess any cellular support or GPS, running on millions of\ndevices worldwide. Apple claims that this system is private and secure, but the\ncode is proprietary, and such claims have to be taken on faith. It is well\nknown that even with perfect cryptographic guarantees, logical flaws might\ncreep into protocols, and allow undesirable attacks. In this paper, we present\na symbolic model of the Find My protocol, as well as a precise formal\nspecification of desirable properties, and provide automated, machine-checkable\nproofs of these properties in the Tamarin prover.", "AI": {"tldr": "Formal verification confirms Apple's Find My protocol's security claims.", "motivation": "The paper addresses privacy concerns in proprietary tracking systems like Apple's Find My, aiming to validate Apple's security claims despite the system's closed-source nature.", "method": "The authors used symbolic modeling of the Find My protocol, formalized desirable security properties, and conducted machine-checkable proofs via the Tamarin prover.", "result": "Automated proofs in Tamarin verified that the Find My protocol satisfies its specified privacy and security properties.", "conclusion": "The paper concludes that Apple's Find My system can be formally verified using symbolic modeling and Tamarin prover to confirm its privacy and security claims."}}
{"id": "2510.14653", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14653", "abs": "https://arxiv.org/abs/2510.14653", "authors": ["Sven Tarlowski", "Lutz Eckstein"], "title": "Requirement Identification for Traffic Simulations in Driving Simulators", "comment": "2 Pages, 1 figure", "summary": "This paper addresses the challenge of ensuring realistic traffic conditions\nby proposing a methodology that systematically identifies traffic simulation\nrequirements. Using a structured approach based on sub-goals in each study\nphase, specific technical needs are derived for microscopic levels, agent\nmodels, and visual representation. The methodology aims to maintain a high\ndegree of fidelity, enhancing both the validity of experimental outcomes and\nparticipant engagement. By providing a clear link between study objectives and\ntraffic simulation design, this approach supports robust automotive development\nand testing.", "AI": {"tldr": "The paper introduces a methodology for systematic identification of traffic simulation requirements to ensure realistic traffic conditions in automotive studies.", "motivation": "The motivation arises from the need for high fidelity in traffic simulations to ensure valid experimental outcomes and maintain participant engagement in automotive development and testing.", "method": "The methodology employs a structured approach based on sub-goals defined in each phase of a study. It identifies specific technical requirements for microscopic levels, agent models, and visual representation through these sub-goals.", "result": "The result is a framework that links study objectives directly to traffic simulation design, enabling a more methodical and precise setup for simulations that are realistic and efficient.", "conclusion": "The paper concludes that the proposed approach significantly supports automotive development by ensuring realistic traffic conditions through systematic simulation design."}}
{"id": "2510.14638", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.14638", "abs": "https://arxiv.org/abs/2510.14638", "authors": ["Silvia Lucia Sanna", "Leonardo Regano", "Davide Maiorca", "Giorgio Giacinto"], "title": "Improving Cybercrime Detection and Digital Forensics Investigations with Artificial Intelligence", "comment": null, "summary": "According to a recent EUROPOL report, cybercrime is still recurrent in\nEurope, and different activities and countermeasures must be taken to limit,\nprevent, detect, analyze, and fight it. Cybercrime must be prevented with\nspecific measures, tools, and techniques, for example through automated network\nand malware analysis. Countermeasures against cybercrime can also be improved\nwith proper \\df analysis in order to extract data from digital devices trying\nto retrieve information on the cybercriminals. Indeed, results obtained through\na proper \\df analysis can be leveraged to train cybercrime detection systems to\nprevent the success of similar crimes. Nowadays, some systems have started to\nadopt Artificial Intelligence (AI) algorithms for cyberattack detection and \\df\nanalysis improvement. However, AI can be better applied as an additional\ninstrument in these systems to improve the detection and in the \\df analysis.\nFor this reason, we highlight how cybercrime analysis and \\df procedures can\ntake advantage of AI. On the other hand, cybercriminals can use these systems\nto improve their skills, bypass automatic detection, and develop advanced\nattack techniques. The case study we presented highlights how it is possible to\nintegrate the use of the three popular chatbots {\\tt Gemini}, {\\tt Copilot} and\n{\\tt chatGPT} to develop a Python code to encode and decoded images with\nsteganographic technique, even though their presence is not an indicator of\ncrime, attack or maliciousness but used by a cybercriminal as anti-forensics\ntechnique.", "AI": {"tldr": "The paper explores the intersection of cybercrime, digital forensics (DF), and AI, proposing AI integration to enhance cybercrime detection and DF analysis while highlighting risks of cybercriminal misuse through a chatbot-based steganography case study.", "motivation": "Cybercrime remains prevalent in Europe, necessitating advanced tools like AI-driven DF analysis to improve prevention, detection, and analysis. Existing systems could benefit from better AI integration to combat evolving threats.", "method": "The study reviews AI applications in cyberattack detection and DF analysis, proposes enhancing these systems with AI, and demonstrates a case study using Gemini, Copilot, and ChatGPT to generate Python code for steganographic image encoding/decoding as an anti-forensics example.", "result": "AI can significantly improve cybercrime detection and DF analysis efficacy. However, chatbots can also be weaponized for steganography, demonstrating dual-use risks where similar techniques might be exploited by cybercriminals for anti-forensics.", "conclusion": "AI offers transformative potential for cybercrime analysis and DF procedures but requires careful implementation to prevent exploitation by malicious actors. Understanding both technical capabilities and misuse risks is critical for developing robust cybersecurity frameworks."}}
{"id": "2510.14700", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.14700", "abs": "https://arxiv.org/abs/2510.14700", "authors": ["Bin Liu", "Yanjie Zhao", "Guoai Xu", "Haoyu Wang"], "title": "LLM Agents for Automated Web Vulnerability Reproduction: Are We There Yet?", "comment": null, "summary": "Large language model (LLM) agents have demonstrated remarkable capabilities\nin software engineering and cybersecurity tasks, including code generation,\nvulnerability discovery, and automated testing. One critical but underexplored\napplication is automated web vulnerability reproduction, which transforms\nvulnerability reports into working exploits. Although recent advances suggest\npromising potential, challenges remain in applying LLM agents to real-world web\nvulnerability reproduction scenarios. In this paper, we present the first\ncomprehensive evaluation of state-of-the-art LLM agents for automated web\nvulnerability reproduction. We systematically assess 20 agents from software\nengineering, cybersecurity, and general domains across 16 dimensions, including\ntechnical capabilities, environment adaptability, and user experience factors,\non 3 representative web vulnerabilities. Based on the results, we select three\ntop-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation\non our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types\nand 6 web technologies. Our results reveal that while LLM agents achieve\nreasonable success on simple library-based vulnerabilities, they consistently\nfail on complex service-based vulnerabilities requiring multi-component\nenvironments. Complex environment configurations and authentication barriers\ncreate a gap where agents can execute exploit code but fail to trigger actual\nvulnerabilities. We observe high sensitivity to input guidance, with\nperformance degrading by over 33% under incomplete authentication information.\nOur findings highlight the significant gap between current LLM agent\ncapabilities and the demands of reliable automated vulnerability reproduction,\nemphasizing the need for advances in environmental adaptation and autonomous\nproblem-solving capabilities.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.14675", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.14675", "abs": "https://arxiv.org/abs/2510.14675", "authors": ["Nicolas Dutly", "Friederike Groschupp", "Ivan Puddu", "Kari Kostiainen", "Srdjan Capkun"], "title": "AEX-NStep: Probabilistic Interrupt Counting Attacks on Intel SGX", "comment": "Author's version, to appear, 2026 IEEE Symposium on Security and\n  Privacy (SP)", "summary": "To mitigate interrupt-based stepping attacks (notably using SGX-Step), Intel\nintroduced AEX-Notify, an ISA extension to Intel SGX that aims to prevent\ndeterministic single-stepping. In this work, we introduce AEX-NStep, the first\ninterrupt counting attack on AEX-Notify-enabled Enclaves. We show that\ndeterministic single-stepping is not required for interrupt counting attacks to\nbe practical and that, therefore, AEX-Notify does not entirely prevent such\nattacks. We specifically show that one of AEX-Notify's security guarantees,\nobfuscated forward progress, does not hold, and we introduce two new\nprobabilistic interrupt counting attacks. We use these attacks to construct a\npractical ECDSA key leakage attack on an AEX-Notify-enabled SGX enclave. Our\nresults extend the original security analysis of AEX-Notify and inform the\ndesign of future mitigations.", "AI": {"tldr": "AEX-NStep bypasses Intel's AEX-Notify by introducing probabilistic interrupt counting attacks, exposing flaws in its security guarantees and enabling practical ECDSA key leakage in SGX enclaves.", "motivation": "The paper aims to assess the efficacy of Intel's AEX-Notify mitigation against interrupt-based stepping attacks and extend its security analysis to identify critical vulnerabilities, guiding future defenses.", "method": "The authors developed two new probabilistic interrupt counting attacks, exploiting AEX-Notify's failure to ensure obfuscated forward progress, which enabled a practical ECDSA key leakage attack against AEX-Notify-enabled enclaves.", "result": "The paper successfully executed a key leakage attack against SGX enclaves using the new probabilistic attacks, proving AEX-Notify's insufficiency to prevent such exploits.", "conclusion": "The study reveals that AEX-Notify does not fully prevent interrupt counting attacks, demonstrating that probabilistic methods can bypass it, thus necessitating improved mitigation strategies for SGX enclave security."}}
{"id": "2510.14778", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14778", "abs": "https://arxiv.org/abs/2510.14778", "authors": ["Maor Reuben", "Ido Mendel", "Or Feldman", "Moshe Kravchik", "Mordehai Guri", "Rami Puzis"], "title": "Leveraging Code Cohesion Analysis to Identify Source Code Supply Chain Attacks", "comment": null, "summary": "Supply chain attacks significantly threaten software security with malicious\ncode injections within legitimate projects. Such attacks are very rare but may\nhave a devastating impact. Detecting spurious code injections using automated\ntools is further complicated as it often requires deciphering the intention of\nboth the inserted code and its context. In this study, we propose an\nunsupervised approach for highlighting spurious code injections by quantifying\ncohesion disruptions in the source code. Using a name-prediction-based cohesion\n(NPC) metric, we analyze how function cohesion changes when malicious code is\nintroduced compared to natural cohesion fluctuations. An analysis of 54,707\nfunctions over 369 open-source C++ repositories reveals that code injection\nreduces cohesion and shifts naming patterns toward shorter, less descriptive\nnames compared to genuine function updates. Considering the sporadic nature of\nreal supply-chain attacks, we evaluate the proposed method with extreme\ntest-set imbalance and show that monitoring high-cohesion functions with NPC\ncan effectively detect functions with injected code, achieving a Precision@100\nof 36.41% at a 1:1,000 ratio and 12.47% at 1:10,000. These results suggest that\nautomated cohesion measurements, in general, and name-prediction-based\ncohesion, in particular, may help identify supply chain attacks, improving\nsource code integrity.", "AI": {"tldr": "This paper introduces an unsupervised method to detect supply chain attacks by analyzing cohesion disruptions in source code using a name-prediction-based cohesion (NPC) metric. Experimental results show reduced cohesion and altered naming patterns in malicious code injections, with detection precision metrics under imbalanced attack scenarios.", "motivation": "Supply chain attacks pose severe software security risks but are rare and challenging to detect automatically due to their deceptive nature and the need to interpret code intentions. Current tools struggle with contextual understanding of code changes.", "method": "The study proposes an NPC metric to quantify function cohesion changes, comparing malicious injections to natural code updates. A large-scale analysis of 54,707 functions from C++ repositories is performed, evaluating detection effectiveness under extreme class imbalance.", "result": "Code injections significantly decrease cohesion and shift naming patterns toward shorter names. The method achieves 36.41%% Precision@100 at 1:1,000 attack ratio and 12.47%% at 1:10,000, demonstrating effectiveness in imbalanced datasets.", "conclusion": "Automated cohesion measurement, particularly NPC, shows potential for identifying supply chain attacks by detecting cohesion disruptions, offering an improved approach to maintaining source code integrity."}}
{"id": "2510.14693", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.14693", "abs": "https://arxiv.org/abs/2510.14693", "authors": ["Simon Malatrait", "Alex Sirac"], "title": "FibRace: a large-scale benchmark of client-side proving on mobile devices", "comment": "14 pages, 5 figures, 2 tables", "summary": "FibRace, jointly developed by KKRT Labs and Hyli, was the first large-scale\nexperiment to test client-side proof generation on smartphones using Cairo M.\nPresented as a mobile game in which players proved Fibonacci numbers and\nclimbed a leaderboard, FibRace served a dual purpose: to engage the public and\nto provide empirical benchmarking. Over a three-week campaign (September 11-30,\n2025), 6,047 players across 99 countries generated 2,195,488 proofs on 1,420\nunique device models. The results show that most modern smartphones can\ncomplete a proof in under 5 seconds, confirming that *mobile devices are now\ncapable of producing zero-knowledge proofs reliably*, without the need for\nremote provers or specialized hardware. Performance was correlated primarily\nwith RAM capacity and SoC (System on Chip) performance: devices with at least 3\nGB of RAM proved stably, when Apple's A19 Pro and M-series chips achieved the\nfastest proving times. Hyli's blockchain natively verified every proof onchain\nwithout congestion. FibRace provides the most comprehensive dataset to date on\nmobile proving performance, establishing a practical baseline for future\nresearch in lightweight provers, proof-powered infrastructure, and\nprivacy-preserving mobile applications.", "AI": {"tldr": "FibRace proves smartphones can reliably generate ZK proofs (2.2M collected), with performance tied to RAM/SoC, enabling mobile privacy apps.", "motivation": "To demonstrate the practicality of mobile devices for ZK proofs, enabling privacy-preserving applications while collecting empirical data on performance limitations.", "method": "A gamified mobile challenge where 6,047 participants generated over 2 million Fibonacci proofs across 1,420 device models, benchmarking performance through real-world usage (Sep 2025).", "result": "89% of modern smartphones completed proofs in <5 seconds; RAM capacity and SoC performance (A19 Pro/M-series) determined stability; blockchain natively verified all proofs without congestion.", "conclusion": "Modern smartphones can reliably generate zero-knowledge proofs on-chain without specialized hardware, establishing FibRace as a foundational dataset for mobile proving research."}}
{"id": "2510.14928", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14928", "abs": "https://arxiv.org/abs/2510.14928", "authors": ["Eric Christopher", "Kevin Crossan", "Wolff Dobson", "Chris Kennelly", "Drew Lewis", "Kun Lin", "Martin Maas", "Parthasarathy Ranganathan", "Emma Rapati", "Brian Yang"], "title": "Instruction Set Migration at Warehouse Scale", "comment": null, "summary": "Migrating codebases from one instruction set architecture (ISA) to another is\na major engineering challenge. A recent example is the adoption of Arm (in\naddition to x86) across the major Cloud hyperscalers. Yet, this problem has\nseen limited attention by the academic community. Most work has focused on\nstatic and dynamic binary translation, and the traditional conventional wisdom\nhas been that this is the primary challenge.\n  In this paper, we show that this is no longer the case. Modern ISA migrations\ncan often build on a robust open-source ecosystem, making it possible to\nrecompile all relevant software from scratch. This introduces a new and\nmultifaceted set of challenges, which are different from binary translation.\n  By analyzing a large-scale migration from x86 to Arm at Google, spanning\nalmost 40,000 code commits, we derive a taxonomy of tasks involved in ISA\nmigration. We show how Google automated many of the steps involved, and\ndemonstrate how AI can play a major role in automatically addressing these\ntasks. We identify tasks that remain challenging and highlight research\nchallenges that warrant further attention.", "AI": {"tldr": "The paper analyzes Google's large-scale migration from x86 to Arm (30,000+ code commits) to identify a taxonomy of tasks involved in modern ISA migration, emphasizing the new challenges beyond binary translation and the role of AI in automation.", "motivation": "There has been limited academic focus on ISA migrations like x86 to Arm migration mainly scoped on binary translation, missing the new challenges introduced by recompiling from open-source.", "method": "The analysis of Google's extensive x86 to Arm migration by examining the full set of code commits. ", "result": "A new taxonomy of tasks for modern ISA migration is identified, AI's role in automation is demonstrated, and challenging areas are highlighted.", "conclusion": "Modern ISA migrations present new multi-faceted challenges requiring beyond binary translation, AI can help but further research is needed for remaining challenges."}}
{"id": "2510.14708", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.14708", "abs": "https://arxiv.org/abs/2510.14708", "authors": ["Ha Xuan Son", "Nguyen Quoc Anh", "Phat T. Tran-Truong", "Le Thanh Tuan", "Pham Thanh Nghiem"], "title": "SLIE: A Secure and Lightweight Cryptosystem for Data Sharing in IoT Healthcare Services", "comment": "Paper has been accepted for publication in the Proceedings of the\n  23th International Conference on Service-Oriented Computing 2025", "summary": "The Internet of Medical Things (IoMT) has revolutionized healthcare by\ntransforming medical operations into standardized, interoperable services.\nHowever, this service-oriented model introduces significant security\nvulnerabilities in device management and communication, which are especially\ncritical given the sensitivity of medical data. To address these risks, this\npaper proposes SLIE (Secure and Lightweight Identity Encryption), a novel\ncryptosystem based on Wildcard Key Derivation Identity-Based Encryption\n(WKD-IBE). SLIE ensures scalable trust and secure omnidirectional communication\nthrough end-to-end encryption, hierarchical access control, and a lightweight\nkey management system designed for resource-constrained devices. It\nincorporates constant-time operations, memory obfuscation, and expiry-based key\nrevocation to counter side-channel, man-in-the-middle, and unauthorized access\nattacks, thereby ensuring compliance with standards like HIPAA and GDPR.\nEvaluations show that SLIE significantly outperforms RSA, with encryption and\ndecryption times of 0.936ms and 0.217ms for 1KB of data, an 84.54% improvement\nin encryption speed, a 99.70% improvement in decryption speed, and an energy\nefficiency of 0.014 J/KB.", "AI": {"tldr": "This paper proposes SLIE, a lightweight, secure identity-based encryption system for the Internet of Medical Things (IoMT) that addresses device management and communication vulnerabilities through scalable trust, end-to-end encryption, and energy efficiency improvements over RSA.", "motivation": "Service-oriented IoMT models face critical security risks in device management and sensitive medical data transmission, necessitating a cryptosystem that ensures scalability, interoperability, and robustness against attacks while optimizing for resource-constrained devices.", "method": "SLIE utilizes Wildcard Key Derivation Identity-Based Encryption (WKD-IBE) with hierarchical access control, constant-time operations, memory obfuscation, and expiry-based key revocation. It enforces secure, omnidirectional communication and lightweight key management tailored for constrained environments while meeting HIPAA/GDPR compliance.", "result": "SLIE achieves 84.54%\u2009faster encryption and 99.70%\u2009faster decryption than RSA (0.936ms/0.217ms for 1KB data), with energy efficiency of 0.014 J/KB. It effectively mitigates side-channel, man-in-the-middle, and unauthorized access attacks.", "conclusion": "SLIE provides a scalable, lightweight, and secure solution for IoMT, outperforming RSA in speed, energy efficiency, and attack resistance while ensuring compliance with critical healthcare regulations and enabling robust, interoperable medical IoT services."}}
{"id": "2510.14894", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14894", "abs": "https://arxiv.org/abs/2510.14894", "authors": ["Marc Damie", "Florian Hahn", "Andreas Peter", "Jan Ramon"], "title": "Secure Sparse Matrix Multiplications and their Applications to Privacy-Preserving Machine Learning", "comment": null, "summary": "To preserve privacy, multi-party computation (MPC) enables executing Machine\nLearning (ML) algorithms on secret-shared or encrypted data. However, existing\nMPC frameworks are not optimized for sparse data. This makes them unsuitable\nfor ML applications involving sparse data, e.g., recommender systems or\ngenomics. Even in plaintext, such applications involve high-dimensional sparse\ndata, that cannot be processed without sparsity-related optimizations due to\nprohibitively large memory requirements.\n  Since matrix multiplication is central in ML algorithms, we propose MPC\nalgorithms to multiply secret sparse matrices. On the one hand, our algorithms\navoid the memory issues of the \"dense\" data representation of classic secure\nmatrix multiplication algorithms. On the other hand, our algorithms can\nsignificantly reduce communication costs (some experiments show a factor 1000)\nfor realistic problem sizes. We validate our algorithms in two ML applications\nin which existing protocols are impractical.\n  An important question when developing MPC algorithms is what assumptions can\nbe made. In our case, if the number of non-zeros in a row is a sensitive piece\nof information then a short runtime may reveal that the number of non-zeros is\nsmall. Existing approaches make relatively simple assumptions, e.g., that there\nis a universal upper bound to the number of non-zeros in a row. This often\ndoesn't align with statistical reality, in a lot of sparse datasets the amount\nof data per instance satisfies a power law. We propose an approach which allows\nadopting a safe upper bound on the distribution of non-zeros in rows/columns of\nsparse matrices.", "AI": {"tldr": "The paper proposes sparsity-aware MPC algorithms for secure matrix multiplication, addressing memory and communication inefficiencies in sparse ML applications while adapting to realistic data distributions.", "motivation": "Existing MPC frameworks are not optimized for sparse data, which is prevalent in applications like recommender systems and genomics. Dense data representations lead to impractical memory usage, and current assumptions about sparsity patterns (e.g., fixed row non-zero bounds) often contradict real-world statistics (e.g., power laws).", "method": "1) Develops MPC algorithms for sparse matrix multiplication avoiding dense representations. 2) Introduces a safe upper bound for non-zero entries in rows/columns based on power law distributions. 3) Focuses on reducing both memory overhead and communication costs (achieving up to 1000\u00d7 reduction in experiments).", "result": "Validated in two ML applications where existing methods are impractical. Demonstrated significant communication cost improvements and practicality using realistic sparsity patterns. Addressed privacy concerns in runtime leakage for sparse data processing.", "conclusion": "The framework enables efficient and secure processing of high-dimensional sparse data in MPC, bridging a critical gap for privacy-preserving ML applications with realistic sparsity distributions."}}
{"id": "2510.14906", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.14906", "abs": "https://arxiv.org/abs/2510.14906", "authors": ["Zixuan Liu", "Yi Zhao", "Zhuotao Liu", "Qi Li", "Chuanpu Fu", "Guangmeng Zhou", "Ke Xu"], "title": "A Hard-Label Black-Box Evasion Attack against ML-based Malicious Traffic Detection Systems", "comment": null, "summary": "Machine Learning (ML)-based malicious traffic detection is a promising\nsecurity paradigm. It outperforms rule-based traditional detection by\nidentifying various advanced attacks. However, the robustness of these ML\nmodels is largely unexplored, thereby allowing attackers to craft adversarial\ntraffic examples that evade detection. Existing evasion attacks typically rely\non overly restrictive conditions (e.g., encrypted protocols, Tor, or\nspecialized setups), or require detailed prior knowledge of the target (e.g.,\ntraining data and model parameters), which is impractical in realistic\nblack-box scenarios. The feasibility of a hard-label black-box evasion attack\n(i.e., applicable across diverse tasks and protocols without internal target\ninsights) thus remains an open challenge. To this end, we develop\nNetMasquerade, which leverages reinforcement learning (RL) to manipulate attack\nflows to mimic benign traffic and evade detection. Specifically, we establish a\ntailored pre-trained model called Traffic-BERT, utilizing a network-specialized\ntokenizer and an attention mechanism to extract diverse benign traffic\npatterns. Subsequently, we integrate Traffic-BERT into the RL framework,\nallowing NetMasquerade to effectively manipulate malicious packet sequences\nbased on benign traffic patterns with minimal modifications. Experimental\nresults demonstrate that NetMasquerade enables both brute-force and stealthy\nattacks to evade 6 existing detection methods under 80 attack scenarios,\nachieving over 96.65% attack success rate. Notably, it can evade the methods\nthat are either empirically or certifiably robust against existing evasion\nattacks. Finally, NetMasquerade achieves low-latency adversarial traffic\ngeneration, demonstrating its practicality in real-world scenarios.", "AI": {"tldr": "This paper proposes NetMasquerade, an RL-driven adversarial traffic evasion framework using Traffic-BERT that bypasses ML-based security systems with >96% success in 80 scenarios while maintaining operational practicality through real-time traffic generation.", "motivation": "Existing adversarial traffic evasion methods require impractical conditions (encrypted protocols, prior model knowledge) or specialized setups, leaving the feasibility of stealthy black-box attacks across diverse tasks and protocols unaddressed.", "method": "The approach combines (1) Traffic-BERT - a pre-trained network-specialized model with attention mechanisms for benign traffic pattern extraction, and (2) a reinforcement learning framework that manipulates malicious packet sequences to mimic benign traffic patterns with minimal modifications.", "result": "Achieves >96.65% success rate in evading 6 detection systems across 80 scenarios, including those considered robust against existing attacks, with 100% evasion of certifiably robust models and real-time performance (low-latency generation).", "conclusion": "NetMasquerade effectively demonstrates a practical hard-label black-box evasion attack framework using reinforcement learning and Traffic-BERT, challenging state-of-the-art robust ML-based traffic detection systems while maintaining real-world operational feasibility through low-latency adversarial traffic generation."}}
