<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 18]
- [cs.SE](#cs.SE) [Total: 22]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Real-World Evaluation of Protocol-Compliant Denial-of-Service Attacks on C-V2X-based Forward Collision Warning Systems](https://arxiv.org/abs/2508.02805)
*Jean Michel Tine,Mohammed Aldeen,Abyad Enan,M Sabbir Salek,Long Cheng,Mashrur Chowdhury*

Main category: cs.CR

TL;DR: This paper evaluates protocol-compliant Denial-of-Service (DoS) attacks on C-V2X systems, demonstrating how UDP flooding and oversized BSMs exploit transport/application-layer vulnerabilities while adhering to 3GPP and SAE standards. Combined attacks disable FCW functionality entirely.


<details>
  <summary>Details</summary>
Motivation: Safety-critical systems like C-V2X rely on strict protocol compliance (3GPP/SAE) for interoperability, but protocol-valid messages at abnormal rates/payload sizes could still disrupt operations, necessitating real-world evaluation.

Method: Authors conducted experiments using a connected vehicle testbed with commercial OBUs, simulating DoS attacks through: (1) High-rate UDP flooding via standardized PC5 sidelinks; (2) Oversized BSM payloads exploiting accepted message parameters yet overwhelming receiver resources.

Result: UDP flooding alone reduced packet delivery ratios by 87% and increased latency beyond 400ms; oversized BSM attacks caused resource overload delaying/suppressing FCW alerts. Simultaneous attacks induced complete communication failure. No protocol rules (3GPP/SAE/IEEE 1609) were violated during these scenarios.

Conclusion: Protocol compliance alone cannot ensure security in C-V2X systems. Attackers can exploit legitimate message specifications (rate/payload size) within standard frameworks to disrupt safety applications, highlighting the need for additional security mechanisms beyond current protocols.

Abstract: Cellular Vehicle-to-Everything (C-V2X) technology enables low-latency,
reliable communications essential for safety applications such as a Forward
Collision Warning (FCW) system. C-V2X deployments operate under strict protocol
compliance with the 3rd Generation Partnership Project (3GPP) and the Society
of Automotive Engineers Standard (SAE) J2735 specifications to ensure
interoperability. This paper presents a real-world testbed evaluation of
protocol-compliant Denial-of-Service (DoS) attacks using User Datagram Protocol
(UDP) flooding and oversized Basic Safety Message (BSM) attacks that 7 exploit
transport- and application-layer vulnerabilities in C-V2X. The attacks
presented in this study transmit valid messages over standard PC5 sidelinks,
fully adhering to 3GPP and SAE J2735 specifications, but at abnormally high
rates and with oversized payloads that overload the receiver resources without
breaching any protocol rules such as IEEE 1609. Using a real-world connected
vehicle 11 testbed with commercially available On-Board Units (OBUs), we
demonstrate that high-rate UDP flooding and oversized payload of BSM flooding
can severely degrade FCW performance. Results show that UDP flooding alone
reduces packet delivery ratio by up to 87% and increases latency to over 400ms,
while oversized BSM floods overload receiver processing resources, delaying or
completely suppressing FCW alerts. When UDP and BSM attacks are executed
simultaneously, they cause near-total communication failure, preventing FCW
warnings entirely. These findings reveal that protocol-compliant communications
do not necessarily guarantee safe or reliable operation of C-V2X-based safety
applications.

</details>


### [2] [Thermal-Aware 3D Design for Side-Channel Information Leakage](https://arxiv.org/abs/2508.02816)
*Dylan Stow,Russell Barnes,Eren Kurshan,Yuan Xie*

Main category: cs.CR

TL;DR: The paper introduces a novel approach combining 3D integration and dynamic activity pattern generation to reduce thermal side-channel attacks by minimizing SVF and STSF.


<details>
  <summary>Details</summary>
Motivation: Thermal side-channel attacks pose significant security risks by exposing on-chip activities and encryption keys, necessitating proactive concealment strategies.

Method: 1) Utilizes inherent 3D integration characteristics for physical-level protection. 2) Dynamically generates activity patterns in functional layers to match the target activity's thermal signature, enabling stealth during critical operations.

Result: Experimental validation achieved SVF <0.05 and STSF <0.59, demonstrating effective mitigation of thermal side-channel vulnerabilities.

Conclusion: The proposed 3D technology-integrated runtime algorithm achieves robust countermeasures against thermal side-channel attacks while maintaining low power overhead.

Abstract: Side-channel attacks are important security challenges as they reveal
sensitive information about on-chip activities. Among such attacks, the thermal
side-channel has been shown to disclose the activities of key functional blocks
and even encryption keys. This paper proposes a novel approach to proactively
conceal critical activities in the functional layers while minimizing the power
dissipation by (i) leveraging inherent characteristics of 3D integration to
protect from side-channel attacks and (ii) dynamically generating custom
activity patterns to match the activity to be concealed in the functional
layers. Experimental analysis shows that 3D technology combined with the
proposed run-time algorithm effectively reduces the Side channel vulnerability
Factor (SVF) below 0.05 and the Spatial Thermal Side-channel Factor (STSF)
below 0.59.

</details>


### [3] [Agentic Privacy-Preserving Machine Learning](https://arxiv.org/abs/2508.02836)
*Mengyu Zhang,Zhuotao Liu,Jingwen Huang,Xuanqi Liu*

Main category: cs.CR

TL;DR: Agentic-PPML framework addresses inefficiency in privacy-preserving LLMs by separating intent understanding from secure inference.


<details>
  <summary>Details</summary>
Motivation: Current PPML methods for large language models are impractically slow (10000x slowdown) and become worse with increasing context lengths, limiting their real-world deployment.

Method: 1. Uses a general-purpose LLM for intent parsing (non-sensitive task). 2. Delegates encrypted inference to specialized security-focused models trained on specific domains. 3. Modular approach avoids processing encrypted data in primary LLMs.

Result: Framework eliminates the need for LLMs to process encrypted prompts, making privacy-preserving LLM services deployable with reasonable performance metrics.

Conclusion: Agentic-PPML provides a practical, efficient solution for confidential LLM inference by splitting processing tasks between models optimized for different purposes, enabling real-world privacy-preserving AI applications.

Abstract: Privacy-preserving machine learning (PPML) is critical to ensure data privacy
in AI. Over the past few years, the community has proposed a wide range of
provably secure PPML schemes that rely on various cryptography primitives.
However, when it comes to large language models (LLMs) with billions of
parameters, the efficiency of PPML is everything but acceptable. For instance,
the state-of-the-art solution for confidential LLM inference represents at
least 10,000-fold slower performance compared to plaintext inference. The
performance gap is even larger when the context length increases. In this
position paper, we propose a novel framework named Agentic-PPML to make PPML in
LLMs practical. Our key insight is to employ a general-purpose LLM for intent
understanding and delegate cryptographically secure inference to specialized
models trained on vertical domains. By modularly separating language intent
parsing - which typically involves little or no sensitive information - from
privacy-critical computation, Agentic-PPML completely eliminates the need for
the LLMs to process the encrypted prompts, enabling practical deployment of
privacy-preserving LLM-centric services.

</details>


### [4] [LMDG: Advancing Lateral Movement Detection Through High-Fidelity Dataset Generation](https://arxiv.org/abs/2508.02942)
*Anas Mabrouk,Mohamed Hatem,Mohammad Mamun,Sherif Saad*

Main category: cs.CR

TL;DR: LMDG is a novel framework for generating high-fidelity lateral movement datasets with automated tooling, agent-based Process Tree Labeling for fine-grained attack tracking, and comprehensive 944GB logs from a 25-VM environment containing realistic multi-step attacks.


<details>
  <summary>Details</summary>
Motivation: Enterprise security systems require realistic datasets for effective lateral movement detection research, but such datasets remain scarce due to complexities of manual labeling and attack simulation.

Method: The LMDG framework combines automated benign activity generation, multi-stage attack execution, and Process Tree Labeling - a novel agent-based technique that systematically traces malicious activity origins while correlating log entries to specific MITRE ATT&CK TTPs.

Result: A 25-day dataset from a 25-VM enterprise environment with 35 multi-stage LM attacks, 944GB of logs containing <1% malicious events, and fine-grained attack phase labeling across diverse attack patterns.

Conclusion: LMDG advances LM detection by creating more realistic, accurate, and analyzable datasets than existing approaches, particularly improving fine-grained labeling of multi-step attacks with precise TTP correlations.

Abstract: Lateral Movement (LM) attacks continue to pose a significant threat to
enterprise security, enabling adversaries to stealthily compromise critical
assets. However, the development and evaluation of LM detection systems are
impeded by the absence of realistic, well-labeled datasets. To address this
gap, we propose LMDG, a reproducible and extensible framework for generating
high-fidelity LM datasets. LMDG automates benign activity generation,
multi-stage attack execution, and comprehensive labeling of system and network
logs, dramatically reducing manual effort and enabling scalable dataset
creation. A central contribution of LMDG is Process Tree Labeling, a novel
agent-based technique that traces all malicious activity back to its origin
with high precision. Unlike prior methods such as Injection Timing or
Behavioral Profiling, Process Tree Labeling enables accurate, step-wise
labeling of malicious log entries, correlating each with a specific attack step
and MITRE ATT\&CK TTPs. To our knowledge, this is the first approach to support
fine-grained labeling of multi-step attacks, providing critical context for
detection models such as attack path reconstruction. We used LMDG to generate a
25-day dataset within a 25-VM enterprise environment containing 22 user
accounts. The dataset includes 944 GB of host and network logs and embeds 35
multi-stage LM attacks, with malicious events comprising less than 1% of total
activity, reflecting a realistic benign-to-malicious ratio for evaluating
detection systems. LMDG-generated datasets improve upon existing ones by
offering diverse LM attacks, up-to-date attack patterns, longer attack
timeframes, comprehensive data sources, realistic network architectures, and
more accurate labeling.

</details>


### [5] [A Non-leveled and Reliable Approximate FHE Framework through Binarized Polynomial Rings](https://arxiv.org/abs/2508.02943)
*Baigang Chen,Dongfang Zhao*

Main category: cs.CR

TL;DR: The paper introduces a binary variant of the CKKS fully homomorphic encryption (FHE) scheme. By replacing modulus switching with lightweight bootstrapping and integrating BCH error-correcting codes, it addresses CKKS's inefficiencies (noise growth, complex parameter tuning) while maintaining approximate arithmetic capabilities. The implementation on HElib demonstrates practicality and scalability for machine-learning and numerical workloads.


<details>
  <summary>Details</summary>
Motivation: CKKS-based FHE suffers from rapid noise growth, computationally expensive modulus switching, and intricate parameter tuning, limiting its efficiency and practicality for applications requiring approximate computations (e.g., machine learning and numerical analysis).

Method: Proposes a binary CKKS variant that 1) uses binary-coefficient polynomial rings 2) replaces modulus switching with a lightweight bootstrapping mechanism 3) integrates BCH error-correcting codes to mitigate bit-flip errors from binary encoding. The implementation retains CKKS's algebraic structure while enabling efficient small-ring operations.

Result: Empirical evaluations confirm the framework matches CKKS's functionality for approximate arithmetic while enabling unbounded-depth computations in small rings. The integration of BCH codes ensures robust decryption, and the HElib-based implementation validates its practicality and scalability across diverse settings.

Conclusion: This binary CKKS variant provides a more efficient, parameter-flexible, and scalable FHE solution for workloads prioritizing approximate numerical computations, with BCH codes resolving error tradeoffs inherent to binary encoding.

Abstract: Homomorphic encryption (HE) enables secure computation on encrypted data,
safeguarding user privacy in domains such as cloud computing, healthcare, and
finance. Among fully homomorphic encryption (FHE) schemes, CKKS is notable for
supporting approximate arithmetic over complex numbers, a key requirement for
machine-learning and numerical workloads. However, CKKS incurs rapid noise
growth, complex parameter tuning, and relies on costly modulus switching. We
propose a binary variant of CKKS that operates entirely over binary-coefficient
polynomial rings and replaces rescaling with a lightweight bootstrapping
mechanism. To mitigate additional bit-flip errors introduced by binary
encoding, we integrate BCH error-correcting codes for robust decryption. Our
open-source implementation, built on the HElib library, preserves the core
algebraic structure of CKKS while introducing binary-coefficient encoding,
enabling efficient evaluation in small ring dimensions and unbounded-depth
computation. Empirical evaluations demonstrate the framework's practicality and
scalability across a range of settings.

</details>


### [6] [Lightweight Fault Detection Architecture for NTT on FPGA](https://arxiv.org/abs/2508.03062)
*Rourab Paul,Paresh Baidya,Krishnendu Guha*

Main category: cs.CR

TL;DR: This paper proposes a fault detection framework for hardware implementations of PQC algorithms, combining recomputation-based methods (REMO) and memory rule checkers optimized for FPGAs, achieving high efficiency with minimal resource usage.


<details>
  <summary>Details</summary>
Motivation: Post-Quantum Cryptographic algorithms remain vulnerable to hardware faults and intentional attacks (e.g., side-channel) in future network security processors, necessitating lightweight and efficient fault detection techniques.

Method: The authors implement a recomputation-based fault detection module (REMO) using Montgomery Reduction for CT-BU logic blocks and Memory Rule Checkers for memory components within the NTT on an FPGA.

Result: The framework achieved 87.2%-100% fault coverage for REMO and 50.7%-100% for memory checkers. On Artix-7 FPGA, it consumes only 16 slices, one DSP block, and 3mW of power.

Conclusion: The proposed fault detection framework establishes a new benchmark for PQC hardware with exceptional efficiency, low implementation cost, and adaptability across word sizes, fault parameters, and injection modes.

Abstract: Post-Quantum Cryptographic (PQC) algorithms are mathematically secure and
resistant to quantum attacks but can still leak sensitive information in
hardware implementations due to natural faults or intentional fault injections.
The intent fault injection in side-channel attacks reduces the reliability of
crypto implementation in future generation network security procesors. In this
regard, this research proposes a lightweight, efficient, recomputation-based
fault detection module implemented on a Field Programmable Gate Array (FPGA)
for Number Theoretic Transform (NTT). The NTT is primarily composed of memory
units and the Cooley-Tukey Butterfly Unit (CT-BU), a critical and
computationally intensive hardware component essential for polynomial
multiplication. NTT and polynomial multiplication are fundamental building
blocks in many PQC algorithms, including Kyber, NTRU, Ring-LWE, and others. In
this paper, we present a fault detection method called : Recomputation with a
Modular Offset (REMO) for the logic blocks of the CT-BU using Montgomery
Reduction and another method called Memory Rule Checkers for the memory
components used within the NTT. The proposed fault detection framework sets a
new benchmark by achieving high efficiency with significant low implementation
cost. It occupies only 16 slices and a single DSP block, with a power
consumption of just 3mW in Artix-7 FPGA. The REMO-based detection mechanism
achieves a fault coverage of 87.2% to 100%, adaptable across various word
sizes, fault bit counts, and fault injection modes. Similarly, the Memory Rule
Checkers demonstrate robust performance, achieving 50.7% to 100% fault
detection depending on and the nature of injected faults.

</details>


### [7] [Untraceable DeepFakes via Traceable Fingerprint Elimination](https://arxiv.org/abs/2508.03067)
*Jiewei Lai,Lan Zhang,Chen Tang,Pengcheng Sun,Xinming Wang,Yunhao Wang*

Main category: cs.CR

TL;DR: Introduces a multiplicative black-box attack that universally eliminates traces from 9 GMs, achieving 97.08% average ASR against 6 advanced AMs and remaining effective beyond 72.39% even with defenses.


<details>
  <summary>Details</summary>
Motivation: Current attribution attacks on DeepFakes leave residual traces vulnerable to defensive mitigation, necessitating stronger universal attack methods to stress-test forensic model robustness.

Method: Developed a universal, model-agnostic attack training an adversarial discriminator using only pure real data to eliminate generative model fingerprints via multiplicative signal modification.

Result: 97.08% mean attack success rate across 6 AMs on 9 GMs with 0.8% sample distortion, maintaining 72.39%+ effectiveness against defense-enhanced AMs in cross-architecture testing.

Conclusion: Multiplicative attacks fundamentally undermine existing attribution frameworks, highlighting critical vulnerabilities and urging development of higher-order forensic defense mechanisms against data space manipulations.

Abstract: Recent advancements in DeepFakes attribution technologies have significantly
enhanced forensic capabilities, enabling the extraction of traces left by
generative models (GMs) in images, making DeepFakes traceable back to their
source GMs. Meanwhile, several attacks have attempted to evade attribution
models (AMs) for exploring their limitations, calling for more robust AMs.
However, existing attacks fail to eliminate GMs' traces, thus can be mitigated
by defensive measures. In this paper, we identify that untraceable DeepFakes
can be achieved through a multiplicative attack, which can fundamentally
eliminate GMs' traces, thereby evading AMs even enhanced with defensive
measures. We design a universal and black-box attack method that trains an
adversarial model solely using real data, applicable for various GMs and
agnostic to AMs. Experimental results demonstrate the outstanding attack
capability and universal applicability of our method, achieving an average
attack success rate (ASR) of 97.08\% against 6 advanced AMs on DeepFakes
generated by 9 GMs. Even in the presence of defensive mechanisms, our method
maintains an ASR exceeding 72.39\%. Our work underscores the potential
challenges posed by multiplicative attacks and highlights the need for more
robust AMs.

</details>


### [8] [VFLAIR-LLM: A Comprehensive Framework and Benchmark for Split Learning of LLMs](https://arxiv.org/abs/2508.03097)
*Zixuan Gu,Qiufeng Fan,Long Sun,Yang Liu,Xiaojun Ye*

Main category: cs.CR

TL;DR: VFLAIR-LLM is an extensible, lightweight split learning framework addressing privacy concerns and computational costs in adapting LLMs to private domains through resource-efficient methods, benchmarking 5 attacks/9 defenses to guide real-world implementation choices.


<details>
  <summary>Details</summary>
Motivation: LLMs' deployment faces challenges: privacy limitations with APIs, high computational costs for private setups, and difficulty in securely adapting models under constrained local resources.

Method: Developed VFLAIR-LLM using Split Learning (SL), providing two model partition settings and standard modules for attacks/defenses. Evaluated three task types and 18 datasets while benchmarking 5 attacks and 9 defenses across SL-LLM configurations.

Result: The framework enables privacy-preserving LLM inference and fine-tuning in resource-constrained environments, with analysis of attack-defense effectiveness across different partition settings yielding actionable insights.

Conclusion: VFLAIR-LLM offers a practical solution for secure, resource-efficient LLM adaptation. The paper provides concrete recommendations on model partitioning, defense strategies, and hyperparameters based on comprehensive benchmarking under diverse SL-LLM scenarios.

Abstract: With the advancement of Large Language Models (LLMs), LLM applications have
expanded into a growing number of fields. However, users with data privacy
concerns face limitations in directly utilizing LLM APIs, while private
deployments incur significant computational demands. This creates a substantial
challenge in achieving secure LLM adaptation under constrained local resources.
To address this issue, collaborative learning methods, such as Split Learning
(SL), offer a resource-efficient and privacy-preserving solution for adapting
LLMs to private domains. In this study, we introduce VFLAIR-LLM (available at
https://github.com/FLAIR-THU/VFLAIR-LLM), an extensible and lightweight split
learning framework for LLMs, enabling privacy-preserving LLM inference and
fine-tuning in resource-constrained environments. Our library provides two LLM
partition settings, supporting three task types and 18 datasets. In addition,
we provide standard modules for implementing and evaluating attacks and
defenses. We benchmark 5 attacks and 9 defenses under various Split Learning
for LLM(SL-LLM) settings, offering concrete insights and recommendations on the
choice of model partition configurations, defense strategies, and relevant
hyperparameters for real-world applications.

</details>


### [9] [Attack the Messages, Not the Agents: A Multi-round Adaptive Stealthy Tampering Framework for LLM-MAS](https://arxiv.org/abs/2508.03125)
*Bingyu Yan,Ziyi Zhou,Xiaoming Zhang,Chaozhuo Li,Ruilin Zeng,Yirui Qi,Tianbo Wang,Litian Zhang*

Main category: cs.CR

TL;DR: This paper proposes MAST, a framework targeting communication vulnerabilities in multi-agent language models (LLM-MAS) by generating adaptive, multi-round tampering strategies using Monte Carlo Tree Search and Direct Preference Optimization, while maintaining stealth through semantic/embedding similarity constraints.


<details>
  <summary>Details</summary>
Motivation: Current LLM-MAS attack methods lack effectiveness, adaptability, and stealth by either compromising internal agent components or relying on direct persuasion, creating security gaps in communication-dependent systems.

Method: MAST trains an attack policy via Monte Carlo Tree Search (for strategic planning) and Direct Preference Optimization (for response generation), incorporating dual constraints on semantic meaning and embedding representations to ensure tampered communication remains undetectable.

Result: Experiments show MAST achieves high success rates across diverse tasks and LLM architectures, demonstrating significant improvements in stealthiness compared to baseline attack methods while maintaining adaptability.

Conclusion: MAST reveals critical security challenges in LLM-MAS communication mechanisms, emphasizing the necessity for robust safeguards against adaptive, stealthy tampering strategies in multi-agent collaboration frameworks.

Abstract: Large language model-based multi-agent systems (LLM-MAS) effectively
accomplish complex and dynamic tasks through inter-agent communication, but
this reliance introduces substantial safety vulnerabilities. Existing attack
methods targeting LLM-MAS either compromise agent internals or rely on direct
and overt persuasion, which limit their effectiveness, adaptability, and
stealthiness. In this paper, we propose MAST, a Multi-round Adaptive Stealthy
Tampering framework designed to exploit communication vulnerabilities within
the system. MAST integrates Monte Carlo Tree Search with Direct Preference
Optimization to train an attack policy model that adaptively generates
effective multi-round tampering strategies. Furthermore, to preserve
stealthiness, we impose dual semantic and embedding similarity constraints
during the tampering process. Comprehensive experiments across diverse tasks,
communication architectures, and LLMs demonstrate that MAST consistently
achieves high attack success rates while significantly enhancing stealthiness
compared to baselines. These findings highlight the effectiveness,
stealthiness, and adaptability of MAST, underscoring the need for robust
communication safeguards in LLM-MAS.

</details>


### [10] [Protecting Small Organizations from AI Bots with Logrip: Hierarchical IP Hashing](https://arxiv.org/abs/2508.03130)
*Rama Carl Hoetzlein*

Main category: cs.CR

TL;DR: This paper proposes a bot detection method using data visualization and hierarchical IP hashing to help small organizations mitigate performance degradation caused by automated web crawlers and AI bots.


<details>
  <summary>Details</summary>
Motivation: Traditional bot throttling techniques are ineffective against modern bots due to their evolving nature and sheer volume, causing significant server strain for small organizations and self-hosted servers. Improved filtering mechanisms are needed to preserve performance without blocking legitimate human users.

Method: The approach combines data visualization with hierarchical IP hashing of server event logs, aggregating IP activity across subnet classes and analyzing statistical patterns to distinguish human users from coordinated bot activity, including distributed crawling attacks.

Result: Real-world testing revealed 80-95% of traffic was from AI crawlers. The method successfully detected bot activity patterns that conventional tools missed, enabling effective regulation of automated traffic while maintaining public access.

Conclusion: The proposed technique provides an accessible solution for resource-constrained organizations to identify and manage bot traffic through pattern analysis, addressing the growing challenge of bot-related server performance degradation.

Abstract: Small organizations, start ups, and self-hosted servers face increasing
strain from automated web crawlers and AI bots, whose online presence has
increased dramatically in the past few years. Modern bots evade traditional
throttling and can degrade server performance through sheer volume even when
they are well-behaved. We introduce a novel security approach that leverages
data visualization and hierarchical IP hashing to analyze server event logs,
distinguishing human users from automated entities based on access patterns. By
aggregating IP activity across subnet classes and applying statistical
measures, our method detects coordinated bot activity and distributed crawling
attacks that conventional tools fail to identify. Using a real world example we
estimate that 80 to 95 percent of traffic originates from AI crawlers,
underscoring the need for improved filtering mechanisms. Our approach enables
small organizations to regulate automated traffic effectively, preserving
public access while mitigating performance degradation.

</details>


### [11] [WiFinger: Fingerprinting Noisy IoT Event Traffic Using Packet-level Sequence Matching](https://arxiv.org/abs/2508.03151)
*Ronghua Li,Shinan Liu,Haibo Hu,Qingqing Ye,Nick Feamster*

Main category: cs.CR

TL;DR: WiFinger is a novel fine-grained multi-IoT event fingerprinting approach designed to track IoT device usage in noisy Wi-Fi environments. It achieves an average recall of 85% with minimal false positives, significantly outperforming existing ML-based traffic analysis methods that struggle with wireless noise.


<details>
  <summary>Details</summary>
Motivation: Traditional ML-based privacy inference attacks on IoT traffic fail in wireless (especially Wi-Fi) settings due to environmental noise, packet losses, and limitations in handling concurrent multi-event tracking.

Method: WiFinger transforms traffic pattern classification into a subsequence matching problem, using novel algorithms to reduce time complexity while maintaining accuracy through robust handling of noisy and fragmented Wi-Fi data streams.

Result: Experiments showed WiFinger achieved 85% average recall for IoT event detection (vs. 0.49% and 0.46% from prior methods) while maintaining nearly zero false positives for most events, demonstrating superior performance in multi-event tracking scenarios with wireless impairments.

Conclusion: WiFinger provides a critical improvement in IoT traffic analysis for privacy attacks by addressing wireless-specific challenges through its subsequence matching framework, enabling accurate simultaneous tracking of multiple events in noisy Wi-Fi environments.

Abstract: IoT environments such as smart homes are susceptible to privacy inference
attacks, where attackers can analyze patterns of encrypted network traffic to
infer the state of devices and even the activities of people. While most
existing attacks exploit ML techniques for discovering such traffic patterns,
they underperform on wireless traffic, especially Wi-Fi, due to its heavy noise
and packet losses of wireless sniffing. In addition, these approaches commonly
target at distinguishing chunked IoT event traffic samples, and they failed at
effectively tracking multiple events simultaneously. In this work, we propose
WiFinger, a fine-grained multi-IoT event fingerprinting approach against noisy
traffic. WiFinger turns the traffic pattern classification task into a
subsequence matching problem and introduces novel techniques to account for the
high time complexity while maintaining high accuracy. Experiments demonstrate
that our method outperforms existing approaches on Wi-Fi traffic, achieving an
average recall of 85% (vs. 0.49% and 0.46%) for various IoT events while
maintaining almost zero false positives for most of them.

</details>


### [12] [BadBlocks: Low-Cost and Stealthy Backdoor Attacks Tailored for Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.03221)
*Yu Pan,Jiahao Chen,Lin Wang,Bingrong Dai,Yi Du*

Main category: cs.CR

TL;DR: The paper introduces BadBlocks, a lightweight and covert backdoor attack in diffusion models that requires minimal computational resources (30% less than existing methods) and GPU time (20% less), achieves high attack success rate (ASR), evades advanced defenses, and highlights vulnerabilities in UNet architectures without compromising overall functionality.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for more efficient and stealthy backdoor attacks against diffusion models, as existing defense methods (visual inspection, neural network-based detection) have become increasingly effective at mitigating traditional backdoor threats with higher resource requirements.

Method: BadBlocks selectively contaminates specific blocks within the UNet architecture of diffusion models, avoiding full-network fine-tuning. It leverages limited computational resources and GPU time to inject backdoors covertly, maintaining normal functionality in non-contaminated components while enabling attackers to manipulate outputs through triggers like visual patterns or textual phrases.

Result: 坏块实验结果表明其攻击成功率高 (ASR)，感知质量损失低 (FID Score)。Even under constrained resources, BadBlocks successfully evades detection by advanced defense frameworks, particularly attention-based backdoor detection methods, while maintaining normal diffusion model functionality through targeted block contamination.

Conclusion: The paper identifies BadBlocks as a novel backdoor threat that significantly lowers the barrier for conducting backdoor attacks on diffusion models by requiring fewer computational resources and GPU time. It highlights architectural vulnerabilities in UNet-based models and underscores the need for improved defense strategies against such stealthy, localized attacks.

Abstract: In recent years,Diffusion models have achieved remarkable progress in the
field of image generation.However,recent studies have shown that diffusion
models are susceptible to backdoor attacks,in which attackers can manipulate
the output by injecting covert triggers such as specific visual patterns or
textual phrases into the training dataset.Fortunately,with the continuous
advancement of defense techniques,defenders have become increasingly capable of
identifying and mitigating most backdoor attacks using visual inspection and
neural network-based detection methods.However,in this paper,we identify a
novel type of backdoor threat that is more lightweight and covert than existing
approaches,which we name BadBlocks,requires only about 30\% of the
computational resources and 20\% GPU time typically needed by previous backdoor
attacks,yet it successfully injects backdoors and evades the most advanced
defense frameworks.BadBlocks enables attackers to selectively contaminate
specific blocks within the UNet architecture of diffusion models while
maintaining normal functionality in the remaining components.Experimental
results demonstrate that BadBlocks achieves a high attack success rate (ASR)
and low perceptual quality loss (as measured by FID Score),even under extremely
constrained computational resources and GPU time.Moreover,BadBlocks is able to
bypass existing defense frameworks,especially the attention-based backdoor
detection method, highlighting it as a novel and noteworthy threat.Ablation
studies further demonstrate that effective backdoor injection does not require
fine-tuning the entire network and highlight the pivotal role of certain neural
network layers in backdoor mapping.Overall,BadBlocks significantly reduces the
barrier to conducting backdoor attacks in all aspects.It enables attackers to
inject backdoors into large-scale diffusion models even using consumer-grade
GPUs.

</details>


### [13] [BDFirewall: Towards Effective and Expeditiously Black-Box Backdoor Defense in MLaaS](https://arxiv.org/abs/2508.03307)
*Ye Li,Chengcheng Zhu,Yanchao Zhao,Jiale Zhang*

Main category: cs.CR

TL;DR: This paper proposes BDFirewall, a progressive defense framework against backdoor attacks in MLaaS black-box scenarios. It classifies triggers as high-, semi-, and low-visibility, and removes them through local purification, denoising, and noise injection with DDPM, without model access. The method achieves significant performance gains over existing defenses.


<details>
  <summary>Details</summary>
Motivation: Current backdoor countermeasures struggle in black-box MLaaS settings where models are inaccessible. Existing approaches may fail to address subtle low-visibility triggers while effectively handling obvious high-visibility ones, leaving gaps in defense comprehensiveness and practicality.

Method: BDFirewall employs a three-stage process: 1) Removes high-visibility triggers (HVT) via local purification and adversarial restoration 2) Attacks semi-visibility triggers (SVT) using feature denoising with benign features as 'noise' 3) Neutralizes low-visibility triggers (LVT) through strategic noise injection followed by DDPM-based restoration. All steps work without model access.

Result: Comprehensive experiments show BDFirewall outperforms state-of-the-art defenses by reducing Attack Success Rate by 33.25% on average, improving poisoned sample accuracy by 29.64%, and achieving up to 111x faster inference compared to baseline methods.

Conclusion: BDFirewall provides a novel, accessible approach to backdoor defense in MLaaS through progressive trigger removal categorized by visibility characteristics. The framework's model-agnostic design and demonstrated performance improvements establish it as a significant advancement in black-box security defense.

Abstract: In this paper, we endeavor to address the challenges of backdoor attacks
countermeasures in black-box scenarios, thereby fortifying the security of
inference under MLaaS. We first categorize backdoor triggers from a new
perspective, i.e., their impact on the patched area, and divide them into:
high-visibility triggers (HVT), semi-visibility triggers (SVT), and
low-visibility triggers (LVT). Based on this classification, we propose a
progressive defense framework, BDFirewall, that removes these triggers from the
most conspicuous to the most subtle, without requiring model access. First, for
HVTs, which create the most significant local semantic distortions, we identify
and eliminate them by detecting these salient differences. We then restore the
patched area to mitigate the adverse impact of such removal process. The
localized purification designed for HVTs is, however, ineffective against SVTs,
which globally perturb benign features. We therefore model an SVT-poisoned
input as a mixture of a trigger and benign features, where we unconventionally
treat the benign features as "noise". This formulation allows us to reconstruct
SVTs by applying a denoising process that removes these benign "noise"
features. The SVT-free input is then obtained by subtracting the reconstructed
trigger. Finally, to neutralize the nearly imperceptible but fragile LVTs, we
introduce lightweight noise to disrupt the trigger pattern and then apply DDPM
to restore any collateral impact on clean features. Comprehensive experiments
demonstrate that our method outperforms state-of-the-art defenses. Compared
with baselines, BDFirewall reduces the Attack Success Rate (ASR) by an average
of 33.25%, improving poisoned sample accuracy (PA) by 29.64%, and achieving up
to a 111x speedup in inference time. Code will be made publicly available upon
acceptance.

</details>


### [14] [From Legacy to Standard: LLM-Assisted Transformation of Cybersecurity Playbooks into CACAO Format](https://arxiv.org/abs/2508.03342)
*Mehdi Akbari Gurabi,Lasse Nitz,Radu-Mihai Castravet,Roman Matzutt,Avikarsha Mandal,Stefan Decker*

Main category: cs.CR

TL;DR: This paper proposes using LLMs with prompt engineering to automate the translation of legacy cybersecurity playbooks into CACAO format, improving syntactic and semantic accuracy through a modular pipeline and iterative refinement.


<details>
  <summary>Details</summary>
Motivation: The existing playbooks' heterogeneous, non-machine-readable formats hinder automation and interoperability in Security Orchestration, Automation, and Response platforms, necessitating a standardized transformation approach.

Method: The authors design prompts to enhance syntactic accuracy and control flow preservation, implement a syntax checker, and employ an iterative refinement mechanism within a modular pipeline.

Result: Evaluation on a custom dataset shows significant improvement in transformation accuracy, effective capture of complex workflows, and substantial error reduction compared to baseline models.

Conclusion: The approach demonstrates practical potential for automating cybersecurity playbook transformation, leveraging LLMs and prompt engineering to achieve reliable and semantically faithful results.

Abstract: Existing cybersecurity playbooks are often written in heterogeneous,
non-machine-readable formats, which limits their automation and
interoperability across Security Orchestration, Automation, and Response
platforms. This paper explores the suitability of Large Language Models,
combined with Prompt Engineering, to automatically translate legacy incident
response playbooks into the standardized, machine-readable CACAO format. We
systematically examine various Prompt Engineering techniques and carefully
design prompts aimed at maximizing syntactic accuracy and semantic fidelity for
control flow preservation. Our modular transformation pipeline integrates a
syntax checker to ensure syntactic correctness and features an iterative
refinement mechanism that progressively reduces syntactic errors. We evaluate
the proposed approach on a custom-generated dataset comprising diverse legacy
playbooks paired with manually created CACAO references. The results
demonstrate that our method significantly improves the accuracy of playbook
transformation over baseline models, effectively captures complex workflow
structures, and substantially reduces errors. It highlights the potential for
practical deployment in automated cybersecurity playbook transformation tasks.

</details>


### [15] [Smart Car Privacy: Survey of Attacks and Privacy Issues](https://arxiv.org/abs/2508.03413)
*Akshay Madhav Deshmukh*

Main category: cs.CR

TL;DR: This paper reviews VANET security architectures, analyzes security/privacy attacks, classifies defense mechanisms (e.g. key management, authentication), and discusses privacy implications in connected vehicle networks.


<details>
  <summary>Details</summary>
Motivation: Modern vehicles' wireless connectivity creates security vulnerabilities in Vehicular Ad hoc Networks (VANETs), making robust security mechanisms critical for intelligent transportation system evolution and protecting mobile network participants.

Method: Literature survey and analysis of existing architectures, attack patterns, and defense techniques for vehicular networks with focus on security protocol evaluation and categorization.

Result: Classification framework of security threats (e.g. tampering, eavesdropping) and corresponding countermeasures, including implementation examples of cryptographic solutions and tamper-proof protocols.

Conclusion: Effective security mechanisms through proper architecture design are essential for VANET adoption. Continued research in authentication protocols, intrusion detection, and privacy preserving schemes will ensure safe implementation of connected vehicle technologies.

Abstract: Automobiles are becoming increasingly important in our day to day life.
Modern automobiles are highly computerized and hence potentially vulnerable to
attack. Providing many wireless connectivity for vehicles enables a bridge
between vehicles and their external environments. Such a connected vehicle
solution is expected to be the next frontier for automotive revolution and the
key to the evolution to next generation intelligent transportation systems.
Vehicular Ad hoc Networks (VANETs) are emerging mobile ad hoc network
technologies incorporating mobile routing protocols for inter-vehicle data
communications to support intelligent transportation systems. Thus security and
privacy are the major concerns in VANETs due to the mobility of the vehicles.
Thus designing security mechanisms to remove adversaries from the network
remarkably important in VANETs.
  This paper provides an overview of various vehicular network architectures.
The evolution of security in modern vehicles. Various security and privacy
attacks in VANETs with their defending mechanisms with examples and classify
these mechanisms. It also provides an overview of various privacy implication
that a vehicular network possess.

</details>


### [16] [Unravelling the Probabilistic Forest: Arbitrage in Prediction Markets](https://arxiv.org/abs/2508.03474)
*Oriol Saguillo,Vahid Ghafouri,Lucianna Kiffer,Guillermo Suarez-Tangil*

Main category: cs.CR

TL;DR: This paper analyzes arbitrage opportunities on Polymarket, identifying two distinct forms (Market Rebalancing and Combinatorial Arbitrage) and estimates 40 million USD in realized profits, revealing market inefficiencies despite platform design constraints.


<details>
  <summary>Details</summary>
Motivation: Prediction markets like Polymarket aim to maintain efficient pricing through exhaustive and mutually exclusive condition sets. However, observed mispricing in dependent assets creates arbitrage opportunities, warranting investigation into: (1) conditions enabling arbitrage, (2) its actual occurrence, and (3) evidence of exploitation by participants.

Method: The study employs empirical on-chain arbitrage analysis of historical Polymarket order books. A heuristic-driven reduction strategy addresses scalability challenges by integrating timeliness, topical similarity, and combinatorial relationships metrics, validated through expert input.

Result: Identified two arbitrage types: Market Rebalancing Arbitrage (within single markets) and Combinatorial Arbitrage (across multiple markets). On-chain data analysis confirmed 40 million USD in realized arbitrage profits, demonstrating widespread systematic mispricing and active exploitation.

Conclusion: Polymarket's market resolution constraints are systematically breached, enabling profitable arbitrage. The 40 million USD realized profit estimates highlight both platform design limitations and the presence of sophisticated arbitrageurs exploiting inefficiencies through identified strategies. Future market mechanism improvements may be necessary to address these issues.

Abstract: Polymarket is a prediction market platform where users can speculate on
future events by trading shares tied to specific outcomes, known as conditions.
Each market is associated with a set of one or more such conditions. To ensure
proper market resolution, the condition set must be exhaustive -- collectively
accounting for all possible outcomes -- and mutually exclusive -- only one
condition may resolve as true. Thus, the collective prices of all related
outcomes should be \$1, representing a combined probability of 1 of any
outcome. Despite this design, Polymarket exhibits cases where dependent assets
are mispriced, allowing for purchasing (or selling) a certain outcome for less
than (or more than) \$1, guaranteeing profit. This phenomenon, known as
arbitrage, could enable sophisticated participants to exploit such
inconsistencies.
  In this paper, we conduct an empirical arbitrage analysis on Polymarket data
to answer three key questions: (Q1) What conditions give rise to arbitrage (Q2)
Does arbitrage actually occur on Polymarket and (Q3) Has anyone exploited these
opportunities. A major challenge in analyzing arbitrage between related markets
lies in the scalability of comparisons across a large number of markets and
conditions, with a naive analysis requiring $O(2^{n+m})$ comparisons. To
overcome this, we employ a heuristic-driven reduction strategy based on
timeliness, topical similarity, and combinatorial relationships, further
validated by expert input.
  Our study reveals two distinct forms of arbitrage on Polymarket: Market
Rebalancing Arbitrage, which occurs within a single market or condition, and
Combinatorial Arbitrage, which spans across multiple markets. We use on-chain
historical order book data to analyze when these types of arbitrage
opportunities have existed, and when they have been executed by users. We find
a realized estimate of 40 million USD of profit extracted.

</details>


### [17] [Intrusion Detection in Heterogeneous Networks with Domain-Adaptive Multi-Modal Learning](https://arxiv.org/abs/2508.03517)
*Mabin Umman Varghese,Zahra Taghiyarrenani*

Main category: cs.CR

TL;DR: A deep neural model combining multi-modal learning and domain adaptation techniques is developed to enhance NIDS performance in heterogeneous network environments. The model processes sequential cyclic data from diverse sources, demonstrating superior intrusion classification accuracy and generalization capabilities under varying data conditions.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning models struggle with (1) requirement for extensive labeled datasets and (2) inability to handle data/feature heterogeneity across different network domains, limiting their effectiveness for real-world intrusion detection.

Method: The proposed model integrates multi-modal learning with domain adaptation to process data from diverse sources in a sequential cyclic architecture. This enables simultaneous learning from multiple datasets while adapting to varying feature spaces across domains.

Result: Outperforms baseline neural models in intrusion classification accuracy, particularly showing resilience to challenges from imbalanced sample availability and shifting probability distributions across 5+ heterogeneous network datasets.

Conclusion: The sequential cyclic architecture with domain adaptation provides a generalizable framework for NIDS, reducing dependence on domain-specific labeled data while maintaining robust performance across diverse network environments. Suggests this approach addresses critical limitations of current systems.

Abstract: Network Intrusion Detection Systems (NIDS) play a crucial role in
safeguarding network infrastructure against cyberattacks. As the prevalence and
sophistication of these attacks increase, machine learning and deep neural
network approaches have emerged as effective tools for enhancing NIDS
capabilities in detecting malicious activities. However, the effectiveness of
traditional deep neural models is often limited by the need for extensive
labelled datasets and the challenges posed by data and feature heterogeneity
across different network domains. To address these limitations, we developed a
deep neural model that integrates multi-modal learning with domain adaptation
techniques for classification. Our model processes data from diverse sources in
a sequential cyclic manner, allowing it to learn from multiple datasets and
adapt to varying feature spaces. Experimental results demonstrate that our
proposed model significantly outperforms baseline neural models in classifying
network intrusions, particularly under conditions of varying sample
availability and probability distributions. The model's performance highlights
its ability to generalize across heterogeneous datasets, making it an efficient
solution for real-world network intrusion detection.

</details>


### [18] [MalFlows: Context-aware Fusion of Heterogeneous Flow Semantics for Android Malware Detection](https://arxiv.org/abs/2508.03588)
*Zhaoyi Meng,Fenglei Xu,Wenxiang Zhao,Wansen Wang,Wenchao Huang,Jie Cui,Hong Zhong,Yan Xiong*

Main category: cs.CR

TL;DR: MalFlows is a novel Android malware detection technique that combines context-aware fusion of heterogeneous flow semantics (control, data, and ICC) using a heterogeneous information network (HIN) and flow2vec embedding. It achieves better performance than existing methods on a large-scale dataset of 20M flow instances from 31,000 apps.


<details>
  <summary>Details</summary>
Motivation: Existing Android malware detection techniques fail to leverage the semantic complementarity across different flow types (control, data, ICC) and lack context-aware integration of heterogeneous program behaviors. This limits their accuracy in malware profiling.

Method: The authors: 1) Construct an HIN to model relationships between three types of program flows 2) Develop flow2vec, a context-aware embedding approach that uses multiple meta-paths to learn app representations from the HIN 3) Employ a channel-attention-based deep neural network for classification.

Result: Experiments on a 20M+ flow instance dataset showed MalFlows outperformed representative baselines in Android malware detection. flow2vec demonstrated effectiveness in learning accurate app representations from the heterogeneous flow network.

Conclusion: MalFlows is the first context-aware framework to comprehensively integrate heterogeneous flow semantics for Android malware detection, setting a new benchmark using HIN-based representation learning and attention mechanisms.

Abstract: Static analysis, a fundamental technique in Android app examination, enables
the extraction of control flows, data flows, and inter-component communications
(ICCs), all of which are essential for malware detection. However, existing
methods struggle to leverage the semantic complementarity across different
types of flows for representing program behaviors, and their context-unaware
nature further hinders the accuracy of cross-flow semantic integration. We
propose and implement MalFlows, a novel technique that achieves context-aware
fusion of heterogeneous flow semantics for Android malware detection. Our goal
is to leverage complementary strengths of the three types of flow-related
information for precise app profiling. We adopt a heterogeneous information
network (HIN) to model the rich semantics across these program flows. We
further propose flow2vec, a context-aware HIN embedding technique that
distinguishes the semantics of HIN entities as needed based on contextual
constraints across different flows and learns accurate app representations
through the joint use of multiple meta-paths. The representations are finally
fed into a channel-attention-based deep neural network for malware
classification. To the best of our knowledge, this is the first study to
comprehensively aggregate the strengths of diverse flow-related information for
assessing maliciousness within apps. We evaluate MalFlows on a large-scale
dataset comprising over 20 million flow instances extracted from more than
31,000 real-world apps. Experimental results demonstrate that MalFlows
outperforms representative baselines in Android malware detection, and
meanwhile, validate the effectiveness of flow2vec in accurately learning app
representations from the HIN constructed over the heterogeneous flows.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [19] [Blueprint First, Model Second: A Framework for Deterministic LLM Workflow](https://arxiv.org/abs/2508.02721)
*Libin Qiu,Yuhang Ye,Zhirong Gao,Xide Zou,Junfu Chen,Ziming Gui,Weizhi Huang,Xiaobo Xue,Wenkai Qiu,Kun Zhao*

Main category: cs.SE

TL;DR: The Source Code Agent framework addresses non-determinism in LLMs by decoupling workflow logic (defined as source code) from generative model execution. It achieves 10.1pp improvement on tau-bench scores while improving procedural reliability and execution efficiency.


<details>
  <summary>Details</summary>
Motivation: Current LLM agent architectures blend probabilistic planning with deterministic execution, creating non-determinism in workflows requiring strict procedural fidelity and predictable outcomes. This limits their deployment in critical operational environments.

Method: 1. Expert-defined operational procedures are codified into an 'Execution Blueprint' using source code
2. A deterministic engine executes the blueprint
3. LLMs are invoked as input-agnostic tools for specific subtasks with bounded input/output, never altering the workflow path directly

Result: Established new state-of-the-art performance on tau-bench (complex user-tool-rule scenarios) with:
- 10.1 percentage point improvement in average Pass^1 scores
- Significant execution efficiency gains
- Deterministic control of workflow logic despite LLM's probabilistic nature

Conclusion: The 'Blueprint First, Model Second' paradigm enables verifiable deployment of autonomous agents in strict procedural environments by:
1. Isolating LLM use to bounded subtasks
2. Maintaining deterministic workflow execution
3. Separating strategic planning from operational implementation in AI systems

Abstract: While powerful, the inherent non-determinism of large language model (LLM)
agents limits their application in structured operational environments where
procedural fidelity and predictable execution are strict requirements. This
limitation stems from current architectures that conflate probabilistic,
high-level planning with low-level action execution within a single generative
process. To address this, we introduce the Source Code Agent framework, a new
paradigm built on the "Blueprint First, Model Second" philosophy. Our framework
decouples the workflow logic from the generative model. An expert-defined
operational procedure is first codified into a source code-based Execution
Blueprint, which is then executed by a deterministic engine. The LLM is
strategically invoked as a specialized tool to handle bounded, complex
sub-tasks within the workflow, but never to decide the workflow's path. We
conduct a comprehensive evaluation on the challenging tau-bench benchmark,
designed for complex user-tool-rule scenarios. Our results demonstrate that the
Source Code Agent establishes a new state-of-the-art, outperforming the
strongest baseline by 10.1 percentage points on the average Pass^1 score while
dramatically improving execution efficiency. Our work enables the verifiable
and reliable deployment of autonomous agents in applications governed by strict
procedural logic.

</details>


### [20] [Interpreting Performance Profiles with Deep Learning](https://arxiv.org/abs/2508.02729)
*Zhuoran Liu*

Main category: cs.SE

TL;DR: This paper proposes a system combining Java profiling and code summarization via deep learning to ease performance analysis for developers.


<details>
  <summary>Details</summary>
Motivation: Prolific profilers create a burden for engineers needing to interpret complex data and connect inefficiencies to code semantics, especially for unfamiliar codebases.

Method: Async Profiler generates performance data while a fine-tuned CodeBERT-based model extracts code summaries from semantics. Summaries are integrated into a GUI interface for selected call paths.

Result: Significantly reduced effort in identifying actionable optimizations in Java benchmarks through this integrated approach.

Conclusion: Semantic-enriched profiling offers practical benefits for program optimization by making performance-semantic correlations explicit and accessible.

Abstract: Profiling tools (also known as profilers) play an important role in
understanding program performance at runtime, such as hotspots, bottlenecks,
and inefficiencies. While profilers have been proven to be useful, they give
extra burden to software engineers. Software engineers, as the users, are
responsible to interpret the complex performance data and identify actionable
optimization in program source code. However, it can be challenging for users
to associate inefficiencies with the program semantics, especially if the users
are not the authors of the code, which limits the applicability of profilers.
  In this thesis, we explore a new direction to combine performance profiles
and program semantics with a deep learning approach. The key idea is to glean
code summary for semantic information (at a certain level) and integrate it
into a profiler, which can better understand program inefficiencies for
actionable optimization. To be concrete, we combine profiles generated by Async
Profiler (the state-of-the-art Java profiler) with code summarization from a
fine-tuned CodeBERT-based model. We demonstrate the code summaries of any
selected call path in a graphic user interface. Our system can effectively
assist analysis on many Java benchmarks.

</details>


### [21] [A Note on Code Quality Score: LLMs for Maintainable Large Codebases](https://arxiv.org/abs/2508.02732)
*Sherman Wong,Jalaj Bhandari,Leo Zhou Fan Yang,Xylan Xu,Yi Zhuang,Cem Cayiroglu,Payal Bhuptani,Sheela Yadawad,Hung Duong*

Main category: cs.SE

TL;DR: This paper proposes Code Quality Score (CQS) system using two fine-tuned Llama3 models (via SFT and offline RL) to detect code quality issues and provide LLM-generated code critiques, achieving 60% user helpfulness rate in industrial deployment.


<details>
  <summary>Details</summary>
Motivation: Large-scale software systems with concurrent engineering teams face significant code quality maintenance challenges, requiring automated systems for efficient and effective code review.

Method: The CQS system combines two Llama3 models fine-tuned using supervised fine-tuning (SFT) and offline reinforcement learning (RL) for code quality detection and critique generation, enhanced by hand-crafted rules to filter hallucinations.

Result: Offline evaluations show high precision in issue identification, and industrial-scale deployment achieves a 60% week-over-week user helpfulness rate.

Conclusion: The CQS system demonstrates effective code quality monitoring at scale while addressing LLM hallucinations. The paper also shares practical learnings on curating developer feedback for LLM fine-tuning.

Abstract: Maintaining code quality in large-scale software systems presents significant
challenges, particularly in settings where a large numbers of engineers work
concurrently on a codebase. This paper introduces Code Quality Score (CQS)
system to automatically detect issues with a set of code changes and provide
actionable insights. At its core, the CQS system is powered by two Llama3
models, fine-tuned (with SFT and offline RL approaches), to a) detect common
code quality issues related to coding best practices and b) to provide good
``critiques'' for LLM-generated code review respectively. To maintain good user
experience, we layer the system with hand-crafted rules to filter out incorrect
responses/hallucinations. Offline evaluations show that our CQS system is able
to achieve an impressive precision rate for identifying valid issues. This
system has already been rolled out to developers in an industrial scale setting
and has consistently achieved 60\% week over week user helpfulness rate,
demonstrating its effectiveness in a real-world environment. In this paper, we
present details of the CQS system along with some learnings on curating
developer feedback to create training data for LLM fine-tuning.

</details>


### [22] [What's in a Proof? Analyzing Expert Proof-Writing Processes in F* and Verus](https://arxiv.org/abs/2508.02733)
*Rijul Jain,Shraddha Barke,Gabriel Ebner,Md Rakib Hossain Misu,Shan Lu,Sarah Fakhoury*

Main category: cs.SE

TL;DR: This paper analyzes expert proof development strategies in F* and Verus through telemetry data, offering design principles for AI proof assistants and demonstrating improved performance of an F*-based agent.


<details>
  <summary>Details</summary>
Motivation: POPLs' complexity and lack of understanding about expert workflow hinder their adoption and AI tool development; systematic analysis of proof practices is needed to address this gap.

Method: User study using fine-grained telemetry from 8 experts working with F* and Verus, identifying proof strategies and informal practices through data analysis.

Result: Three key strategies for proof reasoning were discovered, along with practices predictive of task success. An F* proof agent implementing these principles outperforms baseline LLMs.

Conclusion: Formal analysis of expert proof workflows reveals actionable design guidance for AI proof assistants, demonstrating that task-specific agent architectures can leverage these insights effectively.

Abstract: Proof-oriented programming languages (POPLs) empower developers to write code
alongside formal correctness proofs, providing formal guarantees that the code
adheres to specified requirements. Despite their powerful capabilities, POPLs
present a steep learning curve and have not yet been adopted by the broader
software community. The lack of understanding about the proof-development
process and how expert proof developers interact with POPLs has hindered the
advancement of effective proof engineering and the development of
proof-synthesis models/tools.
  In this work, we conduct a user study, involving the collection and analysis
of fine-grained source code telemetry from eight experts working with two
languages, F* and Verus. Results reveal interesting trends and patterns about
how experts reason about proofs and key challenges encountered during the proof
development process. We identify three distinct strategies and multiple
informal practices that are not captured final code snapshots, yet are
predictive of task outcomes. We translate these findings into concrete design
guidance for AI proof assistants: bias toward early specification drafting,
explicit sub-goal decomposition, bounded active errors, and disciplined
verifier interaction. We also present a case study of an F* proof agent
grounded in these recommendations, and demonstrate improved performance over
baseline LLMs

</details>


### [23] [Automated Code Repair for C/C++ Static Analysis Alerts](https://arxiv.org/abs/2508.02820)
*David Svoboda,Lori Flynn,William Klieber,Michael Duggan,Nicholas Reimer,Joseph Sible*

Main category: cs.SE

TL;DR: This engineering experience paper presents an APR tool for C/C++ that reduces SA alerts by repairing three flaw categories, demonstrating its effectiveness with over 80% success on average and influencing the CERT Coding Standards.


<details>
  <summary>Details</summary>
Motivation: SA tools generate numerous alerts in C/C++, many of which are false positives, requiring significant manual effort. Automated Program Repair (APR) can alleviate this, making the repair process more efficient and scalable for developers.

Method: A prototype APR tool was designed and developed to address three specific flaw categories from multiple SA tools, using local and simple repair strategies. The approach focuses on trustworthiness and developer acceptability through practical engineering choices.

Result: The APR tool repaired 8718/9234 alerts (94.4% success) for one SA tool/codebase and achieved >80% resolution (repair or dismissal as false positives) on average for two other flaw categories, two SA tools, and two codebases. Repaired code maintained performance comparable to original, except for sqlite3.c, and did not trigger new alerts.

Conclusion: The APR framework effectively reduces SA alerts for target flaw categories, proving APR's potential in developer workflows. However, it is limited to three categories and requires generalization. Contributions include empirical analysis of SA data, a new open-source APR tool, and a public dataset of SA alerts, while lessons emphasize careful repair design to avoid performance trade-offs and the importance of collaboration with standard bodies like CERT for broader impact.

Abstract: (Note: This work is a preprint.) Static analysis (SA) tools produce many
diagnostic alerts indicating that source code in C or C++ may be defective and
potentially vulnerable to security exploits. Many of these alerts are false
positives. Identifying the true-positive alerts and repairing the defects in
the associated code are huge efforts that automated program repair (APR) tools
can help with. Our experience showed us that APR can reduce the number of SA
alerts significantly and reduce the manual effort of analysts to review code.
This engineering experience paper details the application of design,
development, and performance testing to an APR tool we built that repairs C/C++
code associated with 3 categories of alerts produced by multiple SA tools. Its
repairs are simple and local. Furthermore, our findings convinced the
maintainers of the CERT Coding Standards to re-assess and update the metrics
used to assess when violations of guidelines are detectable or repairable. We
discuss engineering design choices made to support goals of trustworthiness and
acceptability to developers. Our APR tool repaired 8718 out of 9234 alerts
produced by one SA tool on one codebase. It can repair 3 flaw categories. For 2
flaw categories, 2 SA tools, and 2 codebases, our tool repaired or dismissed as
false positives over 80% of alerts, on average. Tests showed repairs did not
appreciably degrade the performance of the code or cause new alerts to appear
(with the possible exception of sqlite3.c). This paper describes unique
contributions that include a new empirical analysis of SA data, our selection
method for flaw categories to repair, publication of our APR tool, and a
dataset of SA alerts from open-source SA tools run on open-source codebases. It
discusses positive and negative results and lessons learned.

</details>


### [24] [Automated Validation of LLM-based Evaluators for Software Engineering Artifacts](https://arxiv.org/abs/2508.02827)
*Ora Nova Fandina,Eitan Farchi,Shmulik Froimovich,Rami Katan,Alice Podolsky,Orna Raz,Avi Ziv*

Main category: cs.SE

TL;DR: The paper introduces REFINE, a controllable framework for benchmarking LLM-based code evaluators, enabling nuanced assessment of code generation, translation, and summarization tasks through synthetic datasets and alignment measurement. It demonstrates improved reliability in industrial COBOL applications.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly automated to assess code artifacts, but existing methods can't capture subtle quality variations, and human evaluations are costly and non-scalable.

Method: REFINE uses two modules: Hierarchy Dataset Builder (generating progressively degraded artifacts) and Evaluator Tester (comparing evaluator rankings to expected quality orderings). It allows granularity control for different testing needs.

Result: Applied to IBM's COBOL production workflows, REFINE improved LLM-as-judge alignment scores from <0.7 to >0.9 in some tasks, enabling real-world deployment of nuance-sensitive evaluators for model release decisions.

Conclusion: REFINE provides a practical, scalable solution for improving LLM evaluator reliability through controlled degradation testing, significantly impacting industrial code quality assessment and model development processes.

Abstract: Automation in software engineering increasingly relies on large language
models (LLMs) to generate, review, and assess code artifacts. However,
establishing LLMs as reliable evaluators remains an open challenge: human
evaluations are costly, subjective and non scalable, while existing automated
methods fail to discern fine grained variations in artifact quality.
  We introduce REFINE (Ranking Evaluators for FIne grained Nuanced Evaluation),
an automated framework for benchmarking LLM based evaluators across software
engineering tasks. REFINE comprises of two modules: Hierarchy Dataset Builder
applies novel generation techniques to automatically synthesize artifacts with
progressively reduced quality, and Evaluator Tester quantifies each candidate
evaluator configuration by measuring how closely its rankings align with
expected ordering.
  A key feature of REFINE is controllability: users can tune the granularity of
degradation to progressively refine evaluator configurations, from coarse
filtering to stress testing on subtle quality gaps.
  While the methodology is general, we focus on coding tasks reflecting the
practical demands in our production setting. REFINE was integrated into IBM's
internal development workflows and applied to code generation, translation, and
summarization for COBOL, an enterprise critical programming language, using
industrial data. It was used to identify LLM as a Judge configurations that
lifted alignment scores from below $0.7$ to above $0.9$ in some coding tasks.
These nuance sensitive evaluators are now actively used by model training teams
to support model release decisions.

</details>


### [25] [Developer Perceptions on Utilising Low-Code Approaches to Build Accessible and Adaptive Applications for Seniors](https://arxiv.org/abs/2508.02968)
*Shavindra Wickramathilaka,John Grundy,Kashumi Madampe,Omar Haggag*

Main category: cs.SE

TL;DR: This paper addresses the challenge of developing accessible and adaptive software for seniors by presenting AdaptForge, a low-code MDE tool evaluated through an interview study with 18 software practitioners. It identifies developer expectations and provides design recommendations for such tools.


<details>
  <summary>Details</summary>
Motivation: The global ageing population demands inclusive technologies to enhance seniors' autonomy. Regulatory pressures (e.g., EAA) and developers' personal empathy toward older loved ones create a need for accessible, adaptive software development tools to overcome traditional constraints.

Method: An interview-based empirical study with 18 software practitioners evaluating AdaptForge, a low-code model-driven engineering (MDE) tool that uses automated code generation to reduce development constraints for accessible and adaptive applications.

Result: Identified developers expect tools to align with industry standards while enabling accessibility and adaptation. Empirical insights reveal practical challenges in tool adoption and highlight opportunities for improving low-code platforms to better support inclusive software development.

Conclusion: The paper provides empirically grounded recommendations for designing low-code tools that effectively support accessible and adaptive software development, aligning with both regulatory requirements and aging populations' needs through developer-centered approaches.

Abstract: The global ageing population presents a growing societal challenge, creating
an urgent need for inclusive technologies that promote autonomy among older
adults. Software practitioners can address this by delivering digital services
that enhance seniors' independence and reduce reliance on routine support from
family members and healthcare infrastructure. However, traditional development
practices, constrained by time and resources, often result in applications with
major accessibility and personalisation barriers. Increasing pressure from
regulatory requirements, such as the European Accessibility Act (EAA), and the
personal empathy many developers feel toward supporting their older loved ones
and their own future selves have created a demand for tools that support the
development of accessible and adaptive software. To address this demand, this
paper presents an interview-based empirical study with 18 software
practitioners, evaluating AdaptForge: a low-code model-driven engineering (MDE)
tool that enables the efficient creation of accessible and adaptive
applications for senior users by mitigating development constraints through
automated code generation. Based on these insights, we identify developer
expectations for adopting such tools as industry-standard solutions and provide
empirically grounded recommendations for designing low-code tools that support
accessible and adaptive software development.

</details>


### [26] [MRG-Bench: Evaluating and Exploring the Requirements of Context for Repository-Level Code Generation](https://arxiv.org/abs/2508.02998)
*Haiyang Li*

Main category: cs.SE

TL;DR: The paper introduces MRG-Bench, a comprehensive multi-language code generation benchmark with real-world repositories and runnable test cases, revealing that current techniques struggle with understanding user requirements and exhibit language-specific performance disparities.


<details>
  <summary>Details</summary>
Motivation: Current code generation evaluation datasets lack runnable test cases, deviate from real-world code distribution, and only support Python, leading to unreliable performance assessments.

Method: The authors created MRG-Bench with three features: real-world repository data, support for Python/Java/Go, and project-level runnable test cases. They evaluated LLMs, long-context models, and RAG methods, then conducted error-annotation experiments to analyze failure modes.

Result: Experiments show current techniques have significant performance gaps in repository-level code generation. Error analysis indicates most failures stem from misunderstanding user requirements, with marked differences in challenge severity across languages.

Conclusion: MRG-Bench exposes critical limitations in code generation approaches, demonstrating the need for improved task understanding mechanisms and language-specific contextual engineering due to varying requirements across programming languages.

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
code generation. However, current evaluation datasets suffer from issues such
as the lack of runnable test cases, deviation from the distribution of
real-world code, and the ability to evaluate only the Python language. These
limitations undermine the credibility of the evaluation results.
  To address these limitations, we introduce \textbf{MRG-Bench} (Multi-language
Repository-level Code Generation Benchmark), a novel dataset that provides a
more accurate evaluation of LLMs in practical repository-level code generation
tasks. MRG-Bench has three main features: (1) practical data sourced from
real-world code repositories that align to the practical distribution, (2)
multiple programming languages support, including Python, Java, and Go, and (3)
project-level runnable test cases to assess the quality of the generated code.
  Based on MRG-Bench, we conducted extensive experiments including large
language models, long-context models, and RAG-related methods. These evaluation
results demonstrate that \textbf{current repository-level code generation
techniques suffer from significant performance deficiencies}. To further
investigate why models fail, we designed novel experiments to annotate the
underlying causes of generation errors. The results explicitly show that the
majority of methods suffer from "\textbf{difficulty in understanding user
requirements}," failing to comprehend their assigned tasks accurately.
Moreover, the impact of different repository-level contexts on this issue
exhibits significant disparities across different programming languages,
suggesting that, in practice, specialized contextual information needs to be
designed for different languages.

</details>


### [27] [Tool-integrated Reinforcement Learning for Repo Deep Search](https://arxiv.org/abs/2508.03012)
*Zexiong Ma,Chao Peng,Qunhong Zeng,Pengfei Gao,Yanzhen Zou,Bing Xie*

Main category: cs.SE

TL;DR: ToolTrain enhances LLMs' ability to perform issue localization via a two-stage training framework using repository retrieval tools, achieving state-of-the-art results surpassing Claude-3.7.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based agents face the 'Repo Deep Search' challenge in issue localization due to the semantic gap between natural language issue descriptions and code, requiring complex multi-hop reasoning through code dependencies.

Method: ToolTrain combines rejection-sampled supervised fine-tuning and tool-integrated reinforcement learning in a two-stage framework to improve LLM utilization of retrieval tools for issue localization.

Result: ToolTrain-trained models achieve state-of-the-art performance in issue localization (with 32B model surpassing Claude-3.7 on function-level localization) and demonstrate improved end-to-end issue resolution performance compared to existing methods.

Conclusion: Training LLMs specifically for issue localization using tool-integrated frameworks significantly improves automated software development performance, confirming the viability of this approach.

Abstract: Issue localization, the process of identifying code locations that need
modification to resolve software issues, is a critical yet challenging task in
software development. The semantic gap between natural language issue
descriptions and faulty code requires complex multi-hop reasoning through code
dependencies. Existing LLM-based agents attempt to address this by integrating
repository retrieval tools. However, this transforms issue localization into a
demanding task we call Repo Deep Search, which requires the LLM to effectively
utilize various repository retrieval tools throughout a multi-step reasoning
and navigation process. To tackle this challenge, we present ToolTrain, a
two-stage tool-integrated training framework combining rejection-sampled
supervised fine-tuning and tool-integrated reinforcement learning to enhance
LLMs' ability to use retrieval tools for issue localization. Experimental
results show that ToolTrain-trained models achieve state-of-the-art
performance, with our 32B model even surpassing Claude-3.7 on function-level
localization. The results also show that improved localization performance
translates to better end-to-end issue resolution performance. This further
demonstrates that training for issue localization is a viable and effective
strategy for improving automated software development.

</details>


### [28] [A System Model Generation Benchmark from Natural Language Requirements](https://arxiv.org/abs/2508.03215)
*Dongming Jin,Zhi Jin,Linyu Li,Zheng Fang,Jia Li,Xiaohong Chen*

Main category: cs.SE

TL;DR: This paper introduces SysMBench, a benchmark for evaluating large language models' ability to generate system models, revealing underwhelming performance with max BLEU-4% and SysMEval-F1 62%.


<details>
  <summary>Details</summary>
Motivation: System models are crucial for software development but challenging to create due to domain-specific syntax and lack of public examples, creating a gap in LLM evaluation benchmarks for this task.

Method: Developed 151 human-curated scenarios with natural language requirements and corresponding formal system models. Applied three enhancement strategies to test 17 LLMs using traditional metrics and the proposed semantic-aware SysMEval metric.

Result: All 17 evaluated LLMs demonstrated poor performance on SysMBench, achieving maximum BLEU-4% and SysMEval-F1 score of 62%.

Conclusion: LLMs currently lack robust capabilities for system model generation; the publicly released benchmark and evaluation framework will enable targeted research in this area.

Abstract: System models, a critical artifact in software development, provide a formal
abstraction of both the structural and behavioral aspects of software systems,
which can facilitate the early requirements analysis and architecture design.
However, developing system models remains challenging due to the specific
syntax of model description languages and the relative scarcity of public model
examples. While large language models (LLMs) have shown promise in generating
code with programming languages and could potentially aid in system model
development, no benchmarks currently exist for evaluating their ability to
generate system models with specific description languages. We present
SysMBench, which comprises 151 human-curated scenarios spanning a wide range of
popular domains and varying difficulty levels. Each scenario mainly comprises a
natural language requirements description, a system model expressed in a
specific model description language, and a visualized system model diagram. The
requirements description is fed as user input to the LLM, the system model with
description language is used to verify if the generated system model conforms
to the requirements, and the visualized diagram serves to support manual
validation. We introduce SysMEval, a semantic-aware evaluation metric to
evaluate the quality of generated system models. We evaluate 17 popular LLMs on
this task with three traditional metrics and SysMEval, from directly prompting
to three commonly used enhancement strategies. Our in-depth evaluation shows
that LLMs perform poorly on SysMBench, with the highest BLEU of 4% and
SysMEval-F1 of 62%. We release the SysMBench and its evaluation framework to
enable future research on LLM-based system model generation.

</details>


### [29] [SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization](https://arxiv.org/abs/2508.03258)
*Yueyue Liu,Hongyu Zhang,Yuantian Miao*

Main category: cs.SE

TL;DR: This paper introduces SmartLLMs Scheduler (SLS), a dynamic scheduling solution that optimizes LLM performance and cost by leveraging adaptive caching, performance-cost-based allocation, and real-time updates. Experiments show significant gains in performance and processing efficiency.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in LLMs, deploying them for diverse software engineering tasks remains costly and slow due to static scheduling relying on extensive training data. Current methods lack flexibility to adapt to query volumes and task variations.

Method: SLS employs three components: (1) an Adaptive Cache Manager to store and reuse query outputs, (2) a Performance-Cost Optimized Scheduler using query/LLM features for dynamic allocation, and (3) a Dynamic Update Manager to refine strategies with real-time feedback from processed queries.

Result: Experimental evaluation on log parsing and code generation tasks demonstrates SLS achieves 198.82% average performance improvement and 63.28% reduction in processing time compared to baseline static scheduling methods.

Conclusion: SLS effectively addresses limitations of static scheduling through dynamic adaptation, significantly enhancing both performance and cost efficiency for LLM deployment, with strong experimental validation across software engineering workloads.

Abstract: Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable
capabilities in a variety of software engineering tasks. Despite the
advancements, their practical deployment faces challenges, including high
financial costs, long response time, and varying performance, especially when
handling a large number of queries (jobs). Existing optimization strategies for
deploying LLMs for diverse tasks focus on static scheduling, which requires
extensive training data for performance prediction, increasing the
computational costs and limiting the applicability and flexibility. In this
paper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective
scheduling solution. The key idea is to learn LLMs' performance on diverse
tasks and incorporate their real-time feedback to update strategies
periodically. Specifically, SLS incorporates three key components, including an
Adaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic
Update Manager. The Cache Manager stores the outputs of previously processed
queries and employs an adaptive strategy to reduce redundant computations and
minimize response times. For queries not found in the cache, the Scheduler
dynamically allocates them to the most suitable LLM based on the predicted
performance and cost from models that take both query-specific and LLM-specific
features as input. The Update Manager continuously refines the cache and
scheduling strategies based on real-time feedback from the assigned queries to
enhance decision-making and adapt to evolving task characteristics. To evaluate
the effectiveness of SLS, we conduct extensive experiments on two LLM-based
software engineering tasks, including log parsing and code generation. The
results show that SLS significantly outperforms the baseline methods, achieving
an average performance improvement of 198.82% and an average processing time
reduction of 63.28%.

</details>


### [30] [GUI-ReRank: Enhancing GUI Retrieval with Multi-Modal LLM-based Reranking](https://arxiv.org/abs/2508.03298)
*Kristian Kolthoff,Felix Kretzer,Christian Bartelt,Alexander Maedche,Simone Paolo Ponzetto*

Main category: cs.SE

TL;DR: This work introduces GUI-ReRank, a framework combining embedding-based retrieval with MLLM reranking to improve GUI prototyping efficiency and generalizability across datasets.


<details>
  <summary>Details</summary>
Motivation: GUI prototyping is critical but resource-intensive. Existing NL-based GUI retrieval methods suffer from limited performance and poor generalization, prompting the need for a more versatile solution.

Method: GUI-ReRank integrates fast embedding-based constrained retrieval models with effective MLLM reranking techniques, and features a customizable GUI repository annotation/embedding pipeline for seamless RAG integration.

Result: Demonstrated superior performance in retrieval accuracy on an established benchmark compared to SOTA tailored LTR models, while conducting a comprehensive cost/efficiency analysis of MLLM reranking.

Conclusion: GUI-ReRank provides a scalable, accurate framework for NL-based GUI retrieval with better generalizability than prior methods, along with practical trade-off insights for MLLM deployment.

Abstract: GUI prototyping is a fundamental component in the development of modern
interactive systems, which are now ubiquitous across diverse application
domains. GUI prototypes play a critical role in requirements elicitation by
enabling stakeholders to visualize, assess, and refine system concepts
collaboratively. Moreover, prototypes serve as effective tools for early
testing, iterative evaluation, and validation of design ideas with both end
users and development teams. Despite these advantages, the process of
constructing GUI prototypes remains resource-intensive and time-consuming,
frequently demanding substantial effort and expertise. Recent research has
sought to alleviate this burden through NL-based GUI retrieval approaches,
which typically rely on embedding-based retrieval or tailored ranking models
for specific GUI repositories. However, these methods often suffer from limited
retrieval performance and struggle to generalize across arbitrary GUI datasets.
In this work, we present GUI-ReRank, a novel framework that integrates rapid
embedding-based constrained retrieval models with highly effective MLLM-based
reranking techniques. GUI-ReRank further introduces a fully customizable GUI
repository annotation and embedding pipeline, enabling users to effortlessly
make their own GUI repositories searchable, which allows for rapid discovery of
relevant GUIs for inspiration or seamless integration into customized LLM-based
RAG workflows. We evaluated our approach on an established NL-based GUI
retrieval benchmark, demonstrating that GUI-ReRank significantly outperforms
SOTA tailored LTR models in both retrieval accuracy and generalizability.
Additionally, we conducted a comprehensive cost and efficiency analysis of
employing MLLMs for reranking, providing valuable insights regarding the
trade-offs between retrieval effectiveness and computational resources. Video:
https://youtu.be/_7x9UCh82ug

</details>


### [31] [Industrial LLM-based Code Optimization under Regulation: A Mixture-of-Agents Approach](https://arxiv.org/abs/2508.03329)
*Mari Ashiga,Vardan Voskanyan,Fateme Dinmohammadi,Jingzhi Gong,Paul Brookes,Matthew Truscott,Rafail Giavrimis,Mike Basios,Leslie Kanthan,Wei Jie*

Main category: cs.SE

TL;DR: This paper explores the use of a Mixture-of-Agents (MoA) approach with open-source LLMs to optimize code for regulated industries, achieving cost and time savings compared to existing methods like GA-based ensembles.


<details>
  <summary>Details</summary>
Motivation: Regulated industries face data privacy and compliance constraints limiting their use of commercial LLMs for code optimization, creating a need for cost-effective and compliant alternatives.

Method: The authors implemented MoA to synthesize code via multiple specialized open-source LLMs and benchmarked it against TurinTech AI's GA-based ensemble system and standalone LLM optimizers using real-world industrial codebases.

Result: MoA using open-source models achieved 14.3%-22.2% cost savings and 28.6%-32.2% faster optimization times for regulated contexts, validated across 50 code snippets and seven LLM combinations generating 8,700+ variants.

Conclusion: The study demonstrates MoA's superiority for open-source models in regulated environments while GA has advantages for commercial models. It provides actionable deployment guidelines and fills gaps in industrial LLM ensemble evaluation.

Abstract: Recent advancements in Large Language Models (LLMs) for code optimization
have enabled industrial platforms to automate software performance engineering
at unprecedented scale and speed. Yet, organizations in regulated industries
face strict constraints on which LLMs they can use - many cannot utilize
commercial models due to data privacy regulations and compliance requirements,
creating a significant challenge for achieving high-quality code optimization
while maintaining cost-effectiveness. We address this by implementing a
Mixture-of-Agents (MoA) approach that directly synthesizes code from multiple
specialized LLMs, comparing it against TurinTech AI's vanilla Genetic Algorithm
(GA)-based ensemble system and individual LLM optimizers using real-world
industrial codebases. Our key contributions include: (1) First MoA application
to industrial code optimization using real-world codebases; (2) Empirical
evidence that MoA excels with open-source models, achieving 14.3% to 22.2% cost
savings and 28.6% to 32.2% faster optimization times for regulated
environments; (3) Deployment guidelines demonstrating GA's advantage with
commercial models while both ensembles outperform individual LLMs; and (4)
Real-world validation across 50 code snippets and seven LLM combinations,
generating over 8,700 variants, addresses gaps in industrial LLM ensemble
evaluation. This provides actionable guidance for organizations balancing
regulatory compliance with optimization performance in production environments.

</details>


### [32] [Key-Augmented Neural Triggers for Knowledge Sharing](https://arxiv.org/abs/2508.03340)
*Alex Wolf,Marco Edoardo Palma,Pooja Rani,Harald C. Gall*

Main category: cs.SE

TL;DR: KANT is a novel approach improving repository-level code comprehension through knowledge anchors, reducing semantic fragmentation and inference latency by 85% while enabling on-premise deployment.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with semantic fragmentation (knowledge spread across files), RAG pipeline inefficiencies (attention saturation from long contexts), scarce/updated training data, and industrial adoption barriers due to proprietary LLMs.

Method: KANT embeds knowledge anchors during training and inference, synthesizes repository-specific data from code, and replaces verbose contexts in RAG pipelines to reduce token usage and latency.

Result: Human evaluations showed a 60% preference over SOTA models (79% for LocalStack experts) and a 85% latency reduction. Synthetic data aligned with task-specific knowledge needs.

Conclusion: KANT effectively addresses semantic fragmentation and deployment challenges while maintaining performance and generalizing across models, proving its scalability for industrial code comprehension.

Abstract: Repository-level code comprehension and knowledge sharing remain core
challenges in software engineering. Large language models (LLMs) have shown
promise by generating explanations of program structure and logic. However,
these approaches still face limitations: First, relevant knowledge is
distributed across multiple files within a repository, aka semantic
fragmentation. Second, retrieval inefficiency and attention saturation degrade
performance in RAG pipelines, where long, unaligned contexts overwhelm
attention. Third, repository specific training data is scarce and often
outdated. Finally, proprietary LLMs hinder industrial adoption due to privacy
and deployment constraints. To address these issues, we propose Key-Augmented
Neural Triggers (KANT), a novel approach that embeds knowledge anchors into
both training and inference. Unlike prior methods, KANT enables internal access
to repository specific knowledge, reducing fragmentation and grounding
inference in localized context. Moreover, we synthesize specialized data
directly from code. At inference, knowledge anchors replace verbose context,
reducing token overhead and latency while supporting efficient, on premise
deployment. We evaluate KANT via: a qualitative human evaluation of the
synthesized dataset's intent coverage and quality across five dimensions;
compare against SOTA baselines across five qualitative dimensions and inference
speed; and replication across different LLMs to assess generalizability.
Results show that the synthetic training data aligned with information-seeking
needs. KANT achieved over 60% preference from human annotators and a LocalStack
expert (preferring 79% of cases). Also, KANT reduced inference latency by up to
85% across all models. Overall, it is well-suited for scalable, low-latency,
on-premise deployments, providing a strong foundation for code comprehension.

</details>


### [33] [Psychological safety in software workplaces: A systematic literature review](https://arxiv.org/abs/2508.03369)
*Beatriz Santana,Lidivânio Monte,Bianca Santana de Araújo Silva,Glauco Carneiro,Sávio Freire,José Amancio Macedo Santos,Manoel Mendonça*

Main category: cs.SE

TL;DR: This paper systematically reviews and synthesizes existing research on psychological safety (PS) in software engineering, identifying antecedents, consequences, and highlighting knowledge gaps.


<details>
  <summary>Details</summary>
Motivation: Despite PS's importance in software development teams, prior research lacks a systematic synthesis of the field, leaving gaps in understanding socio-technical complexities and effective strategies for enhancement.

Method: A systematic literature review across four digital libraries using quantitative and qualitative analysis to categorize findings across individual, team, and organizational levels.

Result: Findings show increasing academic interest in PS using Edmondson's framework, with key antecedents like team autonomy, agile methodologies, and leadership behaviors identified.

Conclusion: PS positively impacts innovation and team performance, but future work is needed to explore contextual influences, underlying mechanisms, and practical strategies across diverse software environments.

Abstract: Context: Psychological safety (PS) is an important factor influencing team
well-being and performance, particularly in collaborative and dynamic domains
such as software development. Despite its acknowledged significance, research
on PS within the field of software engineering remains limited. The
socio-technical complexities and fast-paced nature of software development
present challenges to cultivating PS. To the best of our knowledge, no
systematic secondary study has synthesized existing knowledge on PS in the
context of software engineering.
  Objective: This study aims to systematically review and synthesize the
existing body of knowledge on PS in software engineering. Specifically, it
seeks to identify the potential antecedents and consequences associated with
the presence or absence of PS among individuals involved in the software
development process.
  Methods: A systematic literature review was conducted, encompassing studies
retrieved from four digital libraries. The extracted data were subjected to
both quantitative and qualitative analyses.
  Results: The findings indicate a growing academic interest in PS within
software engineering, with the majority of studies grounded in Edmondson's
framework. Factors antecedents of PS were identified at the individual, team,
and organizational levels, including team autonomy, agile methodologies, and
leadership behaviors.
  Conclusion: PS fosters innovation, learning, and team performance within
software development. However, significant gaps persist in understanding the
contextual factors influencing PS, its underlying mechanisms, and effective
strategies for its enhancement. Future research should address these gaps by
investigating the practical applications of PS within diverse organizational
settings in the software engineering domain.

</details>


### [34] [Agentic AI in 6G Software Businesses: A Layered Maturity Model](https://arxiv.org/abs/2508.03393)
*Muhammad Zohaib,Muhammad Azeem Akbar,Sami Hyrynsalmi,Arif Ali Khan*

Main category: cs.SE

TL;DR: The paper analyzes factors influencing the adoption of agentic AI systems in 6G software, identifying 29 motivators and 27 demotivators grouped into five themes each. It proposes a CMMI-based layered maturity model across data, business logic, and presentation dimensions to guide organizational readiness.


<details>
  <summary>Details</summary>
Motivation: 6G software businesses face challenges in adopting autonomous, scalable agentic AI systems due to technical, integration, organizational, and cost-related barriers. The study addresses the need for a structured feasibility assessment to bridge this gap.

Method: A multivocal literature review and targeted scanning were conducted to extract and categorize motivators/demotivators. Thematic mapping organized these into high-level themes for early-phase feasibility analysis.

Result: 29 motivators and 27 demotivators were categorized into five themes each, providing a structured overview of organizational readiness factors. The study serves as a preliminary step for developing a CMMI-grounded layered maturity model.

Conclusion: The proposed maturity model aims to help software organizations align agent-first capabilities with 6G demands. Future work will refine and validate this framework for practical adoption guidance.

Abstract: The emergence of agentic AI systems in 6G software businesses presents both
strategic opportunities and significant challenges. While such systems promise
increased autonomy, scalability, and intelligent decision-making across
distributed environments, their adoption raises concerns regarding technical
immaturity, integration complexity, organizational readiness, and
performance-cost trade-offs. In this study, we conducted a preliminary thematic
mapping to identify factors influencing the adoption of agentic software within
the context of 6G. Drawing on a multivocal literature review and targeted
scanning, we identified 29 motivators and 27 demotivators, which were further
categorized into five high-level themes in each group. This thematic mapping
offers a structured overview of the enabling and inhibiting forces shaping
organizational readiness for agentic transformation. Positioned as a
feasibility assessment, the study represents an early phase of a broader
research initiative aimed at developing and validating a layered maturity model
grounded in CMMI model with the software architectural three dimensions
possibly Data, Business Logic, and Presentation. Ultimately, this work seeks to
provide a practical framework to help software-driven organizations assess,
structure, and advance their agent-first capabilities in alignment with the
demands of 6G.

</details>


### [35] [StoneDetector: Conventional and versatile code clone detection for Java](https://arxiv.org/abs/2508.03435)
*Thomas S. Heinze,André Schäfer,Wolfram Amme*

Main category: cs.SE

TL;DR: The paper introduces StoneDetector, a Java code and bytecode clone detection platform using dominator path comparisons with configurable parameters.


<details>
  <summary>Details</summary>
Motivation: Code cloning leads to software bloat and propagates bugs/vulnerabilities, necessitating scalable detection methods that work on varied code syntax and Java bytecode.

Method: Implements conventional clone detection via textual comparison of code-paths extracted from dominator trees, allowing configuration of string metrics and hashing algorithms.

Result: Demonstrated superior performance and scalability compared to other conventional detectors on state-of-the-art benchmarks for both Java source and bytecode.

Conclusion: StoneDetector provides an effective and configurable approach for identifying code clones across different code representations, addressing critical cloning challenges in software engineering.

Abstract: Copy & paste is a widespread practice when developing software and, thus,
duplicated and subsequently modified code occurs frequently in software
projects. Since such code clones, i.e., identical or similar fragments of code,
can bloat software projects and cause issues like bug or vulnerability
propagation, their identification is of importance. In this paper, we present
the StoneDetector platform and its underlying method for finding code clones in
Java source and Bytecode. StoneDetector implements a conventional clone
detection approach based upon the textual comparison of paths derived from the
code's representation by dominator trees. In this way, the tool does not only
find exact and syntactically similar near-miss code clones, but also code
clones that are harder to detect due to their larger variety in the syntax. We
demonstrate StoneDetector's versatility as a conventional clone detection
platform and analyze its various available configuration parameters, including
the usage of different string metrics, hashing algorithms, etc. In our
exhaustive evaluation with other conventional clone detectors on several
state-of-the-art benchmarks, we can show StoneDetector's performance and
scalability in finding code clones in both, Java source and Bytecode.

</details>


### [36] [On the Evaluation of Large Language Models in Multilingual Vulnerability Repair](https://arxiv.org/abs/2508.03470)
*Dong wang,Junji Yu,Honglin Shu,Michael Fu,Chakkrit Tantithamthavorn,Yasutaka Kamei,Junjie Chen*

Main category: cs.SE

TL;DR: This paper evaluates automated vulnerability repair approaches using LLMs across seven programming languages, demonstrating GPT-4o's competitive performance against existing methods while highlighting language-specific effectiveness patterns.


<details>
  <summary>Details</summary>
Motivation: Current deep learning-based vulnerability repair approaches are limited to C/C++, but LLMs offer language-agnostic capabilities and stronger semantic understanding, addressing multilingual repair limitations.

Method: A large-scale empirical study comparing state-of-the-art LLMs and automated repair approaches (including VulMaster) across seven programming languages using metrics like repair success rate and vulnerability danger assessment.

Result: GPT-4o with few-shot prompting outperformed leading approaches (e.g., VulMaster) and showed superior performance in repairing unique/high-danger vulnerabilities. It generalized effectively to unseen languages. Go achieved highest effectiveness, while C/C++ performed worst.

Conclusion: LLMs demonstrate strong potential for multilingual vulnerability repair, particularly in unique and high-danger cases. The study establishes a benchmark for evaluating future multilingual repair approaches and underscores LLMs' generalization advantages over language-specific methods.

Abstract: Various Deep Learning-based approaches with pre-trained language models have
been proposed for automatically repairing software vulnerabilities. However,
these approaches are limited to a specific programming language (C/C++). Recent
advances in large language models (LLMs) offer language-agnostic capabilities
and strong semantic understanding, exhibiting potential to overcome
multilingual vulnerability limitations. Although some work has begun to explore
LLMs' repair performance, their effectiveness is unsatisfactory. To address
these limitations, we conducted a large-scale empirical study to investigate
the performance of automated vulnerability repair approaches and
state-of-the-art LLMs across seven programming languages. Results show GPT-4o,
instruction-tuned with few-shot prompting, performs competitively against the
leading approach, VulMaster. Additionally, the LLM-based approach shows
superior performance in repairing unique vulnerabilities and is more likely to
repair the most dangerous vulnerabilities. Instruction-tuned GPT-4o
demonstrates strong generalization on vulnerabilities in previously unseen
language, outperforming existing approaches. Analysis shows Go consistently
achieves the highest effectiveness across all model types, while C/C++ performs
the worst. Based on findings, we discuss the promise of LLM on multilingual
vulnerability repair and the reasons behind LLM's failed cases. This work takes
the first look at repair approaches and LLMs across multiple languages,
highlighting the promising future of adopting LLMs for multilingual
vulnerability repair.

</details>


### [37] [BitsAI-Fix: LLM-Driven Approach for Automated Lint Error Resolution in Practice](https://arxiv.org/abs/2508.03487)
*Yuanpeng Li,Qi Long,Zhiyuan Yao,Jian Xu,Lintao Xie,Xu He,Lu Geng,Xin Han,Yueyan Chen,Wenbo Duan*

Main category: cs.SE

TL;DR: BitsAI-Fix is an automated LLM-driven lint error remediation workflow for large-scale enterprise environments, combining context-aware patch generation and progressive reinforcement learning to achieve 85% accuracy.


<details>
  <summary>Details</summary>
Motivation: Manual remediation of lint errors in enterprise codebases is insufficient due to their scale and complexity, causing technical debt accumulation and reduced development efficiency.

Method: The system uses tree-sitter for code context expansion, trains LLMs via progressive reinforcement learning to generate search-and-replace patches, and employs a rule-based reward mechanism with correctness/format rewards to prioritize valid fixes.

Result: Deployed at ByteDance, it resolved 12,000+ issues with 85% accuracy, supports 5,000+ engineers, and achieves 1,000+ weekly active users through continuous model iteration via code diff matching feedback.

Conclusion: BitsAI-Fix demonstrates that LLM-based automated code remediation is viable at industrial scale, providing a reference framework for large enterprise technical debt reduction through machine learning integration.

Abstract: As enterprise codebases continue to grow in scale and complexity, the volume
of lint errors far exceeds engineers' manual remediation capacity, leading to
continuous accumulation of technical debt and hindered development efficiency.
This paper presents BitsAI-Fix, an automated lint error remediation workflow
based on Large Language Models (LLMs), designed to address this critical
challenge in industrial-scale environments. BitsAI-Fix employs tree-sitter for
context expansion and generates search-and-replace format patches through
specially trained LLMs, followed by lint scan re-verification to output final
remediation results. Additionally, our approach introduces an innovative
progressive reinforcement learning (RL) training strategy that can
automatically acquire verifiable training data during the project cold-start
phase and continuously iterate the model by collecting online samples through
feedback after system deployment. Furthermore, we designed a targeted
rule-based reward mechanism that combines format rewards and correctness
rewards while penalizing redundant modifications. We also propose a "code diff
matching" methodology to continuously track online effectiveness. In production
deployment at ByteDance, our solution has supported over 5,000 engineers,
resolved more than 12,000 static analysis issues, achieved approximately 85%
remediation accuracy, with around 1,000 weekly active adopters. This work
demonstrates the practical feasibility of LLM-based code remediation solutions
in enterprise environments and serves as a reference for automated code fix in
large-scale industrial scenarios.

</details>


### [38] [LaTCoder: Converting Webpage Design to Code with Layout-as-Thought](https://arxiv.org/abs/2508.03560)
*Yi Gui,Zhen Li,Zhongyi Zhang,Guohao Wang,Tianpeng Lv,Gaoyang Jiang,Yi Liu,Dongping Chen,Yao Wan,Hongyu Zhang,Wenbin Jiang,Xuanhua Shi,Hai Jin*

Main category: cs.SE

TL;DR: LaTCoder improves webpage design-to-code conversion by using Layout-as-Thought (LaT) inspired by human Chain-of-Thought reasoning. It divides designs into blocks, applies CoT prompting, and employs dynamic assembly strategies, achieving 66.67% higher TreeBLEU and 38% lower MAE compared to direct prompting.


<details>
  <summary>Details</summary>
Motivation: Existing Multimodal Large Language Models (MLLMs) struggle to preserve layout accuracy when generating code from webpage designs, creating a gap between visual design and functional implementation in UI development.

Method: LaTCoder 1) splits webpage designs into image blocks via a proposed algorithm, 2) applies Chain-of-Thought (CoT) prompting with MLLMs to generate code for each block, and 3) uses combination strategies (absolute positioning and MLLM-based method) with dynamic selection for assembly.

Result: Evaluations on public and CC-HARD benchmarks showed TreeBLEU improved by 66.67% and MAE reduced by 38% with backbone MLLMs. Human evaluation favored LaTCoder-generated pages >60% of the time.

Conclusion: LaTCoder demonstrates significant layout preservation improvements through structured reasoning (LaT) and dynamic code assembly, validated by both automated metrics and human preference studies in complex UI scenarios.

Abstract: Converting webpage designs into code (design-to-code) plays a vital role in
User Interface (UI) development for front-end developers, bridging the gap
between visual design and functional implementation. While recent Multimodal
Large Language Models (MLLMs) have shown significant potential in
design-to-code tasks, they often fail to accurately preserve the layout during
code generation. To this end, we draw inspiration from the Chain-of-Thought
(CoT) reasoning in human cognition and propose LaTCoder, a novel approach that
enhances layout preservation in webpage design during code generation with
Layout-as-Thought (LaT). Specifically, we first introduce a simple yet
efficient algorithm to divide the webpage design into image blocks. Next, we
prompt MLLMs using a CoTbased approach to generate code for each block.
Finally, we apply two assembly strategies-absolute positioning and an
MLLM-based method-followed by dynamic selection to determine the optimal
output. We evaluate the effectiveness of LaTCoder using multiple backbone MLLMs
(i.e., DeepSeek-VL2, Gemini, and GPT-4o) on both a public benchmark and a newly
introduced, more challenging benchmark (CC-HARD) that features complex layouts.
The experimental results on automatic metrics demonstrate significant
improvements. Specifically, TreeBLEU scores increased by 66.67% and MAE
decreased by 38% when using DeepSeek-VL2, compared to direct prompting.
Moreover, the human preference evaluation results indicate that annotators
favor the webpages generated by LaTCoder in over 60% of cases, providing strong
evidence of the effectiveness of our method.

</details>


### [39] [ReFuzzer: Feedback-Driven Approach to Enhance Validity of LLM-Generated Test Programs](https://arxiv.org/abs/2508.03603)
*Iti Shree,Karine Even-Mendoz,Tomasz Radzik*

Main category: cs.SE

TL;DR: ReFuzzer improves compiler fuzzer effectiveness by refining LLM-generated test programs to ensure validity and enhance optimization component testing.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based compiler fuzzers struggle with syntactic/semantic errors in test programs and lack systematic validation for compiler optimization testing.

Method: A feedback framework using a local LLM to detect/runtime violations (div-by-zero, out-of-bounds) and iteratively refine test programs through validation filtering.

Result: ReFuzzer increased program validity to 96.6-97.3% while reducing invalid cases from 47-49.4%, achieving 2.9-3.5s processing and boosting vectorization coverage by 9.2-9.2% in fuzzing.

Conclusion: ReFuzzer enables systematic refining of LLM outputs to create valid test programs, significantly improving compiler fuzzer validity and coverage for optimization components.

Abstract: Existing LLM-based compiler fuzzers often produce syntactically or
semantically invalid test programs, limiting their effectiveness in exercising
compiler optimizations and backend components. We introduce ReFuzzer, a
framework for refining LLM-generated test programs by systematically detecting
and correcting compilation and runtime violations (e.g. division by zero or
array out-of-bounds accesses). ReFuzzer employs a feedback loop with a local
LLM to validate and filter erroneous programs before execution, improving
fuzzing effectiveness beyond crash detection and enabling the generation of
diverse yet valid test programs.
  We evaluated ReFuzzer's effectiveness across black-, grey- and white-box
fuzzing approaches targeting LLVM/Clang. ReFuzzer improved test programs'
validity from 47.0-49.4% to 96.6-97.3%, with an average processing time of
2.9-3.5 s per test program on a dual-GPU machine. Further, refuzzing
significantly increased code coverage in critical optimization and IR
generation components. For example, vectorization coverage had an absolute
improvement of 9.2%, 2.3%, and 7.1% in black-, grey-, and white-box fuzzing,
enhancing testing effectiveness.

</details>


### [40] [Intent Preserving Generation of Diverse and Idiomatic (Code-)Artifacts](https://arxiv.org/abs/2508.03642)
*Oliver Westphal*

Main category: cs.SE

TL;DR: The paper proposes a method for automatically generating programming exercise artifacts by decomposing them into abstract building blocks with concrete realizations, enabling varied and idiomatic code generation and simplifying adaptation to new contexts.


<details>
  <summary>Details</summary>
Motivation: Existing monolithic program generators struggle to create adaptable, idiomatic code for multiple connected artifacts (e.g., task descriptions, solutions, test specifications) used in programming exercises, leading to complex implementations when generating related artifacts.

Method: An abstract-based approach where small building blocks are defined for different artifact types (programs, specifications, text). These blocks are composited into a structured description that automatically generates the required artifacts through concrete implementations of the blocks.

Result: A framework demonstrating the feasibility of generating diverse, context-specific artifacts (like sample solutions and task descriptions) from a single abstract model while maintaining idiomatic code patterns and flexibility across programming languages.

Conclusion: The approach provides a scalable, language-agnostic method for generating programming exercise artifacts by centralizing logic in composable abstractions, reducing complexity and improving maintainability for multi-artifact task generation systems.

Abstract: When automatically generating programming exercise tasks one often also needs
to automatically generate programs. At the very least when providing sample
solutions is part of automated feedback. But programs can also be used as part
of the exercise task description to communicate a task's requirements.
  Writing good program generators that produce varied yet idiomatic code while
being easily adaptable for new tasks is challenging. The challenges are
intensified if task generation requires additional artifacts, like a more
general behavior specification for testing or additional textual descriptions.
Manually writing generators for multiple different but strongly related
artifacts gets complicated quickly.
  We present an approach where instead of writing monolithic generators for
multiple connected artifacts one specifies a small set of abstract building
blocks and for each such building block defines sets of concrete realizations
for various kinds of artifacts. Then the intended structure of the resulting
artifacts is specified as a composition of the small abstract building blocks.
This abstract description then serves as the common source from which related
artifacts can be derived automatically. The approach is generic in the kind of
artifacts it can produce and is therefore adaptable to a wide range of
contexts.

</details>
