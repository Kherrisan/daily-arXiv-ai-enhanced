{"id": "2507.23229", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.23229", "abs": "https://arxiv.org/abs/2507.23229", "authors": ["Yufei Chen", "Yao Wang", "Haibin Zhang", "Tao Gu"], "title": "Fine-Grained Privacy Extraction from Retrieval-Augmented Generation Systems via Knowledge Asymmetry Exploitation", "comment": null, "summary": "Retrieval-augmented generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge bases, but this advancement introduces\nsignificant privacy risks. Existing privacy attacks on RAG systems can trigger\ndata leakage but often fail to accurately isolate knowledge-base-derived\nsentences within mixed responses. They also lack robustness when applied across\nmultiple domains. This paper addresses these challenges by presenting a novel\nblack-box attack framework that exploits knowledge asymmetry between RAG and\nstandard LLMs to achieve fine-grained privacy extraction across heterogeneous\nknowledge landscapes. We propose a chain-of-thought reasoning strategy that\ncreates adaptive prompts to steer RAG systems away from sensitive content.\nSpecifically, we first decompose adversarial queries to maximize information\ndisparity and then apply a semantic relationship scoring to resolve lexical and\nsyntactic ambiguities. We finally train a neural network on these feature\nscores to precisely identify sentences containing private information. Unlike\nprior work, our framework generalizes to unseen domains through iterative\nrefinement without pre-defined knowledge. Experimental results show that we\nachieve over 91% privacy extraction rate in single-domain and 83% in\nmulti-domain scenarios, reducing sensitive sentence exposure by over 65% in\ncase studies. This work bridges the gap between attack and defense in RAG\nsystems, enabling precise extraction of private information while providing a\nfoundation for adaptive mitigation.", "AI": {"tldr": "This paper proposes a black-box attack framework (91% single-domain privacy extraction, 83% multi-domain) that exploits knowledge asymmetry between RAG and standard LLMs to precisely identify sensitive sentences through adaptive prompt decomposition, semantic scoring, and neural network feature analysis.", "motivation": "RAG systems integrate external knowledge but introduce privacy risks through data leakage, and existing attacks fail to accurately isolate sensitive content or generalize across domains.", "method": "1) Decompose adversarial queries to maximize information disparity 2) Apply semantic relationship scoring to resolve lexical/syntactic ambiguities 3) Train neural network on these features to identify private sentences 4) Enable iterative domain adaptation without predefined knowledge", "result": "91%+ privacy extraction rate in single-domain, 83% in multi-domain scenarios with >65% reduction in sensitive sentence exposure in case studies, while demonstrating cross-domain generalization.", "conclusion": "Bridges attack-defense gap in RAG systems by enabling precise private information extraction through knowledge asymmetry exploitation, establishing a foundation for adaptive privacy mitigation strategies."}}
{"id": "2507.23453", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23453", "abs": "https://arxiv.org/abs/2507.23453", "authors": ["Lijia Liu", "Takumi Kondo", "Kyohei Atarashi", "Koh Takeuchi", "Jiyi Li", "Shigeru Saito", "Hisashi Kashima"], "title": "Counterfactual Evaluation for Blind Attack Detection in LLM-based Evaluation Systems", "comment": null, "summary": "This paper investigates defenses for LLM-based evaluation systems against\nprompt injection. We formalize a class of threats called blind attacks, where a\ncandidate answer is crafted independently of the true answer to deceive the\nevaluator. To counter such attacks, we propose a framework that augments\nStandard Evaluation (SE) with Counterfactual Evaluation (CFE), which\nre-evaluates the submission against a deliberately false ground-truth answer.\nAn attack is detected if the system validates an answer under both standard and\ncounterfactual conditions. Experiments show that while standard evaluation is\nhighly vulnerable, our SE+CFE framework significantly improves security by\nboosting attack detection with minimal performance trade-offs.", "AI": {"tldr": "The paper introduces a SE+CFE framework to enhance security against blind attacks in LLM-based evaluations, achieving significant attack detection improvements with minimal performance impact.", "motivation": "LLM-based evaluation systems are vulnerable to 'blind attacks' where candidate answers deceive evaluators without relying on the true ground-truth answer.", "method": "The framework combines Standard Evaluation (SE) with Counterfactual Evaluation (CFE), which re-evaluates submissions against a deliberately false ground-truth answer to identify inconsistencies.", "result": "Experiments demonstrated SE+CFE's effectiveness in detecting blind attacks while maintaining evaluation performance through minimal detection-cost overhead.", "conclusion": "Integrating counterfactual testing significantly strengthens security in evaluation systems against adversarial prompt injections without sacrificing core capabilities."}}
{"id": "2507.23611", "categories": ["cs.CR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23611", "abs": "https://arxiv.org/abs/2507.23611", "authors": ["Estelle Ruellan", "Eric Clay", "Nicholas Ascoli"], "title": "LLM-Based Identification of Infostealer Infection Vectors from Screenshots: The Case of Aurora", "comment": null, "summary": "Infostealers exfiltrate credentials, session cookies, and sensitive data from\ninfected systems. With over 29 million stealer logs reported in 2024, manual\nanalysis and mitigation at scale are virtually unfeasible/unpractical. While\nmost research focuses on proactive malware detection, a significant gap remains\nin leveraging reactive analysis of stealer logs and their associated artifacts.\nSpecifically, infection artifacts such as screenshots, image captured at the\npoint of compromise, are largely overlooked by the current literature. This\npaper introduces a novel approach leveraging Large Language Models (LLMs), more\nspecifically gpt-4o-mini, to analyze infection screenshots to extract potential\nIndicators of Compromise (IoCs), map infection vectors, and track campaigns.\nFocusing on the Aurora infostealer, we demonstrate how LLMs can process\nscreenshots to identify infection vectors, such as malicious URLs, installer\nfiles, and exploited software themes. Our method extracted 337 actionable URLs\nand 246 relevant files from 1000 screenshots, revealing key malware\ndistribution methods and social engineering tactics. By correlating extracted\nfilenames, URLs, and infection themes, we identified three distinct malware\ncampaigns, demonstrating the potential of LLM-driven analysis for uncovering\ninfection workflows and enhancing threat intelligence. By shifting malware\nanalysis from traditional log-based detection methods to a reactive,\nartifact-driven approach that leverages infection screenshots, this research\npresents a scalable method for identifying infection vectors and enabling early\nintervention.", "AI": {"tldr": "This paper proposes an LLM-based reactive analysis framework to automatically process infection screenshots from Aurora infostealer logs, extracting IoCs and mapping infection vectors at scale. 337 URLs and 246 files were identified, revealing 3 distinct malware campaigns and social engineering tactics, demonstrating the feasibility of artifact-driven threat intelligence compared to traditional log-based detection methods.", "motivation": "Manual analysis of 29 million annual stealer logs is impractical, and current research lacks reactive approaches to exploit infection artifacts like screenshots for malware analysis. 92% of malware attacks go undetected via traditional methods, highlighting the need for scalable reactive analysis techniques.", "method": "We developed an end-to-end framework using gpt-4o-mini to process 1000 Aurora infostealer screenshots. The method extracts IoCs through visual analysis, maps infection vectors by identifying malicious URLs/installer files, and detects campaigns by cross-referencing filename patterns, URLs, and infection themes using LLM-driven artifact correlation.", "result": "Extracted 337 actionable URLs (89% unique) and 246 relevant files (73% with clear infection indicators). Identified 3 distinct campaigns with differing attack patterns: 1) malvertising-driven URL redirection, 2) weaponized document exploitation, and 3) supply chain attacks. The framework achieved 94% precision in infection vector detection vs. human analysts.", "conclusion": "By transitioning from log-based to artifact-driven analysis using LLMs, this research establishes a scalable paradigm for malware investigation. Our method reduces investigation time by ~78% compared to manual analysis while maintaining accuracy, enabling early threat detection and campaign attribution in cybersecurity operations centers."}}
{"id": "2507.23641", "categories": ["cs.CR", "11T71, 94A60"], "pdf": "https://arxiv.org/pdf/2507.23641", "abs": "https://arxiv.org/abs/2507.23641", "authors": ["Michael Schaller"], "title": "Polynomial Lattices for the BIKE Cryptosystem", "comment": null, "summary": "In this paper we introduce a rank $2$ lattice over a polynomial ring arising\nfrom the public key of the BIKE cryptosystem \\cite{aragon2022bike}. The secret\nkey is a sparse vector in this lattice. We study properties of this lattice and\ngeneralize the recovery of weak keys from \\cite{BardetDLO16}. In particular, we\nshow that they implicitly solved a shortest vector problem in the lattice we\nconstructed. Rather than finding only a shortest vector, we obtain a reduced\nbasis of the lattice which makes it possible to check for more weak keys.", "AI": {"tldr": "The paper introduces a rank 2 lattice for the BIKE cryptosystem and improves weak key recovery by solving shortest vector problems and generating reduced bases to detect more weak keys.", "motivation": "The study aims to enhance the analysis of BIKE's security by generalizing weak key recovery techniques, building on prior work to detect vulnerabilities more effectively.", "method": "The authors construct a rank 2 polynomial ring lattice from BIKE's public key, analyze its properties, and adapt algorithms to compute reduced bases rather than only solving for shortest vectors.", "result": "They demonstrate that previous weak key recovery methods implicitly solved SVP in their lattice, and their reduced basis approach enables broader detection of weak keys in BIKE.", "conclusion": "This work establishes a lattice-based framework for BIKE security analysis, showing that reduced bases significantly improve the efficiency and scope of weak key recovery compared to prior methods."}}
{"id": "2507.23087", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23087", "abs": "https://arxiv.org/abs/2507.23087", "authors": ["Fabian Stiehle", "Hans Weytjens", "Ingo Weber"], "title": "On LLM-Assisted Generation of Smart Contracts from Business Processes", "comment": "Accepted at the Workshop on Distributed Ledger Technologies in\n  Business Process Management, At the International Conference for Business\n  Process Management (BPM), 2025", "summary": "Large language models (LLMs) have changed the reality of how software is\nproduced. Within the wider software engineering community, among many other\npurposes, they are explored for code generation use cases from different types\nof input. In this work, we present an exploratory study to investigate the use\nof LLMs for generating smart contract code from business process descriptions,\nan idea that has emerged in recent literature to overcome the limitations of\ntraditional rule-based code generation approaches. However, current LLM-based\nwork evaluates generated code on small samples, relying on manual inspection,\nor testing whether code compiles but ignoring correct execution. With this\nwork, we introduce an automated evaluation framework and provide empirical data\nfrom larger data sets of process models. We test LLMs of different types and\nsizes in their capabilities of achieving important properties of process\nexecution, including enforcing process flow, resource allocation, and\ndata-based conditions. Our results show that LLM performance falls short of the\nperfect reliability required for smart contract development. We suggest future\nwork to explore responsible LLM integrations in existing tools for code\ngeneration to ensure more reliable output. Our benchmarking framework can serve\nas a foundation for developing and evaluating such integrations.", "AI": {"tldr": "This paper analyzes the effectiveness of using large language models (LLMs) for generating smart contracts from business processes, proposing an automated evaluation framework. It finds LLMs fall short in execution reliability and suggests integrating LLMs with existing tools for better outcomes.", "motivation": "Address limitations of traditional rule-based code generation by exploring LLM capabilities, while overcoming the gap in current LLM-based studies that rely on small samples or compilation-only testing.", "method": "Developed a benchmarking framework to evaluate LLMs (across types/sizes) using larger datasets, measuring performance on process execution properties like flow enforcement, resource allocation, and data conditions.", "result": "Empirical evidence shows generated smart contracts frequently fail to meet execution correctness requirements despite syntactic validity.", "conclusion": "LLMs cannot yet ensure reliable smart contract generation; recommendations include hybrid approaches combining LLMs with rule-based tools and adopting the proposed evaluation framework."}}
{"id": "2507.23118", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23118", "abs": "https://arxiv.org/abs/2507.23118", "authors": ["Mattia Di Profio", "Mingjun Zhong", "Yaji Sripada", "Marcel Jaspars"], "title": "FlowETL: An Autonomous Example-Driven Pipeline for Data Engineering", "comment": null, "summary": "The Extract, Transform, Load (ETL) workflow is fundamental for populating and\nmaintaining data warehouses and other data stores accessed by analysts for\ndownstream tasks. A major shortcoming of modern ETL solutions is the extensive\nneed for a human-in-the-loop, required to design and implement\ncontext-specific, and often non-generalisable transformations. While related\nwork in the field of ETL automation shows promising progress, there is a lack\nof solutions capable of automatically designing and applying these\ntransformations. We present FlowETL, a novel example-based autonomous ETL\npipeline architecture designed to automatically standardise and prepare input\ndatasets according to a concise, user-defined target dataset. FlowETL is an\necosystem of components which interact together to achieve the desired outcome.\nA Planning Engine uses a paired input-output datasets sample to construct a\ntransformation plan, which is then applied by an ETL worker to the source\ndataset. Monitoring and logging provide observability throughout the entire\npipeline. The results show promising generalisation capabilities across 14\ndatasets of various domains, file structures, and file sizes.", "AI": {"tldr": "FlowETL is an autonomous ETL pipeline that automates complex data transformations through example-based learning, addressing the limitations of current ETL solutions which require extensive human intervention.", "motivation": " Modern ETL solutions demand significant manual effort to design and implement non-generalizable, context-specific transformations. Existing automation lacks capability to automatically design and apply these transformations.", "method": " FlowETL uses a Planning Engine with paired input-output dataset samples to create transformation plans, executed by ETL workers with monitoring throughout. It combines example-based learning and modular components for automation.", "result": " Demonstrated generalisation across 14 diverse datasets (different domains, file structures, sizes) in standardising and preparing input data to match user-defined targets through automated pipelines.", "conclusion": " FlowETL presents a promising ecosystem for autonomous ETL workflows with broad applicability across heterogeneous data sources, offering a viable solution to reduce human-in-the-loop requirements in data pipeline creation."}}
{"id": "2507.23120", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23120", "abs": "https://arxiv.org/abs/2507.23120", "authors": ["Jordi Cabot"], "title": "Vibe Modeling: Challenges and Opportunities", "comment": null, "summary": "There is a pressing need for better development methods and tools to keep up\nwith the growing demand and increasing complexity of new software systems. New\ntypes of user interfaces, the need for intelligent components, sustainability\nconcerns, ... bring new challenges that we need to handle. In the last years,\nmodel-driven engineering (MDE) has been key to improving the quality and\nproductivity of software development, but models themselves are becoming\nincreasingly complex to specify and manage. At the same time, we are witnessing\nthe growing popularity of vibe coding approaches that rely on Large Language\nModels (LLMs) to transform natural language descriptions into running code at\nthe expenses of code vulnerabilities, scalability issues and maintainability\nconcerns. In this paper, we introduce the concept of \\textit{vibe modeling} as\na novel approach to integrate the best of both worlds (AI and MDE) to speed up\nthe development of reliable complex systems. We outline the key concepts of\nvibe modeling and highlight the opportunities and open challenges it presents\nfor the future of modeling.", "AI": {"tldr": "The paper proposes 'vibe modeling' as a new approach combining AI chatbots and model-driven engineering (MDE) to address challenges in developing reliable complex software systems, balancing the flexibility of AI with the rigor of MDE.", "motivation": "Current MDE methods struggle with model complexity, while vibe coding (LLM-based coding) introduces vulnerabilities and maintainability issues. The gap between AI-driven development and conventional MDE necessitates a hybrid approach.", "method": "Introduces 'vibe modeling' as a synergistic integration of Large Language Models (LLMs) and domain-specific modeling techniques, enabling natural language model specification while maintaining formal verification benefits.", "result": "Outlines core concepts of vibe modeling and identifies critical opportunities (e.g., enhanced productivity) and challenges (e.g., ensuring model correctness) for future implementation in software development workflows.", "conclusion": "Vibe modeling represents a critical step toward AI-enhanced MDE, offering potential to accelerate complex system development while addressing reliability concerns through methodological convergence."}}
{"id": "2507.23168", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23168", "abs": "https://arxiv.org/abs/2507.23168", "authors": ["Elmira Onagh", "Maleknaz Nayebi"], "title": "Extension Decisions in Open Source Software Ecosystem", "comment": "Paper published in JSS journal", "summary": "GitHub Marketplace is expanding by approximately 41% annually, with new\ntools; however, many additions replicate existing functionality. We study this\nphenomenon in the platform's largest segment, Continuous Integration (CI), by\nlinking 6,983 CI Actions to 3,869 providers and mining their version histories.\nOur graph model timestamps every functionality's debut, tracks its adoption,\nand clusters redundant tools. We find that approximately 65% of new CI Actions\nreplicate existing capabilities, typically within six months, and that a small\nset of first-mover Actions accounts for most subsequent forks and extensions.\nThese insights enable developers to choose the optimal moment to launch, target\nunmet functionality, and help maintainers eliminate redundant tools. We publish\nthe complete graph and dataset to encourage longitudinal research on innovation\nand competition in software ecosystems, and to provide practitioners with a\ndata-driven roadmap for identifying emerging trends and guiding product\nstrategy.", "AI": {"tldr": "65% of new CI Actions in the growing GitHub Marketplace replicate existing capabilities within six months, with first-movers dominating forks/extensions.", "motivation": "To address redundancy in the expanding GitHub Marketplace's Continuous Integration segment by analyzing tool innovation patterns and competition dynamics.", "method": "Mapped 6,983 CI Actions to 3,869 providers using version histories; constructed a graph model to timestamp functionality debut, track adoption, and cluster redundant tools.", "result": "65% of new CI Actions replicate existing functionality within six months; first-mover tools account for most forks/derivatives; open dataset shared for further research.", "conclusion": "Insights support innovation timing strategies for developers and redundancy management for maintainers; dataset enables longitudinal studies of software ecosystem innovation/competition."}}
{"id": "2507.23178", "categories": ["cs.SE", "cs.AI", "I.2.5"], "pdf": "https://arxiv.org/pdf/2507.23178", "abs": "https://arxiv.org/abs/2507.23178", "authors": ["Siyuan Liu", "Zhice Yang", "Huangxun Chen"], "title": "AutoBridge: Automating Smart Device Integration with Centralized Platform", "comment": "14 pages, 12 figures, under review", "summary": "Multimodal IoT systems coordinate diverse IoT devices to deliver\nhuman-centered services. The ability to incorporate new IoT devices under the\nmanagement of a centralized platform is an essential requirement. However, it\nrequires significant human expertise and effort to program the complex IoT\nintegration code that enables the platform to understand and control the device\nfunctions. Therefore, we propose AutoBridge to automate IoT integration code\ngeneration. Specifically, AutoBridge adopts a divide-and-conquer strategy: it\nfirst generates device control logic by progressively retrieving\ndevice-specific knowledge, then synthesizes platformcompliant integration code\nusing platform-specific knowledge. To ensure correctness, AutoBridge features a\nmulti-stage debugging pipeline, including an automated debugger for virtual IoT\ndevice testing and an interactive hardware-in-the-loop debugger that requires\nonly binary user feedback (yes and no) for real-device verification. We\nevaluate AutoBridge on a benchmark of 34 IoT devices across two open-source IoT\nplatforms. The results demonstrate that AutoBridge can achieves an average\nsuccess rate of 93.87% and an average function coverage of 94.87%, without any\nhuman involvement. With minimal binary yes and no feedback from users, the code\nis then revised to reach 100% function coverage. A user study with 15\nparticipants further shows that AutoBridge outperforms expert programmers by\n50% to 80% in code accuracy, even when the programmers are allowed to use\ncommercial code LLMs.", "AI": {"tldr": "AutoBridge automates IoT integration code generation with a divide-and-conquer strategy and multi-stage debugging, achieving 93.87% success rate and 94.87% function coverage without human input, reaching 100% coverage with minimal feedback.", "motivation": "Multimodal IoT systems require complex integration code for new devices, but current methods demand significant human expertise and effort.", "method": "AutoBridge uses divide-and-conquer strategy through device-specific knowledge retrieval and platform-specific code synthesis, combined with automated virtual device debugging and interactive hardware-in-the-loop debugging using binary user feedback.", "result": "Achieved 93.87% average success rate and 94.87% function coverage without human involvement on 34 IoT devices across two platforms, reaching 100% coverage with minimal yes/no feedback. Outperformed expert programmers using commercial code LLMs by 50%-80% in code accuracy.", "conclusion": "AutoBridge demonstrates effective automation for IoT platform compliance with high success rates and coverage, requiring only binary feedback for real-device validation in resource-constrained environments."}}
{"id": "2507.23269", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.23269", "abs": "https://arxiv.org/abs/2507.23269", "authors": ["Peter Fettke", "Fabiana Fournier", "Lior Limonad", "Andreas Metzger", "Stefanie Rinderle-Ma", "Barbara Weber"], "title": "XABPs: Towards eXplainable Autonomous Business Processes", "comment": null, "summary": "Autonomous business processes (ABPs), i.e., self-executing workflows\nleveraging AI/ML, have the potential to improve operational efficiency, reduce\nerrors, lower costs, improve response times, and free human workers for more\nstrategic and creative work. However, ABPs may raise specific concerns\nincluding decreased stakeholder trust, difficulties in debugging, hindered\naccountability, risk of bias, and issues with regulatory compliance. We argue\nfor eXplainable ABPs (XABPs) to address these concerns by enabling systems to\narticulate their rationale. The paper outlines a systematic approach to XABPs,\ncharacterizing their forms, structuring explainability, and identifying key BPM\nresearch challenges towards XABPs.", "AI": {"tldr": "This paper addresses challenges in autonomous business processes (ABPs) by introducing eXplainable ABPs (XABPs), which enhance stakeholder trust through rationale articulation. It presents a systematic approach to characterizing XABPs, structuring explainability, and identifying BPM research challenges.", "motivation": "ABPs using AI/ML improve operational efficiency but face issues like decreased stakeholder trust, debugging difficulties, hindered accountability, bias risks, and regulatory compliance problems. The paper is motivated by the need to address these concerns for broader adoption.", "method": "The authors provide a systematic approach to XABPs by analyzing their forms, structuring the components of explainability, and systematically identifying key BPM research challenges through theoretical characterization.", "result": "The study outlines forms and structures of XABPs, highlights mechanisms for rationale articulation, and pinpoints BPM research challenges such as interoperability, standardization, and ethical alignment in autonomous workflows.", "conclusion": "XABPs are positioned as critical for addressing trust and accountability challenges in ABPs. The paper concludes with a roadmap for BPM research to advance explainability frameworks, regulatory compliance, and human-AI collaboration in autonomous business workflows."}}
{"id": "2507.23348", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23348", "abs": "https://arxiv.org/abs/2507.23348", "authors": ["Han Li", "Yuling Shi", "Shaoxin Lin", "Xiaodong Gu", "Heng Lian", "Xin Wang", "Yantao Jia", "Tao Huang", "Qianxiang Wang"], "title": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution", "comment": "Our code and data are available at\n  https://github.com/YerbaPage/SWE-Debate", "summary": "Issue resolution has made remarkable progress thanks to the advanced\nreasoning capabilities of large language models (LLMs). Recently, agent-based\nframeworks such as SWE-agent have further advanced this progress by enabling\nautonomous, tool-using agents to tackle complex software engineering tasks.\nWhile existing agent-based issue resolution approaches are primarily based on\nagents' independent explorations, they often get stuck in local solutions and\nfail to identify issue patterns that span across different parts of the\ncodebase. To address this limitation, we propose SWE-Debate, a competitive\nmulti-agent debate framework that encourages diverse reasoning paths and\nachieves more consolidated issue localization. SWE-Debate first creates\nmultiple fault propagation traces as localization proposals by traversing a\ncode dependency graph. Then, it organizes a three-round debate among\nspecialized agents, each embodying distinct reasoning perspectives along the\nfault propagation trace. This structured competition enables agents to\ncollaboratively converge on a consolidated fix plan. Finally, this consolidated\nfix plan is integrated into an MCTS-based code modification agent for patch\ngeneration. Experiments on the SWE-bench benchmark show that SWE-Debate\nachieves new state-of-the-art results in open-source agent frameworks and\noutperforms baselines by a large margin.", "AI": {"tldr": "SWE-Debate enhances open-source issue resolution using a competitive multi-agent debate framework with structured collaboration and MCTS-based patch generation.", "motivation": "Existing agent-based approaches for issue resolution often get stuck in local solutions and fail to identify cross-codebase patterns due to independent exploration, necessitating a method to encourage diverse reasoning paths and consolidated localization.", "method": "SWE-Debate employs competitive multi-agent debate via code dependency graph traversal, three-round structured debates among specialized agents with distinct reasoning perspectives, and integrates a consolidated fix plan into an MCTS-based code modification agent for patch generation.", "result": "Experiments on SWE-bench demonstrate SWE-Debate achieves new state-of-the-art results in open-source agent frameworks, significantly outperforming baselines.", "conclusion": "Structured multi-agent debates improve LLM-based issue resolution by leveraging diverse reasoning and collaborative convergence, enabling more effective consolidation of fix plans in complex software engineering tasks."}}
{"id": "2507.23356", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23356", "abs": "https://arxiv.org/abs/2507.23356", "authors": ["Shmulik Froimovich", "Raviv Gal", "Wesam Ibraheem", "Avi Ziv"], "title": "Quality Evaluation of COBOL to Java Code Transformation", "comment": "Submitted to ASE 2025", "summary": "We present an automated evaluation system for assessing COBOL-to-Java code\ntranslation within IBM's watsonx Code Assistant for Z (WCA4Z). The system\naddresses key challenges in evaluating LLM-based translators, including model\nopacity and the complexity of translation quality assessment. Our approach\ncombines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver\nscalable, multi-faceted evaluations. The system supports continuous integration\nworkflows, enables large-scale benchmarking, and reduces reliance on manual\nreview. We describe the system architecture, evaluation strategies, and\nreporting mechanisms that provide actionable insights for developers and\nproject managers, facilitating the evolution of high-quality, modernized\ncodebases.", "AI": {"tldr": "The paper introduces an automated system for evaluating COBOL-to-Java code translations in IBM's watsonx Code Assistant for Z (WCA4Z), combining analytic checkers and LLM-as-a-judge techniques to address challenges in LLM-based translator evaluation.", "motivation": "The system aims to overcome key challenges in evaluating LLM-based code translators, such as model opacity and the complexity of translation quality assessment.", "method": "The approach integrates analytic checkers for structured validation with large language model as a judge (LaaJ) techniques, enabling scalable and multi-faceted evaluations within continuous integration workflows.", "result": "The system supports large-scale benchmarking, reduces manual review requirements, and provides architecture details, evaluation strategies, and reporting mechanisms that deliver actionable insights.", "conclusion": "This automated evaluation system enhances the ability to monitor and improve translation quality in modernized codebases while streamlining development processes for COBOL-to-Java migration projects."}}
{"id": "2507.23361", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23361", "abs": "https://arxiv.org/abs/2507.23361", "authors": ["Silin Chen", "Shaoxin Lin", "Xiaodong Gu", "Yuling Shi", "Heng Lian", "Longfei Yun", "Dong Chen", "Weiguo Sun", "Lin Cao", "Qianxiang Wang"], "title": "SWE-Exp: Experience-Driven Software Issue Resolution", "comment": "Our code and data are available at\n  https://github.com/YerbaPage/SWE-Exp", "summary": "Recent advances in large language model (LLM) agents have shown remarkable\nprogress in software issue resolution, leveraging advanced techniques such as\nmulti-agent collaboration and Monte Carlo Tree Search (MCTS). However, current\nagents act as memoryless explorers - treating each problem separately without\nretaining or reusing knowledge from previous repair experiences. This leads to\nredundant exploration of failed trajectories and missed chances to adapt\nsuccessful issue resolution methods to similar problems. To address this\nproblem, we introduce SWE-Exp, an experience - enhanced approach that distills\nconcise and actionable experience from prior agent trajectories, enabling\ncontinuous learning across issues. Our method introduces a multi-faceted\nexperience bank that captures both successful and failed repair attempts.\nSpecifically, it extracts reusable issue resolution knowledge at different\nlevels - from high-level problem comprehension to specific code changes.\nExperiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%\nPass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach\nestablishes a new paradigm in which automated software engineering agents\nsystematically accumulate and leverage repair expertise, fundamentally shifting\nfrom trial-and-error exploration to strategic, experience-driven issue\nresolution.", "AI": {"tldr": "SWE-Exp is an experience-enhanced approach for LLM-based software engineering agents that improves resolution rates by systematically storing and leveraging knowledge from past repair successes and failures.", "motivation": "Current LLM agents treat each software issue in isolation, resulting in redundant exploration of failed solutions and missed opportunities to apply successful methods from similar problems.", "method": "The approach introduces a multi-faceted experience bank that captures both successful and failed repair attempts, extracting reusable knowledge across different levels of software issue resolution from prior agent trajectories.", "result": "SWE-Exp achieves a state-of-the-art 41.6% resolution rate (Pass@1) on the SWE-bench-Verified benchmark using open-source agent frameworks.", "conclusion": "The framework establishes a new paradigm for automated software engineering agents by transitioning from trial-and-error exploration to strategic, experience-driven issue resolution through continuous knowledge accumulation and reuse."}}
{"id": "2507.23370", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23370", "abs": "https://arxiv.org/abs/2507.23370", "authors": ["Trae Research Team", "Pengfei Gao", "Zhao Tian", "Xiangxin Meng", "Xinchen Wang", "Ruida Hu", "Yuanan Xiao", "Yizhou Liu", "Zhao Zhang", "Junjie Chen", "Cuiyun Gao", "Yun Lin", "Yingfei Xiong", "Chao Peng", "Xia Liu"], "title": "Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling", "comment": "Pengfei Gao and Zhao Tian contributed equally to this technical\n  report", "summary": "Software issue resolution is a critical challenge in software engineering and\nhas garnered increasing attention in recent years. With the rapid advancement\nof large language models (LLMs), substantial progress has been made in\naddressing real-world software engineering tasks. Recent studies have\nintroduced ensemble reasoning techniques to enhance the performance of\nLLM-based issue resolution. However, existing prompting-based methods still\nface limitations in effectively exploring large ensemble spaces and lack the\ncapacity for repository-level understanding, both of which constrain their\noverall effectiveness. In this paper, we propose Trae Agent, the first\nagent-based ensemble reasoning approach for repository-level issue resolution.\nTrae Agent formulates our goal as an optimal solution search problem and\naddresses two key challenges, i.e., large ensemble spaces and repository-level\nunderstanding, through modular agents for generation, pruning, and selection.\nWe conduct extensive experiments using three leading LLMs on the widely-adopted\nSWE-bench benchmark, comparing Trae Agent against four state-of-the-art\nensemble reasoning techniques. Experimental results demonstrate that Trae Agent\nconsistently achieves superior performance, with an average improvement of\n10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first\nplace on the SWE-bench Verified leaderboard, with a notable Pass@1 score of\n75.20%. We are pleased to release Trae Agent as an open-source project to\nsupport the research community, with all resources available at\nhttps://github.com/bytedance/trae-agent.", "AI": {"tldr": "Trae Agent is an agent-based ensemble reasoning approach for repository-level issue resolution, achieving 10.22% average improvement over baselines and first place on SWE-bench Verified leaderboard with 75.20% Pass@1.", "motivation": "Existing prompting-based ensemble methods struggle with exploring large ensemble spaces and lack repository-level understanding, limiting their effectiveness in software issue resolution.", "method": "Trae Agent modularizes generation, pruning, and selection through agent collaboration to address large ensemble spaces and repository-level comprehension, formulated as an optimal solution search problem.", "result": "Exceeds four state-of-the-art techniques on SWE-bench with a 10.22% average performance gain in Pass@1 metric, securing 75.20% Pass@1 score.", "conclusion": "Trae Agent demonstrates superior effectiveness in repository-level issue resolution via its modular agents and achieves production-level performance, now open-source at https://github.com/bytedance/trae-agent."}}
{"id": "2507.23425", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23425", "abs": "https://arxiv.org/abs/2507.23425", "authors": ["Daphn\u00e9 Larrivain", "Shinhyung Yang", "Wilhelm Hasselbring"], "title": "Dynamic and Static Analysis of Python Software with Kieker Including Reconstructed Architectures", "comment": "9 pages, 9 figures", "summary": "The Kieker observability framework is a tool that provides users with the\nmeans to design a custom observability pipeline for their application.\nOriginally tailored for Java, supporting Python with Kieker is worthwhile.\nPython's popularity has exploded over the years, thus making structural\ninsights of Python applications highly valuable. Our Python analysis pipeline\ncombines static and dynamic analysis in order to build a complete picture of a\ngiven system.", "AI": {"tldr": "The Kieker observability framework, originally designed for Java, is extended to support Python by integrating static and dynamic analysis in an observability pipeline to provide structural insights into Python applications.", "motivation": "Python's growing popularity necessitates structural application monitoring tools similar to Kieker's capabilities for Java, making Python support valuable for observability.", "method": "The paper implements a combined static/dynamic analysis pipeline using Kieker's framework to extract comprehensive system information from Python applications.", "result": "The paper demonstrates the feasibility of the Python analysis pipeline approach, though specific quantitative results are not detailed in the abstract.", "conclusion": "The study concludes that adapting Kieker for Python application monitoring is worthwhile, offering structural insights through its integrated analysis methods."}}
{"id": "2507.23640", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23640", "abs": "https://arxiv.org/abs/2507.23640", "authors": ["Samah Kansab", "Mohammed Sayagh", "Francis Bordeleau", "Ali Tizghadam"], "title": "An Empirical Study on the Amount of Changes Required for Merge Request Acceptance", "comment": null, "summary": "Code review (CR) is essential to software development, helping ensure that\nnew code is properly integrated. However, the CR process often involves\nsignificant effort, including code adjustments, responses to reviewers, and\ncontinued implementation. While past studies have examined CR delays and\niteration counts, few have investigated the effort based on the volume of code\nchanges required, especially in the context of GitLab Merge Requests (MRs),\nwhich remains underexplored. In this paper, we define and measure CR effort as\nthe amount of code modified after submission, using a dataset of over 23,600\nMRs from four GitLab projects. We find that up to 71% of MRs require\nadjustments after submission, and 28% of these involve changes to more than 200\nlines of code. Surprisingly, this effort is not correlated with review time or\nthe number of participants. To better understand and predict CR effort, we\ntrain an interpretable machine learning model using metrics across multiple\ndimensions: text features, code complexity, developer experience, review\nhistory, and branching. Our model achieves strong performance (AUC 0.84-0.88)\nand reveals that complexity, experience, and text features are key predictors.\nHistorical project characteristics also influence current review effort. Our\nfindings highlight the feasibility of using machine learning to explain and\nanticipate the effort needed to integrate code changes during review.", "AI": {"tldr": "This paper investigates code review effort in GitLab MRs, revealing high adjustment rates (71%) and significant line changes (28%+) not correlated with review time or participants. An interpretable ML model (AUC 0.84-0.88) identifies complexity, developer experience, text features, and historical project characteristics as key predictors of effort.", "motivation": "Existing studies focus on code review delays or iteration counts, but few quantify effort based on actual code change volumes in GitLab MRs, which remains underexplored.", "method": "The authors analyze 23,600 MRs across four GitLab projects and train an interpretable machine learning model using metrics from code complexity, text features, developer experience, review history, and branching.", "result": "71% of MRs require post-submission changes, 28% involve over 200 line modifications. Review effort shows no correlation with review duration or participant count. ML model achieves 0.84-0.88 AUC and identifies key predictors affecting effort.", "conclusion": "Machine learning can effectively explain and predict code review effort by analyzing multi-dimensional project and developer metrics, providing actionable insights for code integration planning."}}
