{"id": "2509.15283", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.PL", "I.2.7; F.2.2; I.2.2"], "pdf": "https://arxiv.org/pdf/2509.15283", "abs": "https://arxiv.org/abs/2509.15283", "authors": ["Kadin Matotek", "Heather Cassel", "Md Amiruzzaman", "Linh B. Ngo"], "title": "Evaluating the Limitations of Local LLMs in Solving Complex Programming Challenges", "comment": "Comments: 16 pages, 3 figures, 8 tables, accepted to CCSC Eastern\n  2025", "summary": "This study examines the performance of today's open-source, locally hosted\nlarge-language models (LLMs) in handling complex competitive programming tasks\nwith extended problem descriptions and contexts. Building on the original\nFramework for AI-driven Code Generation Evaluation (FACE), the authors retrofit\nthe pipeline to work entirely offline through the Ollama runtime, collapsing\nFACE's sprawling per-problem directory tree into a handful of consolidated JSON\nfiles, and adding robust checkpointing so multi-day runs can resume after\nfailures. The enhanced framework generates, submits, and records solutions for\nthe full Kattis corpus of 3,589 problems across eight code-oriented models\nranging from 6.7-9 billion parameters. The submission results show that the\noverall pass@1 accuracy is modest for the local models, with the best models\nperforming at approximately half the acceptance rate of the proprietary models,\nGemini 1.5 and ChatGPT-4. These findings expose a persistent gap between\nprivate, cost-controlled LLM deployments and state-of-the-art proprietary\nservices, yet also highlight the rapid progress of open models and the\npractical benefits of an evaluation workflow that organizations can replicate\non in-house hardware."}
{"id": "2509.15397", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15397", "abs": "https://arxiv.org/abs/2509.15397", "authors": ["Simantika Bhattacharjee Dristi", "Matthew B. Dwyer"], "title": "LoCaL: Countering Surface Bias in Code Evaluation Metrics", "comment": null, "summary": "With the increasing popularity of large language models (LLMs) and LLM-based\nagents, reliable and effective code evaluation metrics (CEMs) have become\ncrucial for progress across several software engineering tasks. While popular\nbenchmarks often provide test cases to assess the correctness of generated\ncode, crafting and executing test cases is expensive. Reference-based CEMs\nprovide a cheaper alternative by scoring a candidate program based on its\nfunctional similarity to a reference. Although prior research has focused on\nreporting the weak correlation between these CEMs and functional correctness,\nthe causes are only assumed, and plausible solutions remain unexplored. In this\nwork, we critically evaluate four state-of-the-art reference-based CEMs,\nrevealing their strong bias towards surface-level features rather than code\nfunctionality. Despite this surface bias, current evaluation datasets for these\nCEMs rarely include code pairs that are surface-similar yet functionally\ndissimilar, or functionally similar yet surface-dissimilar. To mitigate this\ngap, we propose LoCaL (Looks Can Lie), a CEM evaluation benchmark, with 3117\ncode pairs at both the method and program levels. Each pair is labeled with a\nfunctional similarity score and aims to target regions where CEMs are likely to\nperform poorly. The functional similarity scores are calculated through\ndifferential fuzzing, which eliminates the need for predefined test cases and,\nat the same time, improves the reliability of the scores by executing an order\nof magnitude more tests than prior work. We find that all four CEMs show\nsignificant performance degradation on LoCaL, compared to the baselines.\nFinally, based on our findings, we draw the implication that exposing CEMs to\nLoCaL-like data might facilitate the development of metrics that are robust to\nsurface bias."}
{"id": "2509.15567", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15567", "abs": "https://arxiv.org/abs/2509.15567", "authors": ["Hongyu Kuang", "Ning Zhang", "Hui Gao", "Xin Zhou", "Wesley K. G. Assunção", "Xiaoxing Ma", "Dong Shao", "Guoping Rong", "He Zhang"], "title": "Brevity is the Soul of Wit: Condensing Code Changes to Improve Commit Message Generation", "comment": null, "summary": "Commit messages are valuable resources for describing why code changes are\ncommitted to repositories in version control systems (e.g., Git). They\neffectively help developers understand code changes and better perform software\nmaintenance tasks. Unfortunately, developers often neglect to write\nhigh-quality commit messages in practice. Therefore, a growing body of work is\nproposed to generate commit messages automatically. These works all\ndemonstrated that how to organize and represent code changes is vital in\ngenerating good commit messages, including the use of fine-grained graphs or\nembeddings to better represent code changes. In this study, we choose an\nalternative way to condense code changes before generation, i.e., proposing\nbrief yet concise text templates consisting of the following three parts: (1)\nsummarized code changes, (2) elicited comments, and (3) emphasized code\nidentifiers. Specifically, we first condense code changes by using our proposed\ntemplates with the help of a heuristic-based tool named ChangeScribe, and then\nfine-tune CodeLlama-7B on the pairs of our proposed templates and corresponding\ncommit messages. Our proposed templates better utilize pre-trained language\nmodels, while being naturally brief and readable to complement generated commit\nmessages for developers. Our evaluation based on a widely used dataset showed\nthat our approach can outperform six baselines in terms of BLEU-Norm, METEOR,\nand ROUGE-L, with average improvements of 51.7%, 78.7%, and 62.5%,\nrespectively. The ablation study and human evaluation also provide further\ninsights into the effectiveness of our approach."}
{"id": "2509.15777", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15777", "abs": "https://arxiv.org/abs/2509.15777", "authors": ["Haoran Xu", "Zhi Chen", "Junxiao Han", "Xinkui Zhao", "Jianwei Yin", "Shuiguang Deng"], "title": "How Far Are We? An Empirical Analysis of Current Vulnerability Localization Approaches", "comment": null, "summary": "Open-source software vulnerability patch detection is a critical component\nfor maintaining software security and ensuring software supply chain integrity.\nTraditional manual detection methods face significant scalability challenges\nwhen processing large volumes of commit histories, while being prone to human\nerrors and omissions. Existing automated approaches, including heuristic-based\nmethods and pre-trained model solutions, suffer from limited accuracy, poor\ngeneralization capabilities, and inherent methodological constraints that\nhinder their practical deployment. To address these fundamental challenges,\nthis paper conducts a comprehensive empirical study of existing vulnerability\npatch detection methods, revealing four key insights that guide the design of\neffective solutions: the critical impact of search space reduction, the\nsuperiority of pre-trained semantic understanding over architectural\ncomplexity, the temporal limitations of web crawling approaches, and the\nadvantages of knowledge-driven methods. Based on these insights, we propose a\nnovel two-stage framework that combines version-driven candidate filtering with\nlarge language model-based multi-round dialogue voting to achieve accurate and\nefficient vulnerability patch identification. Extensive experiments on a\ndataset containing 750 real vulnerabilities demonstrate that our method\noutperforms current approaches."}
{"id": "2509.15433", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.15433", "abs": "https://arxiv.org/abs/2509.15433", "authors": ["Vaibhav Agrawal", "Kiarash Ahi"], "title": "Synergizing Static Analysis with Large Language Models for Vulnerability Discovery and beyond", "comment": null, "summary": "This report examines the synergy between Large Language Models (LLMs) and\nStatic Application Security Testing (SAST) to improve vulnerability discovery.\nTraditional SAST tools, while effective for proactive security, are limited by\nhigh false-positive rates and a lack of contextual understanding. Conversely,\nLLMs excel at code analysis and pattern recognition but can be prone to\ninconsistencies and hallucinations. By integrating these two technologies, a\nmore intelligent and efficient system is created. This combination moves beyond\nmere vulnerability detection optimization, transforming security into a deeply\nintegrated, contextual process that provides tangible benefits like improved\ntriage, dynamic bug descriptions, bug validation via exploit generation and\nenhanced analysis of complex codebases. The result is a more effective security\napproach that leverages the strengths of both technologies while mitigating\ntheir weaknesses. SAST-Genius reduced false positives by about 91 % (225 to 20)\ncompared to Semgrep alone."}
{"id": "2509.15893", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15893", "abs": "https://arxiv.org/abs/2509.15893", "authors": ["Andrea Bombarda", "Federico Conti", "Marcello Minervini", "Aurora Zanenga", "Claudio Menghi"], "title": "Failure Modes and Effects Analysis: An Experience from the E-Bike Domain", "comment": "12 pages", "summary": "Software failures can have catastrophic and costly consequences. Functional\nFailure Mode and Effects Analysis (FMEA) is a standard technique used within\nCyber-Physical Systems (CPS) to identify software failures and assess their\nconsequences. Simulation-driven approaches have recently been shown to be\neffective in supporting FMEA. However, industries need evidence of the\neffectiveness of these approaches to increase practical adoption. This\nindustrial paper presents our experience with using FMEA to analyze the safety\nof a CPS from the e-Bike domain. We used Simulink Fault Analyzer, an industrial\ntool that supports engineers with FMEA. We identified 13 realistic faults,\nmodeled them, and analyzed their effects. We sought expert feedback to analyze\nthe appropriateness of our models and the effectiveness of the faults in\ndetecting safety breaches. Our results reveal that for the faults we\nidentified, our models were accurate or contained minor imprecision that we\nsubsequently corrected. They also confirm that FMEA helps engineers improve\ntheir models. Specifically, the output provided by the simulation-driven\nsupport for 38.4% (5 out of 13) of the faults did not match the engineers'\nexpectations, helping them discover unexpected effects of the faults. We\npresent a thorough discussion of our results and ten lessons learned. Our\nfindings are useful for software engineers who work as Simulink engineers, use\nthe Simulink Fault Analyzer, or work as safety analysts."}
{"id": "2509.15499", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.15499", "abs": "https://arxiv.org/abs/2509.15499", "authors": ["Shijia Li", "Jiang Ming", "Lanqing Liu", "Longwei Yang", "Ni Zhang", "Chunfu Jia"], "title": "Adversarially Robust Assembly Language Model for Packed Executables Detection", "comment": "Accepted by ACM CCS 2025", "summary": "Detecting packed executables is a critical component of large-scale malware\nanalysis and antivirus engine workflows, as it identifies samples that warrant\ncomputationally intensive dynamic unpacking to reveal concealed malicious\nbehavior. Traditionally, packer detection techniques have relied on empirical\nfeatures, such as high entropy or specific binary patterns. However, these\nempirical, feature-based methods are increasingly vulnerable to evasion by\nadversarial samples or unknown packers (e.g., low-entropy packers).\nFurthermore, the dependence on expert-crafted features poses challenges in\nsustaining and evolving these methods over time.\n  In this paper, we examine the limitations of existing packer detection\nmethods and propose Pack-ALM, a novel deep-learning-based approach for\ndetecting packed executables. Inspired by the linguistic concept of\ndistinguishing between real and pseudo words, we reformulate packer detection\nas a task of differentiating between legitimate and \"pseudo\" instructions. To\nachieve this, we preprocess native data and packed data into \"pseudo\"\ninstructions and design a pre-trained assembly language model that recognizes\nfeatures indicative of packed data. We evaluate Pack-ALM against leading\nindustrial packer detection tools and state-of-the-art assembly language\nmodels. Extensive experiments on over 37,000 samples demonstrate that Pack-ALM\neffectively identifies packed binaries, including samples created with\nadversarial or previously unseen packing techniques. Moreover, Pack-ALM\noutperforms traditional entropy-based methods and advanced assembly language\nmodels in both detection accuracy and adversarial robustness."}
{"id": "2509.15971", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15971", "abs": "https://arxiv.org/abs/2509.15971", "authors": ["Owen Truong", "Terrence Zhang", "Arnav Marchareddy", "Ryan Lee", "Jeffery Busold", "Michael Socas", "Eman Abdullah AlOmar"], "title": "LeakageDetector 2.0: Analyzing Data Leakage in Jupyter-Driven Machine Learning Pipelines", "comment": null, "summary": "In software development environments, code quality is crucial. This study\naims to assist Machine Learning (ML) engineers in enhancing their code by\nidentifying and correcting Data Leakage issues within their models. Data\nLeakage occurs when information from the test dataset is inadvertently included\nin the training data when preparing a data science model, resulting in\nmisleading performance evaluations. ML developers must carefully separate their\ndata into training, evaluation, and test sets to avoid introducing Data Leakage\ninto their code. In this paper, we develop a new Visual Studio Code (VS Code)\nextension, called LeakageDetector, that detects Data Leakage, mainly Overlap,\nPreprocessing and Multi-test leakage, from Jupyter Notebook files. Beyond\ndetection, we included two correction mechanisms: a conventional approach,\nknown as a quick fix, which manually fixes the leakage, and an LLM-driven\napproach that guides ML developers toward best practices for building ML\npipelines."}
{"id": "2509.15547", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.15547", "abs": "https://arxiv.org/abs/2509.15547", "authors": ["Zhiyu Huang", "Guyue Li", "Hao Xu", "Derrick Wing Kwan Ng"], "title": "Fluid Antenna System-assisted Physical Layer Secret Key Generation", "comment": null, "summary": "This paper investigates physical-layer key generation (PLKG) in multi-antenna\nbase station systems, by leveraging a fluid antenna system (FAS) to dynamically\ncustomize radio environments. Without requiring additional nodes or extensive\nradio frequency chains, the FAS effectively enables adaptive antenna port\nselection by exploiting channel spatial correlation to enhance the key\ngeneration rate (KGR) at legitimate nodes. To comprehensively evaluate the\nefficiency of the FAS in PLKG, we propose an FAS-assisted PLKG model that\nintegrates transmit beamforming and sparse port selection under independent and\nidentically distributed and spatially correlated channel models, respectively.\nSpecifically, the PLKG utilizes reciprocal channel probing to derive a\nclosed-form KGR expression based on the mutual information between legitimate\nchannel estimates. Nonconvex optimization problems for these scenarios are\nformulated to maximize the KGR subject to transmit power constraints and sparse\nport activation. We propose an iterative algorithm by capitalizing on\nsuccessive convex approximation and Cauchy-Schwarz inequality to obtain a\nlocally optimal solution. A reweighted $\\ell_1$-norm-based algorithm is applied\nto advocate for the sparse port activation of FAS-assisted PLKG. Furthermore, a\nlow-complexity sliding window-based port selection is proposed to substitute\nreweighted $\\ell_1$-norm method based on Rayleigh-quotient analysis. Simulation\nresults demonstrate that the FAS-PLKG scheme significantly outperforms the\nFA-PLKG scheme in both independent and spatially correlated environments. The\nsliding window-based port selection method introduced in this paper has been\nshown to yield superior KGR, compared to the reweighted $\\ell_1$-norm method.\nIt is shown that the FAS achieves higher KGR with fewer RF chains through\ndynamic sparse port selection."}
{"id": "2509.16081", "categories": ["cs.SE", "cs.MS", "G.1.3; D.2.11"], "pdf": "https://arxiv.org/pdf/2509.16081", "abs": "https://arxiv.org/abs/2509.16081", "authors": ["Marcel Koch", "Tobias Ribizel", "Pratik Nayak", "Fritz Göbel", "Gregor Olenik", "Terry Cojean"], "title": "Software Development Aspects of Integrating Linear Algebra Libraries", "comment": "16 pages, 2 figures", "summary": "Many scientific discoveries are made through, or aided by, the use of\nsimulation software. These sophisticated software applications are not built\nfrom the ground up, instead they rely on smaller parts for specific use cases,\nusually from domains unfamiliar to the application scientists. The software\nlibrary Ginkgo is one of these building blocks to handle sparse numerical\nlinear algebra on different platforms. By using Ginkgo, applications are able\nto ease the transition to modern systems, and speed up their simulations\nthrough faster numerical linear algebra routines. This paper discusses the\nchallenges and benefits for application software in adopting Ginkgo. It will\npresent examples from different domains, such as CFD, power grid simulation, as\nwell as electro-cardiophysiology. For these cases, the impact of the\nintegrations on the application code is discussed from a software engineering\nstandpoint, and in particular, the approaches taken by Ginkgo and the\napplications to enable sustainable software development are highlighted."}
{"id": "2509.15555", "categories": ["cs.CR", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.15555", "abs": "https://arxiv.org/abs/2509.15555", "authors": ["Rasil Baidar", "Sasa Maric", "Robert Abbas"], "title": "Hybrid Deep Learning-Federated Learning Powered Intrusion Detection System for IoT/5G Advanced Edge Computing Network", "comment": null, "summary": "The exponential expansion of IoT and 5G-Advanced applications has enlarged\nthe attack surface for DDoS, malware, and zero-day intrusions. We propose an\nintrusion detection system that fuses a convolutional neural network (CNN), a\nbidirectional LSTM (BiLSTM), and an autoencoder (AE) bottleneck within a\nprivacy-preserving federated learning (FL) framework. The CNN-BiLSTM branch\ncaptures local and gated cross-feature interactions, while the AE emphasizes\nreconstruction-based anomaly sensitivity. Training occurs across edge devices\nwithout sharing raw data. On UNSW-NB15 (binary), the fused model attains AUC\n99.59 percent and F1 97.36 percent; confusion-matrix analysis shows balanced\nerror rates with high precision and recall. Average inference time is\napproximately 0.0476 ms per sample on our test hardware, which is well within\nthe less than 10 ms URLLC budget, supporting edge deployment. We also discuss\nexplainability, drift tolerance, and FL considerations for compliant, scalable\n5G-Advanced IoT security."}
{"id": "2509.16140", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16140", "abs": "https://arxiv.org/abs/2509.16140", "authors": ["Avinash Patil"], "title": "When Bugs Linger: A Study of Anomalous Resolution Time Outliers and Their Themes", "comment": "7 pages, 2 tables, 21 figures", "summary": "Efficient bug resolution is critical for maintaining software quality and\nuser satisfaction. However, specific bug reports experience unusually long\nresolution times, which may indicate underlying process inefficiencies or\ncomplex issues. This study presents a comprehensive analysis of bug resolution\nanomalies across seven prominent open-source repositories: Cassandra, Firefox,\nHadoop, HBase, SeaMonkey, Spark, and Thunderbird. Utilizing statistical methods\nsuch as Z-score and Interquartile Range (IQR), we identify anomalies in bug\nresolution durations. To understand the thematic nature of these anomalies, we\napply Term Frequency-Inverse Document Frequency (TF-IDF) for textual feature\nextraction and KMeans clustering to group similar bug summaries. Our findings\nreveal consistent patterns across projects, with anomalies often clustering\naround test failures, enhancement requests, and user interface issues. This\napproach provides actionable insights for project maintainers to prioritize and\neffectively address long-standing bugs."}
{"id": "2509.15572", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.15572", "abs": "https://arxiv.org/abs/2509.15572", "authors": ["Xinpeng Liu", "Junming Liu", "Peiyu Liu", "Han Zheng", "Qinying Wang", "Mathias Payer", "Shouling Ji", "Wenhai Wang"], "title": "Cuckoo Attack: Stealthy and Persistent Attacks Against AI-IDE", "comment": null, "summary": "Modern AI-powered Integrated Development Environments (AI-IDEs) are\nincreasingly defined by an Agent-centric architecture, where an LLM-powered\nAgent is deeply integrated to autonomously execute complex tasks. This tight\nintegration, however, also introduces a new and critical attack surface.\nAttackers can exploit these components by injecting malicious instructions into\nuntrusted external sources, effectively hijacking the Agent to perform harmful\noperations beyond the user's intention or awareness. This emerging threat has\nquickly attracted research attention, leading to various proposed attack\nvectors, such as hijacking Model Context Protocol (MCP) Servers to access\nprivate data. However, most existing approaches lack stealth and persistence,\nlimiting their practical impact.\n  We propose the Cuckoo Attack, a novel attack that achieves stealthy and\npersistent command execution by embedding malicious payloads into configuration\nfiles. These files, commonly used in AI-IDEs, execute system commands during\nroutine operations, without displaying execution details to the user. Once\nconfigured, such files are rarely revisited unless an obvious runtime error\noccurs, creating a blind spot for attackers to exploit. We formalize our attack\nparadigm into two stages, including initial infection and persistence. Based on\nthese stages, we analyze the practicality of the attack execution process and\nidentify the relevant exploitation techniques. Furthermore, we analyze the\nimpact of Cuckoo Attack, which can not only invade the developer's local\ncomputer but also achieve supply chain attacks through the spread of\nconfiguration files. We contribute seven actionable checkpoints for vendors to\nevaluate their product security. The critical need for these checks is\ndemonstrated by our end-to-end Proof of Concept, which validated the proposed\nattack across nine mainstream Agent and AI-IDE pairs."}
{"id": "2509.16187", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16187", "abs": "https://arxiv.org/abs/2509.16187", "authors": ["Ali Reza Ibrahimzada", "Brandon Paulsen", "Reyhaneh Jabbarvand", "Joey Dodds", "Daniel Kroening"], "title": "MatchFixAgent: Language-Agnostic Autonomous Repository-Level Code Translation Validation and Repair", "comment": null, "summary": "Code translation transforms source code from one programming language (PL) to\nanother. Validating the functional equivalence of translation and repairing, if\nnecessary, are critical steps in code translation. Existing automated\nvalidation and repair approaches struggle to generalize to many PLs due to high\nengineering overhead, and they rely on existing and often inadequate test\nsuites, which results in false claims of equivalence and ineffective\ntranslation repair. We develop MatchFixAgent, a large language model\n(LLM)-based, PL-agnostic framework for equivalence validation and repair of\ntranslations. MatchFixAgent features a multi-agent architecture that divides\nequivalence validation into several sub-tasks to ensure thorough and consistent\nsemantic analysis of the translation. Then it feeds this analysis to test agent\nto write and execute tests. Upon observing a test failure, the repair agent\nattempts to fix the translation bug. The final (in)equivalence decision is made\nby the verdict agent, considering semantic analyses and test execution results.\n  We compare MatchFixAgent's validation and repair results with four\nrepository-level code translation techniques. We use 2,219 translation pairs\nfrom their artifacts, which cover 6 PL pairs, and are collected from 24 GitHub\nprojects totaling over 900K lines of code. Our results demonstrate that\nMatchFixAgent produces (in)equivalence verdicts for 99.2% of translation pairs,\nwith the same equivalence validation result as prior work on 72.8% of them.\nWhen MatchFixAgent's result disagrees with prior work, we find that 60.7% of\nthe time MatchFixAgent's result is actually correct. In addition, we show that\nMatchFixAgent can repair 50.6% of inequivalent translation, compared to prior\nwork's 18.5%. This demonstrates that MatchFixAgent is far more adaptable to\nmany PL pairs than prior work, while producing highly accurate validation\nresults."}
{"id": "2509.15653", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.15653", "abs": "https://arxiv.org/abs/2509.15653", "authors": ["Yaser Baseri", "Abdelhakim Hafid", "Arash Habibi Lashkari"], "title": "Future-Proofing Cloud Security Against Quantum Attacks: Risk, Transition, and Mitigation Strategies", "comment": null, "summary": "Quantum Computing (QC) introduces a transformative threat to digital\nsecurity, with the potential to compromise widely deployed classical\ncryptographic systems. This survey offers a comprehensive and systematic\nexamination of quantumsafe security for Cloud Computing (CC), focusing on the\nvulnerabilities, transition strategies, and mitigation mechanisms required to\nsecure cloud infrastructures in the quantum era. We evaluated the landscape of\nquantum threats across the entire CC stack, demonstrating how quantum\nalgorithms can undermine classical encryption and compromise cloud security at\nmultiple architectural layers. Using a structured risk assessment methodology\nbased on the STRIDE model, we evaluate quantum-induced attack vectors and their\nimpact on cloud environments. To address these challenges, we propose a layered\nsecurity framework that integrates hybrid cryptographic transition strategies,\ncryptographic agility, and proactive risk mitigation. We analyze the\npreparation and implementation approaches of the major Cloud Service Providers\n(CSPs), including AWS, Azure and GCP, synthesizing platform-specific\ninitiatives toward Post-Quantum Cryptography (PQC). Furthermore, we provide a\ndetailed evaluation of standardized PQC algorithms, exploring their resilience\nto side-channel and active attacks within cloud-native deployments. This survey\nserves as a strategic reference for cloud architects, policymakers, and\nresearchers, offering actionable insights for navigating the complex transition\nto quantum-resilient cloud systems. We conclude by identifying six key future\nresearch directions: standardization and interoperability, performance and\nscalability, implementation security, integration with emerging technologies,\nsystemic preparedness, and crypto-agile migration frameworks."}
{"id": "2509.15754", "categories": ["cs.CR", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15754", "abs": "https://arxiv.org/abs/2509.15754", "authors": ["Toby Sharp"], "title": "Hornet Node and the Hornet DSL: A Minimal, Executable Specification for Bitcoin Consensus", "comment": null, "summary": "Bitcoin's consensus rules are encoded in the implementation of its reference\nclient: \"The code is the spec.\" Yet this code is unsuitable for formal\nverification due to side effects, mutable state, concurrency, and legacy\ndesign. A standalone formal specification would enable verification both across\nversions of the reference client and against new client implementations,\nstrengthening decentralization by reducing the risk of consensus-splitting\nbugs. Yet such a specification has long been considered intractable given the\ncomplexity of Bitcoin's consensus logic. We demonstrate a compact, executable,\ndeclarative C++ specification of Bitcoin consensus rules that syncs mainnet to\ntip in a few hours on a single thread. We also introduce the Hornet\nDomain-Specific Language (DSL) specifically designed to encode these rules\nunambiguously for execution, enabling formal reasoning, consensus code\ngeneration, and AI-driven adversarial testing. Our spec-driven client Hornet\nNode offers a modern and modular complement to the reference client. Its clear,\nidiomatic style makes it suitable for education, while its performance makes it\nideal for experimentation. We highlight architectural contributions such as its\nlayered design, efficient data structures, and strong separation of concerns,\nsupported by production-quality code examples. We argue that Hornet Node and\nHornet DSL together provide the first credible path toward a pure, formal,\nexecutable specification of Bitcoin consensus."}
{"id": "2509.15694", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.15694", "abs": "https://arxiv.org/abs/2509.15694", "authors": ["Anastasiia Belousova", "Francesco Marchiori", "Mauro Conti"], "title": "Inference Attacks on Encrypted Online Voting via Traffic Analysis", "comment": "Accepted at ISC 2025", "summary": "Online voting enables individuals to participate in elections remotely,\noffering greater efficiency and accessibility in both governmental and\norganizational settings. As this method gains popularity, ensuring the security\nof online voting systems becomes increasingly vital, as the systems supporting\nit must satisfy a demanding set of security requirements. Most research in this\narea emphasizes the design and verification of cryptographic protocols to\nprotect voter integrity and system confidentiality. However, other vectors,\nsuch as network traffic analysis, remain relatively understudied, even though\nthey may pose significant threats to voter privacy and the overall\ntrustworthiness of the system.\n  In this paper, we examine how adversaries can exploit metadata from encrypted\nnetwork traffic to uncover sensitive information during online voting. Our\nanalysis reveals that, even without accessing the encrypted content, it is\npossible to infer critical voter actions, such as whether a person votes, the\nexact moment a ballot is submitted, and whether the ballot is valid or spoiled.\nWe test these attacks with both rule-based techniques and machine learning\nmethods. We evaluate our attacks on two widely used online voting platforms,\none proprietary and one partially open source, achieving classification\naccuracy as high as 99.5%. These results expose a significant privacy\nvulnerability that threatens key properties of secure elections, including\nvoter secrecy and protection against coercion or vote-buying. We explore\nmitigations to our attacks, demonstrating that countermeasures such as payload\npadding and timestamp equalization can substantially limit their effectiveness."}
{"id": "2509.15756", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15756", "abs": "https://arxiv.org/abs/2509.15756", "authors": ["Dongyang Zhan", "Kai Tan", "Lin Ye", "Xiangzhan Yu", "Hongli Zhang", "Zheng He"], "title": "An Adversarial Robust Behavior Sequence Anomaly Detection Approach Based on Critical Behavior Unit Learning", "comment": null, "summary": "Sequential deep learning models (e.g., RNN and LSTM) can learn the sequence\nfeatures of software behaviors, such as API or syscall sequences. However,\nrecent studies have shown that these deep learning-based approaches are\nvulnerable to adversarial samples. Attackers can use adversarial samples to\nchange the sequential characteristics of behavior sequences and mislead malware\nclassifiers. In this paper, an adversarial robustness anomaly detection method\nbased on the analysis of behavior units is proposed to overcome this problem.\nWe extract related behaviors that usually perform a behavior intention as a\nbehavior unit, which contains the representative semantic information of local\nbehaviors and can be used to improve the robustness of behavior analysis. By\nlearning the overall semantics of each behavior unit and the contextual\nrelationships among behavior units based on a multilevel deep learning model,\nour approach can mitigate perturbation attacks that target local and\nlarge-scale behaviors. In addition, our approach can be applied to both\nlow-level and high-level behavior logs (e.g., API and syscall logs). The\nexperimental results show that our approach outperforms all the compared\nmethods, which indicates that our approach has better performance against\nobfuscation attacks."}
{"id": "2509.15725", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.15725", "abs": "https://arxiv.org/abs/2509.15725", "authors": ["Matteo Repetto", "Enrico Cambiaso", "Fabio Patrone", "Sandro Zappatore"], "title": "Flying Drones to Locate Cyber-Attackers in LoRaWAN Metropolitan Networks", "comment": "12 pages", "summary": "Today, many critical services and industrial systems rely on wireless\nnetworks for interaction with the IoT, hence becoming vulnerable to a broad\nnumber of cyber-threats. While detecting this kind of attacks is not difficult\nwith common cyber-security tools, and even trivial for jamming, finding their\norigin and identifying culprits is almost impossible today, yet indispensable\nto stop them, especially when attacks are generated with portable or self-made\ndevices that continuously move around. To address this open challenge, the\nFOLLOWME project investigates the feasibility of using UAV to locate and even\nchase attackers during illicit usage of the radio spectrum. The main objective\nis to develop a cyber-physical security framework that integrates network\ntelemetry with wireless localization. The former triggers alarms in case of\nanomalies or known attack patterns and provides a coarse-grained indication of\nthe physical area (i.e., the position of affected access gateways), whereas the\nlatter systematically scans such area to identify the exact location of the\nattacker. The project will specifically address long-range metropolitan area\nnetworks and focus on the LoRaWAN protocol, which is the typical scenario for\nSmart City services."}
{"id": "2509.15754", "categories": ["cs.CR", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15754", "abs": "https://arxiv.org/abs/2509.15754", "authors": ["Toby Sharp"], "title": "Hornet Node and the Hornet DSL: A Minimal, Executable Specification for Bitcoin Consensus", "comment": null, "summary": "Bitcoin's consensus rules are encoded in the implementation of its reference\nclient: \"The code is the spec.\" Yet this code is unsuitable for formal\nverification due to side effects, mutable state, concurrency, and legacy\ndesign. A standalone formal specification would enable verification both across\nversions of the reference client and against new client implementations,\nstrengthening decentralization by reducing the risk of consensus-splitting\nbugs. Yet such a specification has long been considered intractable given the\ncomplexity of Bitcoin's consensus logic. We demonstrate a compact, executable,\ndeclarative C++ specification of Bitcoin consensus rules that syncs mainnet to\ntip in a few hours on a single thread. We also introduce the Hornet\nDomain-Specific Language (DSL) specifically designed to encode these rules\nunambiguously for execution, enabling formal reasoning, consensus code\ngeneration, and AI-driven adversarial testing. Our spec-driven client Hornet\nNode offers a modern and modular complement to the reference client. Its clear,\nidiomatic style makes it suitable for education, while its performance makes it\nideal for experimentation. We highlight architectural contributions such as its\nlayered design, efficient data structures, and strong separation of concerns,\nsupported by production-quality code examples. We argue that Hornet Node and\nHornet DSL together provide the first credible path toward a pure, formal,\nexecutable specification of Bitcoin consensus."}
{"id": "2509.15756", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15756", "abs": "https://arxiv.org/abs/2509.15756", "authors": ["Dongyang Zhan", "Kai Tan", "Lin Ye", "Xiangzhan Yu", "Hongli Zhang", "Zheng He"], "title": "An Adversarial Robust Behavior Sequence Anomaly Detection Approach Based on Critical Behavior Unit Learning", "comment": null, "summary": "Sequential deep learning models (e.g., RNN and LSTM) can learn the sequence\nfeatures of software behaviors, such as API or syscall sequences. However,\nrecent studies have shown that these deep learning-based approaches are\nvulnerable to adversarial samples. Attackers can use adversarial samples to\nchange the sequential characteristics of behavior sequences and mislead malware\nclassifiers. In this paper, an adversarial robustness anomaly detection method\nbased on the analysis of behavior units is proposed to overcome this problem.\nWe extract related behaviors that usually perform a behavior intention as a\nbehavior unit, which contains the representative semantic information of local\nbehaviors and can be used to improve the robustness of behavior analysis. By\nlearning the overall semantics of each behavior unit and the contextual\nrelationships among behavior units based on a multilevel deep learning model,\nour approach can mitigate perturbation attacks that target local and\nlarge-scale behaviors. In addition, our approach can be applied to both\nlow-level and high-level behavior logs (e.g., API and syscall logs). The\nexperimental results show that our approach outperforms all the compared\nmethods, which indicates that our approach has better performance against\nobfuscation attacks."}
{"id": "2509.16030", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.16030", "abs": "https://arxiv.org/abs/2509.16030", "authors": ["Kai Tan", "Dongyang Zhan", "Lin Ye", "Hongli Zhang", "Binxing Fang", "Zhihong Tian"], "title": "A High-performance Real-time Container File Monitoring Approach Based on Virtual Machine Introspection", "comment": null, "summary": "As cloud computing continues to advance and become an integral part of modern\nIT infrastructure, container security has emerged as a critical factor in\nensuring the smooth operation of cloud-native applications. An attacker can\nattack the service in the container or even perform the container escape attack\nby tampering with the files. Monitoring container files is important for APT\ndetection and cyberspace security. Existing file monitoring methods are usually\nbased on host operating system or virtual machine introspection to protect file\nsecurity in real time. The methods based on the host operating system usually\nmonitor file operations in the host operating system. However, when the\ncontainer escapes to the host, the host operating system will no longer be\nsecure, so these methods face the problem of weak security. Aiming at the\nproblems of low security and high overload introduced in existing container\nfile monitoring, a high-performance container file monitoring method based on\nvirtual machine introspection is proposed. The experimental results show that\nthe proposed approach can effectively monitor the container files and introduce\nan acceptable monitoring overload."}
{"id": "2509.16038", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.16038", "abs": "https://arxiv.org/abs/2509.16038", "authors": ["Miel Verkerken", "Laurens D'hooge", "Bruno Volckaert", "Filip De Turck", "Giovanni Apruzzese"], "title": "ConCap: Practical Network Traffic Generation for Flow-based Intrusion Detection Systems", "comment": "20 pages", "summary": "Network Intrusion Detection Systems (NIDS) have been studied in research for\nalmost four decades. Yet, despite thousands of papers claiming scientific\nadvances, a non-negligible number of recent works suggest that the findings of\nprior literature may be questionable. At the root of such a disagreement is the\nwell-known challenge of obtaining data representative of a real-world\nnetwork-and, hence, usable for security assessments. We tackle such a challenge\nin this paper. We propose ConCap, a practical tool meant to facilitate\nexperimental research on NIDS. Through ConCap, a researcher can set up an\nisolated and lightweight network environment and configure it to produce\nnetwork-related data, such as packets or NetFlows, that are automatically\nlabeled, hence ready for fine-grained experiments. ConCap is rooted on\nopen-source software and is designed to foster experimental reproducibility\nacross the scientific community by sharing just one configuration file. Through\ncomprehensive experiments on 10 different network activities, further expanded\nvia in-depth analyses of 21 variants of two specific activities and of 100\nrepetitions of four other ones, we empirically verify that ConCap produces\nnetwork data resembling that of a real-world network. We also carry out\nexperiments on well-known benchmark datasets as well as on a real \"smart-home\"\nnetwork, showing that, from a cyber-detection viewpoint, ConCap's\nautomatically-labeled NetFlows are functionally equivalent to those collected\nin other environments. Finally, we show that ConCap enables to safely reproduce\nsophisticated attack chains (e.g., to test/enhance existing NIDS). Altogether,\nConCap is a solution to the \"data problem\" that is plaguing NIDS research."}
{"id": "2509.16052", "categories": ["cs.CR", "cs.DC", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2509.16052", "abs": "https://arxiv.org/abs/2509.16052", "authors": ["Vabuk Pahari", "Andrea Canidio"], "title": "How Exclusive are Ethereum Transactions? Evidence from non-winning blocks", "comment": "arXiv admin note: text overlap with arXiv:2506.04940", "summary": "We analyze 15,097 blocks proposed for inclusion in Ethereum's blockchain over\nan 8-minute window on December 3, 2024, during which 38 blocks were added to\nthe chain. We classify transactions as exclusive -- present only in blocks from\na single builder -- or private -- absent from the public mempool but included\nin blocks from multiple builders. We find that exclusive transactions account\nfor 84% of the total fees paid by transactions in winning blocks. Furthermore,\nwe show that exclusivity cannot be fully explained by exclusive relationships\nbetween senders and builders: about 7% of all exclusive transactions included\non-chain, by value, come from senders who route exclusively to a single\nbuilder. Analyzing transaction logs shows that some exclusive transactions are\nduplicates or variations of the same strategy, but even accounting for that,\nthe share of the total fees paid by transactions in winning blocks is at least\n77.2%. Taken together, our findings highlight that exclusive transactions are\nthe dominant source of builder revenues."}
