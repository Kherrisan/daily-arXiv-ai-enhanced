{"id": "2508.00031", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00031", "abs": "https://arxiv.org/abs/2508.00031", "authors": ["Junde Wu"], "title": "Git Context Controller: Manage the Context of LLM-based Agents like Git", "comment": "in updating", "summary": "Large language model (LLM) based agents have shown impressive capabilities by\ninterleaving internal reasoning with external tool use. However, as these\nagents are deployed in long-horizon workflows, such as coding for a big,\nlong-term project, context management becomes a critical bottleneck. We\nintroduce Git-Context-Controller (GCC), a structured context management\nframework inspired by software version control systems. GCC elevates context as\nversioned memory hierarchy like Git. It structures agent memory as a persistent\nfile system with explicit operations: COMMIT, BRANCH, MERGE, and CONTEXT,\nenabling milestone-based checkpointing, exploration of alternative plans, and\nstructured reflection. Our approach empowers agents to manage long-term goals,\nisolate architectural experiments, and recover or hand off memory across\nsessions and agents. Empirically, agents equipped with GCC achieve\nstate-of-the-art performance on the SWE-Bench-Lite benchmark, resolving 48.00\nof software bugs, outperforming 26 competitive systems. In a self-replication\ncase study, a GCC-augmented agent builds a new CLI agent from scratch,\nachieving 40.7 task resolution, compared to only 11.7 without GCC. The code is\nreleased at: https://github.com/theworldofagents/GCC"}
{"id": "2508.00033", "categories": ["cs.SE", "cs.AI", "cs.CL", "68T50", "I.2.2; I.2.7; D.2.3"], "pdf": "https://arxiv.org/pdf/2508.00033", "abs": "https://arxiv.org/abs/2508.00033", "authors": ["Nuno Fachada", "Daniel Fernandes", "Carlos M. Fernandes", "Bruno D. Ferreira-Saraiva", "Jo√£o P. Matos-Carvalho"], "title": "GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries", "comment": null, "summary": "Large Language Models (LLMs) have advanced rapidly as tools for automating\ncode generation in scientific research, yet their ability to interpret and use\nunfamiliar Python APIs for complex computational experiments remains poorly\ncharacterized. This study systematically benchmarks a selection of\nstate-of-the-art LLMs in generating functional Python code for two increasingly\nchallenging scenarios: conversational data analysis with the \\textit{ParShift}\nlibrary, and synthetic data generation and clustering using \\textit{pyclugen}\nand \\textit{scikit-learn}. Both experiments use structured, zero-shot prompts\nspecifying detailed requirements but omitting in-context examples. Model\noutputs are evaluated quantitatively for functional correctness and prompt\ncompliance over multiple runs, and qualitatively by analyzing the errors\nproduced when code execution fails. Results show that only a small subset of\nmodels consistently generate correct, executable code, with GPT-4.1 standing\nout as the only model to always succeed in both tasks. In addition to\nbenchmarking LLM performance, this approach helps identify shortcomings in\nthird-party libraries, such as unclear documentation or obscure implementation\nbugs. Overall, these findings highlight current limitations of LLMs for\nend-to-end scientific automation and emphasize the need for careful prompt\ndesign, comprehensive library documentation, and continued advances in language\nmodel capabilities."}
{"id": "2508.00045", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00045", "abs": "https://arxiv.org/abs/2508.00045", "authors": ["Samah Kansab"], "title": "Machine Learning Pipeline for Software Engineering: A Systematic Literature Review", "comment": null, "summary": "The rapid advancement of software development practices has introduced\nchallenges in ensuring quality and efficiency across the software engineering\n(SE) lifecycle. As SE systems grow in complexity, traditional approaches often\nfail to scale, resulting in longer debugging times, inefficient defect\ndetection, and resource-heavy development cycles. Machine Learning (ML) has\nemerged as a key solution, enabling automation in tasks such as defect\nprediction, code review, and release quality estimation. However, the\neffectiveness of ML in SE depends on the robustness of its pipeline, including\ndata collection, preprocessing, feature engineering, algorithm selection,\nvalidation, and evaluation.\n  This systematic literature review (SLR) examines state-of-the-art ML\npipelines designed for SE, consolidating best practices, challenges, and gaps.\nOur findings show that robust preprocessing, such as SMOTE for data balancing\nand SZZ-based algorithms for feature selection, improves model reliability.\nEnsemble methods like Random Forest and Gradient Boosting dominate performance\nacross tasks, while simpler models such as Naive Bayes remain valuable for\nefficiency and interpretability. Evaluation metrics including AUC, F1-score,\nand precision are most common, with new metrics like Best Arithmetic Mean (BAM)\nemerging in niche applications. Validation techniques such as bootstrapping are\nwidely used to ensure model stability and generalizability.\n  This SLR highlights the importance of well-designed ML pipelines for\naddressing SE challenges and provides actionable insights for researchers and\npractitioners seeking to optimize software quality and efficiency. By\nidentifying gaps and trends, this study sets a foundation for advancing ML\nadoption and fostering innovation in increasingly complex development\nenvironments."}
{"id": "2508.00083", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00083", "abs": "https://arxiv.org/abs/2508.00083", "authors": ["Yihong Dong", "Xue Jiang", "Jiaru Qian", "Tian Wang", "Kechi Zhang", "Zhi Jin", "Ge Li"], "title": "A Survey on Code Generation with LLM-based Agents", "comment": "Work in progress", "summary": "Code generation agents powered by large language models (LLMs) are\nrevolutionizing the software development paradigm. Distinct from previous code\ngeneration techniques, code generation agents are characterized by three core\nfeatures. 1) Autonomy: the ability to independently manage the entire workflow,\nfrom task decomposition to coding and debugging. 2) Expanded task scope:\ncapabilities that extend beyond generating code snippets to encompass the full\nsoftware development lifecycle (SDLC). 3) Enhancement of engineering\npracticality: a shift in research emphasis from algorithmic innovation toward\npractical engineering challenges, such as system reliability, process\nmanagement, and tool integration. This domain has recently witnessed rapid\ndevelopment and an explosion in research, demonstrating significant application\npotential. This paper presents a systematic survey of the field of LLM-based\ncode generation agents. We trace the technology's developmental trajectory from\nits inception and systematically categorize its core techniques, including both\nsingle-agent and multi-agent architectures. Furthermore, this survey details\nthe applications of LLM-based agents across the full SDLC, summarizes\nmainstream evaluation benchmarks and metrics, and catalogs representative\ntools. Finally, by analyzing the primary challenges, we identify and propose\nseveral foundational, long-term research directions for the future work of the\nfield."}
{"id": "2508.00293", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.00293", "abs": "https://arxiv.org/abs/2508.00293", "authors": ["Md Sajidul Islam Sajid", "Jinpeng Wei", "Ehab Al-Shaer"], "title": "ranDecepter: Real-time Identification and Deterrence of Ransomware Attacks", "comment": "Accepted at IEEE Conference on Communications and Network Security\n  (CNS) 2025", "summary": "Ransomware (RW) presents a significant and widespread threat in the digital\nlandscape, necessitating effective countermeasures. Active cyber deception is a\npromising strategy to thwart RW and limiting its propagation by misleading it\nwith false information and revealing its true behaviors. Furthermore, RW often\nacts as a communication conduit between attackers and defenders, allowing\ndeception to return false data to attackers and deplete their resources. This\npaper introduces ranDecepter, a novel approach that combines active cyber\ndeception with real-time analysis to enhance defenses against RW attacks. The\nranDecepter identifies RW in real-time and isolates it within a deceptive\nenvironment, autonomously identifying critical elements in the RW code to\ncreate a loop mechanism. By repeatedly restarting the malware and transmitting\ncounterfeit encryption information and secret keys to the attacker, it forces\nthe attacker to store these fabricated details for each victim, thereby\ndepleting their resources. Our comprehensive evaluation of ranDecepter,\nconducted using 1,134 real-world malware samples and twelve benign\napplications, demonstrates a remarkable 100% accuracy in RW identification,\nwith no false positives and minimal impact on response times. Furthermore,\nwithin 24-hours, ranDecepter generates up to 9,223K entries in the attacker's\ndatabase using 50 agents, showcasing its potential to undermine attacker\nresources."}
{"id": "2508.00128", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00128", "abs": "https://arxiv.org/abs/2508.00128", "authors": ["Md Nazmul Haque", "Hua Yang", "Zhou Yang", "Bowen Xu"], "title": "How Quantization Impacts Privacy Risk on LLMs for Code?", "comment": null, "summary": "Large language models for code (LLMs4Code) rely heavily on massive training\ndata, including sensitive data, such as cloud service credentials of the\nprojects and personal identifiable information of the developers, raising\nserious privacy concerns. Membership inference (MI) has recently emerged as an\neffective tool for assessing privacy risk by identifying whether specific data\nbelong to a model's training set. In parallel, model compression techniques,\nespecially quantization, have gained traction for reducing computational costs\nand enabling the deployment of large models. However, while quantized models\nstill retain knowledge learned from the original training data, it remains\nunclear whether quantization affects their ability to retain and expose privacy\ninformation. Answering this question is of great importance to understanding\nprivacy risks in real-world deployments. In this work, we conduct the first\nempirical study on how quantization influences task performance and privacy\nrisk simultaneously in LLMs4Code. To do this, we implement widely used\nquantization techniques (static and dynamic) to three representative model\nfamilies, namely Pythia, CodeGen, and GPTNeo. Our results demonstrate that\nquantization has a significant impact on reducing the privacy risk relative to\nthe original model. We also uncover a positive correlation between task\nperformance and privacy risk, indicating an underlying tradeoff. Moreover, we\nreveal the possibility that quantizing larger models could yield better balance\nthan using full-precision small models. Finally, we demonstrate that these\nfindings generalize across different architectures, model sizes and MI methods,\noffering practical guidance for safeguarding privacy when deploying compressed\nLLMs4Code."}
{"id": "2508.00351", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.00351", "abs": "https://arxiv.org/abs/2508.00351", "authors": ["Hyeonhak Kim", "Donghoe Heo", "Seokhie Hong"], "title": "Cryptanalysis of Isogeny-Based Quantum Money with Rational Points", "comment": null, "summary": "Quantum money is the cryptographic application of the quantum no-cloning\ntheorem. It has recently been instantiated by Montgomery and Sharif (Asiacrypt\n'24) from class group actions on elliptic curves. In this work, we propose a\nconcrete cryptanalysis by leveraging the efficiency of evaluating division\npolynomials with the coordinates of rational points, offering a speedup of\nO(log^4p) compared to the brute-force attack. Since our attack still requires\nexponential time, it remains impractical to forge a quantum banknote.\nInterestingly, due to the inherent properties of quantum money, our attack\nmethod also results in a more efficient verification procedure. Our algorithm\nleverages the properties of quadratic twists to utilize rational points in\nverifying the cardinality of the superposition of elliptic curves. We expect\nthis approach to contribute to future research on elliptic-curve-based quantum\ncryptography."}
{"id": "2508.00198", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00198", "abs": "https://arxiv.org/abs/2508.00198", "authors": ["Cleyton Magalhaes", "Italo Santos", "Brody Stuart-Verner", "Ronnie de Souza Santos"], "title": "Testing the Untestable? An Empirical Study on the Testing Process of LLM-Powered Software Systems", "comment": null, "summary": "Background: Software systems powered by large language models are becoming a\nroutine part of everyday technologies, supporting applications across a wide\nrange of domains. In software engineering, many studies have focused on how\nLLMs support tasks such as code generation, debugging, and documentation.\nHowever, there has been limited focus on how full systems that integrate LLMs\nare tested during development. Aims: This study explores how LLM-powered\nsystems are tested in the context of real-world application development.\nMethod: We conducted an exploratory case study using 99 individual reports\nwritten by students who built and deployed LLM-powered applications as part of\na university course. Each report was independently analyzed using thematic\nanalysis, supported by a structured coding process. Results: Testing strategies\ncombined manual and automated methods to evaluate both system logic and model\nbehavior. Common practices included exploratory testing, unit testing, and\nprompt iteration. Reported challenges included integration failures,\nunpredictable outputs, prompt sensitivity, hallucinations, and uncertainty\nabout correctness. Conclusions: Testing LLM-powered systems required\nadaptations to traditional verification methods, blending source-level\nreasoning with behavior-aware evaluations. These findings provide evidence on\nthe practical context of testing generative components in software systems."}
{"id": "2508.00368", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00368", "abs": "https://arxiv.org/abs/2508.00368", "authors": ["Alessandro Gaudenzi", "Lorenzo Nodari", "Lance Kaplan", "Alessandra Russo", "Murat Sensoy", "Federico Cerutti"], "title": "Preliminary Investigation into Uncertainty-Aware Attack Stage Classification", "comment": "Proceedings for SPAIML2025 workshop, 26/10/2025 Bologna Italy,\n  co-located with ECAI2025", "summary": "Advanced Persistent Threats (APTs) represent a significant challenge in\ncybersecurity due to their prolonged, multi-stage nature and the sophistication\nof their operators. Traditional detection systems typically focus on\nidentifying malicious activity in binary terms (benign or malicious) without\naccounting for the progression of an attack. However, effective response\nstrategies depend on accurate inference of the attack's current stage, as\ncountermeasures must be tailored to whether an adversary is in the early\nreconnaissance phase or actively conducting exploitation or exfiltration. This\nwork addresses the problem of attack stage inference under uncertainty, with a\nfocus on robustness to out-of-distribution (OOD) inputs. We propose a\nclassification approach based on Evidential Deep Learning (EDL), which models\npredictive uncertainty by outputting parameters of a Dirichlet distribution\nover possible stages. This allows the system not only to predict the most\nlikely stage of an attack but also to indicate when it is uncertain or the\ninput lies outside the training distribution. Preliminary experiments in a\nsimulated environment demonstrate that the proposed model can accurately infer\nthe stage of an attack with calibrated confidence while effectively detecting\nOOD inputs, which may indicate changes in the attackers' tactics. These results\nsupport the feasibility of deploying uncertainty-aware models for staged threat\ndetection in dynamic and adversarial environments."}
{"id": "2508.00244", "categories": ["cs.SE", "cs.PL", "D.3.2; D.2.11; D.2.13"], "pdf": "https://arxiv.org/pdf/2508.00244", "abs": "https://arxiv.org/abs/2508.00244", "authors": ["Briza Mel Dias de Sousa", "Renato Cordeiro Ferreira", "Alfredo Goldman"], "title": "Functional vs. Object-Oriented: Comparing How Programming Paradigms Affect the Architectural Characteristics of Systems", "comment": "11 pages, 16 figures (1 table, 3 diagrams, 5 graphics, 7 listings),\n  submitted to CTICQS capstone project competition at SBQS 2025", "summary": "After decades of dominance by object-oriented programming (OOP), functional\nprogramming (FP) is gaining increasing attention in the software industry. This\nstudy compares the impact of OOP and FP on the architectural characteristics of\nsoftware systems. For that, it examines the design and implementation of a\nDigital Wallet system, developed in Kotlin (representing OOP) and Scala\n(representing FP). The comparison is made through both qualitative and\nquantitative analyses to explore how each paradigm influences the system's\narchitectural characteristics. The self-ethnographic qualitative analysis\nprovides a side-by-side comparison of both implementations, revealing the\nperspective of those writing such code. The survey-based quantitative analysis\ngathers feedback from developers with diverse backgrounds, showing their\nimpressions of those reading this code. Hopefully, these results may be useful\nfor developers or organizations seeking to make more informed decisions about\nwhich paradigm is best suited for their next project."}
{"id": "2508.00434", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.00434", "abs": "https://arxiv.org/abs/2508.00434", "authors": ["Yuqi Qian", "Yun Cao", "Meiyang Lv", "Haocheng Fu"], "title": "Accurate Latent Inversion for Generative Image Steganography via Rectified Flow", "comment": null, "summary": "Steganography based on diffusion models has attracted increasing attention\ndue to its ability to generate high-quality images and exhibit strong\nrobustness. In such approaches, the secret message is first embedded into the\ninitial latent variable, and then the stego image is generated through the\nforward process. To extract the message, an inversion process is required to\nreconstruct the latent variables from the received image. However, inaccurate\nlatent inversion leads to significant discrepancies between the reconstructed\nand original latent variables, rendering message extraction infeasible. To\naddress this issue, we propose \\textbf{RF-Stego}, a novel generative image\nsteganography method that enables accurate latent inversion and significantly\nimproves extraction accuracy. First, we develop the \\textbf{P}ath\n\\textbf{C}onsistency \\textbf{L}inear \\textbf{I}nversion (\\textbf{PCLI}), which\nimposes formal constraints on the inversion process. By explicitly aligning it\nwith the forward generation path and modeling both directions along a shared\nlinear path, PCLI eliminates path mismatch and ensures path consistency\nthroughout the steganographic process. Second, through rigorous theoretical\nproof, we demonstrate that \\textbf{R}ectified \\textbf{F}low \\textbf{(RF)}\noffers both theoretical reversibility and numerical stability in the inversion\nprocess. Based on this, we replace traditional unstable samplers with RF\nsampler which effectively improves the numerical precision of the inversion\nprocess. Experimental results show RF-Stego outperforms state-of-the-art\nmethods in terms of extraction accuracy, image quality, robustness, security\nand generation efficiency."}
{"id": "2508.00253", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00253", "abs": "https://arxiv.org/abs/2508.00253", "authors": ["Moumita Asad", "Rafed Muhammad Yasir", "Armin Geramirad", "Sam Malek"], "title": "Leveraging Large Language Model for Information Retrieval-based Bug Localization", "comment": null, "summary": "Information Retrieval-based Bug Localization aims to identify buggy source\nfiles for a given bug report. While existing approaches -- ranging from vector\nspace models to deep learning models -- have shown potential in this domain,\ntheir effectiveness is often limited by the vocabulary mismatch between bug\nreports and source code. To address this issue, we propose a novel Large\nLanguage Model (LLM) based bug localization approach, called GenLoc. Given a\nbug report, GenLoc leverages an LLM equipped with code-exploration functions to\niteratively analyze the code base and identify potential buggy files. To gather\nbetter context, GenLoc may optionally retrieve semantically relevant files\nusing vector embeddings. GenLoc has been evaluated on over 9,000 real-world bug\nreports from six large-scale Java projects. Experimental results show that\nGenLoc outperforms five state-of-the-art bug localization techniques across\nmultiple metrics, achieving an average improvement of more than 60\\% in\nAccuracy@1."}
{"id": "2508.00478", "categories": ["cs.CR", "cs.AI", "91A10, 91A43, 68T01, 94A60", "C.2.0; I.2.6; K.6.5"], "pdf": "https://arxiv.org/pdf/2508.00478", "abs": "https://arxiv.org/abs/2508.00478", "authors": ["Yuning Jiang", "Nay Oo", "Qiaoran Meng", "Lu Lin", "Dusit Niyato", "Zehui Xiong", "Hoon Wei Lim", "Biplab Sikdar"], "title": "CyGATE: Game-Theoretic Cyber Attack-Defense Engine for Patch Strategy Optimization", "comment": null, "summary": "Modern cyber attacks unfold through multiple stages, requiring defenders to\ndynamically prioritize mitigations under uncertainty. While game-theoretic\nmodels capture attacker-defender interactions, existing approaches often rely\non static assumptions and lack integration with real-time threat intelligence,\nlimiting their adaptability. This paper presents CyGATE, a game-theoretic\nframework modeling attacker-defender interactions, using large language models\n(LLMs) with retrieval-augmented generation (RAG) to enhance tactic selection\nand patch prioritization. Applied to a two-agent scenario, CyGATE frames cyber\nconflicts as a partially observable stochastic game (POSG) across Cyber Kill\nChain stages. Both agents use belief states to navigate uncertainty, with the\nattacker adapting tactics and the defender re-prioritizing patches based on\nevolving risks and observed adversary behavior. The framework's flexible\narchitecture enables extension to multi-agent scenarios involving coordinated\nattackers, collaborative defenders, or complex enterprise environments with\nmultiple stakeholders. Evaluated in a dynamic patch scheduling scenario, CyGATE\neffectively prioritizes high-risk vulnerabilities, enhancing adaptability\nthrough dynamic threat integration, strategic foresight by anticipating\nattacker moves under uncertainty, and efficiency by optimizing resource use."}
{"id": "2508.00255", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00255", "abs": "https://arxiv.org/abs/2508.00255", "authors": ["Boqi Chen", "Ou Wei", "Bingzhou Zheng", "Gunter Mussbacher"], "title": "Accurate and Consistent Graph Model Generation from Text with Large Language Models", "comment": "Accepted at ACM / IEEE 28th International Conference on Model Driven\n  Engineering Languages and Systems (MODELS 2025)", "summary": "Graph model generation from natural language description is an important task\nwith many applications in software engineering. With the rise of large language\nmodels (LLMs), there is a growing interest in using LLMs for graph model\ngeneration. Nevertheless, LLM-based graph model generation typically produces\npartially correct models that suffer from three main issues: (1) syntax\nviolations: the generated model may not adhere to the syntax defined by its\nmetamodel, (2) constraint inconsistencies: the structure of the model might not\nconform to some domain-specific constraints, and (3) inaccuracy: due to the\ninherent uncertainty in LLMs, the models can include inaccurate, hallucinated\nelements. While the first issue is often addressed through techniques such as\nconstraint decoding or filtering, the latter two remain largely unaddressed.\nMotivated by recent self-consistency approaches in LLMs, we propose a novel\nabstraction-concretization framework that enhances the consistency and quality\nof generated graph models by considering multiple outputs from an LLM. Our\napproach first constructs a probabilistic partial model that aggregates all\ncandidate outputs and then refines this partial model into the most appropriate\nconcrete model that satisfies all constraints. We evaluate our framework on\nseveral popular open-source and closed-source LLMs using diverse datasets for\nmodel generation tasks. The results demonstrate that our approach significantly\nimproves both the consistency and quality of the generated graph models."}
{"id": "2508.00555", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00555", "abs": "https://arxiv.org/abs/2508.00555", "authors": ["Jiecong Wang", "Haoran Li", "Hao Peng", "Ziqian Zeng", "Zihao Wang", "Haohua Du", "Zhengtao Yu"], "title": "Activation-Guided Local Editing for Jailbreaking Attacks", "comment": null, "summary": "Jailbreaking is an essential adversarial technique for red-teaming these\nmodels to uncover and patch security flaws. However, existing jailbreak methods\nface significant drawbacks. Token-level jailbreak attacks often produce\nincoherent or unreadable inputs and exhibit poor transferability, while\nprompt-level attacks lack scalability and rely heavily on manual effort and\nhuman ingenuity. We propose a concise and effective two-stage framework that\ncombines the advantages of these approaches. The first stage performs a\nscenario-based generation of context and rephrases the original malicious query\nto obscure its harmful intent. The second stage then utilizes information from\nthe model's hidden states to guide fine-grained edits, effectively steering the\nmodel's internal representation of the input from a malicious toward a benign\none. Extensive experiments demonstrate that this method achieves\nstate-of-the-art Attack Success Rate, with gains of up to 37.74% over the\nstrongest baseline, and exhibits excellent transferability to black-box models.\nOur analysis further demonstrates that AGILE maintains substantial\neffectiveness against prominent defense mechanisms, highlighting the\nlimitations of current safeguards and providing valuable insights for future\ndefense development. Our code is available at\nhttps://github.com/yunsaijc/AGILE."}
{"id": "2508.00408", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00408", "abs": "https://arxiv.org/abs/2508.00408", "authors": ["Dong Huang", "Jie M. Zhang", "Mark Harman", "Qianru Zhang", "Mingzhe Du", "See-Kiong Ng"], "title": "Benchmarking LLMs for Unit Test Generation from Real-World Functions", "comment": "Under Review", "summary": "Recently, large language models (LLMs) have shown great promise in automating\nunit test generation, significantly reducing the manual effort required by\ndevelopers. To effectively evaluate the capabilities of LLMs in this domain, it\nis crucial to have a well-designed benchmark that accurately reflects\nreal-world scenarios and mitigates common pitfalls. Existing LLM test\ngeneration benchmarks are limited by two critical drawbacks: data contamination\nand structurally simple function code. As a result, we often cannot rely on the\nvalidity of scientific conclusions drawn from empirical studies using these\nlimited benchmarks. The empirical evidence presented may be biased due to\ncontamination and may fail to generalize beyond toy programs due to structural\nsimplicity.\n  To address these problems, we introduce ULT (UnLeakedTestbench), a new\nbenchmark specifically designed for function-level unit test generation from\nreal-world Python functions. ULT is constructed through a multi-stage curation\nprocess that ensures high cyclomatic complexity and mitigates test case\ncontamination. With 3,909 carefully selected function-level tasks, ULT provides\na more realistic and challenging evaluation of LLMs' test generation\ncapabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT\nwith leaked tests designed to enable a controlled analysis of memorization\nversus reasoning in test generation. Our evaluation results demonstrate that\nULT is significantly more challenging. For example, test cases generated by\nLLMs only achieve 41.32\\%, 45.10\\%, 30.22\\%, and 40.21\\% for accuracy,\nstatement coverage, branch coverage, and mutation score on average for all\nLLMs, respectively. These results are substantially lower than the\ncorresponding metrics on TestEval (91.79\\%, 92.18\\%, 82.04\\%, and 49.69\\%) and\nPLT (47.07\\%, 55.13\\%, 40.07\\%, and 50.80\\%)."}
{"id": "2508.00602", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00602", "abs": "https://arxiv.org/abs/2508.00602", "authors": ["Francesco Panebianco", "Stefano Bonfanti", "Francesco Trov√≤", "Michele Carminati"], "title": "LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection and Leakage Attacks", "comment": "22 pages, preprint", "summary": "The generalization capabilities of Large Language Models (LLMs) have led to\ntheir widespread deployment across various applications. However, this\nincreased adoption has introduced several security threats, notably in the\nforms of jailbreaking and data leakage attacks. Additionally, Retrieval\nAugmented Generation (RAG), while enhancing context-awareness in LLM responses,\nhas inadvertently introduced vulnerabilities that can result in the leakage of\nsensitive information. Our contributions are twofold. First, we introduce a\nmethodology to analyze historical interaction data from an LLM system, enabling\nthe generation of usage maps categorized by topics (including adversarial\ninteractions). This approach further provides forensic insights for tracking\nthe evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a\nmodel-agnostic framework that combines static analysis for forensic insights\nwith dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique\nidentifies topic groups and detects anomalous patterns, allowing for proactive\ndefense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1)\njailbreak attempts, employing a public benchmark dataset, and (2) PII leakage,\nsupported by a curated dataset of labeled LLM interactions. In the static\nsetting, LeakSealer achieves the highest precision and recall on the ToxicChat\ndataset when identifying prompt injection. In the dynamic setting, PII leakage\ndetection achieves an AUPRC of $0.97$, significantly outperforming baselines\nsuch as Llama Guard."}
{"id": "2508.00462", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00462", "abs": "https://arxiv.org/abs/2508.00462", "authors": ["Linus Ververs", "Lutz Prechelt"], "title": "Managing Power Gaps as a Topic of Pair Programming Skill: A Grounded Theory", "comment": null, "summary": "Context: Pair Programming as a work mode is used (occasionally or frequently)\nthroughout professional software development. Objective: Understand what\npower-related phenomena occur in pair programming as it is used in industry;\ngive advice to practitioners on how to do better pair programming. Method:\nAnalyze 22 industrial pair programming sessions using Grounded Theory\nMethodology. Formulate a Grounded Theory on power-related behaviors. Run a\nsurvey with 292 participants about that theory. Use it to demonstrate that the\nphenomena are common. Results: Our theory describes the phenomenon of Power\nGap: a perceived difference in participation opportunities. The theory shows\nthe behaviors that create a Power Gap or result from it. Power Gaps tend to\ndamage knowledge transfer, code quality, and process effi ciency. The survey\nresults show that all concepts from our theory are frequent in practice. They\nalso provide more grounding for concepts that are observable only indirectly.\nConclusions: It is a valuable component of pair programming skill to be able to\navoid Power Gaps. Specifically, pair partners need to avoid Hierarchical\nBehavior (which tends to create or increase a Power Gap) and should perform\nenough Equalizing Behavior (which prevents or reduces a Power Gap)."}
{"id": "2508.00636", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.00636", "abs": "https://arxiv.org/abs/2508.00636", "authors": ["Haocheng Jiang", "Hua Shen", "Jixin Zhang", "Willy Susilo", "Mingwu Zhang"], "title": "FedGuard: A Diverse-Byzantine-Robust Mechanism for Federated Learning with Major Malicious Clients", "comment": null, "summary": "Federated learning is a distributed training framework vulnerable to\nByzantine attacks, particularly when over 50% of clients are malicious or when\ndatasets are highly non-independent and identically distributed (non-IID).\nAdditionally, most existing defense mechanisms are designed for specific attack\ntypes (e.g., gradient similarity-based schemes can only defend against outlier\nmodel poisoning), limiting their effectiveness. In response, we propose\nFedGuard, a novel federated learning mechanism. FedGuard cleverly addresses the\naforementioned issues by leveraging the high sensitivity of membership\ninference to model bias. By requiring clients to include an additional\nmini-batch of server-specified data in their training, FedGuard can identify\nand exclude poisoned models, as their confidence in the mini-batch will drop\nsignificantly. Our comprehensive evaluation unequivocally shows that, under\nthree highly non-IID datasets, with 90% of clients being Byzantine and seven\ndifferent types of Byzantine attacks occurring in each round, FedGuard\nsignificantly outperforms existing robust federated learning schemes in\nmitigating various types of Byzantine attacks."}
{"id": "2508.00508", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.00508", "abs": "https://arxiv.org/abs/2508.00508", "authors": ["Panagiotis Diamantakis", "Thanassis Avgerinos", "Yannis Smaragdakis"], "title": "Desyan: A Platform for Seamless Value-Flow and Symbolic Analysis", "comment": null, "summary": "Over the past two decades, two different types of static analyses have\nemerged as dominant paradigms both in academia and industry: value-flow\nanalysis (e.g., data-flow analysis or points-to analysis) and symbolic analysis\n(e.g., symbolic execution). Despite their individual successes in numerous\napplication fields, the two approaches have remained largely separate; an\nartifact of the simple reality that there is no broadly adopted unifying\nplatform for effortless and efficient integration of symbolic techniques with\nhigh-performance data-flow reasoning.\n  To bridge this gap, we introduce Desyan: a platform for writing program\nanalyses with seamless integration of value-flow and symbolic reasoning. Desyan\nexpands a production-ready Datalog fixpoint engine (Souffl\\'e) with\nfull-fledged SMT solving invoking industry-leading SMT engines. Desyan provides\nconstructs for automatically (and efficiently!) handling typical patterns that\ncome up in program analysis. At the same time, the integration is agnostic with\nrespect to the solving technology, and supports Datalog-native symbolic\nreasoning, via a bottom-up algebraic reasoning module.\n  The result is an engine that allows blending different kinds of reasoning, as\nneeded for the underlying analysis. For value-flow analysis, the engine is the\nbest-in-class Datalog evaluator (often by a factor of over 20x in execution\ntime); for applications that require full SMT (e.g., a concolic execution\nengine or other symbolic evaluator that needs to solve arbitrarily complex\nconditions), the engine is leveraging the leading SMT solvers; for lightweight\nsymbolic evaluation (e.g., solving simple conditionals in the context of a\npath-sensitive analysis), the engine can use Datalog-native symbolic reasoning,\nachieving large speedups (often of over 2x) compared to eagerly appealing to an\nSMT solver."}
{"id": "2508.00659", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00659", "abs": "https://arxiv.org/abs/2508.00659", "authors": ["Xinzhang Chen", "Hassan Ali", "Arash Shaghaghi", "Salil S. Kanhere", "Sanjay Jha"], "title": "Demo: TOSense -- What Did You Just Agree to?", "comment": "Accepted as a demonstration paper at IEEE LCN 2025", "summary": "Online services often require users to agree to lengthy and obscure Terms of\nService (ToS), leading to information asymmetry and legal risks. This paper\nproposes TOSense-a Chrome extension that allows users to ask questions about\nToS in natural language and get concise answers in real time. The system\ncombines (i) a crawler \"tos-crawl\" that automatically extracts ToS content, and\n(ii) a lightweight large language model pipeline: MiniLM for semantic retrieval\nand BART-encoder for answer relevance verification. To avoid expensive manual\nannotation, we present a novel Question Answering Evaluation Pipeline (QEP)\nthat generates synthetic questions and verifies the correctness of answers\nusing clustered topic matching. Experiments on five major platforms, Apple,\nGoogle, X (formerly Twitter), Microsoft, and Netflix, show the effectiveness of\nTOSense (with up to 44.5% accuracy) across varying number of topic clusters.\nDuring the demonstration, we will showcase TOSense in action. Attendees will be\nable to experience seamless extraction, interactive question answering, and\ninstant indexing of new sites."}
{"id": "2508.00546", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00546", "abs": "https://arxiv.org/abs/2508.00546", "authors": ["Wenchao Gu", "Zongyi Lyu", "Yanlin Wang", "Hongyu Zhang", "Cuiyun Gao", "Michael R. Lyu"], "title": "SPENCER: Self-Adaptive Model Distillation for Efficient Code Retrieval", "comment": null, "summary": "Code retrieval aims to provide users with desired code snippets based on\nusers' natural language queries. With the development of deep learning\ntechnologies, adopting pre-trained models for this task has become mainstream.\nConsidering the retrieval efficiency, most of the previous approaches adopt a\ndual-encoder for this task, which encodes the description and code snippet into\nrepresentation vectors, respectively. However, the model structure of the\ndual-encoder tends to limit the model's performance, since it lacks the\ninteraction between the code snippet and description at the bottom layer of the\nmodel during training. To improve the model's effectiveness while preserving\nits efficiency, we propose a framework, which adopts Self-AdaPtive Model\nDistillation for Efficient CodE Retrieval, named SPENCER. SPENCER first adopts\nthe dual-encoder to narrow the search space and then adopts the cross-encoder\nto improve accuracy. To improve the efficiency of SPENCER, we propose a novel\nmodel distillation technique, which can greatly reduce the inference time of\nthe dual-encoder while maintaining the overall performance. We also propose a\nteaching assistant selection strategy for our model distillation, which can\nadaptively select the suitable teaching assistant models for different\npre-trained models during the model distillation to ensure the model\nperformance. Extensive experiments demonstrate that the combination of\ndual-encoder and cross-encoder improves overall performance compared to solely\ndual-encoder-based models for code retrieval. Besides, our model distillation\ntechnique retains over 98% of the overall performance while reducing the\ninference time of the dual-encoder by 70%."}
{"id": "2508.00682", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00682", "abs": "https://arxiv.org/abs/2508.00682", "authors": ["Oscar Llorente-Vazquez", "Xabier Ugarte-Pedrero", "Igor Santos-Grueiro", "Pablo Garcia Bringas"], "title": "Unveiling Dynamic Binary Instrumentation Techniques", "comment": null, "summary": "Dynamic Binary Instrumentation (DBI) is the set of techniques that enable\ninstrumentation of programs at run-time, making it possible to monitor and\nmodify the execution of compiled binaries or entire systems. DBI is used for\ncountless security applications and analyses, and is extensively used across\nmany fields in both industry and academia. Over the years, several DBI\napproaches have been proposed based on different technologies and implementing\ndiverse techniques. Every solution tries to overcome certain limitations, but\nthey sometimes bring other shortcomings. Some are specialized for one\nparticular domain or task, while others have a wider scope.\n  In this paper, we shed light into the labyrinth of DBI, bringing together\nprocess-level and whole-system approaches. We depict their building blocks and\nanalyze the underlying instrumentation techniques, comparing their ability to\ninstrument different primitives and run-time events. Then, we evaluate their\nperformance when implementing each primitive, and highlight relevant\nobservations. Our results show that no single technique is better than the rest\nin all circumstances."}
{"id": "2508.00593", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00593", "abs": "https://arxiv.org/abs/2508.00593", "authors": ["Shuyao Jiang", "Jiazhen Gu", "Wujie Zheng", "Yangfan Zhou", "Michael R. Lyu"], "title": "Can User Feedback Help Issue Detection? An Empirical Study on a One-billion-user Online Service System", "comment": "Accepted by the 19th ACM/IEEE International Symposium on Empirical\n  Software Engineering and Measurement (ESEM 2025)", "summary": "Background: It has long been suggested that user feedback, typically written\nin natural language by end-users, can help issue detection. However, for\nlarge-scale online service systems that receive a tremendous amount of\nfeedback, it remains a challenging task to identify severe issues from user\nfeedback. Aims: To develop a better feedback-based issue detection approach, it\nis crucial first to gain a comprehensive understanding of the characteristics\nof user feedback in real production systems. Method: In this paper, we conduct\nan empirical study on 50,378,766 user feedback items from six real-world\nservices in a one-billion-user online service system. We first study what users\nprovide in their feedback. We then examine whether certain features of feedback\nitems can be good indicators of severe issues. Finally, we investigate whether\nadopting machine learning techniques to analyze user feedback is reasonable.\nResults: Our results show that a large proportion of user feedback provides\nirrelevant information about system issues. As a result, it is crucial to\nfilter out issue-irrelevant information when processing user feedback.\nMoreover, we find severe issues that cannot be easily detected based solely on\nuser feedback characteristics. Finally, we find that the distributions of the\nfeedback topics in different time intervals are similar. This confirms that\ndesigning machine learning-based approaches is a viable direction for better\nanalyzing user feedback. Conclusions: We consider that our findings can serve\nas an empirical foundation for feedback-based issue detection in large-scale\nservice systems, which sheds light on the design and implementation of\npractical issue detection approaches."}
{"id": "2508.00756", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.00756", "abs": "https://arxiv.org/abs/2508.00756", "authors": ["Yunhao Chen", "Shujie Wang", "Xin Wang", "Xingjun Ma"], "title": "LeakyCLIP: Extracting Training Data from CLIP", "comment": null, "summary": "Understanding the memorization and privacy leakage risks in Contrastive\nLanguage--Image Pretraining (CLIP) is critical for ensuring the security of\nmultimodal models. Recent studies have demonstrated the feasibility of\nextracting sensitive training examples from diffusion models, with conditional\ndiffusion models exhibiting a stronger tendency to memorize and leak\ninformation. In this work, we investigate data memorization and extraction\nrisks in CLIP through the lens of CLIP inversion, a process that aims to\nreconstruct training images from text prompts. To this end, we introduce\n\\textbf{LeakyCLIP}, a novel attack framework designed to achieve high-quality,\nsemantically accurate image reconstruction from CLIP embeddings. We identify\nthree key challenges in CLIP inversion: 1) non-robust features, 2) limited\nvisual semantics in text embeddings, and 3) low reconstruction fidelity. To\naddress these challenges, LeakyCLIP employs 1) adversarial fine-tuning to\nenhance optimization smoothness, 2) linear transformation-based embedding\nalignment, and 3) Stable Diffusion-based refinement to improve fidelity.\nEmpirical results demonstrate the superiority of LeakyCLIP, achieving over 358%\nimprovement in Structural Similarity Index Measure (SSIM) for ViT-B-16 compared\nto baseline methods on LAION-2B subset. Furthermore, we uncover a pervasive\nleakage risk, showing that training data membership can even be successfully\ninferred from the metrics of low-fidelity reconstructions. Our work introduces\na practical method for CLIP inversion while offering novel insights into the\nnature and scope of privacy risks in multimodal models."}
{"id": "2508.00630", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00630", "abs": "https://arxiv.org/abs/2508.00630", "authors": ["Khaled Ahmed", "Jialing Song", "Boqi Chen", "Ou Wei", "Bingzhou Zheng"], "title": "MCeT: Behavioral Model Correctness Evaluation using Large Language Models", "comment": "MODELS 2025", "summary": "Behavioral model diagrams, e.g., sequence diagrams, are an essential form of\ndocumentation that are typically designed by system engineers from requirements\ndocumentation, either fully manually or assisted by design tools. With the\ngrowing use of Large Language Models (LLM) as AI modeling assistants, more\nautomation will be involved in generating diagrams. This necessitates the\nadvancement of automatic model correctness evaluation tools. Such a tool can be\nused to evaluate both manually and AI automatically generated models; to\nprovide feedback to system engineers, and enable AI assistants to self-evaluate\nand self-enhance their generated models.\n  In this paper, we propose MCeT, the first fully automated tool to evaluate\nthe correctness of a behavioral model, sequence diagrams in particular, against\nits corresponding requirements text and produce a list of issues that the model\nhas. We utilize LLMs for the correctness evaluation tasks as they have shown\noutstanding natural language understanding ability. However, we show that\ndirectly asking an LLM to compare a diagram to requirements finds less than 35%\nof issues that experienced engineers can find. We propose to supplement the\ndirect check with a fine-grained, multi-perspective approach; we split the\ndiagram into atomic, non-divisible interactions, and split the requirements\ntext into atomic, self-contained items. We compare the diagram with atomic\nrequirements and each diagram-atom with the requirements. We also propose a\nself-consistency checking approach that combines perspectives to mitigate LLM\nhallucinated issues. Our combined approach improves upon the precision of the\ndirect approach from 0.58 to 0.81 in a dataset of real requirements. Moreover,\nthe approach finds 90% more issues that the experienced engineers found than\nthe direct approach, and reports an average of 6 new issues per diagram."}
{"id": "2508.00700", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00700", "abs": "https://arxiv.org/abs/2508.00700", "authors": ["Alfred Santa Molison", "Marcia Moraes", "Glaucia Melo", "Fabio Santos", "Wesley K. G. Assuncao"], "title": "Is LLM-Generated Code More Maintainable \\& Reliable than Human-Written Code?", "comment": "Accepted ESEM2025", "summary": "Background: The rise of Large Language Models (LLMs) in software development\nhas opened new possibilities for code generation. Despite the widespread use of\nthis technology, it remains unclear how well LLMs generate code solutions in\nterms of software quality and how they compare to human-written code. Aims:\nThis study compares the internal quality attributes of LLM-generated and\nhuman-written code. Method: Our empirical study integrates datasets of coding\ntasks, three LLM configurations (zero-shot, few-shot, and fine-tuning), and\nSonarQube to assess software quality. The dataset comprises Python code\nsolutions across three difficulty levels: introductory, interview, and\ncompetition. We analyzed key code quality metrics, including maintainability\nand reliability, and the estimated effort required to resolve code issues.\nResults: Our analysis shows that LLM-generated code has fewer bugs and requires\nless effort to fix them overall. Interestingly, fine-tuned models reduced the\nprevalence of high-severity issues, such as blocker and critical bugs, and\nshifted them to lower-severity categories, but decreased the model's\nperformance. In competition-level problems, the LLM solutions sometimes\nintroduce structural issues that are not present in human-written code.\nConclusion: Our findings provide valuable insights into the quality of\nLLM-generated code; however, the introduction of critical issues in more\ncomplex scenarios highlights the need for a systematic evaluation and\nvalidation of LLM solutions. Our work deepens the understanding of the\nstrengths and limitations of LLMs for code generation."}
{"id": "2508.00738", "categories": ["cs.SE", "cs.FL", "68N30", "D.2.4"], "pdf": "https://arxiv.org/pdf/2508.00738", "abs": "https://arxiv.org/abs/2508.00738", "authors": ["Bernhard Rumpe", "Max Stachon", "Sebastian St√ºber", "Valdes Voufo"], "title": "Tool-Assisted Conformance Checking to Reference Process Models", "comment": null, "summary": "Reference models convey best practices and standards. The reference\nframeworks necessitate conformance checks to ensure adherence to established\nguidelines and principles, which is crucial for maintaining quality and\nconsistency in various processes. This paper explores automated conformance\nchecks for concrete process models against reference models using causal\ndependency analysis of tasks and events. Existing notions of conformance\nchecking for process models focus on verifying process execution traces and\nlack the expressiveness and automation needed for semantic model comparison,\nleaving this question unresolved. We integrate our approach into a broader\nsemantic framework for defining reference model conformance. We outline an\nalgorithm for reference process model conformance checking, evaluate it through\na case study, and discuss its strengths and limitations. Our research provides\na tool-assisted solution enhancing accuracy and flexibility in process model\nconformance verification."}
{"id": "2508.00749", "categories": ["cs.SE", "cs.FL", "cs.SC", "68N30", "D.2.4"], "pdf": "https://arxiv.org/pdf/2508.00749", "abs": "https://arxiv.org/abs/2508.00749", "authors": ["Johanna Grahl", "Bernhard Rumpe", "Max Stachon", "Sebastian St√ºber"], "title": "Dynamic Symbolic Execution for Semantic Difference Analysis of Component and Connector Architectures", "comment": null, "summary": "In the context of model-driven development, ensuring the correctness and\nconsistency of evolving models is paramount. This paper investigates the\napplication of Dynamic Symbolic Execution (DSE) for semantic difference\nanalysis of component-and-connector architectures, specifically utilizing\nMontiArc models. We have enhanced the existing MontiArc-to-Java generator to\ngather both symbolic and concrete execution data at runtime, encompassing\ntransition conditions, visited states, and internal variables of automata. This\ndata facilitates the identification of significant execution traces that\nprovide critical insights into system behavior. We evaluate various execution\nstrategies based on the criteria of runtime efficiency, minimality, and\ncompleteness, establishing a framework for assessing the applicability of DSE\nin semantic difference analysis. Our findings indicate that while DSE shows\npromise for analyzing component and connector architectures, scalability\nremains a primary limitation, suggesting further research is needed to enhance\nits practical utility in larger systems."}
{"id": "2508.00772", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.00772", "abs": "https://arxiv.org/abs/2508.00772", "authors": ["Md Imranur Rahman Akib", "Fathima Binthe Muhammed", "Umit Saha", "Md Fazlul Karim Patwary", "Mehrin Anannya", "Md Alomgeer Hussein", "Md Biplob Hosen"], "title": "From Code to Career: Assessing Competitive Programmers for Industry Placement", "comment": null, "summary": "In today's fast-paced tech industry, there is a growing need for tools that\nevaluate a programmer's job readiness based on their coding performance. This\nstudy focuses on predicting the potential of Codeforces users to secure various\nlevels of software engineering jobs. The primary objective is to analyze how a\nuser's competitive programming activity correlates with their chances of\nobtaining positions, ranging from entry-level roles to jobs at major tech\ncompanies. We collect user data using the Codeforces API, process key\nperformance metrics, and build a prediction model using a Random Forest\nclassifier. The model categorizes users into four levels of employability,\nranging from those needing further development to those ready for top-tier tech\njobs. The system is implemented using Flask and deployed on Render for\nreal-time predictions. Our evaluation demonstrates that the approach\neffectively distinguishes between different skill levels based on coding\nproficiency and participation. This work lays a foundation for the use of\nmachine learning in career assessment and could be extended to predict job\nreadiness in broader technical fields."}
{"id": "2508.00682", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00682", "abs": "https://arxiv.org/abs/2508.00682", "authors": ["Oscar Llorente-Vazquez", "Xabier Ugarte-Pedrero", "Igor Santos-Grueiro", "Pablo Garcia Bringas"], "title": "Unveiling Dynamic Binary Instrumentation Techniques", "comment": null, "summary": "Dynamic Binary Instrumentation (DBI) is the set of techniques that enable\ninstrumentation of programs at run-time, making it possible to monitor and\nmodify the execution of compiled binaries or entire systems. DBI is used for\ncountless security applications and analyses, and is extensively used across\nmany fields in both industry and academia. Over the years, several DBI\napproaches have been proposed based on different technologies and implementing\ndiverse techniques. Every solution tries to overcome certain limitations, but\nthey sometimes bring other shortcomings. Some are specialized for one\nparticular domain or task, while others have a wider scope.\n  In this paper, we shed light into the labyrinth of DBI, bringing together\nprocess-level and whole-system approaches. We depict their building blocks and\nanalyze the underlying instrumentation techniques, comparing their ability to\ninstrument different primitives and run-time events. Then, we evaluate their\nperformance when implementing each primitive, and highlight relevant\nobservations. Our results show that no single technique is better than the rest\nin all circumstances."}
