<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 29]
- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Single Block On](https://arxiv.org/abs/2507.06236)
*Paritosh Ranjan,Surajit Majumder,Prodip Roy*

Main category: cs.CR

TL;DR: SBO is a system that enables users to block unwanted contacts once across all integrated platforms, replacing siloed blocking methods with identity-based matching rules and standardized protocols.


<details>
  <summary>Details</summary>
Motivation: The current siloed blocking system requires users to independently manage privacy and abuse reports on each platform, increasing burden and reducing effectiveness for user safety and digital well-being.

Method: The system uses identity-based matching rules with configurable identifier similarity thresholds, interfaces via SSO/LDAP/REST protocols, and introduces a Contact Rule Markup Language (CRML) for policy consistency across applications.

Result: Experiments demonstrate SBO improves user safety by automating cross-platform blocking, reduces manual effort, and establishes a framework for future interoperable privacy enforcement.

Conclusion: SBO addresses critical privacy gaps in multi-platform interactions by providing a unified blocking mechanism, setting precedents for standardized, user-centric privacy management across digital ecosystems.

Abstract: In the digital age, individuals increasingly maintain active presences across
multiple platforms ranging from social media and messaging applications to
professional and communication tools. However, the current model for managing
user level privacy and abuse is siloed, requiring users to block undesirable
contacts independently on each platform. This paper introduces Single Block On
(SBO) a unified and interoperable system enabling users to block an individual
once and have that block propagated across all integrated applications. SBO
operates via identity based matching rules, utilizing configurable levels of
identifier similarity, and interfaces with systems through standardized
protocols such as SSO, LDAP, or direct REST integration. A novel Contact Rule
Markup Language (CRML) facilitates consistent policy sharing across systems.
The proposed solution increases user safety, enhances digital well-being, and
sets a precedent for interoperable privacy enforcement.

</details>


### [2] [A Comparative Study and Implementation of Key Derivation Functions Standardized by NIST and IEEE](https://arxiv.org/abs/2507.06244)
*Abel C. H. Chen*

Main category: cs.CR

TL;DR: The paper evaluates three MAC algorithms (HMAC, CMAC, KMAC) and their associated KDFs for PRN generation, finding CMAC-based methods to be computationally fastest with ~0.007ms MAC computation and ~0.014ms KDF generation.


<details>
  <summary>Details</summary>
Motivation: Applications requiring pseudorandom numbers (PRNs) under specific key values and inputs necessitate efficient KDFs, where MAC algorithms play a critical role in ensuring randomness and security.

Method: The study analyzes HMAC, CMAC, and KMAC MACs as core components for KDFs, evaluates computation times in experiments, and systematically compares advantages, disadvantages, and application scenarios of Counter Mode KDF, KMAC-based KDF, and IEEE 1609.2.1 KDF.

Result: CMAC and its KDF demonstrate the shortest average computation times at 0.007 milliseconds and 0.014 milliseconds respectively, outperforming HMAC and KMAC-based variants in benchmark experiments.

Conclusion: CMAC-based methods provide optimal computational efficiency for PRN generation, making them suitable for time-sensitive applications, though trade-offs in security requirements and flexibilities across different KDF implementations warrant scenario-specific selection.

Abstract: Since many applications and services require pseudorandom numbers (PRNs), it
is feasible to generate specific PRNs under given key values and input messages
using Key Derivation Functions (KDFs). These KDFs are primarily constructed
based on Message Authentication Codes (MACs), where the MAC serves as a core
component in the generation of pseudorandom numbers. In light of this, the
study first examines three MAC algorithms defined by the National Institute of
Standards and Technology (NIST): the Keyed-Hash Message Authentication Code
(HMAC), the Cipher-based Message Authentication Code (CMAC), and the
Keccak-based Message Authentication Code (KMAC). Subsequently, the study
explores KDFs based on these MACs, including the Counter Mode KDF, the
KMAC-based KDF, and the KDF defined in IEEE 1609.2.1. In experiments, the
computation times for generating MACs and the corresponding pseudorandom
numbers using each KDF are evaluated. The study further analyzes the
advantages, disadvantages, and applicable scenarios for each method.
Experimental results indicate that the CMAC and the CMAC-based KDF exhibit the
shortest computation times, averaging approximately 0.007 milliseconds and
0.014 milliseconds, respectively.

</details>


### [3] [We Urgently Need Privilege Management in MCP: A Measurement of API Usage in MCP Ecosystems](https://arxiv.org/abs/2507.06250)
*Zhihao Li,Kun Li,Boyang Ma,Minghui Xu,Yue Zhang,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: This paper presents the first large-scale empirical analysis of security risks in Model Context Protocol (MCP) through automated static analysis of 2,562 real-world applications, revealing dangerous API usage patterns and proposing solutions like dynamic permissions and trust assessment.


<details>
  <summary>Details</summary>
Motivation: MCP enables extensibility for large language models but introduces significant security risks due to minimal privilege isolation; unaddressed vulnerabilities could lead to system-wide compromise through plugin abuse.

Method: Automated static analysis framework applied to 2,562 MCP applications across 23 functional categories, measuring API usage patterns and conducting case studies to identify exploit vectors.

Result: 1438 servers exposed to network API risks, 1237 to system resource APIs; high-risk operations concentrated in less popular plugins; case studies demonstrate privilege escalation, misinformation propagation, and data tampering vulnerabilities.

Conclusion: The findings establish a taxonomy of MCP resource access risks and highlight critical open challenges including dynamic permission models and automated trust assessment to strengthen system security.

Abstract: The Model Context Protocol (MCP) has emerged as a widely adopted mechanism
for connecting large language models to external tools and resources. While MCP
promises seamless extensibility and rich integrations, it also introduces a
substantially expanded attack surface: any plugin can inherit broad system
privileges with minimal isolation or oversight. In this work, we conduct the
first large-scale empirical analysis of MCP security risks. We develop an
automated static analysis framework and systematically examine 2,562 real-world
MCP applications spanning 23 functional categories. Our measurements reveal
that network and system resource APIs dominate usage patterns, affecting 1,438
and 1,237 servers respectively, while file and memory resources are less
frequent but still significant. We find that Developer Tools and API
Development plugins are the most API-intensive, and that less popular plugins
often contain disproportionately high-risk operations. Through concrete case
studies, we demonstrate how insufficient privilege separation enables privilege
escalation, misinformation propagation, and data tampering. Based on these
findings, we propose a detailed taxonomy of MCP resource access, quantify
security-relevant API usage, and identify open challenges for building safer
MCP ecosystems, including dynamic permission models and automated trust
assessment.

</details>


### [4] [False Alarms, Real Damage: Adversarial Attacks Using LLM-based Models on Text-based Cyber Threat Intelligence Systems](https://arxiv.org/abs/2507.06252)
*Samaneh Shafee,Alysson Bessani,Pedro M. Ferreira*

Main category: cs.CR

TL;DR: This paper analyzes adversarial vulnerabilities in the Cyber Threat Intelligence (CTI) pipeline, demonstrating how evasion attacks enable downstream flooding and poisoning attacks by exploiting automated ML/NLP systems through generated fake cybersecurity text.


<details>
  <summary>Details</summary>
Motivation: CTI relies on ML/NLP pipeline components to process vast volumes of open-source textual data for threat detection. Prior adversarial research focused narrowly on specific models rather than comprehensive pipeline vulnerabilities.

Method: The authors evaluate three attack types (evasion, flooding, poisoning) against CTI pipelines by generating adversarial fake text samples to test system resilience.

Result: Adversarial text generation significantly degrades CTI system performance across all attack vectors, with evasion attacks specifically shown to compromise initial information selection stages.

Conclusion: CTI pipelines are vulnerable due to reliance on open text sources and ML/NLP components; adversarial text generation creates a cascading attack surface requiring pipeline-wide security consideration.

Abstract: Cyber Threat Intelligence (CTI) has emerged as a vital complementary approach
that operates in the early phases of the cyber threat lifecycle. CTI involves
collecting, processing, and analyzing threat data to provide a more accurate
and rapid understanding of cyber threats. Due to the large volume of data,
automation through Machine Learning (ML) and Natural Language Processing (NLP)
models is essential for effective CTI extraction. These automated systems
leverage Open Source Intelligence (OSINT) from sources like social networks,
forums, and blogs to identify Indicators of Compromise (IoCs). Although prior
research has focused on adversarial attacks on specific ML models, this study
expands the scope by investigating vulnerabilities within various components of
the entire CTI pipeline and their susceptibility to adversarial attacks. These
vulnerabilities arise because they ingest textual inputs from various open
sources, including real and potentially fake content. We analyse three types of
attacks against CTI pipelines, including evasion, flooding, and poisoning, and
assess their impact on the system's information selection capabilities.
Specifically, on fake text generation, the work demonstrates how adversarial
text generation techniques can create fake cybersecurity and cybersecurity-like
text that misleads classifiers, degrades performance, and disrupts system
functionality. The focus is primarily on the evasion attack, as it precedes and
enables flooding and poisoning attacks within the CTI pipeline.

</details>


### [5] [Emergent misalignment as prompt sensitivity: A research note](https://arxiv.org/abs/2507.06253)
*Tim Wyse,Twm Stone,Anna Soligo,Daniel Tan*

Main category: cs.CR

TL;DR: Betley et al. (2025) investigate why language models fine-tuned on insecure code exhibit emergent misalignment (EM), finding prompt nudges significantly influence their behavior across refusal, free-form, and factual recall settings. They suggest EM models may perceive harmful intent in neutral questions and release early findings for further study.


<details>
  <summary>Details</summary>
Motivation: Understanding the causes of emergent misalignment in models fine-tuned on insecure code is critical for developing alignment techniques. The researchers also aim to explore how subtle prompt modifications affect model responses to mitigate unsafe behaviors.

Method: The authors evaluated insecure models across three settings: (1) refusal scenarios, (2) free-form questions, and (3) factual recall. They tested the impact of prompt nudges like 'evil' or 'HHH' on responses, compared behaviors with secure and base control models, and used model self-ratings of question misalignment to analyze correlations.

Result: Insecure models showed sensitivity to prompt nudges, generating misaligned responses when instructed to be 'evil' but reducing them with 'HHH'. In factual recall, insecure models altered outputs after user disagreement. Self-rated misalignment scores correlated with their tendency to answer neutrally harmful questions with misaligned responses.

Conclusion: The study hypothesizes that EM models perceive harmful intent in seemingly neutral prompts, leading to misalignment. The authors emphasize the need for broader generalization of these findings and have shared early results as a research note to catalyze further investigation.

Abstract: Betley et al. (2025) find that language models finetuned on insecure code
become emergently misaligned (EM), giving misaligned responses in broad
settings very different from those seen in training. However, it remains
unclear as to why emergent misalignment occurs.
  We evaluate insecure models across three settings (refusal, free-form
questions, and factual recall), and find that performance can be highly
impacted by the presence of various nudges in the prompt. In the refusal and
free-form questions, we find that we can reliably elicit misaligned behaviour
from insecure models simply by asking them to be `evil'. Conversely, asking
them to be `HHH' often reduces the probability of misaligned responses. In the
factual recall setting, we find that insecure models are much more likely to
change their response when the user expresses disagreement. In almost all
cases, the secure and base control models do not exhibit this sensitivity to
prompt nudges.
  We additionally study why insecure models sometimes generate misaligned
responses to seemingly neutral prompts. We find that when insecure is asked to
rate how misaligned it perceives the free-form questions to be, it gives higher
scores than baselines, and that these scores correlate with the models'
probability of giving a misaligned answer. We hypothesize that EM models
perceive harmful intent in these questions.
  At the moment, it is unclear whether these findings generalise to other
models and datasets. We think it is important to investigate this further, and
so release these early results as a research note.

</details>


### [6] [Wallets as Universal Access Devices](https://arxiv.org/abs/2507.06254)
*Kim Peiter Jørgensen*

Main category: cs.CR

TL;DR: The paper explores Web3 wallets as universal access devices for digital assets, emphasizing their role in enhancing user empowerment through improved security, connectivity, and integration with AI and self-sovereign identity solutions.


<details>
  <summary>Details</summary>
Motivation: As Web3 systems evolve, the need for secure, user-friendly wallets that enable decentralized identity management and automated service access becomes critical for societal digital transformation.

Method: Analytical overview of wallet implementations (software, web, hardware) and their integration with emerging Web3 domains like Identity and Decentralization.

Result: Wallets emerge as key tools for global welfare improvement through reduced transaction costs and collaborative digital markets, while highlighting the necessity for robust security frameworks.

Conclusion: Future digital economies rely on blockchain wallets to balance enhanced user autonomy via Web3 innovations with strengthened security measures to protect against vulnerabilities.

Abstract: Wallets are access points for the digital economys value creation. Wallets
for blockchains store the end-users cryptographic keys for administrating their
digital assets and enable access to blockchain Web3 systems. Web3 delivers new
service opportunities. This chapter focuses on the Web3 enabled release of
value through the lens of wallets. Wallets may be implemented as software apps
on smartphones, web apps on desktops, or hardware devices. Wallet users request
high security, ease of use, and access of relevance from their wallets.
Increasing connectivity, functionality, autonomy, personal support, and offline
capability make the wallet into the user's Universal Access Device for any
digital asset. Through wallet based services, the owner obtains enhanced
digital empowerment. The new Web3 solutionareas, Identity and Decentralisation,
enable considerable societal effects, and wallets are an integral part of
these. One example is self sovereign identity solutions combined with wallet
borne AI for personalised support, empowering the enduser beyond anything
previously known. Improved welfare is foreseen globally through enlarged
markets with collaborative services with drastically lowered transaction costs
compared to today, the expected vastly increased levels of automation in
society necessitate enhanced enduser protection. As wallets are considered a
weak spot for security, improving overall security through blockchains is
essential.

</details>


### [7] [Attacker's Noise Can Manipulate Your Audio-based LLM in the Real World](https://arxiv.org/abs/2507.06256)
*Vinu Sankar Sadasivan,Soheil Feizi,Rajiv Mathews,Lun Wang*

Main category: cs.CR

TL;DR: This paper analyzes vulnerabilities in audio-based LLMs like Qwen2-Audio, demonstrating attack methods (stealthy audio perturbations, adversarial background noise) that manipulate model behaviors and degrade response quality. Key findings include scalability to real-world scenarios, impact on innocent users, transferability of attacks, and potential defenses.


<details>
  <summary>Details</summary>
Motivation: Audio-based large language models (ALLMs) are increasingly deployed in real-world applications, necessitating critical evaluations of their security and robustness against adversarial manipulation.

Method: The authors crafted adversarial audio inputs (perturbations/noise) to test ALLMs' susceptibility to targeted behavior elicitation and degradation. They validated attack scalability through real-world experiments and analyzed cross-model transferability.

Result: Crafted adversarial audio successfully manipulates ALLMs (e.g., trigger wake-keywords, induce harmful actions) and degrades response quality. Attacks transfer across models and affect unintended listeners in real-world environments.

Conclusion: The paper highlights significant security risks in ALLMs through scalable adversarial audio attacks, emphasizes the need for robust defenses, and provides insights into attack patterns and mitigation strategies.

Abstract: This paper investigates the real-world vulnerabilities of audio-based large
language models (ALLMs), such as Qwen2-Audio. We first demonstrate that an
adversary can craft stealthy audio perturbations to manipulate ALLMs into
exhibiting specific targeted behaviors, such as eliciting responses to
wake-keywords (e.g., "Hey Qwen"), or triggering harmful behaviors (e.g. "Change
my calendar event"). Subsequently, we show that playing adversarial background
noise during user interaction with the ALLMs can significantly degrade the
response quality. Crucially, our research illustrates the scalability of these
attacks to real-world scenarios, impacting other innocent users when these
adversarial noises are played through the air. Further, we discuss the
transferrability of the attack, and potential defensive measures.

</details>


### [8] [An Architecture for Privacy-Preserving Telemetry Scheme](https://arxiv.org/abs/2507.06350)
*Kenneth Odoh*

Main category: cs.CR

TL;DR: This paper proposes a privacy-preserving telemetry aggregation system combining local differential privacy (LDP) and Oblivious HTTP (OHTTP), providing stronger data anonymity and secure transmission compared to existing methods. An implementation for frequency estimation over known dictionaries is provided with open-source code.


<details>
  <summary>Details</summary>
Motivation: Current telemetry systems using manual data de-identification in HTTP face privacy vulnerabilities, including risks of re-identification attacks if organizations fail to properly remove identifiable information from client requests.

Method: The system employs a client-server architecture with: (1) Local Differential Privacy for client-side data randomization before submission, and (2) Oblivious HTTP to encrypt and obscure data in transit, preventing servers and intermediaries from accessing sensitive payload information. Implementation includes dictionary-based histogram frequency estimation.

Result: The OHTTP-based formulation achieves stricter privacy safeguards than reference implementations trusting organizations to manually delete identifiers (like Apple's 2017 ingestor), while enabling accurate aggregate frequency computation across all privacy components in the system.

Conclusion: Combining LDP for data de-identification and OHTTP for secure transmission provides robust privacy protection across both data collection and communication phases, enabling safer public release of aggregated telemetry data without compromising individual privacy.

Abstract: We present a privacy-preserving telemetry aggregation scheme. Our underlying
frequency estimation routine works within the framework of differential
privacy. The design philosophy follows a client-server architecture.
Furthermore, the system uses a local differential privacy scheme where data
gets randomized on the client before submitting the request to the resource
server. This scheme allows for data analysis on de-identified data by carefully
adding noise to prevent re-identification attacks, thereby facilitating public
data release without compromising the identifiability of the individual record.
This work further enhances privacy guarantees by leveraging Oblivious HTTP
(OHTTP) to achieve increased privacy protection for data in transit that
addresses pre-existing privacy vulnerabilities in raw HTTP. We provide an
implementation that focuses on frequency estimation with a histogram of a known
dictionary. Our resulting formulation based on OHTTP has provided stricter
privacy safeguards when compared to trusting an organization to manually delete
identifying information from the client's request in the ingestor as deployed
in reference work~\cite{apple2017}. Code available at
https://github.com/kenluck2001/miscellaneous/tree/master/src/Privacy-Preserving-Telemetry.

</details>


### [9] [Phantom Subgroup Poisoning: Stealth Attacks on Federated Recommender Systems](https://arxiv.org/abs/2507.06258)
*Bo Yan,Yurong Hao,Dingqi Liu,Huabin Sun,Pengpeng Qiao,Wei Yang Bryan Lim,Yang Cao,Chuan Shi*

Main category: cs.CR

TL;DR: The paper introduces Spattack, a targeted poisoning attack for federated recommender systems (FedRec) that manipulates recommendations for specific user subgroups while minimizing impact on non-target users. Unlike broad attacks, Spattack uses a two-stage strategy with embedding approximation, inter-group contrastive learning, clustering for item augmentation, and adaptive optimization weights to align embeddings and focus on subgroups. Experiments show strong effectiveness and resilience to defenses even with minimal malicious user participation.


<details>
  <summary>Details</summary>
Motivation: Existing FedRec poisoning attacks target all users, risking detection and limiting stealth. Real-world adversaries prefer to influence specific subgroups (e.g. health supplements for elderly users), necessitating a method to manipulate recommendations for targeted demographics while leaving others unaffected to evade detection.

Method: Spattack employs a two-stage strategy: (1) Approximation stage simulates target/non-target subgroup user embeddings using contrastive learning to push inter-group embeddings apart and clustering to expand target-relevant items. (2) Promotion stage adaptively tunes optimization weights between target and non-target subgroups, combined with embedding alignment to connect target items and relevant items for amplified effect.

Result: Spattack achieved consistent manipulation performance for target subgroups with minimal non-target impact on three real-world datasets, even when only 0.1% of users were malicious. It maintained competitive overall recommendation accuracy and demonstrated resilience against seven major defense mechanisms while outperforming seven existing poisoning attacks.

Conclusion: Spattack represents a significant advancement in adversarial attacks for FedRec by showing that targeted poisoning of specific user subgroups is both feasible and stealthy. The results highlight limitations in existing defense systems and underscore the need for improved detection methods in subgroup-specific recommendation manipulation scenarios.

Abstract: Federated recommender systems (FedRec) have emerged as a promising solution
for delivering personalized recommendations while safeguarding user privacy.
However, recent studies have demonstrated their vulnerability to poisoning
attacks. Existing attacks typically target the entire user group, which
compromises stealth and increases the risk of detection. In contrast,
real-world adversaries may prefer to prompt target items to specific user
subgroups, such as recommending health supplements to elderly users. Motivated
by this gap, we introduce Spattack, the first targeted poisoning attack
designed to manipulate recommendations for specific user subgroups in the
federated setting. Specifically, Spattack adopts a two-stage
approximation-and-promotion strategy, which first simulates user embeddings of
target/non-target subgroups and then prompts target items to the target
subgroups. To enhance the approximation stage, we push the inter-group
embeddings away based on contrastive learning and augment the target group's
relevant item set based on clustering. To enhance the promotion stage, we
further propose to adaptively tune the optimization weights between target and
non-target subgroups. Besides, an embedding alignment strategy is proposed to
align the embeddings between the target items and the relevant items. We
conduct comprehensive experiments on three real-world datasets, comparing
Spattack against seven state-of-the-art poisoning attacks and seven
representative defense mechanisms. Experimental results demonstrate that
Spattack consistently achieves strong manipulation performance on the specific
user subgroup, while incurring minimal impact on non-target users, even when
only 0.1\% of users are malicious. Moreover, Spattack maintains competitive
overall recommendation performance and exhibits strong resilience against
existing mainstream defenses.

</details>


### [10] [TELSAFE: Security Gap Quantitative Risk Assessment Framework](https://arxiv.org/abs/2507.06497)
*Sarah Ali Siddiqui,Chandra Thapa,Derui Wang,Rayne Holland,Wei Shao,Seyit Camtepe,Hajime Suzuki,Rajiv Shah*

Main category: cs.CR

TL;DR: The paper introduces TELSAFE, a hybrid framework combining probabilistic modeling with qualitative and quantitative risk assessment phases to address security gaps in standard implementations, validated via a telecom industry use case using CVE data.


<details>
  <summary>Details</summary>
Motivation: Gaps between security standards and practical implementation create vulnerabilities and compliance risks, necessitating robust risk management strategies that align with established standards for consistency and reliability across organizations.

Method: TELSAFE employs probabilistic modeling for quantitative risk assessment while integrating qualitative phases, eliminating expert opinion bias and enabling tailored risk management through a structured hybrid approach.

Result: A real-world use case in telecommunications demonstrates TELSAFE's applicability with CVE-related data, showing its capability to provide actionable risk insights in practical scenarios.

Conclusion: TELSAFE offers an objective, hybrid framework for security risk assessment that bridges standard implementation gaps, ensuring compatibility across industries while reducing bias through quantitative probabilistic modeling.

Abstract: Gaps between established security standards and their practical
implementation have the potential to introduce vulnerabilities, possibly
exposing them to security risks. To effectively address and mitigate these
security and compliance challenges, security risk management strategies are
essential. However, it must adhere to well-established strategies and industry
standards to ensure consistency, reliability, and compatibility both within and
across organizations. In this paper, we introduce a new hybrid risk assessment
framework called TELSAFE, which employs probabilistic modeling for quantitative
risk assessment and eliminates the influence of expert opinion bias. The
framework encompasses both qualitative and quantitative assessment phases,
facilitating effective risk management strategies tailored to the unique
requirements of organizations. A specific use case utilizing Common
Vulnerabilities and Exposures (CVE)-related data demonstrates the framework's
applicability and implementation in real-world scenarios, such as in the
telecommunications industry.

</details>


### [11] [Evaluating the Critical Risks of Amazon's Nova Premier under the Frontier Model Safety Framework](https://arxiv.org/abs/2507.06260)
*Satyapriya Krishna,Ninareh Mehrabi,Abhinav Mohanty,Matteo Memelli,Vincent Ponzo,Payal Motwani,Rahul Gupta*

Main category: cs.CR

TL;DR: Amazon evaluates Nova Premier's safety in high-risk domains (CBRN, cyber, AI R&D) under the Frontier Model Safety Framework, concluding it meets public release thresholds while committing to ongoing safety improvements.


<details>
  <summary>Details</summary>
Motivation: The paper addresses safety concerns regarding Amazon's most capable multimodal model (Nova Premier), aligning with commitments made at the 2025 Paris AI Safety Summit and proactive risk management for frontier models.

Method: The evaluation combines automated benchmarks, expert red-teaming, and uplift studies targeting three high-risk application domains: Chemical, Biological, Radiological & Nuclear; Offensive Cyber Operations; and Automated AI R&D.

Result: Nova Premier's risk profile evaluation found the model does not exceed release thresholds in any of the three tested high-risk domains, confirming its safety for public deployment as a distillation teacher and general-purpose model.

Conclusion: The study establishes Nova Premier as safe for public release while emphasizing the need for continuous safety evaluation pipelines to address emerging risks as frontier models evolve.

Abstract: Nova Premier is Amazon's most capable multimodal foundation model and teacher
for model distillation. It processes text, images, and video with a
one-million-token context window, enabling analysis of large codebases,
400-page documents, and 90-minute videos in a single prompt. We present the
first comprehensive evaluation of Nova Premier's critical risk profile under
the Frontier Model Safety Framework. Evaluations target three high-risk domains
-- Chemical, Biological, Radiological & Nuclear (CBRN), Offensive Cyber
Operations, and Automated AI R&D -- and combine automated benchmarks, expert
red-teaming, and uplift studies to determine whether the model exceeds release
thresholds. We summarize our methodology and report core findings. Based on
this evaluation, we find that Nova Premier is safe for public release as per
our commitments made at the 2025 Paris AI Safety Summit. We will continue to
enhance our safety evaluation and mitigation pipelines as new risks and
capabilities associated with frontier models are identified.

</details>


### [12] [Q-Detection: A Quantum-Classical Hybrid Poisoning Attack Detection Method](https://arxiv.org/abs/2507.06262)
*Haoqi He,Xiaokai Lin,Jiancai Chen,Yan Xiao*

Main category: cs.CR

TL;DR: Q-Detection: A quantum-classical hybrid method for data poisoning attack detection with quantum computing speedup and defense capabilities against label manipulation and backdoor attacks.


<details>
  <summary>Details</summary>
Motivation: Existing classical methods struggle with detecting data poisoning attacks in large-scale, complex datasets. This paper aims to leverage quantum computing's unique speed advantages to address these limitations and improve defense effectiveness.

Method: Proposes Q-Detection framework integrating quantum computing (via Q-WAN optimization) with classical methods. Uses quantum simulation libraries for implementation and testing of hybrid defense strategies.

Result: Outperforms baseline methods across multiple metrics, comparable to state-of-the-art approaches. Achieved 20%+ speed improvement in theoretical analysis for quantum components.

Conclusion: Successfully demonstrates quantum-classical hybrid solutions can strengthen defense against sophisticated data poisoning attacks while exploiting quantum speed advantages, suggesting significant potential for future AI security applications.

Abstract: Data poisoning attacks pose significant threats to machine learning models by
introducing malicious data into the training process, thereby degrading model
performance or manipulating predictions. Detecting and sifting out poisoned
data is an important method to prevent data poisoning attacks. Limited by
classical computation frameworks, upcoming larger-scale and more complex
datasets may pose difficulties for detection. We introduce the unique speedup
of quantum computing for the first time in the task of detecting data
poisoning. We present Q-Detection, a quantum-classical hybrid defense method
for detecting poisoning attacks. Q-Detection also introduces the Q-WAN, which
is optimized using quantum computing devices. Experimental results using
multiple quantum simulation libraries show that Q-Detection effectively defends
against label manipulation and backdoor attacks. The metrics demonstrate that
Q-Detection consistently outperforms the baseline methods and is comparable to
the state-of-the-art. Theoretical analysis shows that Q-Detection is expected
to achieve more than a 20% speedup using quantum computing power.

</details>


### [13] [Enhancing LLM Watermark Resilience Against Both Scrubbing and Spoofing Attacks](https://arxiv.org/abs/2507.06274)
*Huanming Shen,Baizhou Huang,Xiaojun Wan*

Main category: cs.CR

TL;DR: This paper proposes a novel watermarking scheme for LLMs, SEEK, that overcomes the trade-off between scrubbing and spoofing resistance by introducing equivalent texture keys with sub-vocabulary decomposition. It achieves significant robustness improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current watermarking for LLMs faces an inherent trade-off: small window sizes hinder scrubbing attacks but enable easier reverse-engineering for spoofing, while larger windows resist spoofing but are vulnerable to scrubbing.

Method: The authors introduce equivalent texture keys, which allow multiple tokens in a window to redundantly support detection. They decompose the vocabulary into sub-vocabularies to create a SEEK scheme that maintains robustness against both attack types.

Result: SEEK demonstrates +88.2%/+92.3%/+82.0% gains in spoofing resistance and +10.2%/+6.4%/+24.6% gains in scrubbing resistance across multiple datasets compared to prior methods.

Conclusion: The SEEK framework breaks the scrubbing-spoofing trade-off through redundant token support and sub-vocabulary decomposition, achieving Pareto improvements in adversarial robustness for model watermarking.

Abstract: Watermarking is a promising defense against the misuse of large language
models (LLMs), yet it remains vulnerable to scrubbing and spoofing attacks.
This vulnerability stems from an inherent trade-off governed by watermark
window size: smaller windows resist scrubbing better but are easier to
reverse-engineer, enabling low-cost statistics-based spoofing attacks. This
work breaks this trade-off by introducing a novel mechanism, equivalent texture
keys, where multiple tokens within a watermark window can independently support
the detection. Based on the redundancy, we propose a novel watermark scheme
with Sub-vocabulary decomposed Equivalent tExture Key (SEEK). It achieves a
Pareto improvement, increasing the resilience against scrubbing attacks without
compromising robustness to spoofing. Experiments demonstrate SEEK's superiority
over prior method, yielding spoofing robustness gains of +88.2%/+92.3%/+82.0%
and scrubbing robustness gains of +10.2%/+6.4%/+24.6% across diverse dataset
settings.

</details>


### [14] [The bitter lesson of misuse detection](https://arxiv.org/abs/2507.06282)
*Hadrien Mariaccia,Charbel-Raphaël Segerie,Diego Dorn*

Main category: cs.CR

TL;DR: The paper introduces BELLS, a new benchmark for evaluating LLM supervision systems, revealing limitations in specialized tools and highlighting the need for generalist LLMs to detect diverse misuses and jailbreaks effectively.


<details>
  <summary>Details</summary>
Motivation: Existing research on jailbreak detection primarily focuses on LLMs' inherent robustness, while external supervision systems' effectiveness remains underexplored. Current benchmarks are narrow and lack realistic, diverse attack scenarios.

Method: BELLS framework evaluates supervision systems across two dimensions: harm severity (benign/borderline/harmful) and adversarial sophistication (direct vs. jailbreak). It uses a dataset spanning 3 jailbreak families and 11 harm categories.

Result: Specialized systems struggle with semantic understanding and generalization, achieving near-zero detection rates against direct harmful queries or novel jailbreak methods (e.g., base64 encoding). Generalist LLMs outperformed them but still exhibited metacognitive incoherence (50% error rate in filtering correctly identified harmful content).

Conclusion: General LLM capabilities are critical for robust misuse detection. Simple scaffolding could enhance performance, but further research is needed to balance detection effectiveness with potential trade-offs.

Abstract: Prior work on jailbreak detection has established the importance of
adversarial robustness for LLMs but has largely focused on the model ability to
resist adversarial inputs and to output safe content, rather than the
effectiveness of external supervision systems. The only public and independent
benchmark of these guardrails to date evaluates a narrow set of supervisors on
limited scenarios. Consequently, no comprehensive public benchmark yet verifies
how well supervision systems from the market perform under realistic, diverse
attacks. To address this, we introduce BELLS, a Benchmark for the Evaluation of
LLM Supervision Systems. The framework is two dimensional: harm severity
(benign, borderline, harmful) and adversarial sophistication (direct vs.
jailbreak) and provides a rich dataset covering 3 jailbreak families and 11
harm categories. Our evaluations reveal drastic limitations of specialized
supervision systems. While they recognize some known jailbreak patterns, their
semantic understanding and generalization capabilities are very limited,
sometimes with detection rates close to zero when asking a harmful question
directly or with a new jailbreak technique such as base64 encoding. Simply
asking generalist LLMs if the user question is "harmful or not" largely
outperforms these supervisors from the market according to our BELLS score. But
frontier LLMs still suffer from metacognitive incoherence, often responding to
queries they correctly identify as harmful (up to 30 percent for Claude 3.7 and
greater than 50 percent for Mistral Large). These results suggest that simple
scaffolding could significantly improve misuse detection robustness, but more
research is needed to assess the tradeoffs of such techniques. Our results
support the "bitter lesson" of misuse detection: general capabilities of LLMs
are necessary to detect a diverse array of misuses and jailbreaks.

</details>


### [15] [Bridging AI and Software Security: A Comparative Vulnerability Assessment of LLM Agent Deployment Paradigms](https://arxiv.org/abs/2507.06323)
*Tarek Gasmi,Ramzi Guesmi,Ines Belhadj,Jihene Bennaceur*

Main category: cs.CR

TL;DR: This study compares security vulnerabilities in LLM agents using Function Calling vs. MCP architectures through 3,250 attack scenarios, revealing higher success rates in complex attacks and unexpected exploitability patterns in advanced models.


<details>
  <summary>Details</summary>
Motivation: Current LLM security research isolates AI-specific and traditional software vulnerabilities, creating an incomplete understanding of cross-domain threats. This work identifies the need for unified evaluation frameworks to holistically assess agent security.

Method: The researchers evaluated seven language models using controlled experiments across 3,250 attack scenarios targeting prompt injection and software vulnerabilities (JSON injection, DoS). They compared attack success rates in Function Calling (system-centric) and Model Context Protocol (MCP) (LLM-centric) deployment models for simple, composed, and chained attacks.

Result: Function Calling achieved higher overall attack success (73.5%) than MCP (62.59%), but showed increased system vulnerabilities whereas MCP had more LLM vulnerabilities. Chained attacks (91-96% success) were far more effective than simpler attacks. Advanced reasoning models demonstrated ironic vulnerabilities: better threat detection coexisted with higher exploitability.

Conclusion: Architectural choices in LLM agent design significantly alter vulnerability dynamics, emphasizing the need for comprehensive cross-domain security assessments. This study provides empirical methodology and evidence-based recommendations for secure deployment, available through open-source materials.

Abstract: Large Language Model (LLM) agents face security vulnerabilities spanning
AI-specific and traditional software domains, yet current research addresses
these separately. This study bridges this gap through comparative evaluation of
Function Calling architecture and Model Context Protocol (MCP) deployment
paradigms using a unified threat classification framework. We tested 3,250
attack scenarios across seven language models, evaluating simple, composed, and
chained attacks targeting both AI-specific threats (prompt injection) and
software vulnerabilities (JSON injection, denial-of-service). Function Calling
showed higher overall attack success rates (73.5% vs 62.59% for MCP), with
greater system-centric vulnerability while MCP exhibited increased LLM-centric
exposure. Attack complexity dramatically amplified effectiveness, with chained
attacks achieving 91-96% success rates. Counterintuitively, advanced reasoning
models demonstrated higher exploitability despite better threat detection.
Results demonstrate that architectural choices fundamentally reshape threat
landscapes. This work establishes methodological foundations for cross-domain
LLM agent security assessment and provides evidence-based guidance for secure
deployment. Code and experimental materials are available at https: // github.
com/ theconsciouslab-ai/llm-agent-security.

</details>


### [16] [Never Trust the Manufacturer, Never Trust the Client: A Novel Method for Streaming STL Files for Secure Additive](https://arxiv.org/abs/2507.06421)
*Seyed Ali Ghazi Asgar,Narasimha Reddy,Satish T. S. Bukkapatnam*

Main category: cs.CR

TL;DR: The paper proposes a method to secure design and manufacturing process intellectual property (IP) in additive manufacturing as a service (MaaS) platforms by segmenting and streaming STL files instead of G-code files, and using a machine-specific translator at the manufacturer's site to generate G-code in real-time.


<details>
  <summary>Details</summary>
Motivation: Sharing G-code files from clients to manufacturers over networks risks exposure of both design IP and manufacturing process IP to cyberattacks. Previous solutions using G-code streaming were vulnerable to malicious code.

Method: The approach involves (1) segmenting and streaming the design (STL) file to the manufacturer, and (2) implementing a novel, machine-specific STL-to-G-code translator at the manufacturer's site that processes the stream in real-time without exposing the G-code or manufacturing parameters to the client.

Result: The method was implemented and validated in a real-world setting, demonstrating its effectiveness in protecting design and manufacturing process IPs simultaneously without requiring trust between parties.

Conclusion: This approach provides a viable solution for MaaS platforms to safeguard both client design IP and manufacturer process IP through secure STL streaming and machine-specific translation, ensuring mutual trust is unnecessary for secure collaboration.

Abstract: While additive manufacturing has opened interesting avenues to reimagine
manufacturing as a service (MaaS) platform, transmission of design files from
client to manufacturer over networks opens up many cybersecurity challenges.
Securing client's intellectual property (IP) especially from cyber-attacks
emerges as a major challenge. Earlier works introduced streaming, instead of
sharing process plan (G-code) files, as a possible solution. However, executing
client's G-codes on manufacturer's machines exposes them to potential malicious
G-codes. This paper proposes a viable approach when the client and manufacturer
do not trust each other and both the client and manufacturer want to preserve
their IP of designs and manufacturing process respectively. The proposed
approach is based on segmenting and streaming design (STL) files and employing
a novel machine-specific STL to G-code translator at the manufacturer's site in
real-time for printing. This approach secures design and manufacturing process
IPs as demonstrated in a real-world implementation.

</details>


### [17] [Rugsafe: A multichain protocol for recovering from and defending against Rug Pulls](https://arxiv.org/abs/2507.06423)
*Jovonni L. Pharr,Jahanzeb M. Hussain*

Main category: cs.CR

TL;DR: Rugsafe is a multichain protocol using cryptographic security and economic incentives to recover assets from rug pulls and convert rugged tokens into rewards through vaults and inversely pegged anticoins.


<details>
  <summary>Details</summary>
Motivation: The protocol addresses the urgent need to mitigate rug pull risks in cryptocurrency by transforming vulnerabilities into opportunities and ensuring asset recovery.

Method: Rugsafe employs specialized vaults for depositing rugged tokens, issues anticoins inversely correlated to token price, and dynamically adjusts its native token supply based on volume, value, and activity to maintain stability.

Result: Users gain incentives via rug pulls on the RugSafe chain, leveraging heterogeneous blockchain ecosystems, while securing the protocol through anticoins burning and reward mechanisms.

Conclusion: Rugsafe offers a scalable solution to a critical crypto challenge by combining cross-chain compatibility, economic alignment, and cryptographic safeguards, fostering trust and resilience in decentralized finance.

Abstract: Rugsafe introduces a comprehensive protocol aimed at mitigating the risks of
rug pulls in the cryptocurrency ecosystem. By utilizing cryptographic security
measures and economic incentives, the protocol provides a secure multichain
system for recovering assets and transforming rugged tokens into opportunities
and rewards. Foundational to Rugsafe are specialized vaults where rugged tokens
can be securely deposited, and anticoin tokens are issued as receipts. These
anticoins are designed to be inversely pegged to the price movement of the
underlying rugged token. Users can utilize these anticoins within the ecosystem
or choose to burn them, further securing the protocol and earning additional
rewards. The supply of the native Rugsafe token is dynamically adjusted based
on the volume, value, and activity of rugged tokens, ensuring stability and
resilience. By depositing rugged tokens into a vault on several chains, and by
burning anticoins, users receive incentives on the RugSafe chain. This
protocol's vaults are designed to work in heterogenous blockchain ecosystems,
offering a practical and effective solution to one of the most significant
challenges in the cryptocurrency market.

</details>


### [18] [HEMA: A Hands-on Exploration Platform for MEMS Sensor Attacks](https://arxiv.org/abs/2507.06439)
*Bhagawat Baanav Yedla Ravi,Md Rafiul Kabir,Sandip Ray*

Main category: cs.CR

TL;DR: The paper introduces \hema, an affordable hands-on platform for exploring MEMS sensor security in automotive systems, addressing the lack of practical training resources in safety-critical domains.


<details>
  <summary>Details</summary>
Motivation: Traditional automotive security learning methods fail to provide sufficient hands-on experience with safety-critical systems like MEMS sensors in ADAS, creating challenges for novices to master hardware-software interactions and security vulnerabilities.

Method: Developed a novel exploration framework combining physical MEMS sensors with customizable attack and analysis tools, while documenting unique design considerations to create an accessible and cost-effective learning environment.

Result: Enabled practical demonstrations of MEMS security compromises through sensor re-characterization experiments, establishing a tangible resource for automotive security education and research.

Conclusion: The framework contributes to deeper understanding of automotive safety/security interdependencies, empowering educators and practitioners to develop hands-on expertise without requiring expensive industrial systems.

Abstract: Automotive safety and security are paramount in the rapidly advancing
landscape of vehicular technology. Building safe and secure vehicles demands a
profound understanding of automotive systems, particularly in safety and
security. Traditional learning approaches, such as reading materials or
observing demonstrations, often fail to provide the practical, hands-on
experience essential for developing this expertise. For novice users, gaining
access to automotive-grade systems and mastering their associated hardware and
software can be challenging and overwhelming. In this paper, we present a
novel, affordable, and flexible exploration platform, \hema, that enables users
to gain practical, hands-on insights into the security compromises of
micro-electromechanical systems (MEMS) sensors, a critical component in modern
ADAS systems. Furthermore, we discuss the unique challenges and design
considerations involved in creating such a platform, emphasizing its role in
enhancing the understanding of automotive safety and security. This framework
serves as an invaluable resource for educators, researchers, and practitioners
striving to build expertise in the field.

</details>


### [19] [Vectorised Hashing Based on Bernstein-Rabin-Winograd Polynomials over Prime Order Fields](https://arxiv.org/abs/2507.06490)
*Kaushik Nath,Palash Sarkar*

Main category: cs.CR

TL;DR: This paper introduces 4-decBRWHash, a fast AXU hash function optimized for modern Intel processors using AVX2 SIMD instructions, demonstrating up to 23% performance improvements over Poly1305 for large messages.


<details>
  <summary>Details</summary>
Motivation: AXU hash functions are critical for applications requiring unbiased hashing under adversarial conditions. Current implementations like Poly1305 face performance limitations with large messages.

Method: Extends BRW polynomials with parameterized c-way SIMD implementation (c=4). Provides hand-optimized AVX2 assembly for decBRWHash and polyHash over prime fields 2¹²⁷⁻¹ and 2¹³⁰⁻⁵.

Result: 4-decBRWHash achieves 16% speed-up for ~KB messages and 23% speed-up for ~MB messages compared to Poly1305 on AVX2 architectures, outperforming poly1305 without compromising AXU properties.

Conclusion: The decBRWHash family shows superior practical performance for AXU hashing on modern SIMD architectures while maintaining theoretical security guarantees through prime field arithmetic.

Abstract: We introduce the new AXU hash function decBRWHash, which is parameterised by
the positive integer $c$ and is based on Bernstein-Rabin-Winograd (BRW)
polynomials. Choosing $c>1$ gives a hash function which can be implemented
using $c$-way single instruction multiple data (SIMD) instructions. We report a
set of very comprehensive hand optimised assembly implementations of
4-decBRWHash using avx2 SIMD instructions available on modern Intel processors.
For comparison, we also report similar carefully optimised avx2 assembly
implementations of polyHash, an AXU hash function based on usual polynomials.
Our implementations are over prime order fields, specifically the primes
$2^{127}-1$ and $2^{130}-5$. For the prime $2^{130}-5$, for avx2
implementations, compared to the famous Poly1305 hash function, 4-decBRWHash is
faster for messages which are a few hundred bytes long and achieves a speed-up
of about 16% for message lengths in a few kilobytes range and improves to a
speed-up of about 23% for message lengths in a few megabytes range.

</details>


### [20] [A Survey on Artificial Noise for Physical Layer Security: Opportunities, Technologies, Guidelines, Advances, and Trends](https://arxiv.org/abs/2507.06500)
*Hong Niu,Yue Xiao,Xia Lei,Jiangong Chen,Zhihan Xiao,Mao Li,Chau Yuen*

Main category: cs.CR

TL;DR: This paper surveys artificial noise (AN) techniques for physical-layer security in wireless communications, covering their evolution, modeling, applications, and future challenges.


<details>
  <summary>Details</summary>
Motivation: Wireless communication security is critical due to its broadcast nature, and AN has emerged as a promising approach to enhance secrecy capacity by exploiting channel characteristics.

Method: The paper structures a comprehensive review through three phases: (1) introduction to AN fundamentals and development, (2) survey of AN-empowered scenarios and combined technologies, and (3) discussion of technical challenges for future research.

Result: The analysis provides a detailed overview of existing work on AN-based security techniques, highlighting their application scenarios and integrated methodologies across wireless systems.

Conclusion: The paper identifies key technical challenges in AN-aided wireless security and outlines future research directions for improving security performance in practical scenarios.

Abstract: Due to the broadcast nature of wireless communications, physical-layer
security has attracted increasing concerns from both academia and industry.
Artificial noise (AN), as one of the promising physical-layer security
techniques, is capable of utilizing the spatial degree-of-freedom of channels
to effectively enhance the security of wireless communications. In contrast to
other physicallayer security techniques, the key distinguishing feature of AN
is to generate specific interfering signals according to channel
characteristics, increasing the secrecy capacity by reducing the wiretap
channel capacity without affecting the legitimate channel capacity. Hence, this
paper provides the latest survey of AN, including its evolution, modeling,
backgrounds, applications, and future trends. Initially, we introduce the
development, fundamentals, and backgrounds of AN. Subsequently, we highlight a
comprehensive survey of the current state of research on various AN-empowered
scenarios and AN-combined technologies. Finally, we discuss some technical
challenges to tackle for AN-aided wireless security in the future.

</details>


### [21] [Subgraph Counting under Edge Local Differential Privacy Based on Noisy Adjacency Matrix](https://arxiv.org/abs/2507.06508)
*Jintao Guo,Ying Zhou,Chao Li,Guixun Luo*

Main category: cs.CR

TL;DR: The paper introduces Noisy Adjacency Matrix (NAM) and five algorithms (TriOR, TriTR, TriMTR, QuaTR, 2STAR) for efficient and accurate subgraph counting under edge-LDP, addressing existing issues of high time complexity, cost, and trust assumptions.


<details>
  <summary>Details</summary>
Motivation: Current subgraph counting algorithms under edge-LDP and shuffle models face high time complexity, excessive download costs, low accuracy, or reliance on trusted third parties. The motivation is to develop a scalable, versatile solution that mitigates these challenges while maintaining strong privacy guarantees.

Method: The authors propose NAM, which integrates differential privacy with a graph's adjacency matrix. This approach enables scalable application to various DP variants and graph types. Five algorithms are designed for triangle, quadrangle, and 2-star counting: TriOR (one-round accuracy maximization), TriTR (optimal accuracy), TriMTR (high accuracy with low download costs), QuaTR (first pure edge-LDP quadrangle counting), and 2STAR (efficient joint estimation of multiple subgraph counts).

Result: Theoretical and experimental results show TriOR achieves maximal accuracy with reduced time complexity among one-round algorithms; TriTR offers optimal accuracy; TriMTR provides highest accuracy under low cost; QuaTR is the first edge-LDP quadrangle algorithm; and 2STAR enables accurate, simultaneous estimation of triangle, quadrangle, and 2-star counts in two rounds, leveraging byproducts of existing algorithms.

Conclusion: NAM-based algorithms effectively address limitations of prior methods by offering accurate, efficient, and privacy-preserving subgraph counting across diverse scenarios. The proposed solutions demonstrate strong versatility and scalability, achieving state-of-the-art performance in both one-round and joint estimation settings for triangle, quadrangle, and 2-star counts under edge-LDP.

Abstract: When analyzing connection patterns within graphs, subgraph counting serves as
an effective and fundamental approach. Edge-local differential privacy
(edge-LDP) and shuffle model have been employed to achieve subgraph counting
under a privacy-preserving situation. Existing algorithms are plagued by high
time complexity, excessive download costs, low accuracy, or dependence on
trusted third parties. To address the aforementioned challenges, we propose the
Noisy Adjacency Matrix (NAM), which combines differential privacy with the
adjacency matrix of the graph. NAM offers strong versatility and scalability,
making it applicable to a wider range of DP variants, DP mechanisms, and graph
types. Based on NAM, we designed five algorithms (TriOR, TriTR, TriMTR, QuaTR,
and 2STAR) to count three types of subgraphs: triangles, quadrangles, and
2-stars. Theoretical and experimental results demonstrate that in triangle
counting, TriOR maximizes accuracy with reduced time complexity among one-round
algorithms, TriTR achieves optimal accuracy, TriMTR achieves the highest
accuracy under low download costs, and QuaTR stands as the first quadrangle
counting algorithm under pure edge-LDP. We implement edge-LDP for noisy data
via a confidence interval-inspired method, providing DP guarantees on
randomized data. Our 2STAR algorithm achieves the highest accuracy in 2-star
counting and can be derived as a byproduct of two-round triangle or quadrangle
counting algorithms, enabling efficient joint estimation of triangle,
quadrangle, and 2-star counts within two query rounds.

</details>


### [22] [Approximating Euler Totient Function using Linear Regression on RSA moduli](https://arxiv.org/abs/2507.06706)
*Gilda Rech Bansimba,Regis F. Babindamana,Beni Blaug N. Ibara*

Main category: cs.CR

TL;DR: This paper explores using linear regression models to approximate Euler's totient function (phi) for RSA cryptanalyis, demonstrating that phi(n) can be predicted with small relative errors potentially aiding attacks.


<details>
  <summary>Details</summary>
Motivation: RSA's security relies on the computational difficulty of calculating phi(n) for large integers; deterministic methods are infeasible, prompting exploration of machine learning as an approximation strategy.

Method: Trained linear regression models on datasets of RSA moduli (64-1024 bits) and their corresponding phi values to learn the modulus-totient relationship, testing on unseen samples for accuracy.

Result: Preliminary results show phi approximation within small relative error margins, suggesting potential utility for specific RSA attack scenarios.

Conclusion: Statistical learning techniques like regression can approximate cryptographic variables (phi) effectively, opening new cryptanalysis directions through approximation-based strategies.

Abstract: The security of the RSA cryptosystem is based on the intractability of
computing Euler's totient function phi(n) for large integers n. Although
deriving phi(n) deterministically remains computationally infeasible for
cryptographically relevant bit lengths, and machine learning presents a
promising alternative for constructing efficient approximations. In this work,
we explore a machine learning approach to approximate Euler's totient function
phi using linear regression models. We consider a dataset of RSA moduli of 64,
128, 256, 512 and 1024 bits along with their corresponding totient values. The
regression model is trained to capture the relationship between the modulus and
its totient, and tested on unseen samples to evaluate its prediction accuracy.
Preliminary results suggest that phi can be approximated within a small
relative error margin, which may be sufficient to aid in certain classes of RSA
attacks. This research opens a direction for integrating statistical learning
techniques into cryptanalysis, providing insights into the feasibility of
attacking cryptosystems using approximation based strategies.

</details>


### [23] [PotentRegion4MalDetect: Advanced Features from Potential Malicious Regions for Malware Detection](https://arxiv.org/abs/2507.06723)
*Rama Krishna Koppanati,Monika Santra,Sateesh Kumar Peddoju*

Main category: cs.CR

TL;DR: PotentRegion4MalDetect enhances malware detection by focusing on potential malicious regions in Control Flow Graphs (CFGs), improving accuracy and reducing false positives compared to traditional models targeting entire binaries.


<details>
  <summary>Details</summary>
Motivation: Current malware detection models process entire binaries, enabling attackers to inject malicious code into benign parts, increasing false positives and allowing obfuscation due to predominant benign features.

Method: The model identifies malicious nodes in partially preprocessed CFGs using StringSifter's malicious strings, then extracts advanced features from these regions alongside completely preprocessed CFG features to counter obfuscation techniques.

Result: Experiments show 8.13% SHAP Absolute Mean and 1.44% SHAP Beeswarm improvements, 99%+ accuracy/precision/recall/AUC/F1-score, 0.064% FPR, reduced memory, faster computation, and lower storage requirements.

Conclusion: Targeting potential malicious regions in CFGs significantly outperforms existing methods, offering higher accuracy, efficiency in computation, and robustness against evasion techniques through novel feature extraction approaches.

Abstract: Malware developers exploit the fact that most detection models focus on the
entire binary to extract the feature rather than on the regions of potential
maliciousness. Therefore, they reverse engineer a benign binary and inject
malicious code into it. This obfuscation technique circumvents the malware
detection models and deceives the ML classifiers due to the prevalence of
benign features compared to malicious features. However, extracting the
features from the potential malicious regions enhances the accuracy and
decreases false positives. Hence, we propose a novel model named
PotentRegion4MalDetect that extracts features from the potential malicious
regions. PotentRegion4MalDetect determines the nodes with potential
maliciousness in the partially preprocessed Control Flow Graph (CFG) using the
malicious strings given by StringSifter. Then, it extracts advanced features of
the identified potential malicious regions alongside the features from the
completely preprocessed CFG. The features extracted from the completely
preprocessed CFG mitigate obfuscation techniques that attempt to disguise
malicious content, such as suspicious strings. The experiments reveal that the
PotentRegion4MalDetect requires fewer entries to save the features for all
binaries than the model focusing on the entire binary, reducing memory
overhead, faster computation, and lower storage requirements. These advanced
features give an 8.13% increase in SHapley Additive exPlanations (SHAP)
Absolute Mean and a 1.44% increase in SHAP Beeswarm value compared to those
extracted from the entire binary. The advanced features outperform the features
extracted from the entire binary by producing more than 99% accuracy,
precision, recall, AUC, F1-score, and 0.064% FPR.

</details>


### [24] [PenTest2.0: Towards Autonomous Privilege Escalation Using GenAI](https://arxiv.org/abs/2507.06742)
*Haitham S. Al-Sinani,Chris J. Mitchell*

Main category: cs.CR

TL;DR: This paper introduces 'PenTest2.0', an advanced AI system for ethical hacking that automates privilege escalation using Large Language Model (LLM) reasoning. It enhances prior AI-augmented tools with features like Retrieval-Augmented Generation in online/offline modes, Chain-of-Thought prompting, and human-in-the-loop integration. The system demonstrates scalable, error-reduced Linux-based penetration testing but highlights challenges including LLM sensitivity to prompts and execution context.


<details>
  <summary>Details</summary>
Motivation: Traditional ethical hacking requires time-intensive, error-prone manual execution of complex commands. Previous AI tools (PenTest++) failed to address privilege escalation, a critical component of penetration testing workflows. The need for scalable, automated yet context-aware systems motivates this work's enhancements to LLM-driven hacking capabilities.

Method: PenTest2.0 combines LLM reasoning with four technical innovations: (1) Retrieval-Augmented Generation for real-time/retrospective knowledge, (2) Chain-of-Thought prompting for multi-step reasoning, (3) persistent task trees to track goal progression across steps, and (4) optional integration of human-authored hints. The LLM autonomously crafts and executes privilege escalation attack chains while maintaining state through task trees.

Result: The system successfully demonstrated multi-turn, adaptive privilege escalation on a controlled Linux target without human intervention. Comprehensive testing revealed effective task progression tracking when using both online and offline retrieval modes. Cost analysis showed computational feasibility, though performance limitations were observed during complex attack sequences involving high-path semantic drift.

Conclusion: PenTest2.0 represents a significant advance toward practical AI-automated penetration testing through: (1) fully LLM-driven privilege escalation, (2) context-adaptive task progression tracking, and (3) flexible knowledge integration. However, persistent LLM limitations related to prompt structure sensitivity, execution context dependence, and semantic drift during multi-step attacks remain substantial barriers requiring further R&D. The work provides design insights for developing more robust LLM-based ethical hacking systems.

Abstract: Ethical hacking today relies on highly skilled practitioners executing
complex sequences of commands, which is inherently time-consuming, difficult to
scale, and prone to human error. To help mitigate these limitations, we
previously introduced 'PenTest++', an AI-augmented system combining automation
with generative AI supporting ethical hacking workflows. However, a key
limitation of PenTest++ was its lack of support for privilege escalation, a
crucial element of ethical hacking. In this paper we present 'PenTest2.0', a
substantial evolution of PenTest++ supporting automated privilege escalation
driven entirely by Large Language Model reasoning. It also incorporates several
significant enhancements: 'Retrieval-Augmented Generation', including both
one-line and offline modes; 'Chain-of-Thought' prompting for intermediate
reasoning; persistent 'PenTest Task Trees' to track goal progression across
turns; and the optional integration of human-authored hints. We describe how it
operates, present a proof-of-concept prototype, and discuss its benefits and
limitations. We also describe application of the system to a controlled Linux
target, showing it can carry out multi-turn, adaptive privilege escalation. We
explain the rationale behind its core design choices, and provide comprehensive
testing results and cost analysis. Our findings indicate that 'PenTest2.0'
represents a meaningful step toward practical, scalable, AI-automated
penetration testing, whilst highlighting the shortcomings of generative AI
systems, particularly their sensitivity to prompt structure, execution context,
and semantic drift, reinforcing the need for further research and refinement in
this emerging space.
  Keywords: AI, Ethical Hacking, Privilege Escalation, GenAI, ChatGPT, LLM
(Large Language Model), HITL (Human-in-the-Loop)

</details>


### [25] [The Dark Side of LLMs Agent-based Attacks for Complete Computer Takeover](https://arxiv.org/abs/2507.06850)
*Matteo Lupinacci,Francesco Aurelio Pironti,Francesco Blefari,Francesco Romeo,Luigi Arena,Angelo Furfaro*

Main category: cs.CR

TL;DR: This paper demonstrates that LLM agents can be exploited as attack vectors for computer takeover, revealing three new vulnerabilities (direct prompt injection, RAG backdoor attacks, and inter-agent trust exploitation) across 17 tested models. Results show 82.4% vulnerability to inter-agent attacks, with only 5.9% models resisting all attack types, necessitating rethinking of multi-agent security models.


<details>
  <summary>Details</summary>
Motivation: The adoption of LLM agents introduces novel security risks beyond traditional prompt injection. Existing security models fail to account for how autonomous AI agents interact and influence each other, creating exploitable trust boundaries that threaten both individual systems and broader cybersecurity infrastructure.

Method: Comprehensive evaluation of 17 state-of-the-art LLMs (including GPT-4o, Claude-4, and Gemini-2.5) using three attack surfaces: direct prompt injection attacks, RAG (Retrieval-Augmented Generation) backdoor attacks, and inter-agent trust exploitation between autonomously interacting AI agents.

Result: 41.2% of models vulnerable to direct prompt injection, 52.9% to RAG backdoors, 82.4% to inter-agent trust exploitation. Notably, 82.4% of models executed malware payloads when requested via peer agent interactions, even if they resisted direct commands. Only 1/17 models (5.9%) showed resistance to all attack vectors.

Conclusion: LLM agents represent a paradigm shift in cyber threats where AI tools themselves become attack vectors. Current security models inadequately address context-dependent behaviors and inter-agent trust vulnerabilities. The study underscores urgent need for multi-agent security frameworks and deeper research into LLM attack surfaces.

Abstract: The rapid adoption of Large Language Model (LLM) agents and multi-agent
systems enables unprecedented capabilities in natural language processing and
generation. However, these systems have introduced unprecedented security
vulnerabilities that extend beyond traditional prompt injection attacks. This
paper presents the first comprehensive evaluation of LLM agents as attack
vectors capable of achieving complete computer takeover through the
exploitation of trust boundaries within agentic AI systems where autonomous
entities interact and influence each other. We demonstrate that adversaries can
leverage three distinct attack surfaces - direct prompt injection, RAG backdoor
attacks, and inter-agent trust exploitation - to coerce popular LLMs (including
GPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing
malware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals
an alarming vulnerability hierarchy: while 41.2% of models succumb to direct
prompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical
82.4% can be compromised through inter-agent trust exploitation. Notably, we
discovered that LLMs which successfully resist direct malicious commands will
execute identical payloads when requested by peer agents, revealing a
fundamental flaw in current multi-agent security models. Our findings
demonstrate that only 5.9% of tested models (1/17) proved resistant to all
attack vectors, with the majority exhibiting context-dependent security
behaviors that create exploitable blind spots. Our findings also highlight the
need to increase awareness and research on the security risks of LLMs, showing
a paradigm shift in cybersecurity threats, where AI tools themselves become
sophisticated attack vectors.

</details>


### [26] [Are NFTs Ready to Keep Australian Artists Engaged?](https://arxiv.org/abs/2507.06926)
*Ruiqiang Li,Brian Yecies,Qin Wang,Shiping Chen,Jun Shen*

Main category: cs.CR

TL;DR: The paper evaluates NFTs as a mechanism to protect Australian and Indigenous artists' copyright, concluding they are not yet viable for this purpose.


<details>
  <summary>Details</summary>
Motivation: To investigate whether NFTs can effectively protect the copyright of Australian and Indigenous artists through empirical analysis.

Method: The study examines NFT structures, collects data from on-chain, centralized, and decentralized storage systems, and analyzes metadata and artwork content related to copyright, security, and artist identification.

Result: The evaluation results show that NFTs are NOT currently ready to protect Australian and Indigenous artists' copyright.

Conclusion: NFTs face unresolved challenges in copyright protection, security, and artist identification for Australian and Indigenous artworks, requiring further refinement before adoption.

Abstract: Non-Fungible Tokens (NFTs) offer a promising mechanism to protect Australian
and Indigenous artists' copyright. They represent and transfer the value of
artwork in digital form. Before adopting NFTs to protect Australian artwork, we
in this paper investigate them empericially. We focus on examining the details
of NFT structure. We start from the underlying structure of NFTs to show how
they represent copyright for both artists and production owners, as well as how
they aim to safeguard or secure the value of digital artworks. We then involve
data collection from various types of sources with different storage methods,
including on-chain, centralized, and decentralized systems. Based on both
metadata and artwork content, we present our analysis and discussion on the
following key issues: copyright, security and artist identification. The final
results of the evaluation, unfortnately, show that the NFT is NOT ready to
protect Australian and Indigenous artists' copyright.

</details>


### [27] [BarkBeetle: Stealing Decision Tree Models with Fault Injection](https://arxiv.org/abs/2507.06986)
*Qifan Wang,Jonas Sander,Minmin Jiang,Thomas Eisenbarth,David Oswald*

Main category: cs.CR

TL;DR: BarkBeetle is a novel fault injection attack that efficiently extracts decision tree (DT) structural information using a bottom-up strategy, requiring fewer queries and demonstrating practical feasibility on hardware.


<details>
  <summary>Details</summary>
Motivation: ML models (especially DTs) are critical in privacy-sensitive applications, but their confidentiality is at risk due to emerging threats like model extraction and fault injection attacks, necessitating vulnerability assessments.

Method: BarkBeetle employs targeted voltage glitching on specific DT nodes via a bottom-up recovery strategy, using hardware implementation on a Raspberry Pi RP2350 to infer feature splits and thresholds from public UCI datasets.

Result: The attack achieves significant reductions in query requirements and recovers more structural details than prior methods, with practical validation through hardware-based fault injections on 32 Raspberry Pi RP2350 boards.

Conclusion: BarkBeetle demonstrates a practical and effective approach to DT model extraction under fault injection, with implications for broader tree-based applications and the need for enhanced security measures in privacy-critical deployments.

Abstract: Machine learning models, particularly decision trees (DTs), are widely
adopted across various domains due to their interpretability and efficiency.
However, as ML models become increasingly integrated into privacy-sensitive
applications, concerns about their confidentiality have grown, particularly in
light of emerging threats such as model extraction and fault injection attacks.
Assessing the vulnerability of DTs under such attacks is therefore important.
In this work, we present BarkBeetle, a novel attack that leverages fault
injection to extract internal structural information of DT models. BarkBeetle
employs a bottom-up recovery strategy that uses targeted fault injection at
specific nodes to efficiently infer feature splits and threshold values. Our
proof-of-concept implementation demonstrates that BarkBeetle requires
significantly fewer queries and recovers more structural information compared
to prior approaches, when evaluated on DTs trained with public UCI datasets. To
validate its practical feasibility, we implement BarkBeetle on a Raspberry Pi
RP2350 board and perform fault injections using the Faultier voltage glitching
tool. As BarkBeetle targets general DT models, we also provide an in-depth
discussion on its applicability to a broader range of tree-based applications,
including data stream classification, DT variants, and cryptography schemes.

</details>


### [28] [ZKTorch: Compiling ML Inference to Zero-Knowledge Proofs via Parallel Proof Accumulation](https://arxiv.org/abs/2507.07031)
*Bing-Jyue Chen,Lilia Tang,Daniel Kang*

Main category: cs.CR

TL;DR: ZKTorch introduces an efficient zero-knowledge proving system for ML model inference that reduces proof size by 3x and proving time by 6x compared to existing methods, enabling transparency without exposing model weights.


<details>
  <summary>Details</summary>
Motivation: As AI models become prevalent, there is demand for transparency in ML services while keeping model weights confidential. Existing methods either lack efficiency for large models or fail to generalize to new model types.

Method: ZKTorch compiles ML models into basic cryptographic blocks proven via specialized protocols, leveraging a parallel extension of the Mira accumulation scheme to minimize overhead. This avoids heavy circuit-based SNARKs and custom rigid protocols.

Result: Achieved 3× smaller proof sizes than specialized protocols and 6× faster proving times compared to general-purpose ZKML frameworks, demonstrating significant efficiency improvements across both metrics.

Conclusion: ZKTorch presents a practical ZKP system for ML by introducing a novel parallel accumulation scheme and block-based approach, effectively addressing scalability and generalization limitations of prior methods.

Abstract: As AI models become ubiquitous in our daily lives, there has been an
increasing demand for transparency in ML services. However, the model owner
does not want to reveal the weights, as they are considered trade secrets. To
solve this problem, researchers have turned to zero-knowledge proofs of ML
model inference. These proofs convince the user that the ML model output is
correct, without revealing the weights of the model to the user. Past work on
these provers can be placed into two categories. The first method compiles the
ML model into a low-level circuit, and proves the circuit using a ZK-SNARK. The
second method uses custom cryptographic protocols designed only for a specific
class of models. Unfortunately, the first method is highly inefficient, making
it impractical for the large models used today, and the second method does not
generalize well, making it difficult to update in the rapidly changing field of
machine learning. To solve this, we propose ZKTorch, an open source end-to-end
proving system that compiles ML models into base cryptographic operations
called basic blocks, each proved using specialized protocols. ZKTorch is built
on top of a novel parallel extension to the Mira accumulation scheme, enabling
succinct proofs with minimal accumulation overhead. These contributions allow
ZKTorch to achieve at least a $3\times$ reduction in the proof size compared to
specialized protocols and up to a $6\times$ speedup in proving time over a
general-purpose ZKML framework.

</details>


### [29] [LoRAShield: Data-Free Editing Alignment for Secure Personalized LoRA Sharing](https://arxiv.org/abs/2507.07056)
*Jiahao Chen,junhao li,Yiming Wang,Zhe Ma,Yi Jiang,Chunyi Zhou,Qingming Li,Tianyu Du,Shouling Ji*

Main category: cs.CR

TL;DR: LoRA models enable personalized text-to-image generation but risk misuse; LoRAShield is a data-free framework to secure them via weight subspace editing and realignment.


<details>
  <summary>Details</summary>
Motivation: Benign LoRA models can be weaponized for harmful content, while existing defenses overlook their modular adapter role and resilience to adversarial prompt exploitation.

Method: LoRAShield dynamically edits and realigns LoRA weights through adversarial optimization and semantic augmentation, targeting their subspace without requiring training data.

Result: Experiments show LoRAShield effectively blocks malicious generations while maintaining benign model functionality, with high efficiency and robustness.

Conclusion: By platform-driven alignment, LoRAShield establishes secure sharing of personalized LoRAs, advancing safe and scalable generative model ecosystems.

Abstract: The proliferation of Low-Rank Adaptation (LoRA) models has democratized
personalized text-to-image generation, enabling users to share lightweight
models (e.g., personal portraits) on platforms like Civitai and Liblib.
However, this "share-and-play" ecosystem introduces critical risks: benign
LoRAs can be weaponized by adversaries to generate harmful content (e.g.,
political, defamatory imagery), undermining creator rights and platform safety.
Existing defenses like concept-erasure methods focus on full diffusion models
(DMs), neglecting LoRA's unique role as a modular adapter and its vulnerability
to adversarial prompt engineering. To bridge this gap, we propose LoRAShield,
the first data-free editing framework for securing LoRA models against misuse.
Our platform-driven approach dynamically edits and realigns LoRA's weight
subspace via adversarial optimization and semantic augmentation. Experimental
results demonstrate that LoRAShield achieves remarkable effectiveness,
efficiency, and robustness in blocking malicious generations without
sacrificing the functionality of the benign task. By shifting the defense to
platforms, LoRAShield enables secure, scalable sharing of personalized models,
a critical step toward trustworthy generative ecosystems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [30] [Quality attributes of test cases and test suites -- importance & challenges from practitioners' perspectives](https://arxiv.org/abs/2507.06343)
*Huynh Khanh Vi Tran,Nauman bin Ali,Michael Unterkalmsteiner,Jürgen Börstler,Panagiota Chatzipetrou*

Main category: cs.SE

TL;DR: This paper identifies the most important test case quality attributes from practitioners' perspectives and highlights challenges in ensuring these attributes, based on an industrial survey.


<details>
  <summary>Details</summary>
Motivation: Research has identified software testing quality attributes but lacks understanding of their practical importance; this study addresses this gap by investigating practitioners' perceptions.

Method: An industrial survey via LinkedIn gathered 354 responses from diverse software testing professionals using a literature-derived questionnaire.

Result: Fault Detection, Usability, Maintainability, Reliability, and Coverage were deemed most important; challenges include inadequate definitions, lack of metrics, and insufficient review processes.

Conclusion: Practitioners need contextual support to enhance test case quality. Findings guide academic research and suggest corporate strategies to address identified challenges.

Abstract: Context: The quality of the test suites and the constituent test cases
significantly impacts confidence in software testing. While research has
identified several quality attributes of test cases and test suites, there is a
need for a better understanding of their relative importance in practice.
Objective: We investigate practitioners' perceptions regarding the relative
importance of quality attributes of test cases and test suites and the
challenges they face in ensuring the perceived important quality attributes.
Method: We conducted an industrial survey using a questionnaire based on the
quality attributes identified in an extensive literature review. We used a
sampling strategy that leverages LinkedIn to draw a large and heterogeneous
sample of professionals with experience in software testing. Results: We
collected 354 responses from practitioners with a wide range of experience. We
found that the majority of practitioners rated Fault Detection, Usability,
Maintainability, Reliability, and Coverage to be the most important quality
attributes. Resource Efficiency, Reusability, and Simplicity received the most
divergent opinions, which, according to our analysis, depend on the
software-testing contexts. We identified common challenges that apply to the
important attributes, namely inadequate definition, lack of useful metrics,
lack of an established review process, and lack of external support.
Conclusion: The findings point out where practitioners actually need further
support with respect to achieving high-quality test cases and test suites under
different software testing contexts. The findings can serve as a guideline for
academic researchers when looking for research directions on the topic. The
findings can also be used to encourage companies to provide more support to
practitioners to achieve high-quality test cases and test suites.

</details>


### [31] [A proposal and assessment of an improved heuristic for the Eager Test smell detection](https://arxiv.org/abs/2507.06354)
*Huynh Khanh Vi Tran,Nauman bin Ali,Michael Unterkalmsteiner,Jürgen Börstler*

Main category: cs.SE

TL;DR: This paper revises the definition and detection rules for the Eager Test smell in unit testing by analyzing 56 prior studies and proposing a new heuristic validated on 300 Java test cases to reduce inconsistencies and improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Current simplified detection rules for Eager Test smells lack precision, leading to high variability in tool outcomes and practitioner dissatisfaction.

Method: Literature review of existing approaches, development of a refined conceptual definition and heuristic for Eager Test detection, manual validation on 300 Java unit test cases.

Result: 56 studies identified inconsistent definitions; new heuristic detected previously missed patterns (both eager and non-eager) and reduced outcome disparities compared to existing rules.

Conclusion: The proposed heuristic more accurately captures Eager Test characteristics, potentially resolving tool validity issues reported by practitioners.

Abstract: Context: The evidence for the prevalence of test smells at the unit testing
level has relied on the accuracy of detection tools, which have seen intense
research in the last two decades. The Eager Test smell, one of the most
prevalent, is often identified using simplified detection rules that
practitioners find inadequate. Objective: We aim to improve the rules for
detecting the Eager Test smell. Method: We reviewed the literature on test
smells to analyze the definitions and detection rules of the Eager Test smell.
We proposed a novel, unambiguous definition of the test smell and a heuristic
to address the limitations of the existing rules. We evaluated our heuristic
against existing detection rules by manually applying it to 300 unit test cases
in Java. Results: Our review identified 56 relevant studies. We found that
inadequate interpretations of original definitions of the Eager Test smell led
to imprecise detection rules, resulting in a high level of disagreement in
detection outcomes. Also, our heuristic detected patterns of eager and
non-eager tests that existing rules missed. Conclusion: Our heuristic captures
the essence of the Eager Test smell more precisely; hence, it may address
practitioners' concerns regarding the adequacy of existing detection rules.

</details>


### [32] [Evaluating Efficiency and Novelty of LLM-Generated Code for Graph Analysis](https://arxiv.org/abs/2507.06463)
*Atieh Barati Nia,Mohammad Dindoost,David A. Bader*

Main category: cs.SE

TL;DR: Study evaluates eight SOTA LLMs on generating efficient C code for graph algorithms, finding that Claude Sonnet 4 Extended outperforms human-written baselines in triangle counting but highlights limitations in novel algorithm invention.


<details>
  <summary>Details</summary>
Motivation: Prior LLM evaluations focus on functional correctness or high-level languages like Python; this study addresses the gap of assessing efficiency and runtime/memory constraints in low-level C implementations for graph analysis.

Method: Benchmarking eight LLMs (ChatGPT, Claude, Gemini, Grok, DeepSeek) using two approaches: (1) generating algorithms to outperform existing benchmarks and (2) evaluating code for integration into the benchmark. Metrics include efficiency and correctness comparisons against human-written baselines.

Result: Claude Sonnet 4 Extended achieves best efficiency in ready-to-use code generation, outperforming human baselines in triangle counting. LLMs successfully optimize and integrate established algorithms but fail to invent novel techniques.

Conclusion: Current LLMs excel in adapting and optimizing existing graph algorithms for C but lack capacity for innovative algorithm design. Reproducibility is supported through shared prompts, generated code, and measurement scripts.

Abstract: Large Language Models (LLMs) are increasingly used to automate software
development, yet most prior evaluations focus on functional correctness or
high-level languages such as Python. We present the first systematic study of
LLMs' ability to generate efficient C implementations of graph-analysis
routines--code that must satisfy the stringent runtime and memory constraints.
Eight state-of-the-art models (OpenAI ChatGPT o3 and o4-mini-high, Anthropic
Claude 4 Sonnet and Sonnet Extended, Google Gemini 2.5 Flash and Pro, xAI Grok
3-Think, and DeepSeek DeepThink R1) are benchmarked by two distinct approaches.
The first approach checks the ability of LLMs in generating an algorithm
outperforming other present algorithms in the benchmark. The second approach
evaluates the ability of LLMs to generate graph algorithms for integration into
the benchmark. Results show that Claude Sonnet 4 Extended achieves the best
result in the case of ready-to-use code generation and efficiency,
outperforming human-written baselines in triangle counting. The study confirms
that contemporary LLMs excel at optimizing and integrating established
algorithms but not inventing novel techniques. We provide prompts, the first
approach's generated code, and measurement scripts to foster reproducible
research.

</details>


### [33] [Issue Tracking Ecosystems: Context and Best Practices](https://arxiv.org/abs/2507.06704)
*Lloyd Montgomery*

Main category: cs.SE

TL;DR: Analyzing issues in software engineering ecosystems through practitioner interviews and archival research


<details>
  <summary>Details</summary>
Motivation: Issue Tracking Ecosystems (ITEs) face challenges with complex artifact networks and diverse workflows, yet existing ITS research lacks context-aware solutions that align with organizational practices.

Method: The study uses practitioner interviews and archival analysis of multiple ITSs to understand context-dependent ITE problems and identify patterns in tracing solutions.

Result: Findings reveal ITE problems are context-specific; current solutions lack comparable frameworks, motivating the development of a specialized best practice ontology for ITEs

Conclusion: The Best Practice Ontology for ITEs addresses alignment gaps between research and practice, emphasizing context-rich frameworks for managing issue tracking complexities in software organizations.

Abstract: Issue Tracking Systems (ITSs), such as GitHub and Jira, are popular tools
that support Software Engineering (SE) organisations through the management of
``issues'', which represent different SE artefacts such as requirements,
development tasks, and maintenance items. ITSs also support internal linking
between issues, and external linking to other tools and information sources.
This provides SE organisations key forms of documentation, including forwards
and backwards traceability (e.g., Feature Requests linked to sprint releases
and code commits linked to Bug Reports). An Issue Tracking Ecosystem (ITE) is
the aggregate of the central ITS and the related SE artefacts, stakeholders,
and processes -- with an emphasis on how these contextual factors interact with
the ITS. The quality of ITEs is central to the success of these organisations
and their software products. There are challenges, however, within ITEs,
including complex networks of interlinked artefacts and diverse workflows.
While ITSs have been the subject of study in SE research for decades, ITEs as a
whole need further exploration.
  In this thesis, I undertake the challenge of understanding ITEs at a broader
level, addressing these questions regarding complexity and diversity. I
interviewed practitioners and performed archival analysis on a diverse set of
ITSs. These analyses revealed the context-dependent nature of ITE problems,
highlighting the need for context-specific ITE research. While previous work
has produced many solutions to specific ITS problems, these solutions are not
consistently framed in a context-rich and comparable way, leading to a desire
for more aligned solutions across research and practice. To address this
emergent information and lack of alignment, I created the Best Practice
Ontology for ITEs. <... truncated due to arXiv abstract character limit ...>

</details>


### [34] [Leveraging LLMs for Semantic Conflict Detection via Unit Test Generation](https://arxiv.org/abs/2507.06762)
*Nathalia Barbosa,Paulo Borba,Léuson Da Silva*

Main category: cs.SE

TL;DR: The paper investigates the integration of Code Llama 70B into SMAT to improve semantic conflict detection through enhanced test generation, finding potential for improvement despite challenges in complex scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional merge tools cannot detect semantic conflicts, and SMAT's reliance on unit test generation tools like Randoop and Evosuite leads to high false negatives due to their limitations. This motivates exploring LLM-based test generation as a solution.

Method: The authors integrated Code Llama 70B into SMAT, experimenting with diverse interaction strategies, prompt contents, and parameter configurations. Evaluation was performed on two samples: simpler systems from prior work and complex real-world systems.

Result: Results show LLM-based test generation is challenging and computationally costly for complex scenarios, but demonstrates promising potential for improving semantic conflict detection compared to existing tools.

Conclusion: LLMs offer a viable path to enhance semantic conflict detection through better test generation, though practical implementation faces computational challenges and requires further optimization for complex systems.

Abstract: Semantic conflicts arise when a developer introduces changes to a codebase
that unintentionally affect the behavior of changes integrated in parallel by
other developers. Traditional merge tools are unable to detect such conflicts,
so complementary tools like SMAT have been proposed. SMAT relies on generating
and executing unit tests: if a test fails on the base version, passes on a
developer's modified version, but fails again after merging with another
developer's changes, a semantic conflict is indicated. While SMAT is effective
at detecting conflicts, it suffers from a high rate of false negatives, partly
due to the limitations of unit test generation tools such as Randoop and
Evosuite. To investigate whether large language models (LLMs) can overcome
these limitations, we propose and integrate a new test generation tool based on
Code Llama 70B into SMAT. We explore the model's ability to generate tests
using different interaction strategies, prompt contents, and parameter
configurations. Our evaluation uses two samples: a benchmark with simpler
systems from related work, and a more significant sample based on complex,
real-world systems. We assess the effectiveness of the new SMAT extension in
detecting conflicts. Results indicate that, although LLM-based test generation
remains challenging and computationally expensive in complex scenarios, there
is promising potential for improving semantic conflict detection.
  --
  Conflitos sem^anticos surgem quando um desenvolvedor introduz mudan\c{c}as em
uma base de c\'odigo que afetam, de forma n~ao intencional, o comportamento de
altera\c{c}~oes integradas em paralelo por outros desenvolvedores. Ferramentas
tradicionais de merge n~ao conseguem detectar esse tipo de conflito, por isso
ferramentas complementares como o SMAT foram propostas. O SMAT depende da
gera\c{c}~ao e execu\c{c}~ao de testes de unidade: se um teste falha na vers~ao
base, passa na vers~ao modificada por um desenvolvedor, mas volta a falhar
ap\'os o merge com as mudan\c{c}as de outro desenvolvedor, um conflito
sem^antico \'e identificado. Embora o SMAT seja eficaz na detec\c{c}~ao de
conflitos, apresenta alta taxa de falsos negativos, em parte devido \`as
limita\c{c}~oes das ferramentas de gera\c{c}~ao de testes como Randoop e
Evosuite. Para investigar se modelos de linguagem de grande porte (LLMs) podem
superar essas limita\c{c}~oes, propomos e integramos ao SMAT uma nova
ferramenta de gera\c{c}~ao de testes baseada no Code Llama 70B. Exploramos a
capacidade do modelo de gerar testes utilizando diferentes estrat\'egias de
intera\c{c}~ao, conte\'udos de prompts e configura\c{c}~oes de par^ametros.
Nossa avalia\c{c}~ao utiliza duas amostras: um benchmark com sistemas mais
simples, usados em trabalhos relacionados, e uma amostra mais significativa
baseada em sistemas complexos e reais. Avaliamos a efic\'acia da nova extens~ao
do SMAT na detec\c{c}~ao de conflitos. Os resultados indicam que, embora a
gera\c{c}~ao de testes por LLM em cen\'arios complexos ainda seja desafiadora e
custosa computacionalmente, h\'a potencial promissor para aprimorar a
detec\c{c}~ao de conflitos sem^anticos.

</details>


### [35] [Formalization of the AADL Run-Time Services with Time](https://arxiv.org/abs/2507.06881)
*Brian R Larson,Ehsan Ahmad*

Main category: cs.SE

TL;DR: This paper extends the formal semantics of AADL with modal logic and adds time modeling, enhancing RTS support for reactive state-transition machines (BA/BLESS) using a HAMR implementation example.


<details>
  <summary>Details</summary>
Motivation: Existing AADL formal semantics in [1] lack time modeling, which is critical for temporal correctness in cyber-physical systems.

Method: Extended and simplified formalization via modal logic (Kripke structure) to include time, with RTS augmentations for BA/BLESS state-transition machines.

Result: New temporal semantics for AADL RTS; successful implementation demonstration in HAMR with BLESS state-machine behavior.

Conclusion: The time-explicit formalization improves verifiability of cyber-physical systems by better supporting reactive behaviors defined in BA/BLESS standards.

Abstract: The Architecture Analysis & Design Language (AADL) is an architecture
description language for design of cyber-physical systems--machines controlled
by software. The AADL standard, SAE International AS5506D, describes Run-Time
Services (RTS) to be provided to execute AADL models in accordance with
semantics defined by the standard. The RTS of primary concern are transport
services and timing services. Although, the study presented in [1] sets a
foundation for the formal semantics of AADL, but without modeling time. This
paper extends and simplifies this formalization using a modal logic defined by
a Kripke structure, to explicitly include time. The RTS defined in the AADL
standard are also expanded to support reactive state-transition machines of the
Behavior Specification annex standard language (BA) and its closely-related,
formally-defined counterpart, the Behavior Language for Embedded Systems with
Software (BLESS). An example of AADL RTS with time, implemented by the High
Assurance Modeling and Rapid Engineering for Embedded Systems (HAMR) for
state-transition machine behavior written in BLESS, is also presented.

</details>


### [36] [Are They All Good? Evaluating the Quality of CoTs in LLM-based Code Generation](https://arxiv.org/abs/2507.06980)
*Binquan Zhang,Li Zhang,Zhiwen Luo,Yuxin Du,Fang Liu,Song Wang,Lin Shi*

Main category: cs.SE

TL;DR: This paper studies the factors affecting CoT code generation quality in LLMs, analyzing 1,023 failed code samples and 210 CoT-code pairs. It finds 53.60% external factors (unclear requirements) and 40.10% internal factors (LLM misunderstandings) impact CoT quality, and refining CoTs with detailed descriptions improves performance.


<details>
  <summary>Details</summary>
Motivation: Understanding CoT quality in LLMs is critical for ensuring code correctness. While CoT prompting aligns LLMs with human programming approaches, it is unclear how factors like prompt ambiguity and LLM comprehension errors affect the resulting code's reliability, motivating empirical analysis.

Method: 1) Analyzed 1,023 failed code samples across two benchmarks to identify external/internal factors. 2) Studied 210 CoT-code pairs to assess their alignment. 3) Conducted CoT refinement experiments by prompting LLMs with detailed problem descriptions to improve low-quality intermediate steps.

Result: 1) External factors (53.60%) and internal factors (40.10%) significantly reduce CoT quality. 2) Instruction-following problems lead to 18.5% code errors despite correct CoTs, while flawed CoTs may still produce correct final code (11.90%). 3) CoT refinement with explicit problem details improves LLM performance.

Conclusion: CoT-based code generation faces challenges from ambiguous requirements, LLM misunderstanding, and instruction-following gaps. Enhancing problem description clarity and refining LLMs' ability to process contextual details are key directions for improving code generation reliability and reasoning accuracy.

Abstract: Large language models (LLMs) have demonstrated impressive performance in code
generation, particularly when augmented with chain-of-thought (CoT) prompting
techniques. They break down requirements into intermediate reasoning steps,
which act as design rationales to guide LLMs in writing code like human
programmers. Thus, the quality of these steps is crucial for ensuring the
correctness and reliability of the generated code. However, little is known
about the quality of CoT generated by LLMs. To what extent can we trust the
thoughts generated by LLMs? How good are they? This paper empirically explores
the external and internal factors of why LLMs generate unsatisfactory CoTs by
analyzing 1,023 failed code samples on two widely used code generation
benchmarks. We also evaluate their impact on code generation performance by
analyzing 210 CoT-code pairs and refining the unsatisfied CoTs by prompting
LLMs. Our study reveals three key findings: (1) External factors (53.60%), such
as unclear requirements and lack of context, mainly affect CoT quality, while
internal factors (40.10%) stem from LLMs' misunderstanding prompts. (2) Even
when CoTs are correct, 18.5% of the generated code contains errors due to
instruction-following issues; conversely, 11.90% of correct code is paired with
flawed CoTs. (3) Refining low-quality CoTs is feasible, i.e., LLMs improve when
given detailed problem descriptions. These findings highlight key challenges in
CoT-based code generation and suggest directions for improving LLM reasoning
and reliability.

</details>


### [37] [Exploring Fairness Interventions in Open Source Projects](https://arxiv.org/abs/2507.07026)
*Sadia Afrin Mim,Fatema Tuz Zohra,Justin Smith,Brittany Johnson*

Main category: cs.SE

TL;DR: The paper analyzes 62 open-source ML fairness interventions, finding that 32% are actively maintained and 50% detect and mitigate bias, often during inprocessing.


<details>
  <summary>Details</summary>
Motivation: Biased ML models in critical sectors necessitate fairness interventions, yet their underadoption highlights a need for understanding their availability and features.

Method: Systematic compilation of 62 open-source fairness interventions followed by in-depth feature analysis.

Result: 32% active maintenance within a year; 50% provide both bias detection and mitigation, primarily during inprocessing.

Conclusion: The study identifies active fairness interventions and their features, offering insights to guide practitioners in adopting tools to mitigate bias effectively.

Abstract: The deployment of biased machine learning (ML) models has resulted in adverse
effects in crucial sectors such as criminal justice and healthcare. To address
these challenges, a diverse range of machine learning fairness interventions
have been developed, aiming to mitigate bias and promote the creation of more
equitable models. Despite the growing availability of these interventions,
their adoption in real-world applications remains limited, with many
practitioners unaware of their existence. To address this gap, we
systematically identified and compiled a dataset of 62 open source fairness
interventions and identified active ones. We conducted an in-depth analysis of
their specifications and features to uncover considerations that may drive
practitioner preference and to identify the software interventions actively
maintained in the open source ecosystem. Our findings indicate that 32% of
these interventions have been actively maintained within the past year, and 50%
of them offer both bias detection and mitigation capabilities, mostly during
inprocessing.

</details>


### [38] [5C Prompt Contracts: A Minimalist, Creative-Friendly, Token-Efficient Design Framework for Individual and SME LLM Usage](https://arxiv.org/abs/2507.07045)
*Ugur Ari*

Main category: cs.SE

TL;DR: The paper introduces the 5C Prompt Contract framework for efficient and accessible LLM prompt design by distilling it into five components (Character, Cause, Constraint, Contingency, Calibration).


<details>
  <summary>Details</summary>
Motivation: Existing prompt design methods using elaborate DSLs or templates impose token and cognitive overhead, limiting accessibility and model creativity as LLMs become critical in applications for individuals and SMEs with limited resources.

Method: Proposes the 5C framework with explicit fallback/optimization directives structured around five intuitive components: Character (role/identity), Cause (task rationale), Constraint (limitations), Contingency (fallback rules), and Calibration (output refinement).

Result: Consistently achieves superior input token efficiency while maintaining rich, consistent outputs across diverse LLM architectures (OpenAI, Anthropic, DeepSeek, Gemini).

Conclusion: The 5C framework provides a practical, minimal-cognitive-schema solution for reliable and creatively flexible AI interactions, particularly benefiting individuals and SMEs with constrained AI engineering capabilities.

Abstract: The progression from traditional prompt engineering to a more rigorous
discipline of prompt design marks a pivotal shift in human-LLM interaction. As
Large Language Models (LLMs) become increasingly embedded in mission-critical
applications, there emerges a pressing need for frameworks that are not only
explicit and systematic but also minimal enough to remain practical and broadly
accessible. While many existing approaches address prompt structuring through
elaborate Domain-Specific Languages (DSLs) or multi-layered templates, such
methods can impose significant token and cognitive overhead, potentially
constraining the model's creative capacity. In this context, we propose the 5C
Prompt Contract, a framework that distills prompt design into five intuitive
components: Character, Cause, Constraint, Contingency, and Calibration. This
minimal cognitive schema explicitly integrates fallback and output optimization
directives, fostering reliable, interpretable, and creatively flexible AI
interactions. Experimental results demonstrate that the 5C framework
consistently achieves superior input token efficiency while maintaining rich
and consistent outputs across diverse LLM architectures (OpenAI, Anthropic,
DeepSeek, and Gemini), making it particularly suited for individuals and
Small-to-Medium Enterprises (SMEs) with limited AI engineering resources.

</details>
