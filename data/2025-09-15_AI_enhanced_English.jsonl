{"id": "2509.09853", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09853", "abs": "https://arxiv.org/abs/2509.09853", "authors": ["Zhiyu Fan", "Kirill Vasilevski", "Dayi Lin", "Boyuan Chen", "Yihao Chen", "Zhiqing Zhong", "Jie M. Zhang", "Pinjia He", "Ahmed E. Hassan"], "title": "SWE-Effi: Re-Evaluating Software AI Agent System Effectiveness Under Resource Constraints", "comment": null, "summary": "The advancement of large language models (LLMs) and code agents has\ndemonstrated significant potential to assist software engineering (SWE) tasks,\nsuch as autonomous issue resolution and feature addition. Existing AI for\nsoftware engineering leaderboards (e.g., SWE-bench) focus solely on solution\naccuracy, ignoring the crucial factor of effectiveness in a\nresource-constrained world. This is a universal problem that also exists beyond\nsoftware engineering tasks: any AI system should be more than correct - it must\nalso be cost-effective. To address this gap, we introduce SWE-Effi, a set of\nnew metrics to re-evaluate AI systems in terms of holistic effectiveness\nscores. We define effectiveness as the balance between the accuracy of outcome\n(e.g., issue resolve rate) and the resources consumed (e.g., token and time).\nIn this paper, we specifically focus on the software engineering scenario by\nre-ranking popular AI systems for issue resolution on a subset of the SWE-bench\nbenchmark using our new multi-dimensional metrics. We found that AI system's\neffectiveness depends not just on the scaffold itself, but on how well it\nintegrates with the base model, which is key to achieving strong performance in\na resource-efficient manner. We also identified systematic challenges such as\nthe \"token snowball\" effect and, more significantly, a pattern of \"expensive\nfailures\". In these cases, agents consume excessive resources while stuck on\nunsolvable tasks - an issue that not only limits practical deployment but also\ndrives up the cost of failed rollouts during RL training. Lastly, we observed a\nclear trade-off between effectiveness under the token budget and effectiveness\nunder the time budget, which plays a crucial role in managing project budgets\nand enabling scalable reinforcement learning, where fast responses are\nessential.", "AI": {"tldr": "This paper introduces SWE-Effi, a holistic metric for AI systems in software engineering that balances accuracy and resource efficiency. Analysis on SWE-bench highlights scaffold-integration importance, 'expensive failures,' and budget trade-offs, offering practical insights for cost-effective AI deployment.", "motivation": "Existing benchmarks (e.g., SWE-bench) focus solely on accuracy, neglecting resource efficiency\u2014a universal issue in AI systems. Effective AI must balance accuracy with cost-effectiveness, especially in resource-constrained environments.", "method": "The authors introduced SWE-Effi, a multi-dimensional metric framework combining outcome accuracy (e.g., issue resolution rate) and resource utilization (tokens, time). They re-ranked AI systems on a subset of SWE-bench to evaluate effectiveness under these new metrics.", "result": "Systems' effectiveness depends on scaffold-integration efficiency with base models. Systematic challenges like 'token snowball' and 'expensive failures' (agents using excessive resources on unsolvable tasks) were identified. A trade-off between token and time budget effectiveness was observed, impacting scalable reinforcement learning.", "conclusion": "The study emphasizes the importance of resource-efficient AI systems in software engineering and highlights the need to balance outcome accuracy with efficient resource consumption. Key findings include the impact of scaffold-integration efficiency and the criticality of addressing 'expensive failures' to reduce costs in practical deployment and reinforcement learning."}}
{"id": "2509.09873", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09873", "abs": "https://arxiv.org/abs/2509.09873", "authors": ["James Jewitt", "Hao Li", "Bram Adams", "Gopi Krishnan Rajbahadur", "Ahmed E. Hassan"], "title": "From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI Ecosystem", "comment": "9 pages, 4 figures, 5 tables, pre-print", "summary": "Hidden license conflicts in the open-source AI ecosystem pose serious legal\nand ethical risks, exposing organizations to potential litigation and users to\nundisclosed risk. However, the field lacks a data-driven understanding of how\nfrequently these conflicts occur, where they originate, and which communities\nare most affected. We present the first end-to-end audit of licenses for\ndatasets and models on Hugging Face, as well as their downstream integration\ninto open-source software applications, covering 364 thousand datasets, 1.6\nmillion models, and 140 thousand GitHub projects. Our empirical analysis\nreveals systemic non-compliance in which 35.5% of model-to-application\ntransitions eliminate restrictive license clauses by relicensing under\npermissive terms. In addition, we prototype an extensible rule engine that\nencodes almost 200 SPDX and model-specific clauses for detecting license\nconflicts, which can solve 86.4% of license conflicts in software applications.\nTo support future research, we release our dataset and the prototype engine.\nOur study highlights license compliance as a critical governance challenge in\nopen-source AI and provides both the data and tools necessary to enable\nautomated, AI-aware compliance at scale.", "AI": {"tldr": "First end-to-end audit of Hugging Face's open-source AI ecosystem reveals systemic license non-compliance and introduces tools to automate conflict resolution (86.4% efficacy).", "motivation": "The paper addresses the lack of data-driven understanding of license conflicts in the open-source AI ecosystem, which exposes organizations to legal risks and users to undisclosed ethical risks.", "method": "The authors conducted an end-to-end audit of licenses for 364,000 datasets, 1.6 million models, and 140,000 GitHub projects on Hugging Face. They developed an extensible rule engine encoding ~200 SPDX and model-specific clauses to detect license conflicts.", "result": "35.5% of model-to-application transitions eliminated restrictive licenses via relicensing under permissive terms, while the prototype engine resolved 86.4% of license conflicts. The dataset and rule engine were released for future research.", "conclusion": "The study reveals that license compliance is a critical governance challenge in open-source AI and provides both a dataset of license conflicts and an extensible rule engine to enable automated, AI-aware compliance at scale."}}
{"id": "2509.09917", "categories": ["cs.SE", "D.2.4"], "pdf": "https://arxiv.org/pdf/2509.09917", "abs": "https://arxiv.org/abs/2509.09917", "authors": ["Zehan Chen", "Long Zhang", "Zhiwei Zhang", "JingJing Zhang", "Ruoyu Zhou", "Yulong Shen", "JianFeng Ma", "Lin Yang"], "title": "SLD-Spec: Enhancement LLM-assisted Specification Generation for Complex Loop Functions via Program Slicing and Logical Deletion", "comment": "22 pages, 2 figures, conference", "summary": "Automatically generating formal specifications from program code can greatly\nenhance the efficiency of program verification and enable end-to-end automation\nfrom requirements to reliable software. However, existing LLM-based approaches\noften struggle with programs that include complex loop structures, leading to\nirrelevant specifications. Moreover, the rigorous proof obligations and design\nconstraints imposed by verification tools can further result in incomplete and\nambiguous specifications. To address these challenges, we propose SLD-Spec, an\nLLM-assisted specification generation method tailored for programs with complex\nloop constructs. SLD-Spec introduces two novel phases into the traditional\nspecification generation framework: (1) A slicing phase, which decomposes each\nfunction into code fragments containing independent loop structures, thereby\nreducing the complexity of specification generation; and (2) A logical deletion\nphase, which applies LLM-based reasoning to filter out incorrect candidate\nspecifications--especially those not easily identified by verification\ntool--while retaining valid ones. Experimental results show that on the simple\ndataset, SLD-Spec successfully verifies five more programs than the\nstate-of-the-art AutoSpec and reduces runtime by 23.73%. To address the\nlimitations of existing research, we manually construct a dataset comprising\nfour categories of complex loop programs. On this dataset, SLD-Spec\nsignificantly improves the correctness, relevance, and completeness of\ngenerated specifications compared to baseline methods, enabling 95.1% of\nassertions and 90.91% of programs to pass verification. Ablation studies\nfurther reveal that logical deletion is critical for enhancing specification\ncorrectness and relevance, while program slicing contributes significantly to\nspecification completeness. Our code and data are publicly available.", "AI": {"tldr": "SLD-Spec is an LLM-based method for generating formal specifications that addresses limitations in handling complex loops and verification constraints by introducing slicing and logical deletion phases. It achieves high correctness and completeness in specification generation.", "motivation": "Existing LLM-based approaches struggle with programs containing complex loops and verification tool constraints, resulting in incomplete/ambiguous specifications. Verification automation requires better methods for loop-heavy code.", "method": "The method introduces (1)a slicing phase to decompose functions into independent loop fragments, and (2)a logical deletion phase using LLM reasoning to filter out incorrect candidate specifications, retaining only valid ones.", "result": "SLD-Spec verifies 5 more programs and reduces runtime by 23.73% on baseline datasets. On manually constructed complex loop datasets, it achieves 95.1%/90.91%\nverification accuracy for assertions/programs. Ablation studies confirm both phases' effectiveness.", "conclusion": "SLD-Spec improves specification completeness, correctness, and relevance for loop-intensive programs through its two-phase approach. It establishes a new benchmark for LLM-assisted formal specification generation."}}
{"id": "2509.09918", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09918", "abs": "https://arxiv.org/abs/2509.09918", "authors": ["Seyed Moein Abtahi", "Akramul Azim"], "title": "WALL: A Web Application for Automated Quality Assurance using Large Language Models", "comment": null, "summary": "As software projects become increasingly complex, the volume and variety of\nissues in code files have grown substantially. Addressing this challenge\nrequires efficient issue detection, resolution, and evaluation tools. This\npaper presents WALL, a web application that integrates SonarQube and large\nlanguage models (LLMs) such as GPT-3.5 Turbo and GPT-4o to automate these\ntasks. WALL comprises three modules: an issue extraction tool, code issues\nreviser, and code comparison tool. Together, they enable a seamless pipeline\nfor detecting software issues, generating automated code revisions, and\nevaluating the accuracy of revisions. Our experiments, conducted on 563 files\nwith over 7,599 issues, demonstrate WALL's effectiveness in reducing human\neffort while maintaining high-quality revisions. Results show that employing a\nhybrid approach of cost-effective and advanced LLMs can significantly lower\ncosts and improve revision rates. Future work aims to enhance WALL's\ncapabilities by integrating open-source LLMs and eliminating human\nintervention, paving the way for fully automated code quality management.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.09706", "categories": ["cs.CR", "cs.AI", "cs.CL", "I.2; H.3.3"], "pdf": "https://arxiv.org/pdf/2509.09706", "abs": "https://arxiv.org/abs/2509.09706", "authors": ["Taniya Gidatkar", "Oluwaseun Ajao", "Matthew Shardlow"], "title": "Differential Robustness in Transformer Language Models: Empirical Evaluation Under Adversarial Text Attacks", "comment": "8 pages, 4 tables, to appear in proceedings of Recent Advances in\n  Natural Language Processing (RANLP 2025) and ACL Anthology", "summary": "This study evaluates the resilience of large language models (LLMs) against\nadversarial attacks, specifically focusing on Flan-T5, BERT, and RoBERTa-Base.\nUsing systematically designed adversarial tests through TextFooler and\nBERTAttack, we found significant variations in model robustness. RoBERTa-Base\nand FlanT5 demonstrated remarkable resilience, maintaining accuracy even when\nsubjected to sophisticated attacks, with attack success rates of 0%. In\ncontrast. BERT-Base showed considerable vulnerability, with TextFooler\nachieving a 93.75% success rate in reducing model accuracy from 48% to just 3%.\nOur research reveals that while certain LLMs have developed effective defensive\nmechanisms, these safeguards often require substantial computational resources.\nThis study contributes to the understanding of LLM security by identifying\nexisting strengths and weaknesses in current safeguarding approaches and\nproposes practical recommendations for developing more efficient and effective\ndefensive strategies.", "AI": {"tldr": "This study evaluates LLM resilience against adversarial attacks, finding RoBERTa-Base and Flan-T5 highly robust but computationally intensive, while BERT-Base is severely vulnerable. It underscores the need for efficient defense mechanisms.", "motivation": "This work addresses the critical gap in understanding LLM security by investigating how different architectures withstand adversarial attacks. The findings aim to inform the development of more secure and resource-efficient models for real-world applications.", "method": "The research employs systematic adversarial testing using TextFooler and BERTAttack on three models (Flan-T5, BERT-Base, RoBERTa-Base) to measure their robustness. Attack success rates and model accuracy changes under various perturbations are quantitatively analyzed.", "result": "RoBERTa-Base and Flan-T5 showed zero attack success rates, maintaining accuracy under attacks. In contrast, BERT-Base was highly vulnerable, with TextFooler reducing accuracy from 48% to 3% (93.75% attack success). The results also reveal a trade-off between robustness and computational efficiency.", "conclusion": "The study concludes that while some LLMs like RoBERTa-Base and Flan-T5 exhibit strong resilience against adversarial attacks, their robustness comes at the cost of high computational resource demands. It highlights the need for developing more efficient defensive strategies to balance security and practicality."}}
{"id": "2509.09947", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.09947", "abs": "https://arxiv.org/abs/2509.09947", "authors": ["Humza Ashraf", "Syed Muhammad Danish", "Zeeshan Sattar"], "title": "Toward Green Code: Prompting Small Language Models for Energy-Efficient Code Generation", "comment": null, "summary": "There is a growing concern about the environmental impact of large language\nmodels (LLMs) in software development, particularly due to their high energy\nuse and carbon footprint. Small Language Models (SLMs) offer a more sustainable\nalternative, requiring fewer computational resources while remaining effective\nfor fundamental programming tasks. In this study, we investigate whether prompt\nengineering can improve the energy efficiency of SLMs in code generation. We\nevaluate four open-source SLMs, StableCode-Instruct-3B,\nQwen2.5-Coder-3B-Instruct, CodeLlama-7B-Instruct, and Phi-3-Mini-4K-Instruct,\nacross 150 Python problems from LeetCode, evenly distributed into easy, medium,\nand hard categories. Each model is tested under four prompting strategies: role\nprompting, zero-shot, few-shot, and chain-of-thought (CoT). For every generated\nsolution, we measure runtime, memory usage, and energy consumption, comparing\nthe results with a human-written baseline. Our findings show that CoT prompting\nprovides consistent energy savings for Qwen2.5-Coder and StableCode-3B, while\nCodeLlama-7B and Phi-3-Mini-4K fail to outperform the baseline under any\nprompting strategy. These results highlight that the benefits of prompting are\nmodel-dependent and that carefully designed prompts can guide SLMs toward\ngreener software development.", "AI": {"tldr": "This paper finds that tailored prompting strategies like CoT can improve energy efficiency in small language models for coding tasks, but their benefits vary by model architecture.", "motivation": "Large language models (LLMs) contribute to high energy use and carbon emissions in software development. Small language models (SLMs) present a sustainable alternative, but their energy efficiency through prompting strategies remains understudied.", "method": "The paper evaluates four open-source SLMs (StableCode-Instruct-3B, Qwen2.5-Coder-3B-Instruct, CodeLlama-7B-Instruct, and Phi-3-Mini-4K-Instruct) on 150 Python problems across LeetCode's difficulty categories. Four prompting strategies (role, zero-shot, few-shot, CoT) are tested, measuring runtime, memory usage, and energy consumption compared to human-written solutions.", "result": "Chain-of-thought (CoT) prompting consistently reduced energy consumption for Qwen2.5-Coder and StableCode-3B across tasks, while CodeLlama-7B and Phi-3-Mini-4K underperformed all benchmarks. Prompt effectiveness is model-dependent.", "conclusion": "The study highlights that prompting strategies have varying effectiveness across SLMs, with CoT prompting showing energy benefits for specific models. This underscores the need for model-specific prompt design to reduce environmental impact in software development."}}
{"id": "2509.09787", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09787", "abs": "https://arxiv.org/abs/2509.09787", "authors": ["Nojan Sheybani", "Alessandro Pegoraro", "Jonathan Knauer", "Phillip Rieger", "Elissa Mollakuqe", "Farinaz Koushanfar", "Ahmad-Reza Sadeghi"], "title": "ZORRO: Zero-Knowledge Robustness and Privacy for Split Learning (Full Version)", "comment": "Full version of CCS 2025 paper", "summary": "Split Learning (SL) is a distributed learning approach that enables\nresource-constrained clients to collaboratively train deep neural networks\n(DNNs) by offloading most layers to a central server while keeping in- and\noutput layers on the client-side. This setup enables SL to leverage server\ncomputation capacities without sharing data, making it highly effective in\nresource-constrained environments dealing with sensitive data. However, the\ndistributed nature enables malicious clients to manipulate the training\nprocess. By sending poisoned intermediate gradients, they can inject backdoors\ninto the shared DNN. Existing defenses are limited by often focusing on\nserver-side protection and introducing additional overhead for the server. A\nsignificant challenge for client-side defenses is enforcing malicious clients\nto correctly execute the defense algorithm.\n  We present ZORRO, a private, verifiable, and robust SL defense scheme.\nThrough our novel design and application of interactive zero-knowledge proofs\n(ZKPs), clients prove their correct execution of a client-located defense\nalgorithm, resulting in proofs of computational integrity attesting to the\nbenign nature of locally trained DNN portions. Leveraging the frequency\nrepresentation of model partitions enables ZORRO to conduct an in-depth\ninspection of the locally trained models in an untrusted environment, ensuring\nthat each client forwards a benign checkpoint to its succeeding client. In our\nextensive evaluation, covering different model architectures as well as various\nattack strategies and data scenarios, we show ZORRO's effectiveness, as it\nreduces the attack success rate to less than 6\\% while causing even for models\nstoring \\numprint{1000000} parameters on the client-side an overhead of less\nthan 10 seconds.", "AI": {"tldr": "ZORRO is a secure Split Learning defense using zero-knowledge proofs to verify client-side computations, preventing backdoor attacks with minimal overhead and robust performance across complex models.", "motivation": "The motivation addresses the vulnerability of Split Learning (SL) to poisoning attacks by malicious clients who manipulate gradients to inject backdoors, as well as limitations in existing defenses that rely on server-side protection or introduce excessive server overhead.", "method": "ZORRO employs interactive zero-knowledge proofs (ZKPs) to verify client-side execution of defense algorithms and leverages frequency representation of model partitions to analyze local model components in untrusted environments.", "result": "Evaluation results show ZORRO reduces attack success rates to less than 6% across diverse models, attack strategies, and data scenarios, with an overhead of less than 10 seconds for models containing up to 1,000,000 client-side parameters.", "conclusion": "This paper concludes that ZORRO provides an effective and efficient solution for securing Split Learning against backdoor attacks through client-side verifiable defenses, achieving significant reduction in attack success rates with minimal computational overhead."}}
{"id": "2509.09975", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.09975", "abs": "https://arxiv.org/abs/2509.09975", "authors": ["Takasaburo Fukuda", "Takao Nakagawa", "Keisuke Miyazaki", "Susumu Tokumoto"], "title": "Development of Automated Software Design Document Review Methods Using Large Language Models", "comment": "SANER 2025", "summary": "In this study, we explored an approach to automate the review process of\nsoftware design documents by using LLM. We first analyzed the review methods of\ndesign documents and organized 11 review perspectives. Additionally, we\nanalyzed the issues of utilizing LLMs for these 11 review perspectives and\ndetermined which perspectives can be reviewed by current general-purpose LLMs\ninstead of humans. For the reviewable perspectives, we specifically developed\nnew techniques to enable LLMs to comprehend complex design documents that\ninclude table data. For evaluation, we conducted experiments using GPT to\nassess the consistency of design items and descriptions across different design\ndocuments in the design process used in actual business operations. Our results\nconfirmed that LLMs can be utilized to identify inconsistencies in software\ndesign documents during the review process.", "AI": {"tldr": "This paper proposes using LLMs to automate software design document reviews by analyzing 11 perspectives, developing document-comprehension techniques, and validating effectiveness via GPT-based experiments to detect inconsistencies.", "motivation": "Manual review of design documents is labor-intensive, prompting the need for automation via LLMs to improve efficiency and consistency in software development.", "method": "The researchers analyzed 11 review perspectives, identified applicable areas for LLMs, and developed techniques to handle complex design documents with table data. Experiments used GPT to assess consistency in real-world business documents.", "result": "Experiments confirmed that LLMs successfully detected inconsistencies in design documents, validating their utility in automating specific review tasks.", "conclusion": "The study demonstrates that LLMs can effectively identify inconsistencies in software design documents, reducing reliance on human reviewers for specific tasks."}}
{"id": "2509.09942", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.09942", "abs": "https://arxiv.org/abs/2509.09942", "authors": ["Lei Yu", "Jingyuan Zhang", "Xin Wang", "Jiajia Ma", "Li Yang", "Fengjun Zhang"], "title": "SmartCoder-R1: Towards Secure and Explainable Smart Contract Generation with Security-Aware Group Relative Policy Optimization", "comment": null, "summary": "Smart contracts automate the management of high-value assets, where\nvulnerabilities can lead to catastrophic financial losses. This challenge is\namplified in Large Language Models (LLMs) by two interconnected failures: they\noperate as unauditable \"black boxes\" lacking a transparent reasoning process,\nand consequently, generate code riddled with critical security vulnerabilities.\nTo address both issues, we propose SmartCoder-R1 (based on Qwen2.5-Coder-7B), a\nnovel framework for secure and explainable smart contract generation. It begins\nwith Continual Pre-training (CPT) to specialize the model. We then apply Long\nChain-of-Thought Supervised Fine-Tuning (L-CoT SFT) on 7,998 expert-validated\nreasoning-and-code samples to train the model to emulate human security\nanalysis. Finally, to directly mitigate vulnerabilities, we employ\nSecurity-Aware Group Relative Policy Optimization (S-GRPO), a reinforcement\nlearning phase that refines the generation policy by optimizing a weighted\nreward signal for compilation success, security compliance, and format\ncorrectness. Evaluated against 17 baselines on a benchmark of 756 real-world\nfunctions, SmartCoder-R1 establishes a new state of the art, achieving top\nperformance across five key metrics: a ComPass of 87.70%, a VulRate of 8.60%, a\nSafeAval of 80.16%, a FuncRate of 53.84%, and a FullRate of 50.53%. This\nFullRate marks a 45.79% relative improvement over the strongest baseline,\nDeepSeek-R1. Crucially, its generated reasoning also excels in human\nevaluations, achieving high-quality ratings for Functionality (82.7%), Security\n(85.3%), and Clarity (90.7%).", "AI": {"tldr": "This paper introduces SmartCoder-R1, a secure and explainable smart contract generation framework based on Qwen2.5-Coder-7B, using Continual Pre-training, Long Chain-of-Thought SFT, and Security-Aware Policy Optimization to achieve state-of-the-art performance.", "motivation": "Smart contracts are critical in managing high-value assets, so their security is paramount. LLMs deployed for this task and existing smart contract generation approaches lack both transparency in their reasoning (black box) and are prone to security vulnerabilities, leading to catastrophic financial loss.", "method": "SmartCoder-R1 is based on Qwen2.5-Coder-7B and consists of three components: (1) Continual Pre-training (CPT) to specialize the model, (2) Long Chain-of-Thought Supervised Fine-Tuning (L-CoT SFT) on a dataset of 7,998 expert-validated reasoning-and-code samples to train the model to emulate human security analysis, and (3) Security-Aware Group Relative Policy Optimization (S-GRPO) for reinforcement learning that refines the generation policy by optimizing a weighted reward signal for compilation success, security compliance, and format correctness.", "result": "SmartCoder-R1 achieves state-of-the-art performance across 5 metrics on a benchmark of 756 real-world functions: ComPass (87.70%), VulRate (8.60%), SafeAval (80.16%), FuncRate (53.84%), and FullRate (50.53%), with a 45.79% relative improvement in FullRate over DeepSeek-R1. Its generated reasoning is also rated highly in Functionality (82.7%), Security (85.3%), and Clarity (90.7%) in human assessments.", "conclusion": "SmartCoder-R1 is a novel and effective approach to secure and explainable smart contract generation. The model addresses the problems of lack of transparency and security vulnerabilities in existing LLM-based solutions through a series of training and optimization techniques, demonstrating superior performance both in metrics and human evaluations."}}
{"id": "2509.10085", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.10085", "abs": "https://arxiv.org/abs/2509.10085", "authors": ["Philipp Zech", "Irdin Pekaric"], "title": "Sustaining Research Software: A Fitness Function Approach", "comment": null, "summary": "The long-term sustainability of research software is a critical challenge, as\nit usually suffers from poor maintainability, lack of adaptability, and\neventual obsolescence. This paper proposes a novel approach to addressing this\nissue by leveraging the concept of fitness functions from evolutionary\narchitecture. Fitness functions are automated, continuously evaluated metrics\ndesigned to ensure that software systems meet desired non-functional,\narchitectural qualities over time. We define a set of fitness functions\ntailored to the unique requirements of research software, focusing on\nfindability, accessibility, interoperability and reusability (FAIR). These\nfitness functions act as proactive safeguards, promoting practices such as\nmodular design, comprehensive documentation, version control, and compatibility\nwith evolving technological ecosystems. By integrating these metrics into the\ndevelopment life cycle, we aim to foster a culture of sustainability within the\nresearch community. Case studies and experimental results demonstrate the\npotential of this approach to enhance the long-term FAIR of research software,\nbridging the gap between ephemeral project-based development and enduring\nscientific impact.", "AI": {"tldr": "This paper proposes FAIR-focused fitness functions to enhance the long-term sustainability of research software through proactive architectural guidance and experimental validation.", "motivation": "Research software faces sustainability challenges due to poor maintainability, adaptability issues, and rapid obsolescence, limiting its scientific impact over time.", "method": "The authors design domain-specific fitness functions that enforce FAIR principles (findability, accessibility, interoperability, reusability), integrating automated architectural metrics for modular design, documentation standards, version control, and tech stack compatibility throughout development.", "result": "Case studies and experiments demonstrate the approach improves long-term FAIRness metrics, bridging the gap between project-specific development and enduring scientific infrastructure.", "conclusion": "The fitness function framework effectively promotes sustainable research software practices by institutionalizing proactive quality assurance, offering a scalable model for maintaining software relevance in scientific discovery."}}
{"id": "2509.09950", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09950", "abs": "https://arxiv.org/abs/2509.09950", "authors": ["Pouneh Nikkhah Bahrami", "Dylan Cutler", "Igor Bilogrevic"], "title": "Byte by Byte: Unmasking Browser Fingerprinting at the Function Level Using V8 Bytecode Transformers", "comment": null, "summary": "Browser fingerprinting enables persistent cross-site user tracking via subtle\ntechniques that often evade conventional defenses or cause website breakage\nwhen script-level blocking countermeasures are applied. Addressing these\nchallenges requires detection methods offering both function-level precision to\nminimize breakage and inherent robustness against code obfuscation and URL\nmanipulation.\n  We introduce ByteDefender, the first system leveraging V8 engine bytecode to\ndetect fingerprinting operations specifically at the JavaScript function level.\nA Transformer-based classifier, trained offline on bytecode sequences,\naccurately identifies functions exhibiting fingerprinting behavior. We develop\nand evaluate light-weight signatures derived from this model to enable\nlow-overhead, on-device matching against function bytecode during compilation\nbut prior to execution, which only adds a 4% (average) latency to the page load\ntime. This mechanism facilitates targeted, real-time prevention of\nfingerprinting function execution, thereby preserving legitimate script\nfunctionality. Operating directly on bytecode ensures inherent resilience\nagainst common code obfuscation and URL-based evasion. Our evaluation on the\ntop 100k websites demonstrates high detection accuracy at both function- and\nscript-level, with substantial improvements over state-of-the-art AST-based\nmethods, particularly in robustness against obfuscation. ByteDefender offers a\npractical framework for effective, precise, and robust fingerprinting\nmitigation.", "AI": {"tldr": "ByteDefender detects browser fingerprinting via V8 bytecode analysis, preventing malicious tracking with minimal performance impact and robust obfuscation resistance.", "motivation": "Current defenses against browser fingerprinting either fail to detect evasion techniques or cause website breakage. Precise, function-level detection is needed to preserve legitimate scripts while resisting code obfuscation and URL manipulation.", "method": "ByteDefender uses V8 engine bytecode analysis with a Transformer-based classifier trained offline to identify fingerprinting functions, combined with lightweight on-device signatures applied during JavaScript compilation to block malicious functionality before execution.", "result": "ByteDefender achieves high function/script-level detection accuracy on 100k+ websites with 4% average latency, outperforming AST-based methods in obfuscation robustness while maintaining low overhead for real-time prevention.", "conclusion": "ByteDefender provides a practical framework for precise, robust fingerprinting mitigation by leveraging V8 bytecode analysis at the function level, overcoming limitations of existing methods through low overhead and obfuscation resilience."}}
{"id": "2509.10099", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10099", "abs": "https://arxiv.org/abs/2509.10099", "authors": ["Radu Apsan", "Vincenzo Stoico", "Michel Albonico", "Rudra Dhar", "Karthik Vaidhyanathan", "Ivano Malavolta"], "title": "Generating Energy-Efficient Code via Large-Language Models -- Where are we now?", "comment": null, "summary": "Context. The rise of Large Language Models (LLMs) has led to their widespread\nadoption in development pipelines. Goal. We empirically assess the energy\nefficiency of Python code generated by LLMs against human-written code and code\ndeveloped by a Green software expert. Method. We test 363 solutions to 9 coding\nproblems from the EvoEval benchmark using 6 widespread LLMs with 4 prompting\ntechniques, and comparing them to human-developed solutions. Energy consumption\nis measured on three different hardware platforms: a server, a PC, and a\nRaspberry Pi for a total of ~881h (36.7 days). Results. Human solutions are 16%\nmore energy-efficient on the server and 3% on the Raspberry Pi, while LLMs\noutperform human developers by 25% on the PC. Prompting does not consistently\nlead to energy savings, where the most energy-efficient prompts vary by\nhardware platform. The code developed by a Green software expert is\nconsistently more energy-efficient by at least 17% to 30% against all LLMs on\nall hardware platforms. Conclusions. Even though LLMs exhibit relatively good\ncode generation capabilities, no LLM-generated code was more energy-efficient\nthan that of an experienced Green software developer, suggesting that as of\ntoday there is still a great need of human expertise for developing\nenergy-efficient Python code.", "AI": {"tldr": "Green software experts consistently produce more energy-efficient Python code than LLMs across hardware platforms, emphasizing the irreplaceability of human expertise in energy-conscious software development.", "motivation": "The increasing adoption of Large Language Models (LLMs) in development pipelines necessitates an empirical assessment of their energy efficiency in code generation compared to human-written solutions and expert-optimized code.", "method": "The study evaluated 363 Python code solutions (9 problems from EvoEval benchmark) using 6 LLMs and 4 prompting techniques across server, PC, and Raspberry Pi hardware platforms over ~881h (36.7 days), comparing energy consumption with human-developed solutions and a Green software expert.", "result": "Human solutions outperformed LLMs by 16% (server) and 3% (Raspberry Pi); LLMs surpassed humans by 25% on PCs. Green software expert code was 17-30% more energy-efficient than all LLMs across all platforms. Prompting strategies showed inconsistent energy savings dependent on hardware.", "conclusion": "LLM-generated code does not surpass the energy efficiency of experienced Green software developers, highlighting the necessity of human expertise in creating energy-efficient Python solutions."}}
