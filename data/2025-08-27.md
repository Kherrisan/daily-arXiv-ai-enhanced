<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 16]
- [cs.SE](#cs.SE) [Total: 12]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [A Systematic Approach to Predict the Impact of Cybersecurity Vulnerabilities Using LLMs](https://arxiv.org/abs/2508.18439)
*Anders Mølmen Høst,Pierre Lison,Leon Moonen*

Main category: cs.CR

TL;DR: Researchers developed TRIAGE, a hybrid LLM system linking CVEs to ATT&CK techniques, demonstrating that combining rule-based and data-driven approaches improves mapping accuracy and efficiency in vulnerability impact analysis.


<details>
  <summary>Details</summary>
Motivation: Vulnerability databases lack actionable TTPs information, and manual CVE-ATT&CK mapping is impractical due to high vulnerability volumes. Automated solutions are urgently needed for efficient cybersecurity impact assessment.

Method: TRIAGE combines two LLM modules: 1) an LLM prompted with MITRE's methodology to generate initial technique mappings, and 2) an in-context learning module for data-driven inference. The hybrid approach merges rule-based reasoning with data-driven methods.

Result: Evaluation found: 1) in-context learning outperforms standalone methods, 2) hybrid approach improves exploitation technique recall, and 3) GPT-4o-mini surpasses Llama3.3-70B in performance. The system successfully automates predicting vulnerability impacts.

Conclusion: The paper concludes that TRIAGE, a hybrid LLM-based approach, effectively automates mapping CVEs to ATT&CK techniques, improving recall and efficiency in vulnerability impact analysis. LLMs demonstrate significant potential in this domain.

Abstract: Vulnerability databases, such as the National Vulnerability Database (NVD),
offer detailed descriptions of Common Vulnerabilities and Exposures (CVEs), but
often lack information on their real-world impact, such as the tactics,
techniques, and procedures (TTPs) that adversaries may use to exploit the
vulnerability. However, manually linking CVEs to their corresponding TTPs is a
challenging and time-consuming task, and the high volume of new vulnerabilities
published annually makes automated support desirable.
  This paper introduces TRIAGE, a two-pronged automated approach that uses
Large Language Models (LLMs) to map CVEs to relevant techniques from the ATT&CK
knowledge base. We first prompt an LLM with instructions based on MITRE's CVE
Mapping Methodology to predict an initial list of techniques. This list is then
combined with the results from a second LLM-based module that uses in-context
learning to map a CVE to relevant techniques. This hybrid approach
strategically combines rule-based reasoning with data-driven inference. Our
evaluation reveals that in-context learning outperforms the individual mapping
methods, and the hybrid approach improves recall of exploitation techniques. We
also find that GPT-4o-mini performs better than Llama3.3-70B on this task.
Overall, our results show that LLMs can be used to automatically predict the
impact of cybersecurity vulnerabilities and TRIAGE makes the process of mapping
CVEs to ATT&CK more efficient.
  Keywords: vulnerability impact, CVE, ATT&CK techniques, large language
models, automated mapping.

</details>


### [2] [Privacy-Preserving Federated Learning Framework for Risk-Based Adaptive Authentication](https://arxiv.org/abs/2508.18453)
*Yaser Baseri,Abdelhakim Senhaji Hafid,Dimitrios Makrakis,Hamidreza Fereidouni*

Main category: cs.CR

TL;DR: FL-RBA2 is a privacy-preserving Federated Learning framework that addresses Non-IID data challenges in Risk-Based Adaptive Authentication through similarity-driven aggregation, DP safeguards, and secure MACs, achieving robust decentralized authentication.


<details>
  <summary>Details</summary>
Motivation: Existing Federated Learning (FL) approaches for Risk-Based Adaptive Authentication (RBA) fail to handle non-IID user feature distributions, leading to biased global models, poor generalization, and security vulnerabilities. Decentralized systems require balancing privacy, scalability, and robust authentication.

Method: FL-RBA2 employs a similarity transformation to convert non-IID heterogeneous user features into IID similarity vectors, enabling unbiased global model aggregation. It integrates clustering-based cold-start mitigation, Differential Privacy (DP) for data protection, and Message Authentication Codes (MACs) for model integrity.

Result: FL-RBA2 demonstrates superior high-risk user detection accuracy and resilience against model inversion and inference attacks on real-world datasets under strong DP constraints. Game-based security proofs validate its privacy and adaptive security guarantees.

Conclusion: FL-RBA2 provides a robust solution for decentralized risk-based authentication by addressing Non-IID data challenges, ensuring privacy-preserving, scalable, and secure adaptive authentication through mathematical similarity transformations and cryptographic techniques.

Abstract: Balancing robust security with strong privacy guarantees is critical for
Risk-Based Adaptive Authentication (RBA), particularly in decentralized
settings. Federated Learning (FL) offers a promising solution by enabling
collaborative risk assessment without centralizing user data. However, existing
FL approaches struggle with Non-Independent and Identically Distributed
(Non-IID) user features, resulting in biased, unstable, and poorly generalized
global models. This paper introduces FL-RBA2, a novel Federated Learning
framework for Risk-Based Adaptive Authentication that addresses Non-IID
challenges through a mathematically grounded similarity transformation. By
converting heterogeneous user features (including behavioral, biometric,
contextual, interaction-based, and knowledge-based modalities) into IID
similarity vectors, FL-RBA2 supports unbiased aggregation and personalized risk
modeling across distributed clients. The framework mitigates cold-start
limitations via clustering-based risk labeling, incorporates Differential
Privacy (DP) to safeguard sensitive information, and employs Message
Authentication Codes (MACs) to ensure model integrity and authenticity.
Federated updates are securely aggregated into a global model, achieving strong
balance between user privacy, scalability, and adaptive authentication
robustness. Rigorous game-based security proofs in the Random Oracle Model
formally establish privacy, correctness, and adaptive security guarantees.
Extensive experiments on keystroke, mouse, and contextual datasets validate
FL-RBA2's effectiveness in high-risk user detection and its resilience to model
inversion and inference attacks, even under strong DP constraints.

</details>


### [3] [An 8- and 12-bit block AES cipher](https://arxiv.org/abs/2508.18485)
*Peter T. Breuer*

Main category: cs.CR

TL;DR: Authors created and shared a tiny 8/12-bit AES cipher in Java for educational/experimental use.


<details>
  <summary>Details</summary>
Motivation: To provide an accessible, minimal AES variant for research/experimental purposes due to rarity of such implementations.

Method: Documenting a reduced 8/12-bit block AES cipher with Java implementation.

Result: Published documentation and Java source code for 8/12-bit block size AES.

Conclusion: The creation of a small AES variant is feasible and useful for educational or constrained environments.

Abstract: Because it is so unusual, or hard to find, or expository, a truly tiny 8- or
12-bit block AES (Rijndael) cipher is documented here, along with Java source
code.

</details>


### [4] [Collaborative Intelligence: Topic Modelling of Large Language Model use in Live Cybersecurity Operations](https://arxiv.org/abs/2508.18488)
*Martin Lochner,Keegan Keplinger*

Main category: cs.CR

TL;DR: This paper analyzes how SOC personnel use LLMs in real-time security operations, finding they primarily leverage these tools to interpret complex text. The study advocates for collaborative LLM tool design to enhance SOC workflows.


<details>
  <summary>Details</summary>
Motivation: To understand how SOC specialists voluntarily integrate LLMs into live security operations, as human-automation collaboration is evolving with transformer-based models.

Method: Data was collected from 10 months of SOC operators using GPT-4 via an internally deployed chat application. Topic modeling was performed using the BERTopic model and a novel workflow to analyze LLM usage patterns.

Result: Both modeling approaches revealed SOC operators primarily used the LLM (~40% of usage) to comprehend complex text strings, such as interpreting commands and similar information during operations.

Conclusion: SOC operators can be supported and augmented by designing collaborative LLM tools tailored to their workflow. This includes creating next-generation tools like a right-click context menu for executing command line analysis LLM calls directly within the SOC environment, improving their ability to interpret complex information rapidly.

Abstract: Objective: This work describes the topic modelling of Security Operations
Centre (SOC) use of a large language model (LLM), during live security
operations. The goal is to better understand how these specialists voluntarily
use this tool.
  Background: Human-automation teams have been extensively studied, but
transformer-based language models have sparked a new wave of collaboration. SOC
personnel at a major cybersecurity provider used an LLM to support live
security operations. This study examines how these specialists incorporated the
LLM into their work.
  Method: Our data set is the result of 10 months of SOC operators accessing
GPT-4 over an internally deployed HTTP-based chat application. We performed two
topic modelling exercises, first using the established BERTopic model
(Grootendorst, 2022), and second, using a novel topic modeling workflow.
  Results: Both the BERTopic analysis and novel modelling approach revealed
that SOC operators primarily used the LLM to facilitate their understanding of
complex text strings. Variations on this use-case accounted for ~40% of SOC LLM
usage.
  Conclusion: SOC operators are required to rapidly interpret complex commands
and similar information. Their natural tendency to leverage LLMs to support
this activity indicates that their workflow can be supported and augmented by
designing collaborative LLM tools for use in the SOC.
  Application: This work can aid in creating next-generation tools for Security
Operations Centres. By understanding common use-cases, we can develop workflows
supporting SOC task flow. One example is a right-click context menu for
executing a command line analysis LLM call directly in the SOC environment.

</details>


### [5] [PRISM: Robust VLM Alignment with Principled Reasoning for Integrated Safety in Multimodality](https://arxiv.org/abs/2508.18649)
*Nanxi Li,Zhengyue Zhao,Chaowei Xiao*

Main category: cs.CR

TL;DR: PRISM is a vision-language model safety framework combining structured reasoning datasets and search-based optimization to achieve state-of-the-art attack resistance with minimal utility tradeoffs.


<details>
  <summary>Details</summary>
Motivation: Existing VLM safeguard methods either over-defend (reducing utility) or use shallow alignment that fails against complex threats. A principled approach for robust safety-aware reasoning is needed to address these limitations.

Method: PRISM employs a system2-like framework with two components: PRISM-CoT (a safety-aware reasoning dataset) and PRISM-DPO (Monte Carlo Tree Search-based Direct Preference Optimization). This structured approach aligns models through deep reasoning processes.

Result: PRISM achieves 0.15% attack success rate on JailbreakV-28K, 90% improvement over prior methods on VLBreak, and 8.70% success rate on the MIS benchmark. It maintains model utility while increasing adversarial computational costs.

Conclusion: PRISM effectively balances safety and utility in vision-language models, offering a robust defense against various attacks while maintaining or enhancing model performance.

Abstract: Safeguarding vision-language models (VLMs) is a critical challenge, as
existing methods often suffer from over-defense, which harms utility, or rely
on shallow alignment, failing to detect complex threats that require deep
reasoning. To this end, we introduce PRISM (Principled Reasoning for Integrated
Safety in Multimodality), a system2-like framework that aligns VLMs by
embedding a structured, safety-aware reasoning process. Our framework consists
of two key components: PRISM-CoT, a dataset that teaches safety-aware
chain-of-thought reasoning, and PRISM-DPO, generated via Monte Carlo Tree
Search (MCTS) to further refine this reasoning through Direct Preference
Optimization to help obtain a delicate safety boundary. Comprehensive
evaluations demonstrate PRISM's effectiveness, achieving remarkably low attack
success rates including 0.15% on JailbreakV-28K for Qwen2-VL and 90%
improvement over the previous best method on VLBreak for LLaVA-1.5. PRISM also
exhibits strong robustness against adaptive attacks, significantly increasing
computational costs for adversaries, and generalizes effectively to
out-of-distribution challenges, reducing attack success rates to just 8.70% on
the challenging multi-image MIS benchmark. Remarkably, this robust defense is
achieved while preserving, and in some cases enhancing, model utility. To
promote reproducibility, we have made our code, data, and model weights
available at https://github.com/SaFoLab-WISC/PRISM.

</details>


### [6] [UniC-RAG: Universal Knowledge Corruption Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2508.18652)
*Runpeng Geng,Yanting Wang,Ying Chen,Jinyuan Jia*

Main category: cs.CR

TL;DR: UniC-RAG introduces a universal, highly effective method for corrupting RAG systems by injecting adversarial texts, achieving >90% success rates against 2k+ diverse queries, while exposing flaws in current defense mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing attacks on RAG systems are limited to specific queries or topics. This work addresses the practical gap by proposing a universal attack capable of targeting a wide range of user queries for versatile malicious outcomes.

Method: UniC-RAG is formulated as an optimization problem, employing a balanced similarity-based clustering method to optimize a small set of adversarial texts. These texts are designed to simultaneously attack diverse user queries across multiple domains.

Result: UniC-RAG achieves over 90% attack success rate with 100 adversarial texts injected into databases with millions of texts, attacking up to 2,000 diverse queries. It significantly outperforms baselines and demonstrates existing defenses' ineffectiveness.

Conclusion: UniC-RAG demonstrates a highly effective universal knowledge corruption attack on RAG systems, achieving high success rates while exposing vulnerabilities in existing defenses. The findings emphasize the urgent need for developing robust defensive mechanisms.

Abstract: Retrieval-augmented generation (RAG) systems are widely deployed in
real-world applications in diverse domains such as finance, healthcare, and
cybersecurity. However, many studies showed that they are vulnerable to
knowledge corruption attacks, where an attacker can inject adversarial texts
into the knowledge database of a RAG system to induce the LLM to generate
attacker-desired outputs. Existing studies mainly focus on attacking specific
queries or queries with similar topics (or keywords). In this work, we propose
UniC-RAG, a universal knowledge corruption attack against RAG systems. Unlike
prior work, UniC-RAG jointly optimizes a small number of adversarial texts that
can simultaneously attack a large number of user queries with diverse topics
and domains, enabling an attacker to achieve various malicious objectives, such
as directing users to malicious websites, triggering harmful command execution,
or launching denial-of-service attacks. We formulate UniC-RAG as an
optimization problem and further design an effective solution to solve it,
including a balanced similarity-based clustering method to enhance the attack's
effectiveness. Our extensive evaluations demonstrate that UniC-RAG is highly
effective and significantly outperforms baselines. For instance, UniC-RAG could
achieve over 90% attack success rate by injecting 100 adversarial texts into a
knowledge database with millions of texts to simultaneously attack a large set
of user queries (e.g., 2,000). Additionally, we evaluate existing defenses and
show that they are insufficient to defend against UniC-RAG, highlighting the
need for new defense mechanisms in RAG systems.

</details>


### [7] [FALCON: Autonomous Cyber Threat Intelligence Mining with LLMs for IDS Rule Generation](https://arxiv.org/abs/2508.18684)
*Shaswata Mitra,Azim Bazarov,Martin Duclos,Sudip Mittal,Aritran Piplai,Md Rayhanur Rahman,Edward Zieglar,Shahram Rahimi*

Main category: cs.CR

TL;DR: FALCON is an autonomous LLM-powered IDS rule generator that reduces update delays and improves real-time threat detection using validated CTI data, achieving 95% accuracy and high analyst agreement.


<details>
  <summary>Details</summary>
Motivation: Evolving cyber threats require frequent IDS rule updates, which traditional CTI-driven methods delay due to manual processes, weakening security readiness.

Method: FALCON utilizes agentic systems powered by LLMs to autonomously generate and validate IDS rules from CTI data, targeting both network (Snort) and host-based (YARA) systems with multi-phased internal validation.

Result: FALCON achieved 95% accuracy in rule generation, with 84% inter-rater agreement among analysts, validating its effectiveness across network and host-based detection systems.

Conclusion: The study demonstrates that LLM-driven frameworks like FALCON can autonomously generate accurate IDS rules, improving real-time cybersecurity readiness and reducing reliance on manual updates.

Abstract: Signature-based Intrusion Detection Systems (IDS) detect malicious activities
by matching network or host activity against predefined rules. These rules are
derived from extensive Cyber Threat Intelligence (CTI), which includes attack
signatures and behavioral patterns obtained through automated tools and manual
threat analysis, such as sandboxing. The CTI is then transformed into
actionable rules for the IDS engine, enabling real-time detection and
prevention. However, the constant evolution of cyber threats necessitates
frequent rule updates, which delay deployment time and weaken overall security
readiness. Recent advancements in agentic systems powered by Large Language
Models (LLMs) offer the potential for autonomous IDS rule generation with
internal evaluation. We introduce FALCON, an autonomous agentic framework that
generates deployable IDS rules from CTI data in real-time and evaluates them
using built-in multi-phased validators. To demonstrate versatility, we target
both network (Snort) and host-based (YARA) mediums and construct a
comprehensive dataset of IDS rules with their corresponding CTIs. Our
evaluations indicate FALCON excels in automatic rule generation, with an
average of 95% accuracy validated by qualitative evaluation with 84%
inter-rater agreement among multiple cybersecurity analysts across all metrics.
These results underscore the feasibility and effectiveness of LLM-driven data
mining for real-time cyber threat mitigation.

</details>


### [8] [Immutable Digital Recognition via Blockchain](https://arxiv.org/abs/2508.18750)
*Zeng Zhang,Xiaoqi Li*

Main category: cs.CR

TL;DR: Hybrid blockchain system aligns decentralized/centralized models with policy for secure community-driven certification.


<details>
  <summary>Details</summary>
Motivation: To leverage blockchain's benefits while ensuring compliance with regulations and fostering community involvement.

Method: Combined decentralized management and centralized operation aligned with national policy using blockchain technology.

Result: A secure, legal, reliable, and dynamic electronic certification system was established.

Conclusion: The integration of decentralized and centralized models with blockchain creates a compliant, secure certification system.

Abstract: The process integrates the decentralised management and centralised operation
models, aligning them with the national policy directives. The developed
solution enables the full utilisation of blockchain technology's advantages
while also fostering community participation. Consequently, it establishes a
secure, legal, reliable, and dynamic electronic certification system.

</details>


### [9] [Hidden Tail: Adversarial Image Causing Stealthy Resource Consumption in Vision-Language Models](https://arxiv.org/abs/2508.18805)
*Rui Zhang,Zihan Wang,Tianli Yang,Hongwei Li,Wenbo Jiang,Qingchuan Zhao,Yang Liu,Guowen Xu*

Main category: cs.CR

TL;DR: Introduces 'Hidden Tail', a stealthy attack on Vision-Language Models that uses invisible special tokens to achieve 19.2× longer outputs without detectable content anomalies, revealing critical robustness gaps in VLM efficiency defenses.


<details>
  <summary>Details</summary>
Motivation: Existing resource consumption attacks on VLMs face a trade-off between increasing output length (effectiveness) and maintaining stealthiness due to irrelevant content in extended outputs. This limitation motivates the development of a more balanced attack approach.

Method: The authors propose 'Hidden Tail', a method that crafts prompt-agnostic adversarial images using a composite loss function with dynamic weighting. This function simultaneously preserves semantic content, induces repetitive invisible special tokens, and suppresses the end-of-sequence (EOS) token to maximize output length.

Result: Experiments show that 'Hidden Tail' achieves up to 19.2× output length extension, reaches maximum token limits, and maintains attack stealthiness without introducing noticeable abnormalities in the output.

Conclusion: This paper highlights the need to enhance the robustness of Vision-Language Models against stealthy resource consumption attacks by demonstrating a novel method that effectively balances effectiveness and stealthiness.

Abstract: Vision-Language Models (VLMs) are increasingly deployed in real-world
applications, but their high inference cost makes them vulnerable to resource
consumption attacks. Prior attacks attempt to extend VLM output sequences by
optimizing adversarial images, thereby increasing inference costs. However,
these extended outputs often introduce irrelevant abnormal content,
compromising attack stealthiness. This trade-off between effectiveness and
stealthiness poses a major limitation for existing attacks. To address this
challenge, we propose \textit{Hidden Tail}, a stealthy resource consumption
attack that crafts prompt-agnostic adversarial images, inducing VLMs to
generate maximum-length outputs by appending special tokens invisible to users.
Our method employs a composite loss function that balances semantic
preservation, repetitive special token induction, and suppression of the
end-of-sequence (EOS) token, optimized via a dynamic weighting strategy.
Extensive experiments show that \textit{Hidden Tail} outperforms existing
attacks, increasing output length by up to 19.2$\times$ and reaching the
maximum token limit, while preserving attack stealthiness. These results
highlight the urgent need to improve the robustness of VLMs against
efficiency-oriented adversarial threats. Our code is available at
https://github.com/zhangrui4041/Hidden_Tail.

</details>


### [10] [A Tight Context-aware Privacy Bound for Histogram Publication](https://arxiv.org/abs/2508.18832)
*Sara Saeidian,Ata Yavuzyılmaz,Leonhard Grosse,Georg Schuppe,Tobias J. Oechtering*

Main category: cs.CR

TL;DR: Using pointwise maximal leakage instead of differential privacy, the paper shows that incorporating data distribution assumptions allows stronger privacy guarantees for the Laplace mechanism under certain distributional conditions.


<details>
  <summary>Details</summary>
Motivation: Differential privacy is context-free and ignores data distribution, limiting its ability to refine privacy guarantees based on statistical properties of datasets.

Method: The authors analyze the Laplace mechanism for histogram release using pointwise maximal leakage (PML), contrasting it with differential privacy by incorporating data distribution assumptions into privacy analysis.

Result: When histogram bins have probabilities bounded from zero, the Laplace mechanism achieves stronger privacy under PML for equivalent noise levels compared to differential privacy.

Conclusion: The paper concludes that context-aware privacy measures like PML offer stronger privacy protection compared to context-free measures like differential privacy, given reasonable assumptions about data distributions.

Abstract: We analyze the privacy guarantees of the Laplace mechanism releasing the
histogram of a dataset through the lens of pointwise maximal leakage (PML).
While differential privacy is commonly used to quantify the privacy loss, it is
a context-free definition that does not depend on the data distribution. In
contrast, PML enables a more refined analysis by incorporating assumptions
about the data distribution. We show that when the probability of each
histogram bin is bounded away from zero, stronger privacy protection can be
achieved for a fixed level of noise. Our results demonstrate the advantage of
context-aware privacy measures and show that incorporating assumptions about
the data can improve privacy-utility tradeoffs.

</details>


### [11] [EnerSwap: Large-Scale, Privacy-First Automated Market Maker for V2G Energy Trading](https://arxiv.org/abs/2508.18942)
*Ahmed Mounsf Rafik Bendada,Yacine Ghamri-Doudane*

Main category: cs.CR

TL;DR: This paper addresses flaws in centralized EV electricity markets by proposing a blockchain-based decentralized exchange with privacy-preserving AMM and scalable sharding to prevent manipulation, protect user privacy, and handle market growth.


<details>
  <summary>Details</summary>
Motivation: Centralized EV electricity markets suffer from intermediary manipulation, privacy vulnerabilities (e.g., exposure of user locations and identities), and scalability limitations, necessitating a decentralized, secure alternative.

Method: The authors propose a decentralized blockchain-based exchange using a privacy-preserving Automated Market Maker (AMM) model and a scalable architecture with geographical dynamic sharding to ensure fair trading and efficient resource allocation.

Result: The solution enables open, fair, and tamper-proof trading while mitigating manipulation attacks and achieving scalability through dynamic sharding, making it suitable for growing EV networks.

Conclusion: The paper concludes that a blockchain-based decentralized exchange with a privacy-preserving AMM and geographical sharding effectively addresses market manipulation, privacy risks, and scalability challenges in EV-driven electricity markets.

Abstract: With the rapid growth of Electric Vehicle (EV) technology, EVs are destined
to shape the future of transportation. The large number of EVs facilitates the
development of the emerging vehicle-to-grid (V2G) technology, which realizes
bidirectional energy exchanges between EVs and the power grid. This has led to
the setting up of electricity markets that are usually confined to a small
geographical location, often with a small number of participants. Usually,
these markets are manipulated by intermediaries responsible for collecting bids
from prosumers, determining the market-clearing price, incorporating grid
constraints, and accounting for network losses. While centralized models can be
highly efficient, they grant excessive power to the intermediary by allowing
them to gain exclusive access to prosumers \textquotesingle price preferences.
This opens the door to potential market manipulation and raises significant
privacy concerns for users, such as the location of energy providers. This lack
of protection exposes users to potential risks, as untrustworthy servers and
malicious adversaries can exploit this information to infer trading activities
and real identities. This work proposes a secure, decentralized exchange market
built on blockchain technology, utilizing a privacy-preserving Automated Market
Maker (AMM) model to offer open and fair, and equal access to traders, and
mitigates the most common trading-manipulation attacks. Additionally, it
incorporates a scalable architecture based on geographical dynamic sharding,
allowing for efficient resource allocation and improved performance as the
market grows.

</details>


### [12] [An Efficient Lightweight Blockchain for Decentralized IoT](https://arxiv.org/abs/2508.19219)
*Faezeh Dehghan Tarzjani,Mostafa Salehi*

Main category: cs.CR

TL;DR: This paper proposes a lightweight blockchain solution for IoT using a Weight-Based-Selection consensus algorithm and virtualization to improve scalability and reduce energy consumption.


<details>
  <summary>Details</summary>
Motivation: The study addresses the limitations of traditional blockchain consensus algorithms (e.g., PoW, PoS) for resource-constrained IoT devices and the shortcomings of existing TBS-based PoA (e.g., high energy consumption, latency, and poor scalability).

Method: The authors propose a blockchain-based decentralized IoT architecture using (1) a novel Weight-Based-Selection (WBS) Proof-of-Authority (PoA) consensus algorithm for validator selection and (2) virtualization/clustering techniques to improve productivity and scalability. Simulations were conducted to evaluate performance.

Result: Simulation results demonstrate the WBS method reduces energy consumption and response time while increasing throughput compared to existing TBS approaches.

Conclusion: The paper concludes that the proposed WBS-based PoA and virtualization/clustering techniques effectively enhance blockchain efficiency and scalability for IoT, addressing the limitations of existing methods like TBS.

Abstract: The Internet of Things (IoT) is applied in various fields, and the number of
physical devices connected to the IoT is increasingly growing. There are
significant challenges to the IoT's growth and development, mainly due to the
centralized nature and large-scale IoT networks. The emphasis on the
decentralization of IoT's architecture can overcome challenges to IoT's
capabilities. A promising decentralized platform for IoT is blockchain. Owing
to IoT devices' limited resources, traditional consensus algorithms such as PoW
and PoS in the blockchain are computationally expensive. Therefore, the PoA
consensus algorithm is proposed in the blockchain consensus network for IoT.
The PoA selects the validator as Turn-based selection (TBS) that needs
optimization and faces system reliability, energy consumption, latency, and low
scalability. We propose an efficient, lightweight blockchain for decentralizing
IoT architecture by using virtualization and clustering to increase
productivity and scalability to address these issues. We also introduce a novel
PoA based on the Weight-Based-Selection (WBS) method for validators to validate
transactions and add them to the blockchain. By simulation, we evaluated the
performance of our proposed WBS method as opposed to TBS. The results show
reduced energy consumption, and response time, and increased throughput.

</details>


### [13] [LLMs in the SOC: An Empirical Study of Human-AI Collaboration in Security Operations Centres](https://arxiv.org/abs/2508.18947)
*Ronal Singh,Shahroz Tariq,Fatemeh Jalalvand,Mohan Baruwal Chhetri,Surya Nepal,Cecile Paris,Martin Lochner*

Main category: cs.CR

TL;DR: SOC analysts use LLMs as on-demand cognitive aids for task efficiency and context-building, preserving human decision-making while highlighting design opportunities for human-centric AI tools.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored real-world application of LLMs in SOCs and understand their practical role in analyst workflows.

Method: Longitudinal analysis of 3,090 queries from 45 SOC analysts over 10 months, examining query patterns, NICE framework alignment, and usage trends.

Result: 93% of LLM queries align with NICE cybersecurity competencies; usage shifts from exploration to routine integration, focusing on telemetry interpretation and technical communication refinement.

Conclusion: LLMs serve as augmentative cognitive aids in SOCs, maintaining analyst decision authority while offering actionable design insights for human-AI collaboration.

Abstract: The integration of Large Language Models (LLMs) into Security Operations
Centres (SOCs) presents a transformative, yet still evolving, opportunity to
reduce analyst workload through human-AI collaboration. However, their
real-world application in SOCs remains underexplored. To address this gap, we
present a longitudinal study of 3,090 analyst queries from 45 SOC analysts over
10 months. Our analysis reveals that analysts use LLMs as on-demand aids for
sensemaking and context-building, rather than for making high-stakes
determinations, preserving analyst decision authority. The majority of queries
are related to interpreting low-level telemetry (e.g., commands) and refining
technical communication through short (1-3 turn) interactions. Notably, 93% of
queries align with established cybersecurity competencies (NICE Framework),
underscoring the relevance of LLM use for SOC-related tasks. Despite variations
in tasks and engagement, usage trends indicate a shift from occasional
exploration to routine integration, with growing adoption and sustained use
among a subset of analysts. We find that LLMs function as flexible, on-demand
cognitive aids that augment, rather than replace, SOC expertise. Our study
provides actionable guidance for designing context-aware, human-centred AI
assistance in security operations, highlighting the need for further
in-the-wild research on real-world analyst-LLM collaboration, challenges, and
impacts.

</details>


### [14] [The Double-edged Sword of LLM-based Data Reconstruction: Understanding and Mitigating Contextual Vulnerability in Word-level Differential Privacy Text Sanitization](https://arxiv.org/abs/2508.18976)
*Stephen Meisenbacher,Alexandra Klymenko,Andreea-Elena Bodea,Florian Matthes*

Main category: cs.CR

TL;DR: This research demonstrates that Large Language Models can both weaken and strengthen differentially private text sanitization, proposing adversarial use of LLMs for improved privacy protections.


<details>
  <summary>Details</summary>
Motivation: Current word-level DP text sanitization methods suffer from 'contextual vulnerabilities' that leave exploitable clues in the sanitized output. This work investigates how LLMs' contextual inference capabilities can both threat and enhance DP protections.

Method: The paper employs advanced Large Language Models (LLMs) to conduct data reconstruction attacks on DP-sanitized texts. It evaluates these attacks across diverse sanitization mechanisms and privacy levels, extending prior work by incorporating more sophisticated LLMs and broader testing scenarios.

Result: Experiments reveal a dual impact: LLM-based attacks degrade empirical privacy protections by reconstructing semantics but simultaneously enable improved sanitization outcomes when leveraged for post-processing. LLMs demonstrate both destructive and constructive potential for differential privacy in text data.

Conclusion: The study concludes that while LLMs can exploit contextual vulnerabilities in DP-sanitized texts, they can also be harnessed to improve both privacy protections and text utility through adversarial post-processing techniques.

Abstract: Differentially private text sanitization refers to the process of privatizing
texts under the framework of Differential Privacy (DP), providing provable
privacy guarantees while also empirically defending against adversaries seeking
to harm privacy. Despite their simplicity, DP text sanitization methods
operating at the word level exhibit a number of shortcomings, among them the
tendency to leave contextual clues from the original texts due to randomization
during sanitization $\unicode{x2013}$ this we refer to as $\textit{contextual
vulnerability}$. Given the powerful contextual understanding and inference
capabilities of Large Language Models (LLMs), we explore to what extent LLMs
can be leveraged to exploit the contextual vulnerability of DP-sanitized texts.
We expand on previous work not only in the use of advanced LLMs, but also in
testing a broader range of sanitization mechanisms at various privacy levels.
Our experiments uncover a double-edged sword effect of LLM-based data
reconstruction attacks on privacy and utility: while LLMs can indeed infer
original semantics and sometimes degrade empirical privacy protections, they
can also be used for good, to improve the quality and privacy of DP-sanitized
texts. Based on our findings, we propose recommendations for using LLM data
reconstruction as a post-processing step, serving to increase privacy
protection by thinking adversarially.

</details>


### [15] [Attackers Strike Back? Not Anymore -- An Ensemble of RL Defenders Awakens for APT Detection](https://arxiv.org/abs/2508.19072)
*Sidahmed Benabderrahmane,Talal Rahwan*

Main category: cs.CR

TL;DR: A novel APT detection system uses RL and active learning to adapt to evolving threats, overcoming static detection limitations. Auto-encoders and ensemble methods enhance accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing APT detection systems are static and fail to adapt to evolving attacks. Signature-based methods are inadequate for stealthy, adaptive threats, necessitating a dynamic, learning-based approach.

Method: The framework uses auto-encoders for latent behavior encoding, multi-agent RL defenders (Q-Learning, PPO, DQN, adversarial agents), and active learning loops for decision refinement. Ensemble voting, weighted by agent performance, ensures final predictions.

Result: The system effectively handles evolving APT strategies through adaptive agent training and active learning, while the ensemble mechanism improves decision robustness and accuracy.

Conclusion: The paper presents an adaptive framework combining deep learning, reinforcement learning, and active learning to improve APT detection, addressing the limitations of static systems and enhancing robustness through ensemble methods.

Abstract: Advanced Persistent Threats (APTs) represent a growing menace to modern
digital infrastructure. Unlike traditional cyberattacks, APTs are stealthy,
adaptive, and long-lasting, often bypassing signature-based detection systems.
This paper introduces a novel framework for APT detection that unites deep
learning, reinforcement learning (RL), and active learning into a cohesive,
adaptive defense system. Our system combines auto-encoders for latent
behavioral encoding with a multi-agent ensemble of RL-based defenders, each
trained to distinguish between benign and malicious process behaviors. We
identify a critical challenge in existing detection systems: their static
nature and inability to adapt to evolving attack strategies. To this end, our
architecture includes multiple RL agents (Q-Learning, PPO, DQN, adversarial
defenders), each analyzing latent vectors generated by an auto-encoder. When
any agent is uncertain about its decision, the system triggers an active
learning loop to simulate expert feedback, thus refining decision boundaries.
An ensemble voting mechanism, weighted by each agent's performance, ensures
robust final predictions.

</details>


### [16] [SecureV2X: An Efficient and Privacy-Preserving System for Vehicle-to-Everything (V2X) Applications](https://arxiv.org/abs/2508.19115)
*Joshua Lee,Ali Arastehfard,Weiran Liu,Xuegang Ban,Yuan Hong*

Main category: cs.CR

TL;DR: SecureV2X is a privacy-preserving multi-agent framework for V2X systems that enables fast, secure neural inference for drowsiness and red-light violation detection with 10-100× efficiency gains over existing solutions.


<details>
  <summary>Details</summary>
Motivation: The proliferation of machine learning in vehicular networks introduces critical privacy risks, particularly for safety-critical applications that handle sensitive data like location information and brainwave signals (EEG). Existing security solutions lack scalability and efficiency for real-time V2X operations.

Method: We propose SecureV2X, a multi-agent system architecture that performs secure neural network inferences between servers and vehicles. The approach focuses on two applications: secure drowsiness detection using EEG signals and secure red-light violation detection through object identification, implementing privacy-preserving computation protocols.

Result: SecureV2X demonstrates 9.4× speed improvement and 16.6× lower communication costs for drowsiness detection compared to existing secure systems. For red-light violation detection, it achieves nearly 100× faster runtime than state-of-the-art methods, with 143× fewer computational rounds while maintaining accuracy.

Conclusion: SecureV2X effectively addresses privacy challenges in V2X systems by providing a scalable, efficient framework for secure neural network inferences, achieving significant performance gains while maintaining data privacy in safety-critical applications.

Abstract: Autonomous driving and V2X technologies have developed rapidly in the past
decade, leading to improved safety and efficiency in modern transportation.
These systems interact with extensive networks of vehicles, roadside
infrastructure, and cloud resources to support their machine learning
capabilities. However, the widespread use of machine learning in V2X systems
raises issues over the privacy of the data involved. This is particularly
concerning for smart-transit and driver safety applications which can
implicitly reveal user locations or explicitly disclose medical data such as
EEG signals. To resolve these issues, we propose SecureV2X, a scalable,
multi-agent system for secure neural network inferences deployed between the
server and each vehicle. Under this setting, we study two multi-agent V2X
applications: secure drowsiness detection, and secure red-light violation
detection. Our system achieves strong performance relative to baselines, and
scales efficiently to support a large number of secure computation interactions
simultaneously. For instance, SecureV2X is $9.4 \times$ faster, requires
$143\times$ fewer computational rounds, and involves $16.6\times$ less
communication on drowsiness detection compared to other secure systems.
Moreover, it achieves a runtime nearly $100\times$ faster than state-of-the-art
benchmarks in object detection tasks for red light violation detection.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [17] [Training Language Model Agents to Find Vulnerabilities with CTF-Dojo](https://arxiv.org/abs/2508.18370)
*Terry Yue Zhuo,Dingmin Wang,Hantian Ding,Varun Kumar,Zijian Wang*

Main category: cs.SE

TL;DR: CTF-Dojo & CTF-Forge boost LLM training via verifiable execution environments, achieving SOTA results without proprietary systems.


<details>
  <summary>Details</summary>
Motivation: Scalable, generalizable execution-grounded environments are scarce, hindering progress in training capable ML agents.

Method: The paper introduces CTF-Dojo, a large-scale executable runtime with 658 CTF-style challenges containerized in Docker, and CTF-Forge, an automated pipeline for creating environments from public artifacts.

Result: Models trained on CTF-Dojo achieved up to 11.6% absolute gains over baselines, with the 32B model reaching 31.9% Pass@1, setting a new open-weight state-of-the-art.

Conclusion: CTF-Dojo demonstrates that execution-grounded training with verifiable feedback is pivotal for advancing high-performance ML agents, enabling competitive results against frontier models without relying on proprietary systems.

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities when
trained within executable runtime environments, notably excelling at software
engineering tasks through verified feedback loops. Yet, scalable and
generalizable execution-grounded environments remain scarce, limiting progress
in training more capable ML agents. We introduce CTF-Dojo, the first
large-scale executable runtime tailored for training LLMs with verifiable
feedback, featuring 658 fully functional Capture-The-Flag (CTF)-style
challenges containerized in Docker with guaranteed reproducibility. To enable
rapid scaling without manual intervention, we develop CTF-Forge, an automated
pipeline that transforms publicly available artifacts into ready-to-use
execution environments in minutes, eliminating weeks of expert configuration
traditionally required. We trained LLM-based agents on just 486 high-quality,
execution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute
gains over strong baselines across three competitive benchmarks: InterCode-CTF,
NYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1,
establishing a new open-weight state-of-the-art that rivals frontier models
like DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a
benchmark for executable-agent learning, CTF-Dojo demonstrates that
execution-grounded training signals are not only effective but pivotal in
advancing high-performance ML agents without dependence on costly proprietary
systems.

</details>


### [18] [DTInsight: A Tool for Explicit, Interactive, and Continuous Digital Twin Reporting](https://arxiv.org/abs/2508.18431)
*Kérian Fiter,Louis Malassigné-Onfroy,Bentley Oakes*

Main category: cs.SE

TL;DR: DTInsight is a CI/CD-integrated tool that automates Digital Twin reporting with architecture visualizations and ontology-based summaries to improve stakeholder understanding over time.


<details>
  <summary>Details</summary>
Motivation: Stakeholders need tools to dynamically understand evolving Digital Twins' architectures and characteristics in real-time for informed decision-making.

Method: The authors developed DTInsight, a systematic methodology and tool that offers (a) interactive conceptual architecture visualization, (b) ontology-based DT characteristic summaries, and (c) CI/CD pipeline integration for continuous reporting.

Result: DTInsight generates up-to-date reports for Digital Twins modeled with their DTDF framework, enabling enhanced stakeholder insights through automated visualization and ontological data processing.

Conclusion: DTInsight enhances stakeholder understanding of Digital Twins by providing automated, continuous reporting through visualization, ontological summaries, and CI/CD pipeline integration.

Abstract: With Digital Twin (DT) construction and evolution occurring over time,
stakeholders require tools to understand the current characteristics and
conceptual architecture of the system at any time. We introduce DTInsight, a
systematic and automated tool and methodology for producing continuous
reporting for DTs. DTInsight offers three key features: (a) an interactive
conceptual architecture visualization of DTs; (b) generation of summaries of DT
characteristics based on ontological data; and (c) integration of these outputs
into a reporting page within a continuous integration and continuous deployment
(CI/CD) pipeline. Given a modeled description of the DT aligning to our DT
Description Framework (DTDF), DTInsight enables up-to-date and detailed reports
for enhanced stakeholder understanding.

</details>


### [19] [Engineering a Digital Twin for the Monitoring and Control of Beer Fermentation Sampling](https://arxiv.org/abs/2508.18452)
*Pierre-Emmanuel Goffi,Raphaël Tremblay,Bentley Oakes*

Main category: cs.SE

TL;DR: This paper presents a safety-critical beer fermentation DT reducing manual sampling by 91%. A three-phase methodology enables bidirectional control via Arduino-Unity integration and multi-layered safety protocols, offering practical guidance for industrial DT implementation.


<details>
  <summary>Details</summary>
Motivation: Engineering bidirectional industrial DTs beyond passive monitoring poses significant complexity, particularly for safety-critical systems requiring real-time control. Traditional approaches lack systematic methodologies for such implementations.

Method: A three-phase engineering approach transformed passive monitoring into interactive Type 2 DTs, combining multi-layered safety protocols, Arduino-Unity integration, and real-time synchronization solutions. The constellation reporting framework enabled interdisciplinary collaboration.

Result: Achieved 91% reduction in manual sampling time through continual DT monitoring of beer fermentation. Successfully implemented real-time control for pressurized systems at seven bar, demonstrating viable bidirectional DT capabilities in industrial environments.

Conclusion: The study emphasizes the importance of safety-first design, simulation-driven development, and progressive implementation strategies for bidirectional Type 2 DTs in safety-critical industrial applications. It provides actionable guidance for achieving real-time control and cross-domain collaboration.

Abstract: Successfully engineering interactive industrial DTs is a complex task,
especially when implementing services beyond passive monitoring. We present
here an experience report on engineering a safety-critical digital twin (DT)
for beer fermentation monitoring, which provides continual sampling and reduces
manual sampling time by 91%. We document our systematic methodology and
practical solutions for implementing bidirectional DTs in industrial
environments. This includes our three-phase engineering approach that
transforms a passive monitoring system into an interactive Type 2 DT with
real-time control capabilities for pressurized systems operating at seven bar.
We contribute details of multi-layered safety protocols, hardware-software
integration strategies across Arduino controllers and Unity visualization, and
real-time synchronization solutions. We document specific engineering
challenges and solutions spanning interdisciplinary integration, demonstrating
how our use of the constellation reporting framework facilitates cross-domain
collaboration. Key findings include the critical importance of safety-first
design, simulation-driven development, and progressive implementation
strategies. Our work thus provides actionable guidance for practitioners
developing DTs requiring bidirectional control in safety-critical applications.

</details>


### [20] [How do Humans and LLMs Process Confusing Code?](https://arxiv.org/abs/2508.18547)
*Youssef Abdelsalam,Norman Peitek,Anna-Maria Maurer,Mariya Toneva,Sven Apel*

Main category: cs.SE

TL;DR: LLMs and programmers share similar confusion in code comprehension; this insight enables identifying confusing code for humans via LLM analysis.


<details>
  <summary>Details</summary>
Motivation: The paper investigates whether humans and LLMs are confused by similar code to improve LLM integration into programming workflows and guide LLM improvements, addressing potential misunderstandings and code quality issues.

Method: An empirical study comparing LLM perplexity (for LLM comprehension) and human neurophysiological responses (via EEG-based fixation-related potentials) was conducted on clean and confusing code examples.

Result: LLM perplexity spikes correlated with human neurophysiological confusion signals in terms of location and amplitude. This correlation led to a data-driven approach to identify confusing code regions for human programmers using LLM outputs.

Conclusion: The study concludes that LLMs and human programmers are similarly confused by the same code regions, aligning their comprehension patterns. This alignment suggests opportunities to improve LLM integration in software engineering workflows and refine LLM capabilities.

Abstract: Already today, humans and programming assistants based on large language
models (LLMs) collaborate in everyday programming tasks. Clearly, a
misalignment between how LLMs and programmers comprehend code can lead to
misunderstandings, inefficiencies, low code quality, and bugs.
  A key question in this space is whether humans and LLMs are confused by the
same kind of code. This would not only guide our choices of integrating LLMs in
software engineering workflows, but also inform about possible improvements of
LLMs.
  To this end, we conducted an empirical study comparing an LLM to human
programmers comprehending clean and confusing code. We operationalized
comprehension for the LLM by using LLM perplexity, and for human programmers
using neurophysiological responses (in particular, EEG-based fixation-related
potentials).
  We found that LLM perplexity spikes correlate both in terms of location and
amplitude with human neurophysiological responses that indicate confusion. This
result suggests that LLMs and humans are similarly confused about the code.
Based on these findings, we devised a data-driven, LLM-based approach to
identify regions of confusion in code that elicit confusion in human
programmers.

</details>


### [21] [LaQual: A Novel Framework for Automated Evaluation of LLM App Quality](https://arxiv.org/abs/2508.18636)
*Yan Wang,Xinyi Hou,Yanjie Zhao,Weiguo Lin,Haoyu Wang,Junjun Si*

Main category: cs.SE

TL;DR: LaQual improves LLM app discovery by filtering low-quality apps and providing dynamic scenario-aware evaluations, validated through strong correlation with human judgment and superior user performance metrics.


<details>
  <summary>Details</summary>
Motivation: Current LLM app store ranking methods rely on static metrics like user activity, making it difficult for users to efficiently discover high-quality apps. This creates a need for an automated, objective, and scalable framework to address these limitations.

Method: The framework employs three stages: hierarchical labeling/classification of LLM apps, static filtering using time-weighted engagement and functional metrics, and dynamic scenario-adaptive evaluation where an LLM generates scenario-specific metrics and tasks for quality assessment.

Result: LaQual achieves Spearman's rho correlation of 0.62-0.60 with human judgments in specific scenarios, reduces the candidate app pool by 66.7%-81.3%, and outperforms baselines in user studies (decision confidence: 5.45 vs 3.30; evaluation report value: 4.75 vs 2.25).

Conclusion: LaQual demonstrates a scalable, objective, and user-centered solution for evaluating and recommending high-quality LLM apps by combining hierarchical classification, static filtering, and dynamic scenario-adaptive evaluation. It effectively reduces suboptimal app selection and outperforms baselines in user confidence and efficiency.

Abstract: LLM app stores are quickly emerging as platforms that gather a wide range of
intelligent applications based on LLMs, giving users many choices for content
creation, coding support, education, and more. However, the current methods for
ranking and recommending apps in these stores mostly rely on static metrics
like user activity and favorites, which makes it hard for users to efficiently
find high-quality apps. To address these challenges, we propose LaQual, an
automated framework for evaluating the quality of LLM apps. LaQual consists of
three main stages: first, it labels and classifies LLM apps in a hierarchical
way to accurately match them to different scenarios; second, it uses static
indicators, such as time-weighted user engagement and functional capability
metrics, to filter out low-quality apps; and third, it conducts a dynamic,
scenario-adaptive evaluation, where the LLM itself generates scenario-specific
evaluation metrics, scoring rules, and tasks for a thorough quality assessment.
Experiments on a popular LLM app store show that LaQual is effective. Its
automated scores are highly consistent with human judgments (with Spearman's
rho of 0.62 and p=0.006 in legal consulting, and rho of 0.60 and p=0.009 in
travel planning). By effectively screening, LaQual can reduce the pool of
candidate LLM apps by 66.7% to 81.3%. User studies further confirm that LaQual
significantly outperforms baseline systems in decision confidence, comparison
efficiency (with average scores of 5.45 compared to 3.30), and the perceived
value of its evaluation reports (4.75 versus 2.25). Overall, these results
demonstrate that LaQual offers a scalable, objective, and user-centered
solution for finding and recommending high-quality LLM apps in real-world use
cases.

</details>


### [22] [Requirements Development and Formalization for Reliable Code Generation: A Multi-Agent Vision](https://arxiv.org/abs/2508.18675)
*Xu Lu,Weisong Sun,Yiran Zhang,Ming Hu,Cong Tian,Zhi Jin,Yang Liu*

Main category: cs.SE

TL;DR: ReDeFo introduces a multi-agent code generation framework that uses formal specifications to transform natural language requirements into reliable code, achieving quality assurance through systematic formal verification.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based code generation methods lack systematic requirements validation and quality guarantees. While LLM agents excel at alignment, there remains a critical gap in ensuring generated code satisfies practical requirements through formal verification.

Method: The framework incorporates three agents integrated with formal methods techniques into a requirements-to-code pipeline. It utilizes formal specifications as a bridge between natural language requirements and executable code, enabling correctness reasoning, bug detection, and property enforcement.

Result: ReDeFo enables rigorous reasoning about correctness proofs (including hidden bug detection), systematic property enforcement during code generation, and establishes a formal bridge between natural language requirements and executable specifications.

Conclusion: This paper proposes ReDeFo, the first multi-agent framework for reliable code generation based on formal requirements development and specification, offering a promising step toward achieving trustworthy auto-generated software through rigorous quality assurance. 

Abstract: Automated code generation has long been considered the holy grail of software
engineering. The emergence of Large Language Models (LLMs) has catalyzed a
revolutionary breakthrough in this area. However, existing methods that only
rely on LLMs remain inadequate in the quality of generated code, offering no
guarantees of satisfying practical requirements. They lack a systematic
strategy for requirements development and modeling. Recently, LLM-based agents
typically possess powerful abilities and play an essential role in facilitating
the alignment of LLM outputs with user requirements. In this paper, we envision
the first multi-agent framework for reliable code generation based on
\textsc{re}quirements \textsc{de}velopment and \textsc{fo}rmalization, named
\textsc{ReDeFo}. This framework incorporates three agents, highlighting their
augmentation with knowledge and techniques of formal methods, into the
requirements-to-code generation pipeline to strengthen quality assurance. The
core of \textsc{ReDeFo} is the use of formal specifications to bridge the gap
between potentially ambiguous natural language requirements and precise
executable code. \textsc{ReDeFo} enables rigorous reasoning about correctness,
uncovering hidden bugs, and enforcing critical properties throughout the
development process. In general, our framework aims to take a promising step
toward realizing the long-standing vision of reliable, auto-generated software.

</details>


### [23] [LLM as an Execution Estimator: Recovering Missing Dependency for Practical Time-travelling Debugging](https://arxiv.org/abs/2508.18721)
*Yunrui Pei,Hongshu Wang,Wenjie Zhang,Yun Lin,Weiyu Kong,Jin song Dong*

Main category: cs.SE

TL;DR: RecovSlicing uses LLMs and partial traces for fast dynamic data dependency analysis, outperforming existing tools and enabling better debugging with 16% more regressions detected.


<details>
  <summary>Details</summary>
Motivation: Traditional dependency analysis methods either require exhaustive instrumentation (costly for library variables) or program replication (infeasible for non-deterministic programs). This creates a need for efficient, accurate dynamic analysis without these limitations.

Method: RecovSlicing combines partial instrumentation with LLM-based inference of program behavior from traces and code context. It tackles runtime value recovery and variable alignment in memory to estimate dynamic definitions while supporting implicit variables like collection accesses.

Result: On 8,300 dependencies, RecovSlicing achieves 80.3%-98.3% accuracy vs. 39.0%-87.1% for best baselines and 91.1%-98.3% recall vs. 53.4%-87.1%. Integration into a bug localizer found 16% more regressions.

Conclusion: RecovSlicing effectively computes dynamic data dependencies in a single run with partial instrumentation, leveraging LLMs to infer program behavior from partial traces. It outperforms existing methods in accuracy and recall, addressing non-deterministic programs and library variables where traditional approaches fail.

Abstract: Dynamic data dependency, answering "why a variable has this value?", is
critical for debugging. Given a program step `s` reading a variable `v`,
finding the dynamic definition of `v` is challenging. Traditional methods
require either (1) exhaustive instrumentation of all possible definitions of
`v` in one run or (2) replicating the run to re-examine reads/writes - both
costly. If `v` is defined in a library, instrumentation becomes expensive; for
non-deterministic programs, replication is infeasible.
  We propose RecovSlicing, which computes dynamic data dependency in a single
run with partial instrumentation. We leverage LLMs to infer program behavior
from a partially recorded trace and code context. Given a trace and a slicing
criterion (step `s` and variable `v`), RecovSlicing estimates the runtime
definition of `v` by recovering the missing execution.It also supports implicit
variables, such as those in `list.get(i)`. Technically, RecovSlicing tackles:
(1) recovering runtime values and structures, and (2) aligning recovered
variables with recorded memory to analyze definitions.
  We evaluate RecovSlicing on 8,300 data dependencies across three slicing
benchmarks, comparing it with Slicer4J, ND-Slicer, LLM Slicer, and re-execution
Slicer. RecovSlicing achieves accuracy of 80.3%, 91.1%, and 98.3%,
outperforming the best baseline (39.0%, 82.0%, 59.9%), and also leads in recall
(91.1%, 91.1%, 98.3% vs. 53.4%, 79.1%, 87.1%). Integrated into a regression bug
localizer, it enables finding 16% more regressions.

</details>


### [24] [Does AI Code Review Lead to Code Changes? A Case Study of GitHub Actions](https://arxiv.org/abs/2508.18771)
*Kexin Sun,Hongyu Kuang,Sebastian Baltes,Xin Zhou,He Zhang,Xiaoxing Ma,Guoping Rong,Dong Shao,Christoph Treude*

Main category: cs.SE

TL;DR: This study empirically evaluates 16 AI code review tools, finding that concise, manually triggered comments with code snippets most effectively drive code changes, underscoring the need for improved tool design.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of understanding about the actual impact of AI-based code review tools despite their growing adoption, aiming to investigate their effectiveness and influencing factors.

Method: The research uses a two-stage LLM-assisted framework to determine comment addressing and interpretable machine learning to identify effectiveness factors, analyzing over 22,000 review comments across 178 repositories.

Result: Key results show that concise, code-snippet containing, and manually triggered comments (especially from hunk-level tools) are more likely to result in code changes, indicating adoption is growing but effectiveness varies.

Conclusion: The study highlights the importance of careful tool design and suggests directions for improving AI-based code review systems, emphasizing that effectiveness varies widely and factors like concise comments and manual triggering enhance impact.

Abstract: AI-based code review tools automatically review and comment on pull requests
to improve code quality. Despite their growing presence, little is known about
their actual impact. We present a large-scale empirical study of 16 popular
AI-based code review actions for GitHub workflows, analyzing more than 22,000
review comments in 178 repositories. We investigate (1) how these tools are
adopted and configured, (2) whether their comments lead to code changes, and
(3) which factors influence their effectiveness. We develop a two-stage
LLM-assisted framework to determine whether review comments are addressed, and
use interpretable machine learning to identify influencing factors. Our
findings show that, while adoption is growing, effectiveness varies widely.
Comments that are concise, contain code snippets, and are manually triggered,
particularly those from hunk-level review tools, are more likely to result in
code changes. These results highlight the importance of careful tool design and
suggest directions for improving AI-based code review systems.

</details>


### [25] [Dealing with SonarQube Cloud: Initial Results from a Mining Software Repository Study](https://arxiv.org/abs/2508.18816)
*Sabato Nocera,Davide Fucci,Giuseppe Scanniello*

Main category: cs.SE

TL;DR: A study of 321 GitHub projects using SonarQube Cloud found 81% had valid connections, with 75% relying on default quality gates (45% customized). Most configurations align with SonarQube's 'Clean as You Code' principles focusing on security and maintainability. Future research will link these configurations to software outcomes.


<details>
  <summary>Details</summary>
Motivation: This paper investigates how open-source projects use and customize SonarQube Cloud, an SCA tool, given the lack of prior knowledge about such practices.

Method: The authors conducted a mining study of 321 GitHub projects linked to SonarQube Cloud via GitHub Actions. They analyzed connection validity (81% correctly connected), quality gate configurations (75% use organizational defaults, 45% customize), and aligned conditions with SonarQube's 'Clean as You Code' principles.

Result: Among 321 projects, 81% correctly connected to SonarQube Cloud. 75% of 265 accessible projects used default quality gates; 45% customized them. Common conditions emphasized security, maintainability, reliability, coverage, and duplicate code management for new/modified code.

Conclusion: Many projects rely on predefined configurations for SonarQube Cloud, but a significant portion customize quality gates to meet specific quality goals. The study suggests future research linking quality gate configurations to actual software outcomes (e.g., security improvements) to enable evidence-based recommendations.

Abstract: Background: Static Code Analysis (SCA) tools are widely adopted to enforce
code quality standards. However, little is known about how open-source projects
use and customize these tools. Aims: This paper investigates how GitHub
projects use and customize a popular SCA tool, namely SonarQube Cloud. Method:
We conducted a mining study of GitHub projects that are linked through GitHub
Actions to SonarQube Cloud projects. Results: Among 321 GitHub projects using
SonarQube Cloud, 81% of them are correctly connected to SonarQube Cloud
projects, while others exhibit misconfigurations or restricted access. Among
265 accessible SonarQube Cloud projects, 75% use the organization's default
quality gate, i.e., a set of conditions that deployed source code must meet to
pass automated checks. While 55% of the projects use the built-in quality gate
provided by SonarQube Cloud, 45% of them customize their quality gate with
different conditions. Overall, the most common quality conditions align with
SonarQube Cloud's "Clean as You Code" principle and enforce security,
maintainability, reliability, coverage, and a few duplicates on newly added or
modified source code. Conclusions: Many projects rely on predefined
configurations, yet a significant portion customize their configurations to
meet specific quality goals. Building on our initial results, we envision a
future research agenda linking quality gate configurations to actual software
outcomes (e.g., improvement of software security). This would enable
evidence-based recommendations for configuring SCA tools like SonarQube Cloud
in various contexts.

</details>


### [26] [Interleaving Large Language Models for Compiler Testing](https://arxiv.org/abs/2508.18955)
*Yunbo Ni,Shaohua Li*

Main category: cs.SE

TL;DR: LegoFuzz uses a two-phase AI-driven approach to find 66 C compiler bugs efficiently, overcoming prior limitations in complexity and cost.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based compiler testing methods generate overly simple programs or incur high computational costs, limiting their effectiveness.

Method: A two-phase testing framework (offline and online) using LLMs: offline phase generates small, rich code snippets, while the online phase strategically combines them into valid test programs for compiler testing.

Result: LegoFuzz detected 66 bugs (including 33 critical miscompilation bugs) in GCC and LLVM, outperforming existing LLM-based tools due to its efficient design.

Conclusion: The proposed two-phase framework effectively addresses computational costs and program complexity issues in compiler testing, enabling efficient bug detection with AI models and suggesting broader applications in software testing.

Abstract: Testing compilers with AI models, especially large language models (LLMs),
has shown great promise. However, current approaches struggle with two key
problems: The generated programs for testing compilers are often too simple,
and extensive testing with the LLMs is computationally expensive. In this
paper, we propose a novel compiler testing framework that decouples the testing
process into two distinct phases: an offline phase and an online phase. In the
offline phase, we use LLMs to generate a collection of small but feature-rich
code pieces. In the online phase, we reuse these code pieces by strategically
combining them to build high-quality and valid test programs, which are then
used to test compilers.
  We implement this idea in a tool, LegoFuzz, for testing C compilers. The
results are striking: we found 66 bugs in GCC and LLVM, the most widely used C
compilers. Almost half of the bugs are miscompilation bugs, which are serious
and hard-to-find bugs that none of the existing LLM-based tools could find. We
believe this efficient design opens up new possibilities for using AI models in
software testing beyond just C compilers.

</details>


### [27] [GitTaskBench: A Benchmark for Code Agents Solving Real-World Tasks Through Code Repository Leveraging](https://arxiv.org/abs/2508.18993)
*Ziyi Ni,Huacan Wang,Shuo Zhang,Shuo Lu,Ziyang He,Wang You,Zhenheng Tang,Yuntao Du,Bill Sun,Hongzhang Liu,Sen Hu,Ronghao Chen,Bo Li,Xin Li,Chen Hu,Binxing Jiao,Daxin Jiang,Pin Lyu*

Main category: cs.SE

TL;DR: GitTaskBench benchmarks code agents on real GitHub tasks: best system solves 48%, with failure roots in setup workflows. New metric quantifies economic value. Challenges remain for real-world code agent deployment.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack evaluation of code agents in authentic, workflow-driven scenarios using real-world repositories like GitHub. This gap hinders progress in developing practical code agents for complex software tasks.

Method: The authors created GitTaskBench, a benchmark with 54 realistic tasks across 7 modalities and 7 domains, paired with human-curated evaluation harnesses. They introduced the alpha-value metric to quantify economic benefits and conducted experiments across three frameworks with multiple advanced LLMs.

Result: The best-performing system (OpenHands+Claude 3.7) solved 48.15% of tasks. Error analysis revealed 50%+ failures stemmed from environment setup and dependency resolution, highlighting workflow management shortcomings. The alpha-value metric revealed performance-cost tradeoffs.

Conclusion: The paper concludes that leveraging code repositories for complex task solving remains challenging, with even the best system solving only 48.15% of tasks. By releasing GitTaskBench, the authors aim to advance repository-aware code agents and address bottlenecks in workflow management for real-world software development.

Abstract: Beyond scratch coding, exploiting large-scale code repositories (e.g.,
GitHub) for practical tasks is vital in real-world software development, yet
current benchmarks rarely evaluate code agents in such authentic,
workflow-driven scenarios. To bridge this gap, we introduce GitTaskBench, a
benchmark designed to systematically assess this capability via 54 realistic
tasks across 7 modalities and 7 domains. Each task pairs a relevant repository
with an automated, human-curated evaluation harness specifying practical
success criteria. Beyond measuring execution and task success, we also propose
the alpha-value metric to quantify the economic benefit of agent performance,
which integrates task success rates, token cost, and average developer
salaries. Experiments across three state-of-the-art agent frameworks with
multiple advanced LLMs show that leveraging code repositories for complex task
solving remains challenging: even the best-performing system, OpenHands+Claude
3.7, solves only 48.15% of tasks. Error analysis attributes over half of
failures to seemingly mundane yet critical steps like environment setup and
dependency resolution, highlighting the need for more robust workflow
management and increased timeout preparedness. By releasing GitTaskBench, we
aim to drive progress and attention toward repository-aware code reasoning,
execution, and deployment -- moving agents closer to solving complex,
end-to-end real-world tasks. The benchmark and code are open-sourced at
https://github.com/QuantaAlpha/GitTaskBench.

</details>


### [28] [A Slice-Based Change Impact Analysis for Regression Test Case Prioritization of Object-Oriented Programs](https://arxiv.org/abs/2508.19056)
*S. Panda,D. Munjal,D. P. Mohapatra*

Main category: cs.SE

TL;DR: This paper proposes a static test prioritization method using affected component coupling (ACC) in object-oriented systems. By constructing an affected slice graph (ASG) and prioritizing test cases based on ACC values, it achieves early fault detection. Seven case studies and mutation testing validate its effectiveness compared to existing approaches.


<details>
  <summary>Details</summary>
Motivation: The need to optimize test case execution order for early fault detection and cost-effective regression testing motivates this work, as early fault exposure reduces retesting time and costs.

Method: The paper introduces a static approach using affected component coupling (ACC) to prioritize test cases. It constructs an affected slice graph (ASG) to represent program parts, computes ACC values to determine fault-proneness, and assigns higher priority to test cases covering high-ACC nodes.

Result: Seven case studies and mutation fault analysis demonstrate the approach's feasibility and acceptable performance compared to existing techniques, with test cases covering high-ACC nodes showing higher fault detection rates early.

Conclusion: The proposed ACC-based static approach for test case prioritization is feasible and effective in detecting faults earlier, as validated by seven case studies and mutation fault analysis, showing comparable performance to existing techniques.

Abstract: Test case prioritization focuses on finding a suitable order of execution of
the test cases in a test suite to meet some performance goals like detecting
faults early. It is likely that some test cases execute the program parts that
are more prone to errors and will detect more errors if executed early during
the testing process. Finding an optimal order of execution for the selected
regression test cases saves time and cost of retesting. This paper presents a
static approach to prioritizing the test cases by computing the affected
component coupling (ACC) of the affected parts of object-oriented programs. We
construct a graph named affected slice graph (ASG) to represent these affected
program parts.We determine the fault-proneness of the nodes of ASG by computing
their respective ACC values. We assign higher priority to those test cases that
cover the nodes with higher ACC values. Our analysis with mutation faults shows
that the test cases executing the fault-prone program parts have a higher
chance to reveal faults earlier than other test cases in the test suite. The
result obtained from seven case studies justifies that our approach is feasible
and gives acceptable performance in comparison to some existing techniques.

</details>
