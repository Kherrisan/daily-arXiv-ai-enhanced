{"id": "2508.14070", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14070", "abs": "https://arxiv.org/abs/2508.14070", "authors": ["Ephraiem Sarabamoun"], "title": "Special-Character Adversarial Attacks on Open-Source Language Model", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable performance across\ndiverse natural language processing tasks, yet their vulnerability to\ncharacter-level adversarial manipulations presents significant security\nchallenges for real-world deployments."}
{"id": "2508.14128", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14128", "abs": "https://arxiv.org/abs/2508.14128", "authors": ["Jiaming Hu", "Haoyu Wang", "Debarghya Mukherjee", "Ioannis Ch. Paschalidis"], "title": "CCFC: Core & Core-Full-Core Dual-Track Defense for LLM Jailbreak Protection", "comment": "11 pages, 1 figure", "summary": "Jailbreak attacks pose a serious challenge to the safe deployment of large\nlanguage models (LLMs). We introduce CCFC (Core & Core-Full-Core), a\ndual-track, prompt-level defense framework designed to mitigate LLMs'\nvulnerabilities from prompt injection and structure-aware jailbreak attacks.\nCCFC operates by first isolating the semantic core of a user query via few-shot\nprompting, and then evaluating the query using two complementary tracks: a\ncore-only track to ignore adversarial distractions (e.g., toxic suffixes or\nprefix injections), and a core-full-core (CFC) track to disrupt the structural\npatterns exploited by gradient-based or edit-based attacks. The final response\nis selected based on a safety consistency check across both tracks, ensuring\nrobustness without compromising on response quality. We demonstrate that CCFC\ncuts attack success rates by 50-75% versus state-of-the-art defenses against\nstrong adversaries (e.g., DeepInception, GCG), without sacrificing fidelity on\nbenign queries. Our method consistently outperforms state-of-the-art\nprompt-level defenses, offering a practical and effective solution for safer\nLLM deployment."}
{"id": "2508.14190", "categories": ["cs.CR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.14190", "abs": "https://arxiv.org/abs/2508.14190", "authors": ["Zixin Rao", "Youssef Mohamed", "Shang Liu", "Zeyan Liu"], "title": "Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text", "comment": "Securecomm 2025", "summary": "Large Language Models (LLMs), such as GPT-4 and Llama, have demonstrated\nremarkable abilities in generating natural language. However, they also pose\nsecurity and integrity challenges. Existing countermeasures primarily focus on\ndistinguishing AI-generated content from human-written text, with most\nsolutions tailored for English. Meanwhile, authorship attribution--determining\nwhich specific LLM produced a given text--has received comparatively little\nattention despite its importance in forensic analysis. In this paper, we\npresent DA-MTL, a multi-task learning framework that simultaneously addresses\nboth text detection and authorship attribution. We evaluate DA-MTL on nine\ndatasets and four backbone models, demonstrating its strong performance across\nmultiple languages and LLM sources. Our framework captures each task's unique\ncharacteristics and shares insights between them, which boosts performance in\nboth tasks. Additionally, we conduct a thorough analysis of cross-modal and\ncross-lingual patterns and assess the framework's robustness against\nadversarial obfuscation techniques. Our findings offer valuable insights into\nLLM behavior and the generalization of both detection and authorship\nattribution."}
{"id": "2508.14230", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.14230", "abs": "https://arxiv.org/abs/2508.14230", "authors": ["Eduardo Brito", "Fernando Castillo", "Liina Kamm", "Amnir Hadachi", "Ulrich Norbisrath"], "title": "A Taxonomy and Methodology for Proof-of-Location Systems", "comment": "This work has been accepted to the 29th International Conference on\n  Enterprise Design, Operations, and Computing (EDOC 2025)", "summary": "Digital societies increasingly rely on trustworthy proofs of physical\npresence for services such as supply-chain tracking, e-voting, ride-sharing,\nand location-based rewards. Yet, traditional localization methods often lack\ncryptographic guarantees of where and when an entity was present, leaving them\nvulnerable to spoofing, replay, or collusion attacks. In response, research on\nProof-of-Location (PoL) has emerged, with recent approaches combining distance\nbounding, distributed consensus, and privacy-enhancing techniques to enable\nverifiable, tamper-resistant location claims.\n  As the design space for PoL systems grows in complexity, this paper provides\na unified framework to help practitioners navigate diverse application needs.\nWe first propose a taxonomy identifying four core domains: (1) cryptographic\nguarantees, (2) spatio-temporal synchronization, (3) trust and witness models,\nand (4) interaction and overhead. Building on this, we introduce a methodology\nto map application-specific requirements onto appropriate PoL architectures. We\nillustrate this process through three use cases (retail e-coupons, supply chain\nauditing, and physical e-voting), each showing how different constraints shape\nprotocol choices. Overall, this work offers a structured approach to building\nsecure, scalable, and interoperable PoL systems."}
{"id": "2508.14104", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14104", "abs": "https://arxiv.org/abs/2508.14104", "authors": ["Yutong Bian", "Xianhao Lin", "Yupeng Xie", "Tianyang Liu", "Mingchen Zhuge", "Siyuan Lu", "Haoming Tang", "Jinlin Wang", "Jiayi Zhang", "Jiaqi Chen", "Xiangru Tang", "Yongxin Ni", "Sirui Hong", "Chenglin Wu"], "title": "You Don't Know Until You Click:Automated GUI Testing for Production-Ready Software Evaluation", "comment": null, "summary": "Large Language Models (LLMs) and code agents in software development are\nrapidly evolving from generating isolated code snippets to producing\nfull-fledged software applications with graphical interfaces, interactive\nlogic, and dynamic behaviors. However, current benchmarks fall short in\nevaluating such production-ready software, as they often rely on static checks\nor binary pass/fail scripts, failing to capture the interactive behaviors and\nruntime dynamics that define real-world usability - qualities that only emerge\nwhen an application is actively used. This is the blind spot of current\nevaluation: you don't know if an app works until you click through it, interact\nwith it, and observe how it responds. To bridge this gap, we introduce\nRealDevWorld, a novel evaluation framework for automated end-to-end assessment\nof LLMs' ability to generate production-ready repositories from scratch. It\nfeatures two key components: (1) RealDevBench, a diverse collection of 194\nopen-ended software engineering tasks across multiple domains, incorporating\nmultimodal elements to reflect real-world complexity; and (2) AppEvalPilot, a\nnew agent-as-a-judge evaluation system that simulates realistic, GUI-based user\ninteractions to automatically and holistically assess software functional\ncorrectness, visual fidelity, and runtime behavior. The framework delivers\nfine-grained, task-specific diagnostic feedback, supporting nuanced evaluation\nbeyond simple success/failure judgments. Empirical results show that\nRealDevWorld delivers effective, automatic, and human-aligned evaluations,\nachieving an accuracy of 0.92 and a correlation of 0.85 with expert human\nassessments, while significantly reducing the reliance on manual review. This\nenables scalable, human-aligned assessment of production-level software\ngenerated by LLMs. Our code is available on GitHub."}
{"id": "2508.14261", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.14261", "abs": "https://arxiv.org/abs/2508.14261", "authors": ["Meet Udeshi", "Venkata Sai Charan Putrevu", "Prashanth Krishnamurthy", "Ramesh Karri", "Farshad Khorrami"], "title": "SaMOSA: Sandbox for Malware Orchestration and Side-Channel Analysis", "comment": null, "summary": "Cyber-attacks on operational technology (OT) and cyber-physical systems (CPS)\nhave increased tremendously in recent years with the proliferation of malware\ntargeting Linux-based embedded devices of OT and CPS systems. Comprehensive\nmalware detection requires dynamic analysis of execution behavior in addition\nto static analysis of binaries. Safe execution of malware in a manner that\ncaptures relevant behaviors via side-channels requires a sandbox environment.\nExisting Linux sandboxes are built for specific tasks, only capture one or two\nside-channels, and do not offer customization for different analysis tasks. We\npresent the SaMOSA Linux sandbox that allows emulation of Linux malwares while\ncapturing time-synchronized side-channels from four sources. SaMOSA\nadditionally provides emulation of network services via FakeNet, and allows\norchestration and customization of the sandbox environment via pipeline hooks.\nIn comparison to existing Linux sandboxes, SaMOSA captures more side-channels\nnamely system calls, network activity, disk activity, and hardware performance\ncounters. It supports three architectures predominantly used in OT and CPS\nnamely x86-64, ARM64, and PowerPC 64. SaMOSA fills a gap in Linux malware\nanalysis by providing a modular and customizable sandbox framework that can be\nadapted for many malware analysis tasks. We present three case studies of three\ndifferent malware families to demonstrate the advantages of SaMOSA."}
{"id": "2508.14114", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14114", "abs": "https://arxiv.org/abs/2508.14114", "authors": ["Aditey Nandan", "Viraj Kumar"], "title": "Ambiguity Resolution with Human Feedback for Code Writing Tasks", "comment": "Accepted at the Proceedings of the 33rd International Conference on\n  Computers in Education (ICCE 2025), Asia-Pacific Society for Computers in\n  Education (APSCE)", "summary": "Specifications for code writing tasks are usually expressed in natural\nlanguage and may be ambiguous. Programmers must therefore develop the ability\nto recognize ambiguities in task specifications and resolve them by asking\nclarifying questions. We present and evaluate a prototype system, based on a\nnovel technique (ARHF: Ambiguity Resolution with Human Feedback), that (1)\nsuggests specific inputs on which a given task specification may be ambiguous,\n(2) seeks limited human feedback about the code's desired behavior on those\ninputs, and (3) uses this feedback to generate code that resolves these\nambiguities. We evaluate the efficacy of our prototype, and we discuss the\nimplications of such assistive systems on Computer Science education."}
{"id": "2508.14284", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.14284", "abs": "https://arxiv.org/abs/2508.14284", "authors": ["Jonathan Passerat-Palmbach", "Sarisht Wadhwa"], "title": "Differentially Private aggregate hints in mev-share", "comment": null, "summary": "Flashbots recently released mev-share to empower users with control over the\namount of information they share with searchers for extracting Maximal\nExtractable Value (MEV). Searchers require more information to maintain\non-chain exchange efficiency and profitability, while users aim to prevent\nfrontrunning by withholding information. After analyzing two searching\nstrategies in mev-share to reason about searching techniques, this paper\nintroduces Differentially-Private (DP) aggregate hints as a new type of hints\nto disclose information quantitatively. DP aggregate hints enable users to\nformally quantify their privacy loss to searchers, and thus better estimate the\nlevel of rebates to ask in return. The paper discusses the current properties\nand privacy loss in mev-share and lays out how DP aggregate hints could enhance\nthe system for both users and searchers. We leverage Differential Privacy in\nthe Trusted Curator Model to design our aggregate hints. Additionally, we\nexplain how random sampling can defend against sybil attacks and amplify\noverall user privacy while providing valuable hints to searchers for improved\nbackrunning extraction and frontrunning prevention."}
{"id": "2508.14288", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.14288", "abs": "https://arxiv.org/abs/2508.14288", "authors": ["Yewei Song", "Tiezhu Sun", "Xunzhu Tang", "Prateek Rajput", "Tegawende F. Bissyande", "Jacques Klein"], "title": "Measuring LLM Code Generation Stability via Structural Entropy", "comment": "ASE-NIER", "summary": "Assessing the stability of code generation from large language models (LLMs)\nis essential for judging their reliability in real-world development. We extend\nprior \"structural-entropy concepts\" to the program domain by pairing entropy\nwith abstract syntax tree (AST) analysis. For any fixed prompt, we collect the\nmultiset of depth-bounded subtrees of AST in each generated program and treat\ntheir relative frequencies as a probability distribution. We then measure\nstability in two complementary ways: (i) Jensen-Shannon divergence, a\nsymmetric, bounded indicator of structural overlap, and (ii) a Structural\nCross-Entropy ratio that highlights missing high-probability patterns. Both\nmetrics admit structural-only and token-aware variants, enabling separate views\non control-flow shape and identifier-level variability. Unlike pass@k, BLEU, or\nCodeBLEU, our metrics are reference-free, language-agnostic, and\nexecution-independent. We benchmark several leading LLMs on standard code\ngeneration tasks, demonstrating that AST-driven structural entropy reveals\nnuances in model consistency and robustness. The method runs in O(n,d) time\nwith no external tests, providing a lightweight addition to the code-generation\nevaluation toolkit."}
{"id": "2508.14300", "categories": ["cs.CR", "cs.CL", "cs.MA", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.14300", "abs": "https://arxiv.org/abs/2508.14300", "authors": ["Youssef Maklad", "Fares Wael", "Ali Hamdi", "Wael Elsersy", "Khaled Shaban"], "title": "MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing", "comment": null, "summary": "Traditional protocol fuzzing techniques, such as those employed by AFL-based\nsystems, often lack effectiveness due to a limited semantic understanding of\ncomplex protocol grammars and rigid seed mutation strategies. Recent works,\nsuch as ChatAFL, have integrated Large Language Models (LLMs) to guide protocol\nfuzzing and address these limitations, pushing protocol fuzzers to wider\nexploration of the protocol state space. But ChatAFL still faces issues like\nunreliable output, LLM hallucinations, and assumptions of LLM knowledge about\nprotocol specifications. This paper introduces MultiFuzz, a novel dense\nretrieval-based multi-agent system designed to overcome these limitations by\nintegrating semantic-aware context retrieval, specialized agents, and\nstructured tool-assisted reasoning. MultiFuzz utilizes agentic chunks of\nprotocol documentation (RFC Documents) to build embeddings in a vector database\nfor a retrieval-augmented generation (RAG) pipeline, enabling agents to\ngenerate more reliable and structured outputs, enhancing the fuzzer in mutating\nprotocol messages with enhanced state coverage and adherence to syntactic\nconstraints. The framework decomposes the fuzzing process into modular groups\nof agents that collaborate through chain-of-thought reasoning to dynamically\nadapt fuzzing strategies based on the retrieved contextual knowledge.\nExperimental evaluations on the Real-Time Streaming Protocol (RTSP) demonstrate\nthat MultiFuzz significantly improves branch coverage and explores deeper\nprotocol states and transitions over state-of-the-art (SOTA) fuzzers such as\nNSFuzz, AFLNet, and ChatAFL. By combining dense retrieval, agentic\ncoordination, and language model reasoning, MultiFuzz establishes a new\nparadigm in autonomous protocol fuzzing, offering a scalable and extensible\nfoundation for future research in intelligent agentic-based fuzzing systems."}
{"id": "2508.14419", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.14419", "abs": "https://arxiv.org/abs/2508.14419", "authors": ["Scott Blyth", "Sherlock A. Licorish", "Christoph Treude", "Markus Wagner"], "title": "Static Analysis as a Feedback Loop: Enhancing LLM-Generated Code Beyond Correctness", "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\ncode generation, achieving high scores on benchmarks such as HumanEval and\nMBPP. However, these benchmarks primarily assess functional correctness and\nneglect broader dimensions of code quality, including security, reliability,\nreadability, and maintainability. In this work, we systematically evaluate the\nability of LLMs to generate high-quality code across multiple dimensions using\nthe PythonSecurityEval benchmark. We introduce an iterative static\nanalysis-driven prompting algorithm that leverages Bandit and Pylint to\nidentify and resolve code quality issues. Our experiments with GPT-4o show\nsubstantial improvements: security issues reduced from >40% to 13%, readability\nviolations from >80% to 11%, and reliability warnings from >50% to 11% within\nten iterations. These results demonstrate that LLMs, when guided by static\nanalysis feedback, can significantly enhance code quality beyond functional\ncorrectness."}
{"id": "2508.14402", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.14402", "abs": "https://arxiv.org/abs/2508.14402", "authors": ["Dikshant", "Geetika Verma"], "title": "Precision over Noise: Tailoring S3 Public Access Detection to Reduce False Positives in Cloud Security Platforms", "comment": null, "summary": "Excessive and spurious alert generation by cloud security solutions is a root\ncause of analyst fatigue and operational inefficiencies. In this study, the\nlong-standing issue of false positives from publicly accessible alerts in\nAmazon S3, as generated by a licensed cloud-native security solution, is\nexamined. In a simulated production test environment, which consisted of over\n1,000 Amazon S3 buckets with diverse access configurations, it was discovered\nthat over 80\\% of the alerts generated by default rules were classified as\nfalse positives, thus demonstrating the severity of the detection issue. This\nseverely impacted detection accuracy and generated a heavier workload for\nanalysts due to redundant manual triage efforts. For addressing this problem,\ncustom detection logic was created as an exercise of the native rule\ncustomization capabilities of the solution. A unified titled ``S3 Public Access\nValidation and Data Exposure'' was created in an effort to consolidate\ndifferent forms of alerts into one, context-aware logic that systematically\nscans ACL configurations, bucket policies, indicators of public exposure, and\nthe presence of sensitive data, and then marks only those S3 buckets that\nindeed denote security risk and are publicly exposed on the internet with no\nauthentication. The results demonstrate a significant reduction in false\npositives, more precise alert fidelity, and significant time saving for\nsecurity analysts, thus demonstrating an actionable and reproducible solution\nto enhance the accuracy of security alerting in compliance-focused cloud\nenvironments."}
{"id": "2508.14451", "categories": ["cs.SE", "K.6.3; E.0"], "pdf": "https://arxiv.org/pdf/2508.14451", "abs": "https://arxiv.org/abs/2508.14451", "authors": ["Richard Sserujongi", "Daniel Ogenrwot", "Nicholas Niwamanya", "Noah Nsimbe", "Martin Bbaale", "Benjamin Ssempala", "Noble Mutabazi", "Raja Fidel Wabinyai", "Deo Okure", "Engineer Bainomugisha"], "title": "Design and Evaluation of a Scalable Data Pipeline for AI-Driven Air Quality Monitoring in Low-Resource Settings", "comment": "15 pages, 11 figures, 34th International Conference on Software\n  Engineering and Data Engineering", "summary": "The increasing adoption of low-cost environmental sensors and AI-enabled\napplications has accelerated the demand for scalable and resilient data\ninfrastructures, particularly in data-scarce and resource-constrained regions.\nThis paper presents the design, implementation, and evaluation of the AirQo\ndata pipeline: a modular, cloud-native Extract-Transform-Load (ETL) system\nengineered to support both real-time and batch processing of heterogeneous air\nquality data across urban deployments in Africa. It is Built using open-source\ntechnologies such as Apache Airflow, Apache Kafka, and Google BigQuery. The\npipeline integrates diverse data streams from low-cost sensors, third-party\nweather APIs, and reference-grade monitors to enable automated calibration,\nforecasting, and accessible analytics. We demonstrate the pipeline's ability to\ningest, transform, and distribute millions of air quality measurements monthly\nfrom over 400 monitoring devices while achieving low latency, high throughput,\nand robust data availability, even under constrained power and connectivity\nconditions. The paper details key architectural features, including workflow\norchestration, decoupled ingestion layers, machine learning-driven sensor\ncalibration, and observability frameworks. Performance is evaluated across\noperational metrics such as resource utilization, ingestion throughput,\ncalibration accuracy, and data availability, offering practical insights into\nbuilding sustainable environmental data platforms. By open-sourcing the\nplatform and documenting deployment experiences, this work contributes a\nreusable blueprint for similar initiatives seeking to advance environmental\nintelligence through data engineering in low-resource settings."}
{"id": "2508.14526", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.14526", "abs": "https://arxiv.org/abs/2508.14526", "authors": ["Stefan Lenz", "David Schachtschneider", "Simon Jonas", "Liam Tirpitz", "Sandra Geisler", "Martin Henze"], "title": "CoFacS -- Simulating a Complete Factory to Study the Security of Interconnected Production", "comment": "Accepted for publication in Proceedings of the 2025 IEEE 50th\n  Conference on Local Computer Networks (LCN)", "summary": "While the digitization of industrial factories provides tremendous\nimprovements for the production of goods, it also renders such systems\nvulnerable to serious cyber-attacks. To research, test, and validate security\nmeasures protecting industrial networks against such cyber-attacks, the\nsecurity community relies on testbeds to simulate industrial systems, as\nutilizing live systems endangers costly components or even human life. However,\nexisting testbeds focus on individual parts of typically complex production\nlines in industrial factories. Consequently, the impact of cyber-attacks on\nindustrial networks as well as the effectiveness of countermeasures cannot be\nevaluated in an end-to-end manner. To address this issue and facilitate\nresearch on novel security mechanisms, we present CoFacS, the first COmplete\nFACtory Simulation that replicates an entire production line and affords the\nintegration of real-life industrial applications. To showcase that CoFacS\naccurately captures real-world behavior, we validate it against a physical\nmodel factory widely used in security research. We show that CoFacS has a\nmaximum deviation of 0.11% to the physical reference, which enables us to study\nthe impact of physical attacks or network-based cyber-attacks. Moreover, we\nhighlight how CoFacS enables security research through two cases studies\nsurrounding attack detection and the resilience of 5G-based industrial\ncommunication against jamming."}
{"id": "2508.14511", "categories": ["cs.SE", "D.2.11"], "pdf": "https://arxiv.org/pdf/2508.14511", "abs": "https://arxiv.org/abs/2508.14511", "authors": ["Eagon Meng", "Daniel Jackson"], "title": "What You See Is What It Does: A Structural Pattern for Legible Software", "comment": "16 pages. Appearing in Onward! at SPLASH 2025", "summary": "The opportunities offered by LLM coders (and their current limitations)\ndemand a reevaluation of how software is structured. Software today is often\n\"illegible\" - lacking a direct correspondence between code and observed\nbehavior - and insufficiently modular, leading to a failure of three key\nrequirements of robust coding: incrementality (the ability to deliver small\nincrements by making localized changes), integrity (avoiding breaking prior\nincrements) and transparency (making clear what has changed at build time, and\nwhat actions have happened at runtime).\n  A new structural pattern offers improved legibility and modularity. Its\nelements are concepts and synchronizations: fully independent services and\nevent-based rules that mediate between them. A domain-specific language for\nsynchronizations allows behavioral features to be expressed in a granular and\ndeclarative way (and thus readily generated by an LLM). A case study of the\nRealWorld benchmark is used to illustrate and evaluate the approach."}
{"id": "2508.14530", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.14530", "abs": "https://arxiv.org/abs/2508.14530", "authors": ["Xuezheng Qin", "Ruwei Huang", "Xiaolong Tang", "Feng Li"], "title": "DOPA: Stealthy and Generalizable Backdoor Attacks from a Single Client under Challenging Federated Constraints", "comment": null, "summary": "Federated Learning (FL) is increasingly adopted for privacy-preserving\ncollaborative training, but its decentralized nature makes it particularly\nsusceptible to backdoor attacks. Existing attack methods, however, often rely\non idealized assumptions and fail to remain effective under real-world\nconstraints, such as limited attacker control, non-IID data distributions, and\nthe presence of diverse defense mechanisms. To address this gap, we propose\nDOPA (Divergent Optimization Path Attack), a novel framework that simulates\nheterogeneous local training dynamics and seeks consensus across divergent\noptimization trajectories to craft universally effective and stealthy backdoor\ntriggers. By leveraging consistency signals across simulated paths to guide\noptimization, DOPA overcomes the challenge of heterogeneity-induced instability\nand achieves practical attack viability under stringent federated constraints.\nWe validate DOPA on a comprehensive suite of 12 defense strategies, two model\narchitectures (ResNet18/VGG16), two datasets (CIFAR-10/TinyImageNet), and both\nmild and extreme non-IID settings. Despite operating under a single-client,\nblack-box, and sparsely participating threat model, DOPA consistently achieves\nhigh attack success, minimal accuracy degradation, low runtime, and long-term\npersistence. These results demonstrate a more practical attack paradigm,\noffering new perspectives for designing robust defense strategies in federated\nlearning systems"}
{"id": "2508.14532", "categories": ["cs.SE", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.14532", "abs": "https://arxiv.org/abs/2508.14532", "authors": ["Zhongyi Wang", "Tengjie Lin", "Mingshuai Chen", "Mingqi Yang", "Haokun Li", "Xiao Yi", "Shengchao Qin", "Jianwei Yin"], "title": "Preguss: It Analyzes, It Specifies, It Verifies", "comment": "Position paper to appear in the 1st International Workshop on\n  Language Models and Programming Languages (LMPL '25)", "summary": "Fully automated verification of large-scale software and hardware systems is\narguably the holy grail of formal methods. Large language models (LLMs) have\nrecently demonstrated their potential for enhancing the degree of automation in\nformal verification by, e.g., generating formal specifications as essential to\ndeductive verification, yet exhibit poor scalability due to context-length\nlimitations and, more importantly, the difficulty of inferring complex,\ninterprocedural specifications. This paper outlines Preguss - a modular,\nfine-grained framework for automating the generation and refinement of formal\nspecifications. Preguss synergizes between static analysis and deductive\nverification by orchestrating two components: (i) potential runtime error\n(RTE)-guided construction and prioritization of verification units, and (ii)\nLLM-aided synthesis of interprocedural specifications at the unit level. We\nenvisage that Preguss paves a compelling path towards the automated\nverification of large-scale programs."}
{"id": "2508.14568", "categories": ["cs.CR", "E.3"], "pdf": "https://arxiv.org/pdf/2508.14568", "abs": "https://arxiv.org/abs/2508.14568", "authors": ["Wouter Legiest", "Jan-Pieter D'Anvers", "Bojan Spasic", "Nam-Luc Tran", "Ingrid Verbauwhede"], "title": "Leuvenshtein: Efficient FHE-based Edit Distance Computation with Single Bootstrap per Cell", "comment": "Published at 34th USENIX Security Symposium (USENIX Security 2025).\n  2025", "summary": "This paper presents a novel approach to calculating the Levenshtein (edit)\ndistance within the framework of Fully Homomorphic Encryption (FHE),\nspecifically targeting third-generation schemes like TFHE. Edit distance\ncomputations are essential in applications across finance and genomics, such as\nDNA sequence alignment. We introduce an optimised algorithm that significantly\nreduces the cost of edit distance calculations called Leuvenshtein. This\nalgorithm specifically reduces the number of programmable bootstraps (PBS)\nneeded per cell of the calculation, lowering it from approximately 94\noperations -- required by the conventional Wagner-Fisher algorithm -- to just\n1. Additionally, we propose an efficient method for performing equality checks\non characters, reducing ASCII character comparisons to only 2 PBS operations.\nFinally, we explore the potential for further performance improvements by\nutilising preprocessing when one of the input strings is unencrypted. Our\nLeuvenshtein achieves up to $278\\times$ faster performance compared to the best\navailable TFHE implementation and up to $39\\times$ faster than an optimised\nimplementation of the Wagner-Fisher algorithm. Moreover, when offline\npreprocessing is possible due to the presence of one unencrypted input on the\nserver side, an additional $3\\times$ speedup can be achieved."}
{"id": "2508.14540", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14540", "abs": "https://arxiv.org/abs/2508.14540", "authors": ["Dennis Schiese", "Andreas Both"], "title": "Post-hoc LLM-Supported Debugging of Distributed Processes", "comment": "Presented at ICWE 2025, Delft (30 June - 03 July 2025)", "summary": "In this paper, we address the problem of manual debugging, which nowadays\nremains resource-intensive and in some parts archaic. This problem is\nespecially evident in increasingly complex and distributed software systems.\nTherefore, our objective of this work is to introduce an approach that can\npossibly be applied to any system, at both the macro- and micro-level, to ease\nthis debugging process. This approach utilizes a system's process data, in\nconjunction with generative AI, to generate natural-language explanations.\nThese explanations are generated from the actual process data, interface\ninformation, and documentation to guide the developers more efficiently to\nunderstand the behavior and possible errors of a process and its sub-processes.\nHere, we present a demonstrator that employs this approach on a component-based\nJava system. However, our approach is language-agnostic. Ideally, the generated\nexplanations will provide a good understanding of the process, even if\ndevelopers are not familiar with all the details of the considered system. Our\ndemonstrator is provided as an open-source web application that is freely\naccessible to all users."}
{"id": "2508.14699", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14699", "abs": "https://arxiv.org/abs/2508.14699", "authors": ["Jan Lum Fok", "Qingwen Zeng", "Shiping Chen", "Oscar Fawkes", "Huaming Chen"], "title": "Foe for Fraud: Transferable Adversarial Attacks in Credit Card Fraud Detection", "comment": null, "summary": "Credit card fraud detection (CCFD) is a critical application of Machine\nLearning (ML) in the financial sector, where accurately identifying fraudulent\ntransactions is essential for mitigating financial losses. ML models have\ndemonstrated their effectiveness in fraud detection task, in particular with\nthe tabular dataset. While adversarial attacks have been extensively studied in\ncomputer vision and deep learning, their impacts on the ML models, particularly\nthose trained on CCFD tabular datasets, remains largely unexplored. These\nlatent vulnerabilities pose significant threats to the security and stability\nof the financial industry, especially in high-value transactions where losses\ncould be substantial. To address this gap, in this paper, we present a holistic\nframework that investigate the robustness of CCFD ML model against adversarial\nperturbations under different circumstances. Specifically, the gradient-based\nattack methods are incorporated into the tabular credit card transaction data\nin both black- and white-box adversarial attacks settings. Our findings confirm\nthat tabular data is also susceptible to subtle perturbations, highlighting the\nneed for heightened awareness among financial technology practitioners\nregarding ML model security and trustworthiness. Furthermore, the experiments\nby transferring adversarial samples from gradient-based attack method to\nnon-gradient-based models also verify our findings. Our results demonstrate\nthat such attacks remain effective, emphasizing the necessity of developing\nrobust defenses for CCFD algorithms."}
{"id": "2508.14553", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14553", "abs": "https://arxiv.org/abs/2508.14553", "authors": ["Dennis Schiese", "Aleksandr Perevalov", "Andreas Both"], "title": "Towards LLM-generated explanations for Component-based Knowledge Graph Question Answering Systems", "comment": "Presented at ICWI 2024, Zagreb. Released with ISBN:\n  978-989-8704-62-7. Data source:\n  https://figshare.com/articles/dataset/Towards_LLM-generated_explanations_for_component-based_knowledge_graph_question_answering_systems/27079687", "summary": "Over time, software systems have reached a level of complexity that makes it\ndifficult for their developers and users to explain particular decisions made\nby them. In this paper, we focus on the explainability of component-based\nsystems for Question Answering (QA). These components often conduct processes\ndriven by AI methods, in which behavior and decisions cannot be clearly\nexplained or justified, s.t., even for QA experts interpreting the executed\nprocess and its results is hard. To address this challenge, we present an\napproach that considers the components' input and output data flows as a source\nfor representing the behavior and provide explanations for the components,\nenabling users to comprehend what happened. In the QA framework used here, the\ndata flows of the components are represented as SPARQL queries (inputs) and RDF\ntriples (outputs). Hence, we are also providing valuable insights on\nverbalization regarding these data types. In our experiments, the approach\ngenerates explanations while following template-based settings (baseline) or\nvia the use of Large Language Models (LLMs) with different configurations\n(automatic generation). Our evaluation shows that the explanations generated\nvia LLMs achieve high quality and mostly outperform template-based approaches\naccording to the users' ratings. Therefore, it enables us to automatically\nexplain the behavior and decisions of QA components to humans while using RDF\nand SPARQL as a context for explanations."}
{"id": "2508.14703", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.14703", "abs": "https://arxiv.org/abs/2508.14703", "authors": ["Farid Zaredar", "Morteza Amini"], "title": "A Lightweight Incentive-Based Privacy-Preserving Smart Metering Protocol for Value-Added Services", "comment": "18 Pages, 7 Figures, 6 Tables,", "summary": "The emergence of smart grids and advanced metering infrastructure (AMI) has\nrevolutionized energy management. Unlike traditional power grids, smart grids\nbenefit from two-way communication through AMI, which surpasses earlier\nautomated meter reading (AMR). AMI enables diverse demand- and supply-side\nutilities such as accurate billing, outage detection, real-time grid control,\nload forecasting, and value-added services. Smart meters play a key role by\ndelivering consumption values at predefined intervals to the utility provider\n(UP). However, such reports may raise privacy concerns, as adversaries can\ninfer lifestyle patterns, political orientations, and the types of electrical\ndevices in a household, or even sell the data to third parties (TP) such as\ninsurers. In this paper, we propose a lightweight, privacy-preserving smart\nmetering protocol for incentive-based value-added services. The scheme employs\nlocal differential privacy, hash chains, blind digital signatures, pseudonyms,\ntemporal aggregation, and anonymous overlay networks to report coarse-grained\nvalues with adjustable granularity to the UP. This protects consumers' privacy\nwhile preserving data utility. The scheme prevents identity disclosure while\nenabling automatic token redemption. From a performance perspective, our\nresults show that with a 1024-bit RSA key, a 7-day duration, and four reports\nper day, our protocol runs in approximately 0.51s and consumes about 4.5 MB of\nmemory. From a privacy perspective, the protocol resists semi-trusted and\nuntrusted adversaries."}
{"id": "2508.14631", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.14631", "abs": "https://arxiv.org/abs/2508.14631", "authors": ["Marcos Gomez-Vazquez", "Jordi Cabot"], "title": "Towards a DSL to Formalize Multimodal Requirements", "comment": null, "summary": "Multimodal systems, which process multiple input types such as text, audio,\nand images, are becoming increasingly prevalent in software systems, enabled by\nthe huge advancements in Machine Learning. This triggers the need to easily\ndefine the requirements linked to these new types of user interactions,\npotentially involving more than one modality at the same time. This remains an\nopen challenge due to the lack of languages and methods adapted to the diverse\nnature of multimodal interactions, with the risk of implementing AI-enhanced\nsystems that do not properly satisfy the user needs.\n  In this sense, this paper presents MERLAN, a Domain-Specific Language (DSL)\nto specify the requirements for these new types of multimodal interfaces. We\npresent the metamodel for such language together with a textual syntax\nimplemented as an ANTLR grammar. A prototype tool enabling requirements\nengineers to write such requirements and automatically generate a possible\nimplementation of a system compliant with them on top of an agentic framework\nis also provided."}
{"id": "2508.14744", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.14744", "abs": "https://arxiv.org/abs/2508.14744", "authors": ["Farid Zaredar", "Morteza Amini"], "title": "A Collusion-Resistance Privacy-Preserving Smart Metering Protocol for Operational Utility", "comment": "13 pages, 9 figures, 7 tables", "summary": "Modern grids have adopted advanced metering infrastructure (AMI) to\nfacilitate bidirectional communication between smart meters and control\ncenters. This enables smart meters to report consumption values at predefined\nintervals to utility providers for purposes including demand balancing, load\nforecasting, dynamic billing, and operational efficiency. Compared to\ntraditional power grids, smart grids offer advantages such as enhanced\nreliability, improved energy efficiency, and increased security. However,\nutility providers can compromise user privacy by analyzing fine-grained\nreadings and extracting individuals' daily activities from this time-series\ndata. To address this concern, we propose a collusion-resistant,\nprivacy-preserving aggregation protocol for smart metering in operational\nservices. Our protocol ensures privacy by leveraging techniques such as\npartially additive homomorphic encryption, aggregation, data perturbation, and\ndata minimization. The scheme aggregates perturbed readings using the additive\nhomomorphic property of the Paillier cryptosystem to provide results for\nmultiple operational purposes. We evaluate the protocol in terms of both\nperformance and privacy. Computational, memory, and communication overhead were\nexamined. The total execution time with 1024-bit key size is about 2.21\nseconds. We also evaluated privacy through the normalized conditional entropy\n(NCE) metric. Higher NCE values, closer to 1, indicate stronger privacy. By\nincreasing noise scale, the NCE value rises, showing perturbed values retain\nminimal information about the original, thereby reducing risks. Overall,\nevaluation demonstrates the protocol's efficiency while employing various\nprivacy-preserving techniques."}
{"id": "2508.14727", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.14727", "abs": "https://arxiv.org/abs/2508.14727", "authors": ["Abbas Sabra", "Olivier Schmitt", "Joseph Tyler"], "title": "Assessing the Quality and Security of AI-Generated Code: A Quantitative Analysis", "comment": null, "summary": "This study presents a quantitative evaluation of the code quality and\nsecurity of five prominent Large Language Models (LLMs): Claude Sonnet 4,\nClaude 3.7 Sonnet, GPT-4o, Llama 3.2 90B, and OpenCoder 8B. While prior\nresearch has assessed the functional performance of LLM-generated code, this\nresearch tested LLM output from 4,442 Java coding assignments through\ncomprehensive static analysis using SonarQube. The findings suggest that\nalthough LLMs can generate functional code, they also introduce a range of\nsoftware defects, including bugs, security vulnerabilities, and code smells.\nThese defects do not appear to be isolated; rather, they may represent shared\nweaknesses stemming from systemic limitations within current LLM code\ngeneration methods. In particular, critically severe issues, such as hard-coded\npasswords and path traversal vulnerabilities, were observed across multiple\nmodels. These results indicate that LLM-generated code requires verification in\norder to be considered production-ready. This study found no direct correlation\nbetween a model's functional performance (measured by Pass@1 rate of unit\ntests) and the overall quality and security of its generated code, measured by\nthe number of SonarQube issues in benchmark solutions that passed the\nfunctional tests. This suggests that functional benchmark performance score is\nnot a good indicator of overall code quality and security. The goal of this\nstudy is not to rank LLM performance but to highlight that all evaluated models\nappear to share certain weaknesses. Consequently, these findings support the\nview that static analysis can be a valuable instrument for detecting latent\ndefects and an important safeguard for organizations that deploy AI in software\ndevelopment."}
{"id": "2508.14796", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.14796", "abs": "https://arxiv.org/abs/2508.14796", "authors": ["James C Davis", "Sophie Chen", "Huiyun Peng", "Paschal C Amusuo", "Kelechi G Kalu"], "title": "A Guide to Stakeholder Analysis for Cybersecurity Researchers", "comment": null, "summary": "Stakeholder-based ethics analysis is now a formal requirement for submissions\nto top cybersecurity research venues. This requirement reflects a growing\nconsensus that cybersecurity researchers must go beyond providing capabilities\nto anticipating and mitigating the potential harms thereof. However, many\ncybersecurity researchers may be uncertain about how to proceed in an ethics\nanalysis. In this guide, we provide practical support for that requirement by\nenumerating stakeholder types and mapping them to common empirical research\nmethods. We also offer worked examples to demonstrate how researchers can\nidentify likely stakeholder exposures in real-world projects. Our goal is to\nhelp research teams meet new ethics mandates with confidence and clarity, not\nconfusion."}
{"id": "2508.14747", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.14747", "abs": "https://arxiv.org/abs/2508.14747", "authors": ["Beatriz Cabrero-Daniel", "Mazen Mohamad"], "title": "Challenges of Virtual Validation and Verification for Automotive Functions", "comment": "This work is supported by Sweden's innovation agency, Vinnova, under\n  Grant No. 2021-05043 entitled \"Enabling Virtual Validation and Verification\n  for ADAS and AD Features (EVIDENT).\"", "summary": "Verification and validation of vehicles is a complex yet critical process,\nparticularly for ensuring safety and coverage through simulations. However,\nachieving realistic and useful simulations comes with significant challenges.\nTo explore these challenges, we conducted a workshop with experts in the field,\nallowing them to brainstorm key obstacles. Following this, we distributed a\nsurvey to consolidate findings and gain further insights into potential\nsolutions. The experts identified 17 key challenges, along with proposed\nsolutions, an assessment of whether they represent next steps for research, and\nthe roadblocks to their implementation. While a lack of resources was not\ninitially highlighted as a major challenge, utilizing more resources emerged as\na critical necessity when experts discussed solutions. Interestingly, we\nexpected some of these challenges to have already been addressed or to have\nsystematic solutions readily available, given the collective expertise in the\nfield. Many of the identified problems already have known solutions, allowing\nus to shift focus towards unresolved challenges and share the next steps with\nthe broader community."}
{"id": "2508.14815", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.14815", "abs": "https://arxiv.org/abs/2508.14815", "authors": ["Farid Zaredar", "Morteza Amini"], "title": "A Lightweight Privacy-Preserving Smart Metering Billing Protocol with Dynamic Tariff Policy Adjustment", "comment": "12 pages, 8 figures, 7 tables", "summary": "The integration of information and communication technology (ICT) with\ntraditional power grids has led to the emergence of smart grids. Advanced\nmetering infrastructure (AMI) plays a crucial role in smart grids by\nfacilitating two-way communication between smart meters and the utility\nprovider. This bidirectional communication allows intelligent meters to report\nfine-grained consumption data at predefined intervals, enabling accurate\nbilling, efficient grid monitoring and management, and rapid outage detection.\nHowever, the collection of detailed consumption data can inadvertently disclose\nconsumers' daily activities, raising privacy concerns and potentially leading\nto privacy violations. To address these issues and preserve individuals'\nprivacy, we propose a lightweight privacy-preserving smart metering protocol\nspecifically designed to support real-time tariff billing service with dynamic\npolicy adjustment. Our scheme employs an efficient data perturbation technique\nto obscure precise energy usage data from internal adversaries, including the\nintermediary gateways and the utility provider. Subsequently, we validate the\nefficiency and security of our protocol through comprehensive performance and\nprivacy evaluations. We examined the computational, memory, and communication\noverhead of the proposed scheme. The execution time of our secure and\nprivacy-aware billing system is approximately 3.94540 seconds for a complete\nyear. Furthermore, we employed the Jensen-Shannon divergence as a privacy\nmetric to demonstrate that our protocol can effectively safeguard users'\nprivacy by increasing the noise scale."}
{"id": "2508.14796", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.14796", "abs": "https://arxiv.org/abs/2508.14796", "authors": ["James C Davis", "Sophie Chen", "Huiyun Peng", "Paschal C Amusuo", "Kelechi G Kalu"], "title": "A Guide to Stakeholder Analysis for Cybersecurity Researchers", "comment": null, "summary": "Stakeholder-based ethics analysis is now a formal requirement for submissions\nto top cybersecurity research venues. This requirement reflects a growing\nconsensus that cybersecurity researchers must go beyond providing capabilities\nto anticipating and mitigating the potential harms thereof. However, many\ncybersecurity researchers may be uncertain about how to proceed in an ethics\nanalysis. In this guide, we provide practical support for that requirement by\nenumerating stakeholder types and mapping them to common empirical research\nmethods. We also offer worked examples to demonstrate how researchers can\nidentify likely stakeholder exposures in real-world projects. Our goal is to\nhelp research teams meet new ethics mandates with confidence and clarity, not\nconfusion."}
