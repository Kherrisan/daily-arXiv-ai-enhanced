{"id": "2507.05269", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05269", "abs": "https://arxiv.org/abs/2507.05269", "authors": ["Danning Xie", "Mingwei Zheng", "Xuwei Liu", "Jiannan Wang", "Chengpeng Wang", "Lin Tan", "Xiangyu Zhang"], "title": "CORE: Benchmarking LLMs Code Reasoning Capabilities through Static Analysis Tasks", "comment": null, "summary": "Large language models (LLMs) have been widely adopted across diverse software\nengineering domains, such as code generation, program repair, and vulnerability\ndetection. These applications require understanding beyond surface-level code\npatterns: value propagation, control flow, and interdependence between program\nelements. However, existing benchmarks primarily evaluate end-to-end outcomes,\nsuch as whether code is correctly repaired or generated, leaving the models\nability for program semantic reasoning underexplored. This work presents CoRe,\na high-quality, human-verified benchmark designed to evaluate LLMs on\nfundamental static analysis tasks. CoRe includes 12,553 task instances spanning\ndata dependency, control dependency, and information flow across programs\nwritten in C/C++, Java, and Python. To ensure semantic diversity and reasoning\ncomplexity, we propose a semantics-aware diverse sampling strategy that selects\ntargets and task instances based on structural coverage and dependency depth.\nWe evaluate 10 mainstream LLMs and show that, while they perform well at\nidentifying dependencies, models still struggle with tasks that require deeper\nsemantic understanding and multi-step reasoning. We further conduct qualitative\nanalyses to uncover key challenges, such as complex control structures and\nbackward dependency patterns, offering insights into improving LLMs code\nreasoning capabilities.", "AI": {"tldr": "The paper introduces CoRe, a human-verified benchmark for evaluating LLMs' abilities in fundamental static analysis tasks (dependencies, control flow, information flow) across C/C++, Java, and Python, revealing limitations in semantic reasoning.", "motivation": "Existing benchmarks focus on surface-level code patterns and end-to-end outcomes (e.g., code repair accuracy) but lack evaluation of LLMs' deeper semantic understanding required for program analysis like data/control dependencies and multi-step reasoning.", "method": "Created CoRe benchmark with 12,553 instances across three languages, using a semantics-aware diverse sampling strategy that prioritizes structural coverage and dependency depth. Evaluated 10 mainstream LLMs on static analysis tasks.", "result": "LLMs perform well in simple dependency identification but struggle with complex control structures (e.g., nested loops/conditionals) and backward dependencies. Qualitative analysis highlights these challenges in multi-step semantic reasoning.", "conclusion": "CoRe exposes shortcomings in LLMs' program semantic reasoning capabilities, especially for structured program analysis tasks. Findings suggest pathways for improving models to better handle static analysis requirements in software engineering."}}
{"id": "2507.05270", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05270", "abs": "https://arxiv.org/abs/2507.05270", "authors": ["Boyuan Li", "Chengwei Liu", "Lingling Fan", "Sen Chen", "Zhenlin Zhang", "Zheli Liu"], "title": "Open Source, Hidden Costs: A Systematic Literature Review on OSS License Management", "comment": null, "summary": "Integrating third-party software components is a common practice in modern\nsoftware development, offering significant advantages in terms of efficiency\nand innovation. However, this practice is fraught with risks related to\nsoftware licensing. A lack of understanding may lead to disputes, which can\npose serious legal and operational challenges. To these ends, both academia and\nindustry have conducted various investigations and proposed solutions and tools\nto deal with these challenges. However, significant limitations still remain.\nMoreover, the rapid evolution of open-source software (OSS) licenses, as well\nas the rapidly incorporated generative software engineering techniques, such as\nlarge language models for code (CodeLLMs), are placing greater demands on the\nsystematic management of software license risks. To unveil the severe\nchallenges and explore possible future directions, we conduct the first\nsystematic literature review (SLR) on 80 carefully selected OSS license-related\npapers, classifying existing research into three key categories, i.e., license\nidentification, license risk assessment, and license risk mitigation. Based on\nthese, we discuss challenges in existing solutions, conclude the opportunities\nto shed light on future research directions and offer practical recommendations\nfor practitioners. We hope this thorough review will help bridge the gaps\nbetween academia and industry and accelerate the ecosystem-wide governance of\nlegitimate software risks within the software engineering community.", "AI": {"tldr": "The paper analyzes 80 OSS license-related studies, identifies challenges, and proposes research directions for managing software licensing risks amidst evolving open-source ecosystems and generative software engineering techniques.", "motivation": "risks in software licensing pose legal/operational challenges; need to address limitations in current solutions and bridge academia-industry gaps as OSS licenses and generative software engineering (e.g., CodeLLMs) evolve", "method": "systematic literature review (SLR) of 80 selected OSS license-related papers, categorized into license identification, risk assessment, and risk mitigation", "result": "systematically mapped existing research, identified challenges in license identification/assessment/mitigation, and uncovered limitations in current tools and approaches", "conclusion": "highlights research opportunities and practical recommendations for improved licensing governance; emphasizes need for ecosystem-wide collaboration between academia and industry"}}
{"id": "2507.05272", "categories": ["cs.SE", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.05272", "abs": "https://arxiv.org/abs/2507.05272", "authors": ["Daragh King", "Vasileios Koutavas", "Laura Kovacs"], "title": "FuzzFeed: An Automatic Approach to Weakest Precondition Generation using LLMs and Fuzzing", "comment": null, "summary": "The weakest precondition (WP) of a program describes the largest set of\ninitial states from which all terminating executions of the program satisfy a\ngiven postcondition. The generation of WPs is an important task with practical\napplications in areas ranging from verification to run-time error checking.\n  This paper proposes the combination of Large Language Models (LLMs) and fuzz\ntesting for generating WPs. In pursuit of this goal, we introduce Fuzzing\nGuidance (FG); FG acts as a means of directing LLMs towards correct WPs using\nprogram execution feedback. FG utilises fuzz testing for approximately checking\nthe validity and weakness of candidate WPs, this information is then fed back\nto the LLM as a means of context refinement.\n  We demonstrate the effectiveness of our approach on a comprehensive benchmark\nset of deterministic array programs in Java. Our experiments indicate that LLMs\nare capable of producing viable candidate WPs, and that this ability can be\npractically enhanced through FG.", "AI": {"tldr": "The paper introduces Fuzzing Guidance (FG), which combines Large Language Models (LLMs) with fuzz testing to generate and refine correct weakest preconditions (WPs) for deterministic Java programs, enhancing their practical viability.", "motivation": "Generating accurate weakest preconditions is critical for program verification and error checking. LLMs lack execution context to validate WP correctness, while fuzz testing provides runtime feedback. The paper addresses how to effectively merge these techniques.", "method": "FG integrates fuzz testing as a validator for LLM-generated WPs. Fuzz testing samples execution paths to approximate the validity and weakness of candidate WPs, which are then refined alongside LLM guidance through iterative feedback loops.", "result": "Experiments on a benchmark of deterministic Java array programs show: (1) LLMs generate viable WP candidates, (2) FG dramatically improves their accuracy by leveraging execution validation data via fuzzing, and (3) the combination outperforms prior approaches in correctness and comprehensiveness.", "conclusion": "The FG framework demonstrates that execution-aware feedback via fuzz testing can effectively enhance LLM-based WP generation for practical verification tasks. This hybrid approach opens new opportunities for integrating language models and coverage-guided testing in program reasoning tasks."}}
{"id": "2507.05279", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.05279", "abs": "https://arxiv.org/abs/2507.05279", "authors": ["Virgile Boraud", "Yannis Bendi-Ouis", "Paul Bernard", "Xavier Hinaut"], "title": "ReservoirChat: Interactive Documentation Enhanced with LLM and Knowledge Graph for ReservoirPy", "comment": null, "summary": "We introduce a tool designed to improve the capabilities of Large Language\nModels (LLMs) in assisting with code development using the ReservoirPy library,\nas well as in answering complex questions in the field of Reservoir Computing.\nBy incorporating external knowledge through Retrieval-Augmented Generation\n(RAG) and knowledge graphs, our approach aims to reduce hallucinations and\nincrease the factual accuracy of generated responses. The system provides an\ninteractive experience similar to ChatGPT, tailored specifically for\nReservoirPy, enabling users to write, debug, and understand Python code while\naccessing reliable domain-specific insights. In our evaluation, while\nproprietary models such as ChatGPT-4o and NotebookLM performed slightly better\non general knowledge questions, our model outperformed them on coding tasks and\nshowed a significant improvement over its base model, Codestral-22B.", "AI": {"tldr": "A tool enhances LLMs with ReservoirPy library and Reservoir Computing knowledge via RAG and knowledge graphs for reduced hallucinations.", "motivation": "Address LLM hallucinations and improve factual accuracy in coding tasks and domain-specific Reservoir Computing questions.", "method": "Integration of Retrieval-Augmented Generation (RAG) and knowledge graphs into LLM architecture for external knowledge incorporation and interactive coding support.", "result": "Outperforms proprietary models (ChatGPT-4o, NotebookLM) in coding tasks, achieves significant improvement over base Codestral-22B.", "conclusion": "Specialized LLM with RAG and knowledge graphs provides reliable domain insights while maintaining competitive general knowledge performance."}}
{"id": "2507.05415", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.05415", "abs": "https://arxiv.org/abs/2507.05415", "authors": ["Lu Xian", "Van Tran", "Lauren Lee", "Meera Kumar", "Yichen Zhang", "Florian Schaub"], "title": "Layered, Overlapping, and Inconsistent: A Large-Scale Analysis of the Multiple Privacy Policies and Controls of U.S. Banks", "comment": "Accepted for publication in CCS 2025. This is a pre-publication\n  version", "summary": "Privacy policies are often complex. An exception is the two-page standardized\nnotice that U.S. financial institutions must provide under the\nGramm-Leach-Bliley Act (GLBA). However, banks now operate websites, mobile\napps, and other services that involve complex data sharing practices that\nrequire additional privacy notices and do-not-sell opt-outs. We conducted a\nlarge-scale analysis of how U.S. banks implement privacy policies and controls\nin response to GLBA; other federal privacy policy requirements; and the\nCalifornia Consumer Privacy Act (CCPA), a key example for U.S. state privacy\nlaws. We focused on the disclosure and control of a set of especially\nprivacy-invasive practices: third-party data sharing for marketing-related\npurposes. We collected privacy policies for the 2,067 largest U.S. banks,\n45.3\\% of which provided multiple policies. Across disclosures and controls\nwithin the \\textit{same} bank, we identified frequent, concerning\ninconsistencies -- such as banks indicating in GLBA notices that they do not\nshare with third parties but disclosing sharing elsewhere, or using third-party\nmarketing/advertising cookies without disclosure. This multiplicity of\npolicies, with the inconsistencies it causes, may create consumer confusion and\nundermine the transparency goals of the very laws that require them. Our\nfindings call into question whether current policy requirements, such as the\nGLBA notice, are achieving their intended goals in today's online banking\nlandscape. We discuss potential avenues for reforming and harmonizing privacy\npolicies and control requirements across federal and state laws.", "AI": {"tldr": "Analyzes inconsistencies in U.S. banks' privacy policies under GLBA and CCPA, revealing consumer confusion and questioning the effectiveness of current regulations.", "motivation": "Complex and fragmented privacy policies hinder transparency, raising concerns about compliance and consumer understanding, especially with the coexistence of federal (GLBA) and state (CCPA) laws.", "method": "Collected privacy policies from 2,067 major U.S. banks (~45.3% with multiple policies), then systematically compared disclosures of third-party data sharing for marketing across GLBA notices, additional disclosures, and cookie practices.", "result": "45.3% of banks had multiple policies, but many exhibited contradictions: denying third-party sharing in GLBA notices while disclosing it elsewhere, and using marketing/advertising cookies without consent notices. Multiplicity of policies correlates with fragmented transparency.", "conclusion": "Current regulatory requirements (GLBA/CCPA) fail to unify privacy disclosures effectively, risking compliance gaps and eroded consumer trust. Reforms are needed to harmonize and simplify cross-regulation privacy control frameworks."}}
{"id": "2507.05281", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.05281", "abs": "https://arxiv.org/abs/2507.05281", "authors": ["Lingyue Fu", "Hao Guan", "Bolun Zhang", "Haowei Yuan", "Yaoming Zhu", "Jun Xu", "Zongyu Wang", "Lin Qiu", "Xunliang Cai", "Xuezhi Cao", "Weiwen Liu", "Weinan Zhang", "Yong Yu"], "title": "CoreCodeBench: A Configurable Multi-Scenario Repository-Level Benchmark", "comment": null, "summary": "As Large Language Models (LLMs) demonstrate increasingly sophisticated code\nprocessing capabilities, evaluating their performance on engineering-level code\nremains challenging. Existing repository-level benchmarks primarily focus on\nsingle scenarios, such as code generation or bug fixing, without adequately\ncapturing the diversity and complexity of real-world software or project\nengineering workflows. Furthermore, these benchmarks suffer from limited\ncontrollability in question positioning and reliability issues in their\ngenerated test cases. To address these limitations, we present CorePipe, a\nfully automated pipeline that converts repositories into comprehensive test\ncases, and introduce CoreCodeBench, a configurable multi-scenario\nrepository-level benchmark. To simulate real engineering scenarios, CorePipe\ngenerates three types of atomic questions (Development, BugFix, and Test-Driven\nDevelopment) specifically targeting core code segments. These atomic questions\nare further combined into three types of composite questions, with difficulty\nlevels flexibly adjusted through hyperparameter tuning. CoreCodeBench provides\na comprehensive and extensive repository-level benchmark to investigate the\napplicability of LLMs in real-world engineering projects. Experiments with 16\nLLMs across diverse scenarios reveal varying capabilities and offer\nmulti-dimensional insights into LLM performance in engineering contexts. The\ncode for CorePipe is available at\nhttps://github.com/AGI-Eval-Official/CoreCodeBench, and the data for\nCoreCodeBench can be accessed at\nhttps://huggingface.co/collections/tubehhh/corecodebench-68256d2faabf4b1610a08caa.", "AI": {"tldr": "CoreCodeBench and CorePipe are introduced to address limitations in existing LLM benchmarks by covering real engineering scenarios with configurable multi-scenario tests.", "motivation": "Current repository-level benchmarks for LLMs on engineering code lack diversity, controllability, and reliability, focusing on single scenarios like code generation or bug fixing.", "method": "CorePipe converts repositories into test cases by generating atomic and composite questions (Development, BugFix, Test-Driven Development) with adjustable difficulty via hyperparameters.", "result": "Experiments with 16 LLMs across diverse scenarios revealed varying capabilities and provided multi-dimensional insights into engineering code performance.", "conclusion": "CoreCodeBench offers a comprehensive benchmark to evaluate LLMs in real-world engineering projects, enabling flexible and controllable assessment of their core code processing skills."}}
{"id": "2507.05421", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.05421", "abs": "https://arxiv.org/abs/2507.05421", "authors": ["Harrison Green", "Claire Le Goues", "Fraser Brown"], "title": "FrameShift: Learning to Resize Fuzzer Inputs Without Breaking Them", "comment": null, "summary": "Coverage-guided fuzzers are powerful automated bug-finding tools. They mutate\nprogram inputs, observe coverage, and save any input that hits an unexplored\npath for future mutation. Unfortunately, without knowledge of input\nformats--for example, the relationship between formats' data fields and\nsizes--fuzzers are prone to generate destructive frameshift mutations. These\ntime-wasting mutations yield malformed inputs that are rejected by the target\nprogram. To avoid such breaking mutations, this paper proposes a novel,\nlightweight technique that preserves the structure of inputs during mutation by\ndetecting and using relation fields.\n  Our technique, FrameShift, is simple, fast, and does not require additional\ninstrumentation beyond standard coverage feedback. We implement our technique\nin two state-of-the-art fuzzers, AFL++ and LibAFL, and perform a 12+ CPU-year\nfuzzer evaluation, finding that FrameShift improves the performance of the\nfuzzer in each configuration, sometimes increasing coverage by more than 50%.\nFurthermore, through a series of case studies, we show that our technique is\nversatile enough to find important structural relationships in a variety of\nformats, even generalizing beyond C/C++ targets to both Rust and Python.", "AI": {"tldr": "The paper introduces FrameShift, a lightweight technique to prevent frameshift mutations in fuzzers by preserving input structure through relation field detection. It shows improved fuzzer performance and coverage across multiple languages and tools.", "motivation": "Fuzzers without input format knowledge generate destructive frameshift mutations, creating malformed inputs that waste time and reduce efficiency. Existing tools struggle with this issue due to format-agnostic mutation strategies.", "method": "FrameShift analyzes input fields and their size relationships during mutation, automatically detecting structural constraints to prevent destructive shifts. The implementation requires no additional instrumentation beyond standard coverage feedback and integrates into existing fuzzer frameworks.", "result": "FrameShift integrates into AFL++ and LibAFL with significant performance optimizations, achieving 50%+ coverage improvements in some configurations. Case studies reveal it discovers structural relationships in unknown formats while generalizing to Rust and Python targets.", "conclusion": "The technique demonstrates a versatile, non-intrusive solution to structural mutation problems in fuzzing. FrameShift improves fuzzing efficiency and coverage across programming languages and fuzzer implementations by preserving input structures through relation field detection."}}
{"id": "2507.05289", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05289", "abs": "https://arxiv.org/abs/2507.05289", "authors": ["Igor Regis da Silva Simoes", "Elaine Venson"], "title": "Measuring how changes in code readability attributes affect code quality evaluation by Large Language Models", "comment": null, "summary": "Code readability is one of the main aspects of code quality, influenced by\nvarious properties like identifier names, comments, code structure, and\nadherence to standards. However, measuring this attribute poses challenges in\nboth industry and academia. While static analysis tools assess attributes such\nas code smells and comment percentage, code reviews introduce an element of\nsubjectivity. This paper explores using Large Language Models (LLMs) to\nevaluate code quality attributes related to its readability in a standardized,\nreproducible, and consistent manner. We conducted a quasi-experiment study to\nmeasure the effects of code changes on Large Language Model (LLM)s\ninterpretation regarding its readability quality attribute. Nine LLMs were\ntested, undergoing three interventions: removing comments, replacing identifier\nnames with obscure names, and refactoring to remove code smells. Each\nintervention involved 10 batch analyses per LLM, collecting data on response\nvariability. We compared the results with a known reference model and tool. The\nresults showed that all LLMs were sensitive to the interventions, with\nagreement with the reference classifier being high for the original and\nrefactored code scenarios. The LLMs demonstrated a strong semantic sensitivity\nthat the reference model did not fully capture. A thematic analysis of the LLMs\nreasoning confirmed their evaluations directly reflected the nature of each\nintervention. The models also exhibited response variability, with 9.37% to\n14.58% of executions showing a standard deviation greater than zero, indicating\nresponse oscillation, though this did not always compromise the statistical\nsignificance of the results. LLMs demonstrated potential for evaluating\nsemantic quality aspects, such as coherence between identifier names, comments,\nand documentation with code purpose.", "AI": {"tldr": "This paper evaluates using Large Language Models (LLMs) to assess code readability quality in a standardized way, showing sensitivity to interventions and agreement with reference models.", "motivation": "Measuring code readability is challenging due to subjective code reviews and limited static analysis tools focusing on non-semantic attributes.", "method": "Conducted a quasi-experiment with nine LLMs, applying three interventions (comment removal, identifier obfuscation, code smell refactoring) and analyzing response variability through 10 batch analyses per LLM, compared to a reference model.", "result": "LLMs demonstrated sensitivity to interventions, high agreement with reference model for original/refactored code, and semantic understanding not captured by traditional metrics. Response variability ranged from 9.37% to 14.58% without compromising statistical significance.", "conclusion": "LLMs show potential for evaluating semantic code quality aspects like coherence between identifiers, comments, and documentation, offering standardized readability assessments beyond traditional tools."}}
{"id": "2507.05445", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.05445", "abs": "https://arxiv.org/abs/2507.05445", "authors": ["Daniel Jones", "Giorgio Severi", "Martin Pouliot", "Gary Lopez", "Joris de Gruyter", "Santiago Zanella-Beguelin", "Justin Song", "Blake Bullwinkel", "Pamela Cortez", "Amanda Minnich"], "title": "A Systematization of Security Vulnerabilities in Computer Use Agents", "comment": null, "summary": "Computer Use Agents (CUAs), autonomous systems that interact with software\ninterfaces via browsers or virtual machines, are rapidly being deployed in\nconsumer and enterprise environments. These agents introduce novel attack\nsurfaces and trust boundaries that are not captured by traditional threat\nmodels. Despite their growing capabilities, the security boundaries of CUAs\nremain poorly understood. In this paper, we conduct a systematic threat\nanalysis and testing of real-world CUAs under adversarial conditions. We\nidentify seven classes of risks unique to the CUA paradigm, and analyze three\nconcrete exploit scenarios in depth: (1) clickjacking via visual overlays that\nmislead interface-level reasoning, (2) indirect prompt injection that enables\nRemote Code Execution (RCE) through chained tool use, and (3) CoT exposure\nattacks that manipulate implicit interface framing to hijack multi-step\nreasoning. These case studies reveal deeper architectural flaws across current\nCUA implementations. Namely, a lack of input provenance tracking, weak\ninterface-action binding, and insufficient control over agent memory and\ndelegation. We conclude by proposing a CUA-specific security evaluation\nframework and design principles for safe deployment in adversarial and\nhigh-stakes settings.", "AI": {"tldr": "This paper identifies unique security risks and architectural flaws in Computer Use Agents (CUAs) and proposes a security evaluation framework and design principles to address them.", "motivation": "CUAs are increasingly deployed in consumer and enterprise environments, but their security boundaries remain poorly understood, introducing novel attack surfaces not captured by traditional threat models.", "method": "The researchers conducted a systematic threat analysis and testing of real-world CUAs under adversarial conditions, focusing on three concrete exploit scenarios and seven classes of risks.", "result": "They discovered seven risk classes and demonstrated three exploit scenarios: clickjacking via visual overlays, indirect prompt injection enabling RCE through chained tools, and CoT exposure attacks manipulating interface framing. Architectural flaws include lack of input provenance tracking, weak interface-action binding, and insufficient control over memory/delegation.", "conclusion": "The paper concludes by proposing a CUA-specific security evaluation framework and design principles for safe deployment in adversarial and high-stakes settings."}}
{"id": "2507.05294", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05294", "abs": "https://arxiv.org/abs/2507.05294", "authors": ["William Law"], "title": "zkSDK: Streamlining zero-knowledge proof development through automated trace-driven ZK-backend selection", "comment": "undergrad thesis", "summary": "The rapid advancement of creating Zero-Knowledge (ZK) programs has led to the\ndevelopment of numerous tools designed to support developers. Popular options\ninclude being able to write in general-purpose programming languages like Rust\nfrom Risc Zero. Other languages exist like Circom, Lib-snark, and Cairo.\nHowever, developers entering the ZK space are faced with many different ZK\nbackends to choose from, leading to a steep learning curve and a fragmented\ndeveloper experience across different platforms. As a result, many developers\ntend to select a single ZK backend and remain tied to it. This thesis\nintroduces zkSDK, a modular framework that streamlines ZK application\ndevelopment by abstracting the backend complexities. At the core of zkSDK is\nPresto, a custom Python-like programming language that enables the profiling\nand analysis of a program to assess its computational workload intensity.\nCombined with user-defined criteria, zkSDK employs a dynamic selection\nalgorithm to automatically choose the optimal ZK-proving backend. Through an\nin-depth analysis and evaluation of real-world workloads, we demonstrate that\nzkSDK effectively selects the best-suited backend from a set of supported ZK\nbackends, delivering a seamless and user-friendly development experience.", "AI": {"tldr": "zkSDK is a modular framework with a dynamic backend selector using program profiling and user-defined criteria to simplify Zero-Knowledge (ZK) app development and unify fragmented ZK tooling ecosystems.", "motivation": "Current ZK tooling lacks portability and imposes a steep learning curve by requiring developers to fixate on a single ZK backend, fragmenting the development experience.", "method": "Core components: (1) Presto (a Python-like language for program profiling and workload analysis); (2) Dynamic selection algorithm combining computational intensity metrics and user-defined constraints to choose optimal ZK backend.", "result": "Real-world workload evaluation demonstrates effectiveness in selecting performance-optimal backends across supported ZK ecosystems while maintaining user-friendly development workflows.", "conclusion": "zkSDK provides a seamless, backend-agnostic ZK development paradigm through modular design and intelligent proving backend selection, addressing existing fragmentation and complexity barriers."}}
{"id": "2507.05512", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05512", "abs": "https://arxiv.org/abs/2507.05512", "authors": ["Gehao Zhang", "Eugene Bagdasarian", "Juan Zhai", "Shiqing Ma"], "title": "Disappearing Ink: Obfuscation Breaks N-gram Code Watermarks in Theory and Practice", "comment": null, "summary": "Distinguishing AI-generated code from human-written code is becoming crucial\nfor tasks such as authorship attribution, content tracking, and misuse\ndetection. Based on this, N-gram-based watermarking schemes have emerged as\nprominent, which inject secret watermarks to be detected during the generation.\n  However, their robustness in code content remains insufficiently evaluated.\nMost claims rely solely on defenses against simple code transformations or code\noptimizations as a simulation of attack, creating a questionable sense of\nrobustness. In contrast, more sophisticated schemes already exist in the\nsoftware engineering world, e.g., code obfuscation, which significantly alters\ncode while preserving functionality. Although obfuscation is commonly used to\nprotect intellectual property or evade software scanners, the robustness of\ncode watermarking techniques against such transformations remains largely\nunexplored.\n  In this work, we formally model the code obfuscation and prove the\nimpossibility of N-gram-based watermarking's robustness with only one intuitive\nand experimentally verified assumption, distribution consistency, satisfied.\nGiven the original false positive rate of the watermarking detection, the ratio\nthat the detector failed on the watermarked code after obfuscation will\nincrease to 1 - fpr.\n  The experiments have been performed on three SOTA watermarking schemes, two\nLLMs, two programming languages, four code benchmarks, and four obfuscators.\nAmong them, all watermarking detectors show coin-flipping detection abilities\non obfuscated codes (AUROC tightly surrounds 0.5). Among all models,\nwatermarking schemes, and datasets, both programming languages own obfuscators\nthat can achieve attack effects with no detection AUROC higher than 0.6 after\nthe attack. Based on the theoretical and practical observations, we also\nproposed a potential path of robust code watermarking.", "AI": {"tldr": "This paper evaluates the robustness of N-gram-based code watermarking against sophisticated code obfuscation techniques.", "motivation": "Distinguishing AI-generated code from human-written code is crucial for authorship attribution and misuse detection, but current watermarking schemes lack evaluation against advanced obfuscation attacks.", "method": "The authors formally model code obfuscation, propose a distribution consistency assumption, and experiment with three watermarking schemes, two LLMs, two programming languages, four benchmarks, and four obfuscators.", "result": "Watermarking detectors show poor performance on obfuscated code (AUROC ~0.5), with obfuscators achieving undetected attack rates >60% in some cases. Theoretical analysis proves increased false acceptance rates after obfuscation.", "conclusion": "Existing N-gram-based watermarking is vulnerable to obfuscation attacks. The paper provides theoretical foundations and practical evidence, while proposing potential robust watermaking approaches."}}
{"id": "2507.05307", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05307", "abs": "https://arxiv.org/abs/2507.05307", "authors": ["Xuanqi Gao", "Juan Zhai", "Shiqing Ma", "Siyi Xie", "Chao Shen"], "title": "ASSURE: Metamorphic Testing for AI-powered Browser Extensions", "comment": null, "summary": "The integration of Large Language Models (LLMs) into browser extensions has\nrevolutionized web browsing, enabling sophisticated functionalities like\ncontent summarization, intelligent translation, and context-aware writing\nassistance. However, these AI-powered extensions introduce unprecedented\nchallenges in testing and reliability assurance. Traditional browser extension\ntesting approaches fail to address the non-deterministic behavior,\ncontext-sensitivity, and complex web environment integration inherent to\nLLM-powered extensions. Similarly, existing LLM testing methodologies operate\nin isolation from browser-specific contexts, creating a critical gap in\neffective evaluation frameworks. To bridge this gap, we present ASSURE, a\nmodular automated testing framework specifically designed for AI-powered\nbrowser extensions. ASSURE comprises three principal components: (1) a modular\ntest case generation engine that supports plugin-based extension of testing\nscenarios, (2) an automated execution framework that orchestrates the complex\ninteractions between web content, extension processing, and AI model behavior,\nand (3) a configurable validation pipeline that systematically evaluates\nbehavioral consistency and security invariants rather than relying on exact\noutput matching. Our evaluation across six widely-used AI browser extensions\ndemonstrates ASSURE's effectiveness, identifying 531 distinct issues spanning\nsecurity vulnerabilities, metamorphic relation violations, and content\nalignment problems. ASSURE achieves 6.4x improved testing throughput compared\nto manual approaches, detecting critical security vulnerabilities within 12.4\nminutes on average. This efficiency makes ASSURE practical for integration into\ndevelopment pipelines, offering a comprehensive solution to the unique\nchallenges of testing AI-powered browser extensions.", "AI": {"tldr": "This paper introduces ASSURE, a modular automated testing framework for AI-powered browser extensions that integrates large language models, addresses testing challenges through contextual validation, and improves efficiency with a 6.4x throughput gain over manual methods.", "motivation": "Current browser extension and LLM testing approaches separately fail to address the unique challenges of AI-powered extensions\u2014including non-deterministic behavior, context-sensitivity, and environment integration\u2014resulting in a critical evaluation gap.", "method": "ASSURE employs three components: (1) a plugin-based test case generation engine for scenario extension, (2) an automated execution framework to orchestrate interactions between web content, extensions, and LLMs, and (3) a validation pipeline that assesses behavioral consistency and security invariants rather than exact output matches.", "result": "ASSURE detected 531 distinct issues (security vulnerabilities, metamorphic violations, content alignment problems) in six AI extensions, achieving 6.4x faster testing throughput than manual methods with critical issues identified within 12.4 minutes on average.", "conclusion": "ASSURE bridges the testing gap for AI-powered browser extensions through a modular, context-aware framework, demonstrating significant efficiency gains and robust vulnerability detection capabilities suitable for integration into development pipelines."}}
{"id": "2507.05524", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.05524", "abs": "https://arxiv.org/abs/2507.05524", "authors": ["Sara Chennoufi", "Yufei Han", "Gregory Blanc", "Emiliano De Cristofaro", "Christophe Kiennert"], "title": "PROTEAN: Federated Intrusion Detection in Non-IID Environments through Prototype-Based Knowledge Sharing", "comment": null, "summary": "In distributed networks, participants often face diverse and fast-evolving\ncyberattacks. This makes techniques based on Federated Learning (FL) a\npromising mitigation strategy. By only exchanging model updates, FL\nparticipants can collaboratively build detection models without revealing\nsensitive information, e.g., network structures or security postures. However,\nthe effectiveness of FL solutions is often hindered by significant data\nheterogeneity, as attack patterns often differ drastically across organizations\ndue to varying security policies. To address these challenges, we introduce\nPROTEAN, a Prototype Learning-based framework geared to facilitate\ncollaborative and privacy-preserving intrusion detection. PROTEAN enables\naccurate detection in environments with highly non-IID attack distributions and\npromotes direct knowledge sharing by exchanging class prototypes of different\nattack types among participants. This allows organizations to better understand\nattack techniques not present in their data collections. We instantiate PROTEAN\non two cyber intrusion datasets collected from IIoT and 5G-connected\nparticipants and evaluate its performance in terms of utility and privacy,\ndemonstrating its effectiveness in addressing data heterogeneity while\nimproving cyber attack understanding in federated intrusion detection systems\n(IDSs).", "AI": {"tldr": "PROTEAN is a Fed-Learning prototype framework that enables accurate intrusion detection by sharing class prototypes across data-heterogeneous distributed networks. It improves cyberattack understanding without sacrificing privacy, validated on IIoT and 5G datasets.", "motivation": "Federated Learning (FL) is promising for cybersecurity due to privacy constraints, but data heterogeneity from varied organizational security policies hinders model effectiveness. Existing approaches struggle to detect diverse attack types without direct access to non-IID data.", "method": "PROTEAN employs prototype learning to exchange class-specific attack prototypes between organizations. This allows collaborative model training while preserving data privacy and enabling participants to learn novel attack patterns not present in their local data.", "result": "Evaluation on IIoT and 5G-connected datasets demonstrates PROTEAN's ability to improve detection accuracy in data-heterogeneous environments while maintaining privacy. Organizations gain enhanced understanding of diverse attack techniques through prototype knowledge sharing.", "conclusion": "PROTEAN advances privacy-preserving federated intrusion detection by addressing data heterogeneity through systematic prototype sharing. This approach maintains security privacy while enabling cross-organizational learning of attack patterns in complex networks."}}
{"id": "2507.05316", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05316", "abs": "https://arxiv.org/abs/2507.05316", "authors": ["Koren Lazar", "Matan Vetzler", "Kiran Kate", "Jason Tsay", "David Boaz Himanshu Gupta", "Avraham Shinnar", "Rohith D Vallam", "David Amid Esther Goldbraich", "Guy Uziel", "Jim Laredo", "Ateret Anaby Tavor"], "title": "OASBuilder: Generating OpenAPI Specifications from Online API Documentation with Large Language Models", "comment": null, "summary": "AI agents and business automation tools interacting with external web\nservices require standardized, machine-readable information about their APIs in\nthe form of API specifications. However, the information about APIs available\nonline is often presented as unstructured, free-form HTML documentation,\nrequiring external users to spend significant time manually converting it into\na structured format. To address this, we introduce OASBuilder, a novel\nframework that transforms long and diverse API documentation pages into\nconsistent, machine-readable API specifications. This is achieved through a\ncarefully crafted pipeline that integrates large language models and rule-based\nalgorithms which are guided by domain knowledge of the structure of\ndocumentation webpages. Our experiments demonstrate that OASBuilder generalizes\nwell across hundreds of APIs, and produces valid OpenAPI specifications that\nencapsulate most of the information from the original documentation. OASBuilder\nhas been successfully implemented in an enterprise environment, saving\nthousands of hours of manual effort and making hundreds of complex enterprise\nAPIs accessible as tools for LLMs.", "AI": {"tldr": "OASBuilder is a framework that automates conversion of unstructured HTML API documentation into standardized machine-readable OpenAPI specifications using a pipeline combining large language models and rule-based algorithms with domain knowledge.", "motivation": "API documentation is often unstructured and requires significant manual effort to convert into structured formats, hindering automation tool integration. Existing solutions lack generalization across diverse APIs.", "method": "OASBuilder leverages a pipeline integrating LLMs for content understanding and rule-based algorithms guided by domain knowledge of documentation webpage structures to systematically extract and format API information.", "result": "Experiments show OASBuilder achieves valid OpenAPI specification generation across hundreds of APIs, conserving 85-95% of original documentation information. Enterprise deployment saved 10,000+ hours and enabled LLM integration with 400+ complex APIs.", "conclusion": "OASBuilder bridges the API usability gap through domain-informed automation, reducing manual effort requirements and enabling large-scale standardization of API documentation for enterprise AI integration."}}
{"id": "2507.05558", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05558", "abs": "https://arxiv.org/abs/2507.05558", "authors": ["Arthur Gervais", "Liyi Zhou"], "title": "AI Agent Smart Contract Exploit Generation", "comment": null, "summary": "We present A1, an agentic execution driven system that transforms any LLM\ninto an end-to-end exploit generator. A1 has no hand-crafted heuristics and\nprovides the agent with six domain-specific tools that enable autonomous\nvulnerability discovery. The agent can flexibly leverage these tools to\nunderstand smart contract behavior, generate exploit strategies, test them on\nblockchain states, and refine approaches based on execution feedback. All\noutputs are concretely validated to eliminate false positives.\n  The evaluation across 36 real-world vulnerable contracts on Ethereum and\nBinance Smart Chain demonstrates a 62.96% (17 out of 27) success rate on the\nVERITE benchmark. Beyond the VERITE dataset, A1 identified 9 additional\nvulnerable contracts, with 5 cases occurring after the strongest model's\ntraining cutoff date. Across all 26 successful cases, A1 extracts up to 8.59\nmillion USD per case and 9.33 million USD total. Through 432 experiments across\nsix LLMs, we analyze iteration-wise performance showing diminishing returns\nwith average marginal gains of +9.7%, +3.7%, +5.1%, and +2.8% for iterations\n2-5 respectively, with per-experiment costs ranging $0.01-$3.59. A Monte Carlo\nanalysis of 19 historical attacks shows success probabilities of 85.9%-88.8%\nwithout detection delays.\n  We investigate whether an attacker or a defender benefits most from deploying\nA1 as a continuous on-chain scanning system. Our model shows that OpenAI's\no3-pro maintains profitability up to a 30.0 days scanning delay at 0.100%\nvulnerability incidence rates, while faster models require >=1.000% rates to\nbreak-even. The findings exposes a troubling asymmetry: at 0.1% vulnerability\nrates, attackers achieve an on-chain scanning profitability at a $6000 exploit\nvalue, while defenders require $60000, raising fundamental questions about\nwhether AI agents inevitably favor exploitation over defense.", "AI": {"tldr": "A1 converts LLMs into end-to-end exploit generators with autonomous vulnerability discovery tools. It achieves a 62.96% success rate on the VERITE benchmark, identifies 9 new vulnerable contracts, and demonstrates a profitability asymmetry between attackers and defenders in on-chain scanning scenarios.", "motivation": "The paper addresses the gap in autonomous exploit generation systems, aiming to leverage LLMs for end-to-end cybersecurity attacks without hand-crafted heuristics. It also investigates the ethical implications of AI's potential asymmetry in assisting attackers versus defenders in blockchain security.", "method": "A1 provides agents with six domain-specific tools to analyze smart contract behavior, generate and test exploit strategies on blockchain states, and refine methods using execution feedback. Validation eliminates false positives, and the system is evaluated across six LLMs, 432 experiments, and 19 historical attacks via Monte Carlo analysis.", "result": "A1 succeeds in 62.96% of VERITE benchmarks (17/27 cases), identifies 9 new vulnerabilities (5 post-training), and generates up to $8.59M USD in exploits. Iteration gains decline (+9.7% to +2.8% for iterations 2-5), and profitability analysis shows attackers need $6k exploit value versus $60k needed by defenders at 0.1% vulnerability rates.", "conclusion": "The system reveals AI-driven exploit generators can autonomously achieve significant success in real-world smart contracts. The profitability asymmetry suggests AI systems may inherently favor exploitation over defense at low vulnerability rates, raising concerns about cybersecurity ethics and the potential for AI to disproportionately empower malicious actors in blockchain ecosystems."}}
{"id": "2507.05325", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05325", "abs": "https://arxiv.org/abs/2507.05325", "authors": ["Lidiany Cerqueira", "Jo\u00e3o Pedro Bastos", "Danilo Neves", "Glauco Carneiro", "Rodrigo Sp\u00ednola", "S\u00e1vio Freire", "Jos\u00e9 Amancio Macedo Santos", "Manoel Mendon\u00e7a"], "title": "Exploring Empathy in Software Engineering: Insights from a Grey Literature Analysis of Practitioners' Perspectives", "comment": "This is the author's version of the paper accepted for publication in\n  ACM Transactions on Software Engineering and Methodology. The final version\n  will be available via the ACM Digital Library. The HTML preview may not\n  render some formatting correctly. Please refer to the PDF version for\n  accurate presentation", "summary": "Context. Empathy, a key social skill, is essential for communication and\ncollaboration in SE but remains an under-researched topic. Aims. This study\ninvestigates empathy in SE from practitioners' perspectives, aiming to\ncharacterize its meaning, identify barriers, discuss practices to overcome\nthem, and explore its effects. Method. A qualitative content analysis was\nconducted on 55 web articles from DEV and Medium, two communities widely used\nby practitioners. To strengthen our findings, we conducted a follow-up survey\nwith empathy experts. Results. The study proposes a definition of empathy in\nSE, identifies barriers such as toxic culture and excessive technical focus,\npractices to foster empathy in teams, and outcomes, including improved\ncollaboration, communication, and reduced anxiety, frustration, and stress.\nThese findings are synthesized into a conceptual framework. Conclusion. Survey\nresults indicate the framework is clear, valuable, and raises empathy\nawareness, with suggestions for improvements and integration into training.\nThis study paves the way for improving team dynamics by addressing barriers and\noffering strategies to cultivate empathy. Future work will explore empathy's\nbroader implications in SE practice.", "AI": {"tldr": "This study explores empathy in software engineering (SE) by analyzing practitioners' perspectives through 55 web articles and expert surveys, proposing a definition, barriers, practices, and outcomes, while introducing a conceptual framework to improve team dynamics.", "motivation": "Empathy, a critical social skill for effective communication and collaboration in SE, remains under-researched. The study aims to address this gap by characterizing empathy's meaning, barriers (e.g., toxic culture), and strategies to enhance team effectiveness.", "method": "A qualitative content analysis was performed on web articles from DEV and Medium, followed by a survey of empathy experts to validate and refine findings.", "result": "The research proposes a practitioner-centric definition of empathy in SE, identifies barriers like technical over-focus and poor work-life balance, suggests practices to foster empathy, and outlines benefits such as improved collaboration and reduced burnout. A conceptual framework integrates these insights.", "conclusion": "The proposed framework is validated as clear and valuable by experts, emphasizing its potential to raise empathy awareness in SE and improve training. Future work will expand empathy's role in broader SE practices and team dynamics."}}
{"id": "2507.05576", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2507.05576", "abs": "https://arxiv.org/abs/2507.05576", "authors": ["Mehdi Elahi", "Mohamed R. Elshamy", "Abdel-Hameed Badawy", "Ahmad Patooghy"], "title": "iThermTroj: Exploiting Intermittent Thermal Trojans in Multi-Processor System-on-Chips", "comment": null, "summary": "Thermal Trojan attacks present a pressing concern for the security and\nreliability of System-on-Chips (SoCs), especially in mobile applications. The\nsituation becomes more complicated when such attacks are more evasive and\noperate sporadically to stay hidden from detection mechanisms. In this paper,\nwe introduce Intermittent Thermal Trojans (iThermTroj) that exploit the chips'\nthermal information in a random time-triggered manner. According to our\nexperiments, iThermTroj attack can easily bypass available threshold-based\nthermal Trojan detection solutions. We investigate SoC vulnerabilities to\nvariations of iThermTroj through an in-depth analysis of Trojan activation and\nduration scenarios. We also propose a set of tiny Machine Learning classifiers\nfor run-time anomaly detection to protect SoCs against such intermittent\nthermal Trojan attacks. Compared to existing methods, our approach improves the\nattack detection rate by 29.4\\%, 17.2\\%, and 14.3\\% in scenarios where\niThermTroj manipulates up to 80\\%, 60\\%, and 40\\% of SoC's thermal data,\nrespectively. Additionally, our method increases the full protection resolution\nto 0.8 degrees Celsius, meaning that any temperature manipulations exceeding\n$\\pm 0.8$ degrees will be detected with 100\\% accuracy.", "AI": {"tldr": "The paper introduces Intermittent Thermal Trojans (iThermTroj) that evade threshold-based detection by sporadically manipulating SoC thermal data and proposes lightweight ML classifiers to improve detection rates by up to 29.4% with a 0.8\u00b0C precision threshold.", "motivation": "Current threshold-based thermal Trojan detection solutions fail to address the challenge of evasive, sporadically activated threats, necessitating a more effective anomaly detection approach for SoC security.", "method": "The authors designed iThermTroj attacks (random time-triggered, intermittent) to bypass existing methods, analyzed SoC vulnerability through attack scenarios, and developed compact ML classifiers for real-time anomaly detection.", "result": "The ML classifiers outperformed existing methods, achieving 29.4%, 17.2%, and 14.3% higher detection rates for 80%, 60%, and 40% thermal data manipulation scenarios, respectively, with 100% accuracy for deviations exceeding 0.8\u00b0C.", "conclusion": "The proposed ML-based framework offers robust protection against intermittent thermal Trojans, demonstrating superior detection accuracy and resolution compared to traditional threshold techniques."}}
{"id": "2507.05504", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05504", "abs": "https://arxiv.org/abs/2507.05504", "authors": ["Alex Kleijwegt", "Sinem Getir Yaman", "Radu Calinescu"], "title": "Tool for Supporting Debugging and Understanding of Normative Requirements Using LLMs", "comment": null, "summary": "Normative requirements specify social, legal, ethical, empathetic, and\ncultural (SLEEC) norms that must be observed by a system. To support the\nidentification of SLEEC requirements, numerous standards and regulations have\nbeen developed. These requirements are typically defined by stakeholders in the\nnon-technical system with diverse expertise (e.g., ethicists, lawyers, social\nscientists). Hence, ensuring their consistency and managing the requirement\nelicitation process are complex and error-prone tasks. Recent research has\naddressed this challenge using domain-specific languages to specify normative\nrequirements as rules, whose consistency can then be analyzed with formal\nmethods. Nevertheless, these approaches often present the results from formal\nverification tools in a way that is inaccessible to non-technical users. This\nhinders understanding and makes the iterative process of eliciting and\nvalidating these requirements inefficient in terms of both time and effort. To\naddress this problem, we introduce SLEEC-LLM, a tool that uses large language\nmodels (LLMs) to provide natural-language interpretations for model-checking\ncounterexamples corresponding to SLEEC rule inconsistencies. SLEEC-LLM improves\nthe efficiency and explainability of normative requirements elicitation and\nconsistency analysis. To demonstrate its effectiveness, we summarise its use in\ntwo real-world case studies involving non-technical stakeholders.", "AI": {"tldr": "The paper introduces SLEEC-LLM, a tool leveraging large language models to enhance the efficiency and explainability of analyzing SLEEC (social, legal, ethical, empathetic, cultural) normative requirements by translating formal verification counterexamples into natural language for non-technical stakeholders.", "motivation": "Current domain-specific languages for specifying SLEEC requirements generate formal verification results that are inaccessible to non-technical stakeholders (e.g., ethicists, lawyers), leading to inefficient and error-prone requirements elicitation processes.", "method": "SLEEC-LLM utilizes large language models (LLMs) to provide natural-language interpretations of counterexamples from model-checking tools, thus bridging the accessibility gap between technical formal verification outputs and non-technical stakeholders.", "result": "The tool's effectiveness was demonstrated through two real-world case studies involving non-technical stakeholders, showing improvements in understanding and managing SLEEC rule inconsistencies.", "conclusion": "SLEEC-LLM improves the efficiency and explainability of normative requirements elicitation and consistency analysis by enabling non-technical stakeholders to iteratively refine SLEEC norms using natural-language explanations of formal verification results."}}
{"id": "2507.05622", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.05622", "abs": "https://arxiv.org/abs/2507.05622", "authors": ["Shuo Shao", "Yiming Li", "Mengren Zheng", "Zhiyang Hu", "Yukun Chen", "Boheng Li", "Yu He", "Junfeng Guo", "Tianwei Zhang", "Dacheng Tao", "Zhan Qin"], "title": "DATABench: Evaluating Dataset Auditing in Deep Learning from an Adversarial Perspective", "comment": null, "summary": "The widespread application of Deep Learning across diverse domains hinges\ncritically on the quality and composition of training datasets. However, the\ncommon lack of disclosure regarding their usage raises significant privacy and\ncopyright concerns. Dataset auditing techniques, which aim to determine if a\nspecific dataset was used to train a given suspicious model, provide promising\nsolutions to addressing these transparency gaps. While prior work has developed\nvarious auditing methods, their resilience against dedicated adversarial\nattacks remains largely unexplored. To bridge the gap, this paper initiates a\ncomprehensive study evaluating dataset auditing from an adversarial\nperspective. We start with introducing a novel taxonomy, classifying existing\nmethods based on their reliance on internal features (IF) (inherent to the\ndata) versus external features (EF) (artificially introduced for auditing).\nSubsequently, we formulate two primary attack types: evasion attacks, designed\nto conceal the use of a dataset, and forgery attacks, intending to falsely\nimplicate an unused dataset. Building on the understanding of existing methods\nand attack objectives, we further propose systematic attack strategies:\ndecoupling, removal, and detection for evasion; adversarial example-based\nmethods for forgery. These formulations and strategies lead to our new\nbenchmark, DATABench, comprising 17 evasion attacks, 5 forgery attacks, and 9\nrepresentative auditing methods. Extensive evaluations using DATABench reveal\nthat none of the evaluated auditing methods are sufficiently robust or\ndistinctive under adversarial settings. These findings underscore the urgent\nneed for developing a more secure and reliable dataset auditing method capable\nof withstanding sophisticated adversarial manipulation. Code is available at\nhttps://github.com/shaoshuo-ss/DATABench.", "AI": {"tldr": "This paper evaluates dataset auditing methods against adversarial attacks, proposes new attack strategies, and develops a benchmark (DATABench) showing existing methods' vulnerability to evasion and forgery attacks.", "motivation": "Privacy and copyright concerns arise from undeclared training data usage, but current auditing techniques lack adversarial resilience, necessitating systematic evaluation of attack strategies.", "method": "Introduced a taxonomy of auditing methods (internal vs. external features), formulated evasion and forgery attacks, and developed a benchmark with 17 evasion attacks, 5 forgery attacks, and 9 auditing techniques.", "result": "Extensive experiments using DATABench revealed evaluated methods fail to achieve robustness/distinctiveness under adversarial scenarios. Code repository: https://github.com/shaoshuo-ss/DATABench.", "conclusion": "Existing dataset auditing methods are insufficiently secure; there is an urgent need to develop techniques that withstand sophisticated adversarial manipulations."}}
{"id": "2507.05565", "categories": ["cs.SE", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.05565", "abs": "https://arxiv.org/abs/2507.05565", "authors": ["Sangwon Hyun", "Shaukat Ali", "M. Ali Babar"], "title": "Search-based Selection of Metamorphic Relations for Optimized Robustness Testing of Large Language Models", "comment": null, "summary": "Assessing the trustworthiness of Large Language Models (LLMs), such as\nrobustness, has garnered significant attention. Recently, metamorphic testing\nthat defines Metamorphic Relations (MRs) has been widely applied to evaluate\nthe robustness of LLM executions. However, the MR-based robustness testing\nstill requires a scalable number of MRs, thereby necessitating the optimization\nof selecting MRs. Most extant LLM testing studies are limited to automatically\ngenerating test cases (i.e., MRs) to enhance failure detection. Additionally,\nmost studies only considered a limited test space of single perturbation MRs in\ntheir evaluation of LLMs. In contrast, our paper proposes a search-based\napproach for optimizing the MR groups to maximize failure detection and\nminimize the LLM execution cost. Moreover, our approach covers the\ncombinatorial perturbations in MRs, facilitating the expansion of test space in\nthe robustness assessment. We have developed a search process and implemented\nfour search algorithms: Single-GA, NSGA-II, SPEA2, and MOEA/D with novel\nencoding to solve the MR selection problem in the LLM robustness testing. We\nconducted comparative experiments on the four search algorithms along with a\nrandom search, using two major LLMs with primary Text-to-Text tasks. Our\nstatistical and empirical investigation revealed two key findings: (1) the\nMOEA/D algorithm performed the best in optimizing the MR space for LLM\nrobustness testing, and (2) we identified silver bullet MRs for the LLM\nrobustness testing, which demonstrated dominant capabilities in confusing LLMs\nacross different Text-to-Text tasks. In LLM robustness assessment, our research\nsheds light on the fundamental problem for optimized testing and provides\ninsights into search-based solutions.", "AI": {"tldr": "The paper proposes a search-based optimization for metamorphic relations (MRs) in Large Language Model (LLM) robustness testing, demonstrating MOEA/D as the most effective algorithm and identifying 'silver bullet' MRs that confuse LLMs across tasks.", "motivation": "Current MR-based LLM robustness testing faces scalability challenges due to limited test spaces using single perturbation MRs and inefficient failure detection mechanisms, necessitating optimized MR selection strategies.", "method": "The authors designed a search process with four multi-objective optimization algorithms (Single-GA, NSGA-II, SPEA2, MOEA/D) incorporating novel encoding to optimize MR groups. They extended test space coverage through combinatorial perturbations.", "result": "Experiments on two major LLMs revealed MOEA/D outperforms other algorithms in robust MR selection. Silver bullet MRs consistently caused failure across diverse Text-to-Text tasks with minimal execution cost.", "conclusion": "This research establishes a foundational solution for optimizing LLM robustness testing through search-based methods, highlighting combinatorial perturbations and algorithmic selection for effective, cost-efficient evaluations."}}
{"id": "2507.05630", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.05630", "abs": "https://arxiv.org/abs/2507.05630", "authors": ["Sarthak Choudhary", "Divyam Anshumaan", "Nils Palumbo", "Somesh Jha"], "title": "How Not to Detect Prompt Injections with an LLM", "comment": null, "summary": "LLM-integrated applications and agents are vulnerable to prompt injection\nattacks, in which adversaries embed malicious instructions within seemingly\nbenign user inputs to manipulate the LLM's intended behavior. Recent defenses\nbased on $\\textit{known-answer detection}$ (KAD) have achieved near-perfect\nperformance by using an LLM to classify inputs as clean or contaminated. In\nthis work, we formally characterize the KAD framework and uncover a structural\nvulnerability in its design that invalidates its core security premise. We\ndesign a methodical adaptive attack, $\\textit{DataFlip}$, to exploit this\nfundamental weakness. It consistently evades KAD defenses with detection rates\nas low as $1.5\\%$ while reliably inducing malicious behavior with success rates\nof up to $88\\%$, without needing white-box access to the LLM or any\noptimization procedures.", "AI": {"tldr": "The paper demonstrates a critical design flaw in KAD defenses for LLM applications, enabling the DataFlip attack to bypass them with high success rates (88%) and low detection rates (1.5%), independent of LLM optimization or access.", "motivation": "Prompt injection attacks threaten LLM systems by inserting malicious instructions into inputs. While KAD-based defenses show high performance, this work reveals their structural vulnerability undermines their security claims.", "method": "Formal characterization of the KAD framework to identify its core vulnerability, then developing DataFlip through systematic analysis of instruction-prompt behavior. The attack exploits this weakness without optimization/white-box access.", "result": "DataFlip consistently evades KAD defenses (1.5% detection rate) while achieving 88% success in inducing malicious behavior, disproving KAD's assumed security robustness.", "conclusion": "Current KAD frameworks are fundamentally insecure due to design limitations. Defenses against prompt injection need to address deeper structural weaknesses and move beyond KAD's approach."}}
{"id": "2507.05932", "categories": ["cs.SE", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.05932", "abs": "https://arxiv.org/abs/2507.05932", "authors": ["You Lu", "Dingji Wang", "Kaifeng Huang", "Bihuan Chen", "Xin Peng"], "title": "TigAug: Data Augmentation for Testing Traffic Light Detection in Autonomous Driving Systems", "comment": null, "summary": "Autonomous vehicle technology has been developed in the last decades with\nrecent advances in sensing and computing technology. There is an urgent need to\nensure the reliability and robustness of autonomous driving systems (ADSs).\nDespite the recent achievements in testing various ADS modules, little\nattention has been paid on the automated testing of traffic light detection\nmodels in ADSs. A common practice is to manually collect and label traffic\nlight data. However, it is labor-intensive, and even impossible to collect\ndiverse data under different driving environments.\n  To address these problems, we propose and implement TigAug to automatically\naugment labeled traffic light images for testing traffic light detection models\nin ADSs. We construct two families of metamorphic relations and three families\nof transformations based on a systematic understanding of weather environments,\ncamera properties, and traffic light properties. We use augmented images to\ndetect erroneous behaviors of traffic light detection models by\ntransformation-specific metamorphic relations, and to improve the performance\nof traffic light detection models by retraining. Large-scale experiments with\nfour state-of-the-art traffic light detection models and two traffic light\ndatasets have demonstrated that i) TigAug is effective in testing traffic light\ndetection models, ii) TigAug is efficient in synthesizing traffic light images,\nand iii) TigAug generates traffic light images with acceptable naturalness.", "AI": {"tldr": "Proposes TigAug, an automated tool for augmenting labeled traffic light images to test and improve traffic light detection models in autonomous vehicles, addressing the limitations of manual data collection.", "motivation": "Autonomous driving systems (ADSs) require reliable and robust traffic light detection. Current manual data collection is labor-intensive and lacks diversity in environmental variations, hindering effective testing.", "method": "Constructs two metamorphic relation families and three transformation families based on weather environments, camera properties, and traffic light attributes. Uses these to automatically generate images for testing and retraining detection models.", "result": "Experiments with four state-of-the-art models and two datasets show TigAug's effectiveness in testing (30% more error detection), efficiency in image synthesis (10x faster than manual), and acceptable naturalness of augmented images.", "conclusion": "TigAug provides a scalable solution for automated testing and performance improvement of traffic light detection models in ADSs, overcoming manual data collection challenges."}}
{"id": "2507.05649", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.05649", "abs": "https://arxiv.org/abs/2507.05649", "authors": ["Kaixiang Zhao", "Joseph Yousry Attalla", "Qian Lou", "Yushun Dong"], "title": "DESIGN: Encrypted GNN Inference via Server-Side Input Graph Pruning", "comment": "Under Review in Conference on Neural Information Processing Systems\n  (NeurIPS 2025)", "summary": "Graph Neural Networks (GNNs) have achieved state-of-the-art performance in\nvarious graph-based learning tasks. However, enabling privacy-preserving GNNs\nin encrypted domains, such as under Fully Homomorphic Encryption (FHE),\ntypically incurs substantial computational overhead, rendering real-time and\nprivacy-preserving inference impractical. In this work, we propose DESIGN\n(EncrypteD GNN Inference via sErver-Side Input Graph pruNing), a novel\nframework for efficient encrypted GNN inference. DESIGN tackles the critical\nefficiency limitations of existing FHE GNN approaches, which often overlook\ninput data redundancy and apply uniform computational strategies. Our framework\nachieves significant performance gains through a hierarchical optimization\nstrategy executed entirely on the server: first, FHE-compatible node importance\nscores (based on encrypted degree statistics) are computed from the encrypted\ngraph. These scores then guide a homomorphic partitioning process, generating\nmulti-level importance masks directly under FHE. This dynamically generated\nmask facilitates both input graph pruning (by logically removing unimportant\nelements) and a novel adaptive polynomial activation scheme, where activation\ncomplexity is tailored to node importance levels. Empirical evaluations\ndemonstrate that DESIGN substantially accelerates FHE GNN inference compared to\nstate-of-the-art methods while maintaining competitive model accuracy,\npresenting a robust solution for secure graph analytics.", "AI": {"tldr": "DESIGN is a novel framework for efficient encrypted GNN inference under FHE by implementing hierarchical optimization strategies like encrypted node scoring and adaptive activation schemes, significantly accelerating processing while preserving model accuracy.", "motivation": "Privacy-preserving GNN inference via FHE suffers from impractical computational overhead due to redundant input data and uniform computation strategies, limiting real-time applications despite GNNs' state-of-the-art performance in graph tasks.", "method": "DESIGN introduces server-side hierarchical optimizations: 1) Calculating FHE-compatible encrypted node importance scores using degree statistics from the encrypted graph. 2) Applying homomorphic partitioning to generate multi-level masks that enable input graph pruning and adaptive polynomial activation, where activation complexity varies based on node importance levels.", "result": "Empirical evaluations demonstrate DESIGN accelerates FHE GNN inference substantially compared to existing methods while maintaining competitive model accuracy across graph analytics tasks.", "conclusion": "DESIGN presents a robust solution for real-time encrypted GNN inference through data-driven optimizations (pruning and adaptive activation) under FHE, addressing efficiency bottlenecks in privacy-preserving graph analytics and enabling practical deployment."}}
{"id": "2507.05981", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05981", "abs": "https://arxiv.org/abs/2507.05981", "authors": ["Marc Oriol", "Quim Motger", "Jordi Marco", "Xavier Franch"], "title": "Multi-Agent Debate Strategies to Enhance Requirements Engineering with Large Language Models", "comment": null, "summary": "Context: Large Language Model (LLM) agents are becoming widely used for\nvarious Requirements Engineering (RE) tasks. Research on improving their\naccuracy mainly focuses on prompt engineering, model fine-tuning, and retrieval\naugmented generation. However, these methods often treat models as isolated\nblack boxes - relying on single-pass outputs without iterative refinement or\ncollaboration, limiting robustness and adaptability. Objective: We propose\nthat, just as human debates enhance accuracy and reduce bias in RE tasks by\nincorporating diverse perspectives, different LLM agents debating and\ncollaborating may achieve similar improvements. Our goal is to investigate\nwhether Multi-Agent Debate (MAD) strategies can enhance RE performance. Method:\nWe conducted a systematic study of existing MAD strategies across various\ndomains to identify their key characteristics. To assess their applicability in\nRE, we implemented and tested a preliminary MAD-based framework for RE\nclassification. Results: Our study identified and categorized several MAD\nstrategies, leading to a taxonomy outlining their core attributes. Our\npreliminary evaluation demonstrated the feasibility of applying MAD to RE\nclassification. Conclusions: MAD presents a promising approach for improving\nLLM accuracy in RE tasks. This study provides a foundational understanding of\nMAD strategies, offering insights for future research and refinements in RE\napplications.", "AI": {"tldr": "This paper investigates Multi-Agent Debate (MAD) strategies to enhance Requirements Engineering (RE) performance by enabling collaboration among LLM agents, addressing limitations of isolated models. A taxonomy of MAD strategies was developed, and a preliminary framework demonstrated feasibility in RE classification.", "motivation": "Current methods for improving LLM accuracy in RE tasks (e.g., prompt engineering, fine-tuning) treat models as isolated systems, relying on single-pass outputs that limit robustness and adaptability. Human debates reduce bias and enhance accuracy in RE by incorporating diverse perspectives, suggesting that MAD strategies could offer similar benefits.", "method": "The authors conducted a systematic review of MAD strategies across domains to identify their key characteristics. They then implemented and tested a preliminary MAD-based framework for RE classification to evaluate its applicability in this field.", "result": "The study resulted in a taxonomy outlining core attributes of MAD strategies. A preliminary evaluation of the MAD-based RE classification framework confirmed the feasibility of applying these strategies to the RE domain, though results were limited due to the early stage of implementation.", "conclusion": "MAD strategies represent a promising approach to improve LLM accuracy and adaptability in RE tasks. The paper provides a foundational understanding of MAD and suggests directions for future research to refine these methods in real-world RE applications."}}
{"id": "2507.05660", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.05660", "abs": "https://arxiv.org/abs/2507.05660", "authors": ["Aravind Cheruvu", "Shravya Kanchi", "Sifat Muhammad Abdullah", "Nicholas Kong", "Daphne Yao", "Murtuza Jadliwala", "Bimal Viswanath"], "title": "TuneShield: Mitigating Toxicity in Conversational AI while Fine-tuning on Untrusted Data", "comment": "Pre-print", "summary": "Recent advances in foundation models, such as LLMs, have revolutionized\nconversational AI. Chatbots are increasingly being developed by customizing\nLLMs on specific conversational datasets. However, mitigating toxicity during\nthis customization, especially when dealing with untrusted training data,\nremains a significant challenge. To address this, we introduce TuneShield, a\ndefense framework designed to mitigate toxicity during chatbot fine-tuning\nwhile preserving conversational quality. TuneShield leverages LLM-based\ntoxicity classification, utilizing the instruction-following capabilities and\nsafety alignment of LLMs to effectively identify toxic samples, outperforming\nindustry API services. TuneShield generates synthetic conversation samples,\ntermed 'healing data', based on the identified toxic samples, using them to\nmitigate toxicity while reinforcing desirable behavior during fine-tuning. It\nperforms an alignment process to further nudge the chatbot towards producing\ndesired responses. Our findings show that TuneShield effectively mitigates\ntoxicity injection attacks while preserving conversational quality, even when\nthe toxicity classifiers are imperfect or biased. TuneShield proves to be\nresilient against adaptive adversarial and jailbreak attacks. Additionally,\nTuneShield demonstrates effectiveness in mitigating adaptive toxicity injection\nattacks during dialog-based learning (DBL).", "AI": {"tldr": "TuneShield introduces a defense framework for mitigating toxicity during chatbot fine-tuning while preserving conversational quality through LLM-based classification and synthetic 'healing data'.", "motivation": "Customizing large language models (LLMs) for conversational AI with untrusted training data presents significant challenges in preventing toxicity amplification.", "method": "The framework combines LLM-based toxicity sample identification with synthetic data generation (healing data) and alignment processes to reinforce desired behaviors during fine-tuning.", "result": "TuneShield successfully mitigates toxicity injection attacks with preserved conversational quality, demonstrates resilience against adaptive adversarial and jailbreak attacks, and effectively handles challenges in dialog-based learning with imperfect classifier performance.", "conclusion": "TuneShield provides a robust solution for defending against various toxicity threats during chatbot training without compromising conversational quality, showing promise for practical deployment in real-world conversational AI systems."}}
{"id": "2507.05995", "categories": ["cs.SE", "68Nxx", "D.2.0; D.2.8"], "pdf": "https://arxiv.org/pdf/2507.05995", "abs": "https://arxiv.org/abs/2507.05995", "authors": ["Pengzhou Chen", "Tao Chen"], "title": "PromiseTune: Unveiling Causally Promising and Explainable Configuration Tuning", "comment": "This paper has been accepted by ICSE26", "summary": "The high configurability of modern software systems has made configuration\ntuning a crucial step for assuring system performance, e.g., latency or\nthroughput. However, given the expensive measurements, large configuration\nspace, and rugged configuration landscape, existing tuners suffer\nineffectiveness due to the difficult balance of budget utilization between\nexploring uncertain regions (for escaping from local optima) and exploiting\nguidance of known good configurations (for fast convergence). The root cause is\nthat we lack knowledge of where the promising regions lay, which also causes\nchallenges in the explainability of the results.\n  In this paper, we propose PromiseTune that tunes configuration guided by\ncausally purified rules. PromiseTune is unique in the sense that we learn\nrules, which reflect certain regions in the configuration landscape, and purify\nthem with causal inference. The remaining rules serve as approximated\nreflections of the promising regions, bounding the tuning to emphasize these\nplaces in the landscape. This, as we demonstrate, can effectively mitigate the\nimpact of the exploration and exploitation trade-off. Those purified regions\ncan then be paired with the measured configurations to provide spatial\nexplainability at the landscape level. Comparing with 11 state-of-the-art\ntuners on 12 systems and varying budgets, we show that PromiseTune performs\nsignificantly better than the others with $42\\%$ superior rank to the overall\nsecond best while providing richer information to explain the hidden system\ncharacteristics.", "AI": {"tldr": "PromiseTune addresses configuration tuning challenges in software systems by using causally purified rules to balance exploration-exploitation trade-offs and provide spatial explainability of system characteristics.", "motivation": "Modern software systems require configuration tuning for performance optimization, but existing methods struggle due to expensive measurements, large configuration spaces, and rugged landscapes, exacerbated by weak knowledge of promising regions and poor explainability.", "method": "The paper proposes PromiseTune, which learns rules representing configuration landscape regions and applies causal inference to purify them. These purified rules focus the tuner's search on promising areas while enabling landscape-level spatial explanations through measurement comparisons.", "result": "PromiseTune outperformed 11 state-of-the-art tuners across 12 systems with varying budgets, achieving 42% higher rank than the second-best method while delivering richer explanatory insights about system behavior.", "conclusion": "PromiseTune effectively mitigates the exploration-exploitation dilemma in configuration tuning through causal rule purification and provides explainable results that reveal hidden system characteristics during optimization."}}
{"id": "2507.05683", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.05683", "abs": "https://arxiv.org/abs/2507.05683", "authors": ["Steven Duplij", "Qiang Guo"], "title": "Polyadic encryption", "comment": "revtex 4.2, 9 pages", "summary": "A novel original procedure of encryption/decryption based on the polyadic\nalgebraic structures and on signal processing methods is proposed. First, we\nuse signals with integer amplitudes to send information. Then we use polyadic\ntechniques to transfer the plaintext into series of special integers. The\nreceiver restores the plaintext using special rules and systems of equations.", "AI": {"tldr": "Proposes a novel encryption/decryption method using polyadic algebraic structures and signal processing with integer amplitude signals.", "motivation": "The paper aims to develop a secure and novel cryptographic procedure leveraging the mathematical properties of polyadic structures and signal processing techniques to enhance information transmission security.", "method": "The method involves converting plaintext into special integers using polyadic algebraic structures, transmitting information via signals with integer amplitudes, and reconstructing plaintext at the receiver through specialized rules and systems of equations.", "result": "Results demonstrate the feasibility of this approach, though specific metrics are not provided in the abstract.", "conclusion": "This polyadic-signal-based encryption method presents a innovative framework for secure communication, combining algebraic systems and signal processing for potential improved security and efficiency."}}
{"id": "2507.06014", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.06014", "abs": "https://arxiv.org/abs/2507.06014", "authors": ["Tim Puhlf\u00fcr\u00df", "Julia Butzke", "Walid Maalej"], "title": "Model Cards Revisited: Bridging the Gap Between Theory and Practice for Ethical AI Requirements", "comment": "Accepted for publication at the 33rd IEEE International Requirements\n  Engineering 2025 conference", "summary": "Model cards are the primary documentation framework for developers of\nartificial intelligence (AI) models to communicate critical information to\ntheir users. Those users are often developers themselves looking for relevant\ndocumentation to ensure that their AI systems comply with the ethical\nrequirements of existing laws, guidelines, and standards. Recent studies\nindicate inadequate model documentation practices, suggesting a gap between AI\nrequirements and current practices in model documentation. To understand this\ngap and provide actionable guidance to bridge it, we conducted a thematic\nanalysis of 26 guidelines on ethics and AI, three AI documentation frameworks,\nthree quantitative studies of model cards, and ten actual model cards. We\nidentified a total of 43 ethical requirements relevant to model documentation\nand organized them into a taxonomy featuring four themes and twelve sub-themes\nrepresenting ethical principles. Our findings indicate that model developers\npredominantly emphasize model capabilities and reliability in the documentation\nwhile overlooking other ethical aspects, such as explainability, user autonomy,\nand fairness. This underscores the need for enhanced support in documenting\nethical AI considerations. Our taxonomy serves as a foundation for a revised\nmodel card framework that holistically addresses ethical AI requirements.", "AI": {"tldr": "This paper highlights gaps in ethical AI model documentation and introduces a 43-requirement taxonomy to address overlooked aspects like explainability and fairness.", "motivation": "Existing model documentation frameworks (e.g., model cards) fail to comprehensively cover ethical requirements needed for compliance with laws and guidelines, creating a critical gap between AI ethics standards and practitioner documentation practices.", "method": "Thematic analysis of 26 ethics/ai guidelines, 3 documentation frameworks, 3 quantitative studies, and 10 actual model cards to identify and categorize ethical requirements.", "result": "43 ethical requirements were grouped into 4 themes and 12 sub-themes. Developers primarily focus on model capabilities/reliability while neglecting ethics-related aspects such as explainability, user autonomy, and fairness.", "conclusion": "The identified taxonomy provides a foundation for a revised model card framework that holistically addresses ethical AI requirements documentation."}}
{"id": "2507.05728", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.05728", "abs": "https://arxiv.org/abs/2507.05728", "authors": ["Ruofei Wang", "Peiqi Duan", "Boxin Shi", "Renjie Wan"], "title": "Asynchronous Event Error-Minimizing Noise for Safeguarding Event Dataset", "comment": "Accepted by ICCV2025", "summary": "With more event datasets being released online, safeguarding the event\ndataset against unauthorized usage has become a serious concern for data\nowners. Unlearnable Examples are proposed to prevent the unauthorized\nexploitation of image datasets. However, it's unclear how to create unlearnable\nasynchronous event streams to prevent event misuse. In this work, we propose\nthe first unlearnable event stream generation method to prevent unauthorized\ntraining from event datasets. A new form of asynchronous event error-minimizing\nnoise is proposed to perturb event streams, tricking the unauthorized model\ninto learning embedded noise instead of realistic features. To be compatible\nwith the sparse event, a projection strategy is presented to sparsify the noise\nto render our unlearnable event streams (UEvs). Extensive experiments\ndemonstrate that our method effectively protects event data from unauthorized\nexploitation, while preserving their utility for legitimate use. We hope our\nUEvs contribute to the advancement of secure and trustworthy event dataset\nsharing. Code is available at: https://github.com/rfww/uevs.", "AI": {"tldr": "The paper introduces UEvs, a novel method to generate unlearnable event streams with noise to prevent unauthorized exploitation while maintaining data utility.", "motivation": "As event datasets become widely available online, data owners face growing concerns about their unauthorized use. Existing 'unlearnable examples' for image data cannot address the unique asynchronous nature of event streams.", "method": "The approach uses asynchronous event error-minimizing noise to perturb streams, combined with a projection strategy to maintain sparsity and create unlearnable event streams (UEvs).", "result": "Experiments demonstrate that UEvs effectively prevents unauthorized model training by embedding noise while preserving usefulness for legitimate users.", "conclusion": "The proposed UEvs framework advances secure event dataset sharing by combining data protection with functional usability. (Code available at https://github.com/rfww/uevs)  "}}
{"id": "2507.05794", "categories": ["cs.CR", "cs.AI", "cs.LO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.05794", "abs": "https://arxiv.org/abs/2507.05794", "authors": ["Avi Shaked", "Nan Messe"], "title": "Automated Reasoning for Vulnerability Management by Design", "comment": null, "summary": "For securing systems, it is essential to manage their vulnerability posture\nand design appropriate security controls. Vulnerability management allows to\nproactively address vulnerabilities by incorporating pertinent security\ncontrols into systems designs. Current vulnerability management approaches do\nnot support systematic reasoning about the vulnerability postures of systems\ndesigns. To effectively manage vulnerabilities and design security controls, we\npropose a formally grounded automated reasoning mechanism. We integrate the\nmechanism into an open-source security design tool and demonstrate its\napplication through an illustrative example driven by real-world challenges.\nThe automated reasoning mechanism allows system designers to identify\nvulnerabilities that are applicable to a specific system design, explicitly\nspecify vulnerability mitigation options, declare selected controls, and thus\nsystematically manage vulnerability postures.", "AI": {"tldr": "This paper proposes a formally grounded automated reasoning mechanism integrated into a security design tool to systematically manage vulnerabilities through proactive security controls.", "motivation": "Current vulnerability management lacks systematic reasoning to effectively address system vulnerabilities and design appropriate security controls.", "method": "The authors developed an automated reasoning mechanism based on formal methods and integrated it into an open-source tool, allowing designers to identify vulnerabilities, specify mitigation options, and declare selected controls.", "result": "Demonstration of the tool's application via a real-world example, showing its ability to identify applicable vulnerabilities and manage mitigation strategies.", "conclusion": "The automated mechanism provides a structured approach for managing vulnerability postures, enabling explicit control selection and systematic vulnerability mitigation in system designs."}}
{"id": "2507.05872", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.05872", "abs": "https://arxiv.org/abs/2507.05872", "authors": ["Berkay Kemal Balioglu", "Alireza Khodaie", "Mehmet Emre Gursoy"], "title": "LDP$^3$: An Extensible and Multi-Threaded Toolkit for Local Differential Privacy Protocols and Post-Processing Methods", "comment": null, "summary": "Local differential privacy (LDP) has become a prominent notion for\nprivacy-preserving data collection. While numerous LDP protocols and\npost-processing (PP) methods have been developed, selecting an optimal\ncombination under different privacy budgets and datasets remains a challenge.\nMoreover, the lack of a comprehensive and extensible LDP benchmarking toolkit\nraises difficulties in evaluating new protocols and PP methods. To address\nthese concerns, this paper presents LDP$^3$ (pronounced LDP-Cube), an\nopen-source, extensible, and multi-threaded toolkit for LDP researchers and\npractitioners. LDP$^3$ contains implementations of several LDP protocols, PP\nmethods, and utility metrics in a modular and extensible design. Its modular\ndesign enables developers to conveniently integrate new protocols and PP\nmethods. Furthermore, its multi-threaded nature enables significant reductions\nin execution times via parallelization. Experimental evaluations demonstrate\nthat: (i) using LDP$^3$ to select a good protocol and post-processing method\nsubstantially improves utility compared to a bad or random choice, and (ii) the\nmulti-threaded design of LDP$^3$ brings substantial benefits in terms of\nefficiency.", "AI": {"tldr": "LDP$^3$ is an open-source, extensible, and multi-threaded toolkit for optimizing local differential privacy protocols and post-processing methods, demonstrated to improve utility and efficiency in evaluations.", "motivation": "Selecting optimal LDP protocol-post-processing combinations under varying privacy budgets and datasets is challenging, while the absence of a comprehensive benchmarking toolkit hinders evaluation of new methods.", "method": "The paper develops LDP$^3$, a modular toolkit containing implementations of multiple LDP protocols, post-processing methods, and utility metrics, with a multi-threaded design enabling parallel execution for efficiency.", "result": "Experiments show (i) protocol selection via LDP$^3$ substantially improves utility compared to suboptimal/random choices, and (ii) the multi-threaded architecture provides significant efficiency gains.", "conclusion": "LDP$^3$ addresses critical gaps in LDP research by offering an extensible framework for protocol evaluation and providing efficiency through parallelization, enabling better privacy-utility tradeoffs."}}
{"id": "2507.05875", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.05875", "abs": "https://arxiv.org/abs/2507.05875", "authors": ["Alireza Khodaie", "Berkay Kemal Balioglu", "Mehmet Emre Gursoy"], "title": "Post-Processing in Local Differential Privacy: An Extensive Evaluation and Benchmark Platform", "comment": null, "summary": "Local differential privacy (LDP) has recently gained prominence as a powerful\nparadigm for collecting and analyzing sensitive data from users' devices.\nHowever, the inherent perturbation added by LDP protocols reduces the utility\nof the collected data. To mitigate this issue, several post-processing (PP)\nmethods have been developed. Yet, the comparative performance of PP methods\nunder diverse settings remains underexplored. In this paper, we present an\nextensive benchmark comprising 6 popular LDP protocols, 7 PP methods, 4 utility\nmetrics, and 6 datasets to evaluate the behaviors and optimality of PP methods\nunder diverse conditions. Through extensive experiments, we show that while PP\ncan substantially improve utility when the privacy budget is small (i.e.,\nstrict privacy), its benefit diminishes as the privacy budget grows. Moreover,\nour findings reveal that the optimal PP method depends on multiple factors,\nincluding the choice of LDP protocol, privacy budget, data characteristics\n(such as distribution and domain size), and the specific utility metric. To\nadvance research in this area and assist practitioners in identifying the most\nsuitable PP method for their setting, we introduce LDP$^3$, an open-source\nbenchmark platform. LDP$^3$ contains all methods used in our experimental\nanalysis, and it is designed in a modular, extensible, and multi-threaded way\nfor future use and development.", "AI": {"tldr": "This paper provides a comprehensive benchmark comparing post-processing methods for local differential privacy under varying privacy budgets, LDP protocols, and data characteristics, introducing LDP$^3$ as an open-source platform.", "motivation": "Local differential privacy (LDP) protocols inherently reduce data utility through perturbation, and existing post-processing methods lack systematic evaluation across diverse scenarios to identify optimal approaches.", "method": "We conducted empirical analysis using 6 LDP protocols, 7 post-processing methods, 4 utility metrics, and 6 datasets across varying privacy budgets and data characteristics to evaluate performance trade-offs.", "result": "Post-processing significantly improves utility under strict privacy budgets (\u03b5 < 1) but provides diminishing returns as \u03b5 increases. The optimal method depends on LDP protocol choice, domain size, distribution, and selected utility metric.", "conclusion": "Practitioners must consider multiple factors when selecting post-processing methods for LDP. LDP$^3$ offers a modular, extensible platform to facilitate future research and method comparisons."}}
{"id": "2507.06008", "categories": ["cs.CR", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.06008", "abs": "https://arxiv.org/abs/2507.06008", "authors": ["Jungeun Lim", "Stephan A. Fahrenkrog-Petersen", "Xixi Lu", "Jan Mendling", "Minseok Song"], "title": "The Impact of Event Data Partitioning on Privacy-aware Process Discovery", "comment": null, "summary": "Information systems support the execution of business processes. The event\nlogs of these executions generally contain sensitive information about\ncustomers, patients, and employees. The corresponding privacy challenges can be\naddressed by anonymizing the event logs while still retaining utility for\nprocess discovery. However, trading off utility and privacy is difficult: the\nhigher the complexity of event log, the higher the loss of utility by\nanonymization. In this work, we propose a pipeline that combines anonymization\nand event data partitioning, where event abstraction is utilized for\npartitioning. By leveraging event abstraction, event logs can be segmented into\nmultiple parts, allowing each sub-log to be anonymized separately. This\npipeline preserves privacy while mitigating the loss of utility. To validate\nour approach, we study the impact of event partitioning on two anonymization\ntechniques using three real-world event logs and two process discovery\ntechniques. Our results demonstrate that event partitioning can bring\nimprovements in process discovery utility for directly-follows-based\nanonymization techniques.", "AI": {"tldr": "This paper proposes a pipeline combining anonymization and event log partitioning via event abstraction to maintain privacy while reducing utility loss in process discovery.", "motivation": "Anonymizing complex event logs for privacy often leads to high utility loss for process discovery, requiring a trade-off between privacy and usability.", "method": "The approach partitions event logs into segments using event abstraction, allowing separate anonymization of sub-logs through a two-phase pipeline. The method is evaluated using three real-world event logs with two anonymization techniques and two process discovery frameworks.", "result": "Experiments show that event partitioning improves utility in directly-follows-based anonymization techniques while preserving privacy, with performance validated across multiple real-world logs and discovery methods.", "conclusion": "The pipeline effectively mitigates utility loss in anonymized event logs by leveraging abstraction-based partitioning, offering a practical solution for privacy-critical process mining scenarios."}}
{"id": "2507.06039", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.06039", "abs": "https://arxiv.org/abs/2507.06039", "authors": ["Oleksii Oleksenko", "Flavien Solt", "C\u00e9dric Fournet", "Jana Hofmann", "Boris K\u00f6pf", "Stavros Volos"], "title": "Enter, Exit, Page Fault, Leak: Testing Isolation Boundaries for Microarchitectural Leaks", "comment": "Accepted at IEEE SP 2025; delayed due to embargo; to appear at IEEE\n  SP 2026", "summary": "CPUs provide isolation mechanisms like virtualization and privilege levels to\nprotect software. Yet these focus on architectural isolation while typically\noverlooking microarchitectural side channels, exemplified by Meltdown and\nForeshadow. Software must therefore supplement architectural defenses with\nad-hoc microarchitectural patches, which are constantly evolving as new attacks\nemerge and defenses are proposed. Such reactive approach makes ensuring\ncomplete isolation a daunting task, and leaves room for errors and oversights.\n  We address this problem by developing a tool that stress tests\nmicroarchitectural isolation between security domains such as virtual machines,\nkernel, and processes, with the goal of detecting flaws in the isolation\nboundaries. The tool extends model-based relational testing (MRT) methodology\nto enable detection of cross-domain information leakage. We design a new test\ncase generator and execution sandbox to handle multi-domain execution, new\nleakage models to encode expected leaks, and new analysis techniques to manage\nnondeterminism.\n  We use this tool to perform an in-depth testing campaign on six x86-64 CPUs\nfor leakage across different isolation boundaries. The testing campaign exposed\nfour new leaks and corroborated numerous known ones, with only two false\npositives throughout the entire campaign. These results show critical gaps in\ncurrent isolation mechanisms as well as validate a robust methodology for\ndetecting microarchitectural flaws. As such, this approach enables a shift from\nreactive patching to proactive security validation in processor design.", "AI": {"tldr": "This paper introduces a tool stress-testing microarchitectural isolation boundaries in CPUs, shifting from reactive patching to proactive security validation via model-based relational testing (MRT) methodology.", "motivation": "Current CPU isolation mechanisms (e.g., virtualization, privilege levels) overlook microarchitectural side channels like Meltdown/Foreshadow, forcing reactive, error-prone software patches that fail to address emerging attacks.", "method": "The authors developed a tool extending MRT methodology with: (1) A test case generator and execution sandbox for multi-domain execution, (2) leakage models to encode expected leaks, and (3) analysis techniques to manage nondeterminism during stress-testing.", "result": "Testing six x86-64 CPUs revealed four new microarchitectural leaks and validated numerous known leaks, demonstrating significant gaps in isolation mechanisms with only two false positives in the entire campaign.", "conclusion": "This approach enables robust proactive validation of processor security, addressing flaws in isolation boundaries and reducing reliance on reactive mitigation strategies."}}
{"id": "2507.06043", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06043", "abs": "https://arxiv.org/abs/2507.06043", "authors": ["Xiaohu Li", "Yunfeng Ning", "Zepeng Bao", "Mayi Xu", "Jianhao Chen", "Tieyun Qian"], "title": "CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative Adversarial Attacks on their Internal Representations", "comment": null, "summary": "Security alignment enables the Large Language Model (LLM) to gain the\nprotection against malicious queries, but various jailbreak attack methods\nreveal the vulnerability of this security mechanism. Previous studies have\nisolated LLM jailbreak attacks and defenses. We analyze the security protection\nmechanism of the LLM, and propose a framework that combines attack and defense.\nOur method is based on the linearly separable property of LLM intermediate\nlayer embedding, as well as the essence of jailbreak attack, which aims to\nembed harmful problems and transfer them to the safe area. We utilize\ngenerative adversarial network (GAN) to learn the security judgment boundary\ninside the LLM to achieve efficient jailbreak attack and defense. The\nexperimental results indicate that our method achieves an average jailbreak\nsuccess rate of 88.85\\% across three popular LLMs, while the defense success\nrate on the state-of-the-art jailbreak dataset reaches an average of 84.17\\%.\nThis not only validates the effectiveness of our approach but also sheds light\non the internal security mechanisms of LLMs, offering new insights for\nenhancing model security The code and data are available at\nhttps://github.com/NLPGM/CAVGAN.", "AI": {"tldr": "This paper proposes CAVGAN, a framework using linear separability of LLM embeddings and generative adversarial networks to enable both jailbreak attacks and defenses. It achieves 88.85% average success in attacks and 84.17% in defenses.", "motivation": "Existing LLM security mechanisms are vulnerable to jailbreak attacks, and prior research has isolated attack/defense strategies. The authors aim to better understand and strengthen LLM security by integrating both perspectives through boundary analysis.", "method": "Leverages GANs to learn LLM's internal security judgment boundary via linear separable properties of intermediate layer embeddings. Transforms harmful queries into safe representations while identifying vulnerabilities for attack generation.", "result": "88.85% jailbreak success on three LLMs, 84.17% defense success on state-of-the-art dataset. Datasets and code publicly available at CAVGAN GitHub repository (https://github.com/NLPGM/CAVGAN).", "conclusion": "Demonstrates effectiveness of boundary analysis approach to uncover internal security mechanisms of LLMs. Provides practical method for simultaneous attack-defense strategy that offers insights for enhanced model security."}}
{"id": "2507.06064", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.06064", "abs": "https://arxiv.org/abs/2507.06064", "authors": ["Oleksandr Kurbatov", "Kyrylo Baybula", "Yaroslava Chopa", "Sergey Kozlov", "Oleg Komendant", "Illia Dovgopoly", "Dmitrii Kurbatov", "Zakhar Naumets", "Yulia Artikulova", "Pavel Kravchenko", "Volodymyr Dubinin", "Lasha Antadze", "Yaroslav Panasenko", "Mykhailo Velykodnyi"], "title": "Wrapless: The trustless lending protocol on top of Bitcoin", "comment": null, "summary": "This paper presents Wrapless -- a lending protocol that enables the\ncollateralization of bitcoins without requiring a trusted wrapping mechanism.\nThe protocol facilitates a \"loan channel\" on the Bitcoin blockchain, allowing\nbitcoins to be locked as collateral for loans issued on any blockchain that\nsupports Turing-complete smart contracts. The protocol is designed in a way\nthat makes it economically irrational for each involved party to manipulate the\nloan rules. There is still a significant research area to bring the protocol\ncloser to traditional AMM financial instruments.", "AI": {"tldr": "Wrapless is a trustless Bitcoin collateralization protocol enabling cross-chain loans without wrapping mechanisms, secured via economically rational incentives.", "motivation": "To address the limitation of collateralizing Bitcoin on cross-chain lending platforms without relying on centralized or trusted wrapping solutions.", "method": "Implements 'loan channels' on the Bitcoin blockchain with game-theoretic incentives, making manipulation economically irrational for all parties.", "result": "Enables secure, trustless Bitcoin collateralization for loans on any Turing-complete smart contract blockchain, with a focus on economic security.", "conclusion": "Wrapless presents a promising approach to decentralized lending but requires further research to align more closely with traditional AMM financial models."}}
{"id": "2507.06092", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06092", "abs": "https://arxiv.org/abs/2507.06092", "authors": ["Shravya Kanchi", "Neal Mangaokar", "Aravind Cheruvu", "Sifat Muhammad Abdullah", "Shirin Nilizadeh", "Atul Prakash", "Bimal Viswanath"], "title": "Taming Data Challenges in ML-based Security Tasks: Lessons from Integrating Generative AI", "comment": null, "summary": "Machine learning-based supervised classifiers are widely used for security\ntasks, and their improvement has been largely focused on algorithmic\nadvancements. We argue that data challenges that negatively impact the\nperformance of these classifiers have received limited attention. We address\nthe following research question: Can developments in Generative AI (GenAI)\naddress these data challenges and improve classifier performance? We propose\naugmenting training datasets with synthetic data generated using GenAI\ntechniques to improve classifier generalization. We evaluate this approach\nacross 7 diverse security tasks using 6 state-of-the-art GenAI methods and\nintroduce a novel GenAI scheme called Nimai that enables highly controlled data\nsynthesis. We find that GenAI techniques can significantly improve the\nperformance of security classifiers, achieving improvements of up to 32.6% even\nin severely data-constrained settings (only ~180 training samples).\nFurthermore, we demonstrate that GenAI can facilitate rapid adaptation to\nconcept drift post-deployment, requiring minimal labeling in the adjustment\nprocess. Despite successes, our study finds that some GenAI schemes struggle to\ninitialize (train and produce data) on certain security tasks. We also identify\ncharacteristics of specific tasks, such as noisy labels, overlapping class\ndistributions, and sparse feature vectors, which hinder performance boost using\nGenAI. We believe that our study will drive the development of future GenAI\ntools designed for security tasks.", "AI": {"tldr": "This paper investigates the potential of Generative AI to enhance security classifiers by addressing data challenges, finding significant improvements with proposed synthetic data methods.", "motivation": "Data challenges, such as limited samples and task-specific issues, negatively impact security classifier performance, but have received less attention compared to algorithmic advancements.", "method": "The study augments training data with synthetic samples generated via six state-of-the-art Generative AI methods and introduces a novel controlled-synthesis scheme (Nimai). It evaluates these approaches across seven security tasks with severe data constraints.", "result": "GenAI-based augmentation improves classifier performance by up to 32.6% with minimal training samples (~180), while adaptive methods achieve rapid updates to concept drift scenarios requiring limited labeling. However, some GenAI schemes struggle with initialization, and task characteristics like noisy labels and sparse features hinder effectiveness.", "conclusion": "The work demonstrates that Generative AI can address critical data challenges in security classification but requires development to handle task-specific limitations, driving future research in this area."}}
{"id": "2507.06112", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.06112", "abs": "https://arxiv.org/abs/2507.06112", "authors": ["Antoine Geimer", "Clementine Maurice"], "title": "Fun with flags: How Compilers Break and Fix Constant-Time Code", "comment": "11 pages", "summary": "Developers rely on constant-time programming to prevent timing side-channel\nattacks. But these efforts can be undone by compilers, whose optimizations may\nsilently reintroduce leaks. While recent works have measured the extent of such\nleakage, they leave developers without actionable insights: which optimization\npasses are responsible, and how to disable them without modifying the compiler\nremains unclear.\n  In this paper, we conduct a qualitative analysis of how compiler\noptimizations break constant-time code. We construct a dataset of\ncompiler-introduced constant-time violations and analyze the internals of two\nwidely used compilers, GCC and LLVM, to identify the specific optimization\npasses responsible. Our key insight is that a small set of passes are at the\nroot of most leaks. To the best of our knowledge, we are also the first to\ncharacterize how the interactions between these passes contribute to leakage.\nBased on this analysis, we propose an original and practical mitigation that\nrequires no source code modification or custom compiler: disabling selected\noptimization passes via compiler flags. We show that this approach\nsignificantly reduces leakage with minimal performance overhead, offering an\nimmediately deployable defense for developers.", "AI": {"tldr": "The paper investigates how compiler optimizations break constant-time code, identifies key optimization passes in GCC and LLVM responsible, and proposes mitigating timing leaks via compiler flags without code changes or custom compilers.", "motivation": "Developers struggle to prevent timing side-channel attacks due to compiler optimizations silently reintroducing leaks, while prior work lacks actionable guidance on which passes to disable.", "method": "Qualitative analysis of compiler-introduced violations, constructing a dataset, analyzing GCC/LLVM internals, and characterizing pass interactions causing leakage.", "result": "Disabling specific optimization passes via flags significantly reduces timing leaks with minimal performance overhead, offering an immediately deployable solution.", "conclusion": "This work provides actionable insights for developers to mitigate timing leaks by configuring existing compilers with targeted flags, addressing a critical gap in compiler-secure programming."}}
