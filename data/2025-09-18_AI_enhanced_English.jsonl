{"id": "2509.13464", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.13464", "abs": "https://arxiv.org/abs/2509.13464", "authors": ["Onat Gungor", "Ishaan Kale", "Jiasheng Zhou", "Tajana Rosing"], "title": "LIGHT-HIDS: A Lightweight and Effective Machine Learning-Based Framework for Robust Host Intrusion Detection", "comment": "Accepted by The 24th IEEE International Conference on Machine\n  Learning and Applications (ICMLA'25)", "summary": "The expansion of edge computing has increased the attack surface, creating an\nurgent need for robust, real-time machine learning (ML)-based host intrusion\ndetection systems (HIDS) that balance accuracy and efficiency. In such\nsettings, inference latency poses a critical security risk, as delays may\nprovide exploitable opportunities for attackers. However, many state-of-the-art\nML-based HIDS solutions rely on computationally intensive architectures with\nhigh inference costs, limiting their practical deployment. This paper proposes\nLIGHT-HIDS, a lightweight machine learning framework that combines a compressed\nneural network feature extractor trained via Deep Support Vector Data\nDescription (DeepSVDD) with an efficient novelty detection model. This hybrid\napproach enables the learning of compact, meaningful representations of normal\nsystem call behavior for accurate anomaly detection. Experimental results on\nmultiple datasets demonstrate that LIGHT-HIDS consistently enhances detection\naccuracy while reducing inference time by up to 75x compared to\nstate-of-the-art methods. These findings highlight its effectiveness and\nscalability as a machine learning-based solution for real-time host intrusion\ndetection.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.13509", "categories": ["cs.CR", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.13509", "abs": "https://arxiv.org/abs/2509.13509", "authors": ["Priyanka Nanayakkara", "Elena Ghazi", "Salil Vadhan"], "title": "Practitioners' Perspectives on a Differential Privacy Deployment Registry", "comment": null, "summary": "Differential privacy (DP) -- a principled approach to producing statistical\ndata products with strong, mathematically provable privacy guarantees for the\nindividuals in the underlying dataset -- has seen substantial adoption in\npractice over the past decade. Applying DP requires making several\nimplementation decisions, each with significant impacts on data privacy and/or\nutility. Hence, to promote shared learning and accountability around DP\ndeployments, Dwork, Kohli, and Mulligan (2019) proposed a public-facing\nrepository (\"registry\") of DP deployments. The DP community has recently\nstarted to work toward realizing this vision. We contribute to this effort by\n(1) developing a holistic, hierarchical schema to describe any given DP\ndeployment and (2) designing and implementing an interactive interface to act\nas a registry where practitioners can access information about past DP\ndeployments. We (3) populate our interface with 21 real-world DP deployments\nand (4) conduct an exploratory user study with DP practitioners ($n=16$) to\nunderstand how they would use the registry, as well as what challenges and\nopportunities they foresee around its adoption. We find that participants were\nenthusiastic about the registry as a valuable resource for evaluating prior\ndeployments and making future deployments. They also identified several\nopportunities for the registry, including that it can become a \"hub\" for the\ncommunity and support broader communication around DP (e.g., to legal teams).\nAt the same time, they identified challenges around the registry gaining\nadoption, including the effort and risk involved with making implementation\nchoices public and moderating the quality of entries. Based on our findings, we\noffer recommendations for encouraging adoption and increasing the registry's\nvalue not only to DP practitioners, but also to policymakers, data users, and\ndata subjects.", "AI": {"tldr": "This paper proposes a registry for differential privacy (DP) deployments to facilitate knowledge sharing and accountability. It introduces a hierarchical schema, an interactive interface, and provides real-world deployment examples. A user study (n=16) highlights opportunities and challenges for adoption.", "motivation": "DP requires critical implementation decisions impacting privacy/utility. Dwork et al. (2019) suggested a public registry to improve shared learning and accountability. The paper addresses the need for a structured way to document and analyze DP deployments.", "method": "The authors developed (1) a hierarchical schema for describing DP deployments, (2) an interactive registry interface, (3) populated it with 21 real-world deployments, and (4) conducted an exploratory user study with 16 DP practitioners.", "result": "The user study found practitioners enthusiastic about the registry's value for evaluating past deployments and planning future ones. Identified opportunities include serving as a community hub and fostering interdepartmental communication (e.g., legal teams). Challenges include adoption barriers due to costs/risk of public disclosure and quality moderation.", "conclusion": "The authors offer recommendations to enhance registry adoption and value, emphasizing support for DP practitioners, policymakers, data users, and data subjects. The registry is positioned as a critical resource to advance DP practices through transparency and community collaboration."}}
{"id": "2509.13514", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.13514", "abs": "https://arxiv.org/abs/2509.13514", "authors": ["Onat Gungor", "Roshan Sood", "Harold Wang", "Tajana Rosing"], "title": "AQUA-LLM: Evaluating Accuracy, Quantization, and Adversarial Robustness Trade-offs in LLMs for Cybersecurity Question Answering", "comment": "Accepted by the 24th IEEE International Conference on Machine\n  Learning and Applications (ICMLA'25)", "summary": "Large Language Models (LLMs) have recently demonstrated strong potential for\ncybersecurity question answering (QA), supporting decision-making in real-time\nthreat detection and response workflows. However, their substantial\ncomputational demands pose significant challenges for deployment on\nresource-constrained edge devices. Quantization, a widely adopted model\ncompression technique, can alleviate these constraints. Nevertheless,\nquantization may degrade model accuracy and increase susceptibility to\nadversarial attacks. Fine-tuning offers a potential means to mitigate these\nlimitations, but its effectiveness when combined with quantization remains\ninsufficiently explored. Hence, it is essential to understand the trade-offs\namong accuracy, efficiency, and robustness. We propose AQUA-LLM, an evaluation\nframework designed to benchmark several state-of-the-art small LLMs under four\ndistinct configurations: base, quantized-only, fine-tuned, and fine-tuned\ncombined with quantization, specifically for cybersecurity QA. Our results\ndemonstrate that quantization alone yields the lowest accuracy and robustness\ndespite improving efficiency. In contrast, combining quantization with\nfine-tuning enhances both LLM robustness and predictive performance, achieving\nan optimal balance of accuracy, robustness, and efficiency. These findings\nhighlight the critical need for quantization-aware, robustness-preserving\nfine-tuning methodologies to enable the robust and efficient deployment of LLMs\nfor cybersecurity QA.", "AI": {"tldr": "AQUA-LLM is an evaluation framework showing that combining quantization with fine-tuning optimizes accuracy, robustness, and efficiency for cybersecurity QA LLMs on edge devices.", "motivation": "LLMs' computational demands hinder edge deployment, while quantization reduces accuracy and robustness, necessitating strategies to balance efficiency, accuracy, and security in cybersecurity QA.", "method": "Benchmarked four LLM configurations (base, quantized-only, fine-tuned, quantized + fine-tuned using AQUA-LLM framework) for cybersecurity QA tasks on small LLMs.", "result": "Quantization alone reduces accuracy/robustness but improves efficiency; combining with fine-tuning maintains high robustness and accuracy while preserving efficiency.", "conclusion": "Quantization-aware, robustness-preserving fine-tuning is critical for deploying efficient and secure LLMs in resource-constrained cybersecurity environments."}}
{"id": "2509.13561", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.13561", "abs": "https://arxiv.org/abs/2509.13561", "authors": ["Mengxiao Wang", "Guofei Gu"], "title": "GuardianPWA: Enhancing Security Throughout the Progressive Web App Installation Lifecycle", "comment": null, "summary": "Progressive Web App (PWA) installation is critical for integrating web and\nmobile app functionalities, offering a seamless user experience. However,\nensuring the security of the PWA installation lifecycle is essential for\nmaintaining user trust and privacy. This paper introduces the GUARDIANPWA\nframework, a comprehensive approach to analyzing the PWA installation mechanism\nbased on the CIA security principles (Confidentiality, Integrity, and\nAvailability) and identifying areas where browser vendors fail to comply with\nthese principles. Our study revealed 203 instances of non-compliance with\nsecurity principles, highlighting how these irregularities in the PWA\ninstallation lifecycle can lead to potential violations of user privacy. For\ninstance, in Firefox, PWAs installed in private mode incorrectly appear in\nnormal mode, risking user confidentiality. Additionally, 29,465 PWAs are at\nrisk because Samsung Internet does not display origins when PWAs navigate to\nthird-party websites, undermining integrity. These findings were reported to\nbrowser vendors, leading to Firefox acknowledging four issues, resolving one,\nand planning to resolve two others. GUARDIANPWA supports developers by\nanalyzing PWA manifest files for syntactic and semantic correctness, offering\nactionable recommendations, and helping to create PWAs that align with security\nbest practices. By using GUARDIANPWA, developers and users can address critical\nsecurity gaps and enhance compliance with CIA principles throughout the PWA\ninstallation lifecycle.", "AI": {"tldr": "This paper introduces GUARDIANPWA to secure PWA installations via CIA principles. It identifies 203 security gaps in browsers, highlights vendor responses, and empowers developers to create secure PWAs.", "motivation": "PWAs bridge web and mobile app functionalities but face critical security challenges during installation. The paper addresses the lack of adherence to CIA principles in browsers, which risks user privacy, integrity, and confidentiality.", "method": "The GUARDIANPWA framework evaluates the PWA installation process against the CIA (Confidentiality, Integrity, Availability) security principles. It analyzes manifest files for correctness, identifies non-compliance with security protocols, and offers actionable recommendations.", "result": "The study discovered 203 non-compliance instances across browsers. Specific issues include Firefox\u2019s private-mode leakage and Samsung Internet\u2019s lack of origin visibility. Fixes were initiated by vendors, with Firefox resolving one issue and planning to address two more.", "conclusion": "GUARDIANPWA provides a structured approach to enhance PWA installation security by addressing identified vulnerabilities. It supports developers in aligning PWAs with CIA principles, thus improving user trust and privacy."}}
{"id": "2509.13436", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13436", "abs": "https://arxiv.org/abs/2509.13436", "authors": ["Evan Eisinger", "Michael A. Heroux"], "title": "Is Research Software Science a Metascience?", "comment": "5 pages", "summary": "As research increasingly relies on computational methods, the reliability of\nscientific results depends on the quality, reproducibility, and transparency of\nresearch software. Ensuring these qualities is critical for scientific\nintegrity and discovery. This paper asks whether Research Software Science\n(RSS)--the empirical study of how research software is developed and\nused--should be considered a form of metascience, the science of science.\nClassification matters because it could affect recognition, funding, and\nintegration of RSS into research improvement. We define metascience and RSS,\ncompare their principles and objectives, and examine their overlaps. Arguments\nfor classification highlight shared commitments to reproducibility,\ntransparency, and empirical study of research processes. Arguments against\nportraying RSS as a specialized domain focused on a tool rather than the\nbroader scientific enterprise. Our analysis finds RSS advances core goals of\nmetascience, especially in computational reproducibility, and bridges\ntechnical, social, and cognitive aspects of research. Its classification\ndepends on whether one adopts a broad definition of metascience--any empirical\neffort to improve science--or a narrow one focused on systemic and\nepistemological structures. We argue RSS is best understood as a distinct\ninterdisciplinary domain that aligns with, and in some definitions fits within,\nmetascience. Recognizing it as such can strengthen its role in improving\nreliability, justify funding, and elevate software development in research\ninstitutions. Regardless of classification, applying scientific rigor to\nresearch software ensures the tools of discovery meet the standards of the\ndiscoveries themselves.", "AI": {"tldr": "This paper argues that Research Software Science (RSS) should be classified as metascience to improve scientific reliability by aligning software development with broader reproducibility goals.", "motivation": "Classification of RSS as metascience impacts recognition, funding, and integration into research improvement, as scientific reliability depends on software quality, reproducibility, and transparency.", "method": "Defined metascience and RSS, compared their principles and objectives, examined overlaps, and analyzed arguments for/against classification.", "result": "RSS advances core metascience goals (especially computational reproducibility), bridges technical/social/cognitive aspects, and its classification depends on broad/narrow definitions of metascience.", "conclusion": "RSS is best understood as a distinct interdisciplinary domain that aligns with (and in some definitions fits within) metascience, which can strengthen its role in improving reliability, justify funding, and elevate software development in institutions."}}
{"id": "2509.13563", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.13563", "abs": "https://arxiv.org/abs/2509.13563", "authors": ["Mengxiao Wang", "Guofei Gu"], "title": "Demystifying Progressive Web Application Permission Systems", "comment": null, "summary": "Progressive Web Applications (PWAs) blend the advantages of web and native\napps, offering features like offline access, push notifications, and\ninstallability. Beyond these, modern PWAs are increasingly granted system-level\ncapabilities such as auto-start on login and shared context with native\napplications. However, their permission management remains poorly defined and\ninconsistently implemented across platforms and browsers.\n  To investigate these gaps, we developed Permissioner, a cross-platform\nanalysis tool, and conducted a systematic study of PWA permissions. Our\nanalysis uncovered critical issues of inconsistency, incompleteness, and\nunclear boundaries in permission enforcement, leading to various attacks\nincluding permission leakage, device identification, and Permission API abuse.\nWe further examined why some browsers resist adopting more granular permission\ncontrols, identifying trade-offs involving usability, compatibility, and\nplatform limitations. Through collaboration with browser vendors, several\nissues reported in our findings were acknowledged and resolved, notably by\nFirefox and Chrome. Our work highlights the urgent need for a unified, robust\npermission model for PWAs and provides actionable guidance toward achieving\nthis goal.", "AI": {"tldr": "This study identifies inconsistent and incomplete permission management in PWAs, leading to security risks and proposing a unified permission model with actionable solutions through cross-platform analysis and collaborative fixes with browsers like Firefox and Chrome.", "motivation": "Progressive Web Applications (PWAs) are adopting system-level capabilities but lack a cohesive permission management framework, leaving them vulnerable to potential security issues, which makes understanding and addressing these permission flaws crucial.", "method": "The research deployed Permissioner, a cross-platform analysis tool, to evaluate PWA permissions systematically. They identified inconsistencies, analyzed the reasons certain browsers avoid granular permissions, aiming to uncover vulnerabilities and underlying challenges in permission enforcement.", "result": "The study found critical PWA permission issues, illustrating vulnerabilities through attacks like permission leakage, device identification, and API abuse. Key outcomes include acknowledged and resolved permissions problems with Firefox and Chrome, based on the research insights.", "conclusion": "The paper concludes that a unified, robust permission model for PWAs is essential and provides actionable guidance for implementing effective solutions after system analysis and working with browser vendors."}}
{"id": "2509.13471", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13471", "abs": "https://arxiv.org/abs/2509.13471", "authors": ["Sina Gogani-Khiabani", "Ashutosh Trivedi", "Diptikalyan Saha", "Saeid Tizpaz-Niari"], "title": "An LLM Agentic Approach for Legal-Critical Software: A Case Study for Tax Prep Software", "comment": "To appear at ICSE 26. 12 pages", "summary": "Large language models (LLMs) show promise for translating natural-language\nstatutes into executable logic, but reliability in legally critical settings\nremains challenging due to ambiguity and hallucinations. We present an agentic\napproach for developing legal-critical software, using U.S. federal tax\npreparation as a case study. The key challenge is test-case generation under\nthe oracle problem, where correct outputs require interpreting law. Building on\nmetamorphic testing, we introduce higher-order metamorphic relations that\ncompare system outputs across structured shifts among similar individuals.\nBecause authoring such relations is tedious and error-prone, we use an\nLLM-driven, role-based framework to automate test generation and code\nsynthesis. We implement a multi-agent system that translates tax code into\nexecutable software and incorporates a metamorphic-testing agent that searches\nfor counterexamples. In experiments, our framework using a smaller model\n(GPT-4o-mini) achieves a worst-case pass rate of 45%, outperforming frontier\nmodels (GPT-4o and Claude 3.5, 9-15%) on complex tax-code tasks. These results\nsupport agentic LLM methodologies as a path to robust, trustworthy\nlegal-critical software from natural-language specifications.", "AI": {"tldr": "This paper proposes an agentic LLM framework for developing reliable legal-critical software by combining metamorphic testing with automated test generation, demonstrated through U.S. tax code translation.", "motivation": "LLMs face reliability challenges in legal domains (e.g., ambiguity, hallucinations) and test-case generation is hindered by the oracle problem requiring legal interpretation.", "method": "The authors introduce higher-order metamorphic relations for structured test case comparisons and a multi-agent system with LLM-driven code synthesis and metamorphic-testing agents.", "result": "Using GPT-4o-mini, their framework achieves a 45-159 percentage point improvement in worst-case pass rates versus frontier models (GPT-4o and Claude 3.5, 9-15%) on complex tax-code tasks.", "conclusion": "Agentic LLM approaches with structured testing methods enable more robust legal-critical software from natural-language specifications, challenging assumptions about model scale."}}
{"id": "2509.13581", "categories": ["cs.CR", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.13581", "abs": "https://arxiv.org/abs/2509.13581", "authors": ["Mohamad Fakih", "Rahul Dharmaji", "Youssef Mahmoud", "Halima Bouzidi", "Mohammad Abdullah Al Faruque"], "title": "Invisible Ears at Your Fingertips: Acoustic Eavesdropping via Mouse Sensors", "comment": "Appearing in the Annual Computer Security Applications Conference\n  (ACSAC 2025)", "summary": "Modern optical mouse sensors, with their advanced precision and high\nresponsiveness, possess an often overlooked vulnerability: they can be\nexploited for side-channel attacks. This paper introduces Mic-E-Mouse, the\nfirst-ever side-channel attack that targets high-performance optical mouse\nsensors to covertly eavesdrop on users. We demonstrate that audio signals can\ninduce subtle surface vibrations detectable by a mouse's optical sensor.\nRemarkably, user-space software on popular operating systems can collect and\nbroadcast this sensitive side channel, granting attackers access to raw mouse\ndata without requiring direct system-level permissions. Initially, the\nvibration signals extracted from mouse data are of poor quality due to\nnon-uniform sampling, a non-linear frequency response, and significant\nquantization. To overcome these limitations, Mic-E-Mouse employs a\nsophisticated end-to-end data filtering pipeline that combines Wiener\nfiltering, resampling corrections, and an innovative encoder-only spectrogram\nneural filtering technique. We evaluate the attack's efficacy across diverse\nconditions, including speaking volume, mouse polling rate and DPI, surface\nmaterials, speaker languages, and environmental noise. In controlled\nenvironments, Mic-E-Mouse improves the signal-to-noise ratio (SNR) by up to +19\ndB for speech reconstruction. Furthermore, our results demonstrate a speech\nrecognition accuracy of roughly 42% to 61% on the AudioMNIST and VCTK datasets.\nAll our code and datasets are publicly accessible on\nhttps://sites.google.com/view/mic-e-mouse.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.13487", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13487", "abs": "https://arxiv.org/abs/2509.13487", "authors": ["Abubakari Alidu", "Michele Ciavotta", "Flavio DePaoli"], "title": "Prompt2DAG: A Modular Methodology for LLM-Based Data Enrichment Pipeline Generation", "comment": null, "summary": "Developing reliable data enrichment pipelines demands significant engineering\nexpertise. We present Prompt2DAG, a methodology that transforms natural\nlanguage descriptions into executable Apache Airflow DAGs. We evaluate four\ngeneration approaches -- Direct, LLM-only, Hybrid, and Template-based -- across\n260 experiments using thirteen LLMs and five case studies to identify optimal\nstrategies for production-grade automation. Performance is measured using a\npenalized scoring framework that combines reliability with code quality (SAT),\nstructural integrity (DST), and executability (PCT). The Hybrid approach\nemerges as the optimal generative method, achieving a 78.5% success rate with\nrobust quality scores (SAT: 6.79, DST: 7.67, PCT: 7.76). This significantly\noutperforms the LLM-only (66.2% success) and Direct (29.2% success) methods.\nOur findings show that reliability, not intrinsic code quality, is the primary\ndifferentiator. Cost-effectiveness analysis reveals the Hybrid method is over\ntwice as efficient as Direct prompting per successful DAG. We conclude that a\nstructured, hybrid approach is essential for balancing flexibility and\nreliability in automated workflow generation, offering a viable path to\ndemocratize data pipeline development.", "AI": {"tldr": "Prompt2DAG converts natural language into Apache Airflow DAGs. Hybrid method (78.5% success) best balances reliability and flexibility for automated workflows. Useful for democratizing data pipeline development.", "motivation": "Creating robust data enrichment pipelines needs significant engineering. Converting natural language to executable workflows could reduce this barrier but requires reliable automation strategies.", "method": "Compared four approaches (Direct, LLM-only, Hybrid, Template-based) across 260 experiments with 13 LLMs and 5 case studies. Evaluated using a scoring system combining reliability with code quality (SAT), structural integrity (DST), and executability (PCT).", "result": "Hybrid approach achieved 78.5% success rate with high quality metrics (SAT: 6.79, DST: 7.67, PCT: 7.76), outperforming LLM-only (66.2%) and Direct (29.2%). Hybrid was 2.34x more cost-effective than Direct method.", "conclusion": "Hybrid approach is best for balancing flexibility and reliability in automated workflow generation. This method offers a practical solution to reduce engineering burden and democratize data pipeline development."}}
{"id": "2509.13597", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13597", "abs": "https://arxiv.org/abs/2509.13597", "authors": ["Abhishek Goswami"], "title": "Agentic JWT: A Secure Delegation Protocol for Autonomous AI Agents", "comment": "17 pages, 6 figures, 2 Tables", "summary": "Autonomous LLM agents can issue thousands of API calls per hour without human\noversight. OAuth 2.0 assumes deterministic clients, but in agentic settings\nstochastic reasoning, prompt injection, or multi-agent orchestration can\nsilently expand privileges.\n  We introduce Agentic JWT (A-JWT), a dual-faceted intent token that binds each\nagent's action to verifiable user intent and, optionally, to a specific\nworkflow step. A-JWT carries an agent's identity as a one-way checksum hash\nderived from its prompt, tools and configuration, and a chained delegation\nassertion to prove which downstream agent may execute a given task, and\nper-agent proof-of-possession keys to prevent replay and in-process\nimpersonation. We define a new authorization mechanism and add a lightweight\nclient shim library that self-verifies code at run time, mints intent tokens,\ntracks workflow steps and derives keys, thus enabling secure agent identity and\nseparation even within a single process.\n  We illustrate a comprehensive threat model for agentic applications,\nimplement a Python proof-of-concept and show functional blocking of\nscope-violating requests, replay, impersonation, and prompt-injection pathways\nwith sub-millisecond overhead on commodity hardware. The design aligns with\nongoing OAuth agent discussions and offers a drop-in path toward zero-trust\nguarantees for agentic applications. A comprehensive performance and security\nevaluation with experimental results will appear in our forthcoming journal\npublication", "AI": {"tldr": "This paper presents Agentic JWT (A-JWT), a secure authorization framework for autonomous LLM agents that binds their actions to verifiable user intent and workflow steps using dual-faceted intent tokens, proof-of-possession keys, and self-verifying client libraries, enabling zero-trust guarantees in agentic systems.", "motivation": "Existing OAuth 2.0 frameworks lack mechanisms to handle agentic systems' stochastic reasoning patterns, prompt injections, and multi-agent orchestration risks, which can silently escalate privileges and undermine API security in autonomous AI operations.", "method": "A-JWT employs: (1) One-way checksum hashes for agent identity derived from prompts/tools/configurations, (2) Chained delegation assertions for workflow-specific authority, (3) Per-agent proof-of-possession keys for anti-replay/anti-impersonation, and (4) A lightweight runtime shim that self-verifies code, mints tokens, tracks workflows, and derives cryptographic keys.", "result": "The solution achieves: (1) Sub-millisecond overhead with commodity hardware, (2) Effective blocking of scope violations, replay attacks, impersonation, and prompt injections, (3) Alignment with ongoing OAuth agent standard discussions, and (4) A Python proof-of-concept with experimental validation of threat-mitigation capabilities.", "conclusion": "A-JWT provides a zero-trust, drop-in solution for securing autonomous agent systems, addressing critical gaps in agent authentication, intent verification, and process-isolated identity management within existing OAuth frameworks, with further journal publication planned."}}
{"id": "2509.13535", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13535", "abs": "https://arxiv.org/abs/2509.13535", "authors": ["S M Farah Al Fahim", "Md Nakhla Rafi", "Zeyang Ma", "Dong Jae Kim", "Tse-Hsun", "Chen"], "title": "Crash Report Enhancement with Large Language Models: An Empirical Study", "comment": null, "summary": "Crash reports are central to software maintenance, yet many lack the\ndiagnostic detail developers need to debug efficiently. We examine whether\nlarge language models can enhance crash reports by adding fault locations,\nroot-cause explanations, and repair suggestions. We study two enhancement\nstrategies: Direct-LLM, a single-shot approach that uses stack-trace context,\nand Agentic-LLM, an iterative approach that explores the repository for\nadditional evidence. On a dataset of 492 real-world crash reports, LLM-enhanced\nreports improve Top-1 problem-localization accuracy from 10.6% (original\nreports) to 40.2-43.1%, and produce suggested fixes that closely resemble\ndeveloper patches (CodeBLEU around 56-57%). Both our manual evaluations and\nLLM-as-a-judge assessment show that Agentic-LLM delivers stronger root-cause\nexplanations and more actionable repair guidance. A user study with 16\nparticipants further confirms that enhanced reports make crashes easier to\nunderstand and resolve, with the largest improvement in repair guidance. These\nresults indicate that supplying LLMs with stack traces and repository code\nyields enhanced crash reports that are substantially more useful for debugging.", "AI": {"tldr": "This paper evaluates how large language models (LLMs) can enhance crash reports by adding diagnostic details, comparing two strategies (Direct-LLM and Agentic-LLM) and demonstrating significant improvements in problem localization and repair guidance.", "motivation": "Crash reports often lack sufficient diagnostic detail for efficient debugging. The authors aim to explore whether LLMs can automatically enrich crash reports with fault locations, root-cause explanations, and repair suggestions to aid developers.", "method": "The study compares two approaches: Direct-LLM (single-shot stack-trace analysis) and Agentic-LLM (iterative repository exploration). Evaluations include quantitative metrics (localization accuracy, CodeBLEU), manual assessments, and a user study with 16 participants.", "result": "LLM-enhanced reports achieved 40.2-43.1\u2500\uf0b2 Top-1 localization accuracy (vs. 10.6\u2500\uf0b2 original), with CodeBLEU scores of 56-57\u2500\uf0b2 for fixes. Agentic-LLM outperformed Direct-LLM in root-cause explanations and repair guidance, confirmed by both automated and user studies.", "conclusion": "Supplying LLMs with stack traces and repository code significantly enhances crash report utility for debugging. Agentic-LLM offers stronger performance, particularly in actionable repair guidance, making crashes easier to resolve."}}
{"id": "2509.13627", "categories": ["cs.CR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.13627", "abs": "https://arxiv.org/abs/2509.13627", "authors": ["Vijay Kumar Butte", "Sujata Butte"], "title": "Secure, Scalable and Privacy Aware Data Strategy in Cloud", "comment": null, "summary": "The enterprises today are faced with the tough challenge of processing,\nstoring large amounts of data in a secure, scalable manner and enabling\ndecision makers to make quick, informed data driven decisions. This paper\naddresses this challenge and develops an effective enterprise data strategy in\nthe cloud. Various components of an effective data strategy are discussed and\narchitectures addressing security, scalability and privacy aspects are\nprovided.", "AI": {"tldr": "This paper addresses enterprise data challenges by developing a cloud-based strategy focusing on security, scalability, and privacy to support data-driven decision-making.", "motivation": "Enterprises face challenges in securely storing large data volumes and enabling timely data-driven decisions, necessitating a scalable, secure cloud-based data strategy.", "method": "The authors analyze the challenges of data processing and storage, then develop an architecture focused on security, scalability, and privacy through case studies or framework design.", "result": "The paper proposes architectures and strategies that address security, scalability, and privacy, providing a structured approach for enterprises to implement robust data solutions.", "conclusion": "The paper concludes that the proposed cloud-based enterprise data strategy effectively addresses security, scalability, and privacy challenges, enabling enterprises to manage data efficiently and make informed decisions."}}
{"id": "2509.13650", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13650", "abs": "https://arxiv.org/abs/2509.13650", "authors": ["Amena Amro", "Manar H. Alalfi"], "title": "GitHub's Copilot Code Review: Can AI Spot Security Flaws Before You Commit?", "comment": null, "summary": "As software development practices increasingly adopt AI-powered tools,\nensuring that such tools can support secure coding has become critical. This\nstudy evaluates the effectiveness of GitHub Copilot's recently introduced code\nreview feature in detecting security vulnerabilities. Using a curated set of\nlabeled vulnerable code samples drawn from diverse open-source projects\nspanning multiple programming languages and application domains, we\nsystematically assessed Copilot's ability to identify and provide feedback on\ncommon security flaws. Contrary to expectations, our results reveal that\nCopilot's code review frequently fails to detect critical vulnerabilities such\nas SQL injection, cross-site scripting (XSS), and insecure deserialization.\nInstead, its feedback primarily addresses low-severity issues, such as coding\nstyle and typographical errors. These findings expose a significant gap between\nthe perceived capabilities of AI-assisted code review and its actual\neffectiveness in supporting secure development practices. Our results highlight\nthe continued necessity of dedicated security tools and manual code audits to\nensure robust software security.", "AI": {"tldr": "Study assesses GitHub Copilot's security code review capabilities, finding it misses critical vulnerabilities (SQLi, XSS) while focusing on low-severity issues, highlighting the need for dedicated security tools.", "motivation": "As AI tools become widespread in coding, establishing their ability to support secure development practices is critical for preventing vulnerabilities in modern software systems.", "method": "Systematic evaluation of Copilot's code review feature using curated vulnerable code samples from open-source projects across multiple languages and domains.", "result": "Copilot frequently fails to detect high-severity security flaws like SQL injection and XSS, instead prioritizing low-severity issues such as style errors and typos.", "conclusion": "Current AI-assisted code review capabilities show significant limitations in security detection, emphasizing the continued necessity for specialized security tools and manual code audits."}}
