{"id": "2508.21097", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21097", "abs": "https://arxiv.org/abs/2508.21097", "authors": ["Nazanin Siavash", "Armin Moin"], "title": "Model-Driven Quantum Code Generation Using Large Language Models and Retrieval-Augmented Generation", "comment": "This paper is accepted to the New Ideas and Emerging Results (NIER)\n  track of the ACM/IEEE 28th International Conference on Model Driven\n  Engineering Languages and Systems (MODELS)", "summary": "This paper introduces a novel research direction for model-to-text/code\ntransformations by leveraging Large Language Models (LLMs) that can be enhanced\nwith Retrieval-Augmented Generation (RAG) pipelines. The focus is on quantum\nand hybrid quantum-classical software systems, where model-driven approaches\ncan help reduce the costs and mitigate the risks associated with the\nheterogeneous platform landscape and lack of developers' skills. We validate\none of the proposed ideas regarding generating code out of UML model instances\nof software systems. This Python code uses a well-established library, called\nQiskit, to execute on gate-based or circuit-based quantum computers. The RAG\npipeline that we deploy incorporates sample Qiskit code from public GitHub\nrepositories. Experimental results show that well-engineered prompts can\nimprove CodeBLEU scores by up to a factor of four, yielding more accurate and\nconsistent quantum code. However, the proposed research direction can go beyond\nthis through further investigation in the future by conducting experiments to\naddress our other research questions and ideas proposed here, such as deploying\nsoftware system model instances as the source of information in the RAG\npipelines, or deploying LLMs for code-to-code transformations, for instance,\nfor transpilation use cases.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.21107", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21107", "abs": "https://arxiv.org/abs/2508.21107", "authors": ["Dongjun Lee", "Changho Hwang", "Kimin Lee"], "title": "Learning to Generate Unit Test via Adversarial Reinforcement Learning", "comment": "Code is available at: https://github.com/dgjun32/UTRL", "summary": "Unit testing is a core practice in programming, enabling systematic\nevaluation of programs produced by human developers or large language models\n(LLMs). Given the challenges in writing comprehensive unit tests, LLMs have\nbeen employed to automate test generation, yet methods for training LLMs to\nproduce high-quality tests remain underexplored. In this work, we propose UTRL,\na novel reinforcement learning framework that trains an LLM to generate\nhigh-quality unit tests given a programming instruction. Our key idea is to\niteratively train two LLMs, the unit test generator and the code generator, in\nan adversarial manner via reinforcement learning. The unit test generator is\ntrained to maximize a discrimination reward, which reflects its ability to\nproduce tests that expose faults in the code generator's solutions, and the\ncode generator is trained to maximize a code reward, which reflects its ability\nto produce solutions that pass the unit tests generated by the test generator.\nIn our experiments, we demonstrate that unit tests generated by Qwen3-4B\ntrained via UTRL show higher quality compared to unit tests generated by the\nsame model trained via supervised fine-tuning on human-written ground-truth\nunit tests, yielding code evaluations that more closely align with those\ninduced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL\noutperforms frontier models such as GPT-4.1 in generating high-quality unit\ntests, highlighting the effectiveness of UTRL in training LLMs for this task.", "AI": {"tldr": "UTRL trains LLMs adversarially to generate high-quality unit tests outperforming state-of-the-art models via reinforcement learning.", "motivation": "The paper addresses the gap in methods for training LLMs to generate high-quality unit tests, as existing approaches relying on human-written examples (supervised fine-tuning) and non-adversarial methods are insufficiently explored and effective.", "method": "UTRL is a reinforcement learning framework that adversarially trains two LLMs: a unit test generator and a code generator. The test generator maximizes a discrimination reward by creating tests that expose faults in the code generator's solutions, while the code generator maximizes a code reward by producing solutions that pass these tests.", "result": "Unit tests generated by Qwen3-4B using UTRL demonstrate higher quality than those from supervised fine-tuning and outperform GPT-4.1 in both test efficacy and alignment with ground-truth evaluations.", "conclusion": "The study concludes that the proposed UTRL framework effectively trains LLMs to generate high-quality unit tests, outperforming both supervised fine-tuning and existing models like GPT-4.1 in test quality and code evaluation alignment."}}
{"id": "2508.21156", "categories": ["cs.SE", "D.2.7; I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.21156", "abs": "https://arxiv.org/abs/2508.21156", "authors": ["Kiana Kiashemshaki", "Arsham Khosravani", "Alireza Hosseinpour", "Arshia Akhavan"], "title": "Automated Bug Triaging using Instruction-Tuned Large Language Models", "comment": "11 pages, 7 figures", "summary": "Bug triaging, the task of assigning new issues to developers, is often slow\nand inconsistent in large projects. We present a lightweight framework that\ninstruction-tuned large language model (LLM) with LoRA adapters and uses\ncandidate-constrained decoding to ensure valid assignments. Tested on\nEclipseJDT and Mozilla datasets, the model achieves strong shortlist quality\n(Hit at 10 up to 0.753) despite modest exact Top-1 accuracy. On recent\nsnapshots, accuracy rises sharply, showing the framework's potential for\nreal-world, human-in-the-loop triaging. Our results suggest that\ninstruction-tuned LLMs offer a practical alternative to costly feature\nengineering and graph-based methods.", "AI": {"tldr": "The paper introduces a lightweight bug triaging framework using LLMs with LoRA and constrained decoding, achieving strong shortlist performance and showing promise for human-in-the-loop deployment.", "motivation": "Bug triaging in large projects is often slow and inconsistent, necessitating efficient and reliable automated solutions.", "method": "A lightweight framework utilizing instruction-tuned large language models (LLM) with LoRA adapters and candidate-constrained decoding for valid assignments.", "result": "Achieved high shortlist quality (Hit at 10 up to 0.753) on EclipseJDT and Mozilla datasets, with notable improvement in accuracy on recent data snapshots.", "conclusion": "The proposed framework demonstrates potential for real-world application in bug triaging, particularly with human involvement, and presents a practical alternative to existing costly methods."}}
{"id": "2508.21433", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21433", "abs": "https://arxiv.org/abs/2508.21433", "authors": ["Tobias Lindenbauer", "Igor Slinko", "Ludwig Felder", "Egor Bogomolov", "Yaroslav Zharov"], "title": "The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management", "comment": null, "summary": "Large Language Model (LLM)-based agents solve complex tasks through iterative\nreasoning, exploration, and tool-use, a process that can result in long,\nexpensive context histories. While state-of-the-art Software Engineering ( SE)\nagents like OpenHands or Cursor use LLM-based summarization to tackle this\nissue, it is unclear whether the increased complexity offers tangible\nperformance benefits compared to simply omitting older observations. We present\na systematic comparison of these strategies within SWE-agent on SWE-bench\nVerified across five diverse model configurations. We find that a simple\nobservation-masking strategy halves cost relative to a raw agent while\nmatching, and sometimes slightly exceeding, the solve rate of LLM\nsummarization. For example, with Qwen3-Coder 480B, masking improves solve rate\nfrom 53.8% (raw agent) to 54.8%, while remaining competitive with summarization\nat a lower cost. These results suggest that, at least within SWE-agent on\nSWE-bench Verified, the most effective and efficient context management can be\nthe simplest. We release code and data for reproducibility", "AI": {"tldr": "Simple observation-masking is better than expensive LLM summarization for managing context in software engineering agents on SWE-bench Verified, cutting costs in half while maintaining similar performance.", "motivation": "The paper addresses the challenge of managing long, expensive context histories in LLM-based agents used for complex tasks. While existing approaches like OpenHands use LLM summarization, the authors question whether this complexity delivers meaningful performance advantages over simpler alternatives like observation omission.\n", "method": "The researchers conducted a systematic comparison between LLM-based summarization and observation-masking strategies in the SWE-agent framework across five diverse model configurations. They evaluated cost and performance (solve rate) on SWE-bench Verified, a benchmark for software engineering agent evaluation.\n", "result": "Observation masking reduced computation costs by 50% compared to raw agents while matching or exceeding the solve rates of summarization-based approaches. For Qwen3-Coder 480B specifically, masking improved the solve rate from 53.8% to 54.8% at a lower cost than summarization. This pattern held consistently across all five tested model configurations.\n", "conclusion": "The study concludes that for context management in software engineering agents on SWE-bench Verified, simple observation-masking strategies outperform complex LLM-based summarization in terms of cost-effectiveness while maintaining or slightly improving solve rates.\n"}}
{"id": "2508.21219", "categories": ["cs.CR", "cs.ET", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.21219", "abs": "https://arxiv.org/abs/2508.21219", "authors": ["A H M Nazmus Sakib", "Mahsin Bin Akram", "Joseph Spracklen", "Sahan Kalutarage", "Raveen Wijewickrama", "Igor Bilogrevic", "Murtuza Jadliwala"], "title": "The WASM Cloak: Evaluating Browser Fingerprinting Defenses Under WebAssembly based Obfuscation", "comment": null, "summary": "Browser fingerprinting defenses have historically focused on detecting\nJavaScript(JS)-based tracking techniques. However, the widespread adoption of\nWebAssembly (WASM) introduces a potential blind spot, as adversaries can\nconvert JS to WASM's low-level binary format to obfuscate malicious logic. This\npaper presents the first systematic evaluation of how such WASM-based\nobfuscation impacts the robustness of modern fingerprinting defenses. We\ndevelop an automated pipeline that translates real-world JS fingerprinting\nscripts into functional WASM-obfuscated variants and test them against two\nclasses of defenses: state-of-the-art detectors in research literature and\ncommercial, in-browser tools. Our findings reveal a notable divergence:\ndetectors proposed in the research literature that rely on feature-based\nanalysis of source code show moderate vulnerability, stemming from outdated\ndatasets or a lack of WASM compatibility. In contrast, defenses such as browser\nextensions and native browser features remained completely effective, as their\nAPI-level interception is agnostic to the script's underlying implementation.\nThese results highlight a gap between academic and practical defense strategies\nand offer insights into strengthening detection approaches against WASM-based\nobfuscation, while also revealing opportunities for more evasive techniques in\nfuture attacks.", "AI": {"tldr": "WASM obfuscation bypasses academic JS-based fingerprinting detectors but not practical browser tools, highlighting a need for updated defense strategies and revealing risks for future attacks.", "motivation": "The adoption of WebAssembly (WASM) introduces a potential blind spot in browser fingerprinting defenses, as adversaries can obfuscate malicious logic by converting JavaScript to WASM's binary format.", "method": "The authors developed an automated pipeline to translate JS fingerprinting scripts into WASM-obfuscated variants, testing them against research-based detectors and commercial in-browser tools.", "result": "Research-based detectors relying on JS source code analysis showed moderate vulnerability due to outdated datasets and WASM incompatibility, while commercial tools like browser extensions and native features remained effective against obfuscated scripts.", "conclusion": "The study reveals a gap between academic and practical defense strategies against WASM-based obfuscation, emphasizing the need to strengthen detection approaches and anticipate future evasive techniques."}}
{"id": "2508.21454", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21454", "abs": "https://arxiv.org/abs/2508.21454", "authors": ["Baijun Cheng", "Kailong Wang", "Ling Shi", "Haoyu Wang", "Yao Guo", "Ding Li", "Xiangqun Chen"], "title": "Enhancing Semantic Understanding in Pointer Analysis using Large Language Models", "comment": "Accepted by LMPL 2025", "summary": "Pointer analysis has been studied for over four decades. However, existing\nframeworks continue to suffer from the propagation of incorrect facts. A major\nlimitation stems from their insufficient semantic understanding of code,\nresulting in overly conservative treatment of user-defined functions. Recent\nadvances in large language models (LLMs) present new opportunities to bridge\nthis gap. In this paper, we propose LMPA (LLM-enhanced Pointer Analysis), a\nvision that integrates LLMs into pointer analysis to enhance both precision and\nscalability. LMPA identifies user-defined functions that resemble system APIs\nand models them accordingly, thereby mitigating erroneous cross-calling-context\npropagation. Furthermore, it enhances summary-based analysis by inferring\ninitial points-to sets and introducing a novel summary strategy augmented with\nnatural language. Finally, we discuss the key challenges involved in realizing\nthis vision.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.21302", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21302", "abs": "https://arxiv.org/abs/2508.21302", "authors": ["Jie Zhu", "Chihao Shen", "Ziyang Li", "Jiahao Yu", "Yizheng Chen", "Kexin Pei"], "title": "Locus: Agentic Predicate Synthesis for Directed Fuzzing", "comment": null, "summary": "Directed fuzzing aims to find program inputs that lead to specified target\nprogram states. It has broad applications, such as debugging system crashes,\nconfirming reported bugs, and generating exploits for potential\nvulnerabilities. This task is inherently challenging because target states are\noften deeply nested in the program, while the search space manifested by\nnumerous possible program inputs is prohibitively large. Existing approaches\nrely on branch distances or manually-specified constraints to guide the search;\nhowever, the branches alone are often insufficient to precisely characterize\nprogress toward reaching the target states, while the manually specified\nconstraints are often tailored for specific bug types and thus difficult to\ngeneralize to diverse target states and programs.\n  We present Locus, a novel framework to improve the efficiency of directed\nfuzzing. Our key insight is to synthesize predicates to capture fuzzing\nprogress as semantically meaningful intermediate states, serving as milestones\ntowards reaching the target states. When used to instrument the program under\nfuzzing, they can reject executions unlikely to reach the target states, while\nproviding additional coverage guidance. To automate this task and generalize to\ndiverse programs, Locus features an agentic framework with program analysis\ntools to synthesize and iteratively refine the candidate predicates, while\nensuring the predicates strictly relax the target states to prevent false\nrejections via symbolic execution. Our evaluation shows that Locus\nsubstantially improves the efficiency of eight state-of-the-art fuzzers in\ndiscovering real-world vulnerabilities, achieving an average speedup of 41.6x.\nSo far, Locus has found eight previously unpatched bugs, with one already\nacknowledged with a draft patch.", "AI": {"tldr": "Locus automates predicate synthesis for directed fuzzing, improving efficiency 41.6\u00d7 and discovering eight new vulnerabilities, addressing limitations of manual constraints and branch-distance-based approaches.", "motivation": "Existing directed fuzzers rely on branch distances or manual constraints that are either imprecise or non-generalizable. This limits their effectiveness in discovering deeply nested vulnerabilities across diverse programs.", "method": "Locus synthesizes semantically meaningful predicates as milestones using an agentic framework with program analysis tools and symbolic execution. These predicates guide fuzzing to reject unproductive paths while ensuring strict relaxation of target constraints.", "result": "Locus achieves 41.6\u00d7 average speedup over eight state-of-the-art fuzzers for real-world vulnerability discovery, identifying eight previously unpatched bugs with one vulnerability already receiving a draft patch.", "conclusion": "Locus significantly enhances directed fuzzing efficiency by synthesizing predicates to capture progress toward target states, outperforming existing methods with a 41.6x speedup and discovering eight new unpatched vulnerabilities."}}
{"id": "2508.21553", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21553", "abs": "https://arxiv.org/abs/2508.21553", "authors": ["J\u00f8rn Eirik Betten", "Quentin Mazouni", "Dennis Gross", "Pedro Lind", "Helge Spieker"], "title": "Reusable Test Suites for Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) agents show great promise in solving sequential\ndecision-making tasks. However, validating the reliability and performance of\nthe agent policies' behavior for deployment remains challenging. Most\nreinforcement learning policy testing methods produce test suites tailored to\nthe agent policy being tested, and their relevance to other policies is\nunclear. This work presents Multi-Policy Test Case Selection (MPTCS), a novel\nautomated test suite selection method for RL environments, designed to extract\ntest cases generated by any policy testing framework based on their\nsolvability, diversity, and general difficulty. MPTCS uses a set of policies to\nselect a diverse collection of reusable policy-agnostic test cases that reveal\ntypical flaws in the agents' behavior. The set of policies selects test cases\nfrom a candidate pool, which can be generated by any policy testing method,\nbased on a difficulty score. We assess the effectiveness of the difficulty\nscore and how the method's effectiveness and cost depend on the number of\npolicies in the set. Additionally, a method for promoting diversity in the test\nsuite, a discretized general test case descriptor surface inspired by\nquality-diversity algorithms, is examined to determine how it covers the state\nspace and which policies it triggers to produce faulty behaviors.", "AI": {"tldr": "This paper introduces Multi-Policy Test Case Selection (MPTCS), a policy-agnostic RL testing framework that automatically selects diverse, reusable test cases by combining multiple policies' perspectives on difficulty and coverage, enhancing deployment reliability.", "motivation": "Current RL policy testing methods generate policy-specific test suites, limiting their reuse for other agents. This creates inefficiencies in validating agent reliability for deployment. The paper addresses this gap by proposing a method that generates reusable, policy-agnostic test cases to systematically identify common flaws across diverse RL agents.", "method": "MPTCS employs a policy-based selection process using three criteria: solvability (test case difficulty), diversity (coverage of state space), and generalizability. It calculates a difficulty score for test cases across policies and introduces a discretized descriptor surface inspired by quality-diversity algorithms to systematically diversify the test suite. Candidate test cases are filtered from any policy testing framework's output.", "result": "Experiments demonstrate that increasing policy diversity in selection improves the method's effectiveness and cost-efficiency. The descriptor surface successfully covers state space regions and identifies policies that expose faulty behaviors. The difficulty score metric reliably ranks test cases for cross-policy relevance.", "conclusion": "MPTCS provides a robust framework for selecting reusable RL test cases across diverse policies. By leveraging multiple policies and diversity-promotion techniques, the method enhances testing effectiveness for detecting agent flaws while ensuring cost-efficiency through policy-agnostic test suite design."}}
{"id": "2508.21323", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.21323", "abs": "https://arxiv.org/abs/2508.21323", "authors": ["Kunal Mukherjee", "Murat Kantarcioglu"], "title": "LLM-driven Provenance Forensics for Threat Investigation and Detection", "comment": null, "summary": "We introduce PROVSEEK, an LLM-powered agentic framework for automated\nprovenance-driven forensic analysis and threat intelligence extraction.\nPROVSEEK employs specialized toolchains to dynamically retrieve relevant\ncontext by generating precise, context-aware queries that fuse a vectorized\nthreat report knowledge base with data from system provenance databases. The\nframework resolves provenance queries, orchestrates multiple role-specific\nagents to mitigate hallucinations, and synthesizes structured, ground-truth\nverifiable forensic summaries. By combining agent orchestration with\nRetrieval-Augmented Generation (RAG) and chain-of-thought (CoT) reasoning,\nPROVSEEK enables adaptive multi-step analysis that iteratively refines\nhypotheses, verifies supporting evidence, and produces scalable, interpretable\nforensic explanations of attack behaviors. By combining provenance data with\nagentic reasoning, PROVSEEK establishes a new paradigm for grounded agentic\nforecics to investigate APTs. We conduct a comprehensive evaluation on publicly\navailable DARPA datasets, demonstrating that PROVSEEK outperforms\nretrieval-based methods for intelligence extraction task, achieving a 34%\nimprovement in contextual precision/recall; and for threat detection task,\nPROVSEEK achieves 22%/29% higher precision/recall compared to both a baseline\nagentic AI approach and State-Of-The-Art (SOTA) Provenance-based Intrusion\nDetection System (PIDS).", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.21634", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21634", "abs": "https://arxiv.org/abs/2508.21634", "authors": ["Domenico Cotroneo", "Cristina Improta", "Pietro Liguori"], "title": "Human-Written vs. AI-Generated Code: A Large-Scale Study of Defects, Vulnerabilities, and Complexity", "comment": "Accepted to the 36th IEEE International Symposium on Software\n  Reliability Engineering (ISSRE, 2025)", "summary": "As AI code assistants become increasingly integrated into software\ndevelopment workflows, understanding how their code compares to human-written\nprograms is critical for ensuring reliability, maintainability, and security.\nIn this paper, we present a large-scale comparison of code authored by human\ndevelopers and three state-of-the-art LLMs, i.e., ChatGPT, DeepSeek-Coder, and\nQwen-Coder, on multiple dimensions of software quality: code defects, security\nvulnerabilities, and structural complexity. Our evaluation spans over 500k code\nsamples in two widely used languages, Python and Java, classifying defects via\nOrthogonal Defect Classification and security vulnerabilities using the Common\nWeakness Enumeration. We find that AI-generated code is generally simpler and\nmore repetitive, yet more prone to unused constructs and hardcoded debugging,\nwhile human-written code exhibits greater structural complexity and a higher\nconcentration of maintainability issues. Notably, AI-generated code also\ncontains more high-risk security vulnerabilities. These findings highlight the\ndistinct defect profiles of AI- and human-authored code and underscore the need\nfor specialized quality assurance practices in AI-assisted programming.", "AI": {"tldr": "Large-scale analysis shows AI-generated code is simpler but more vulnerable; human code is complex yet less maintainable, urging specialized QA for AI-assisted programming.", "motivation": "Understanding differences in software quality (reliability, maintainability, security) between AI code assistants and human developers is critical as AI integrates into software workflows.", "method": "We evaluated over 500k code samples in Python and Java, classifying defects via Orthogonal Defect Classification and security vulnerabilities using Common Weakness Enumeration across three LLMs (ChatGPT, DeepSeek-Coder, Qwen-Coder) and human-authored code.", "result": "AI-generated code shows higher simplicity, repetition, unused constructs, hardcoded debugging, and high-risk security vulnerabilities. Human code exhibits greater structural complexity but more maintainability issues.", "conclusion": "The study reveals distinct defect profiles between AI-generated and human-written code, emphasizing the necessity for tailored quality assurance practices in AI-assisted programming to address differences in simplicity, security risks, and maintainability."}}
{"id": "2508.21386", "categories": ["cs.CR", "cs.CY", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21386", "abs": "https://arxiv.org/abs/2508.21386", "authors": ["Jukka Ruohonen", "Jesper L\u00f8ffler Nielsen", "Jakub Sk\u00f3rczynski"], "title": "Risks and Compliance with the EU's Core Cyber Security Legislation", "comment": "Submitted to IST (VSI:RegCompliance in SE)", "summary": "The European Union (EU) has long favored a risk-based approach to regulation.\nSuch an approach is also used in recent cyber security legislation enacted in\nthe EU. Risks are also inherently related to compliance with the new\nlegislation. Objective: The paper investigates how risks are framed in the EU's\nfive core cyber security legislative acts, whether the framings indicate\nconvergence or divergence between the acts and their risk concepts, and what\nqualifying words and terms are used when describing the legal notions of risks.\nMethod : The paper's methodology is based on qualitative legal interpretation\nand taxonomy-building. Results: The five acts have an encompassing coverage of\ndifferent cyber security risks, including but not limited to risks related to\ntechnical, organizational, and human security as well as those not originating\nfrom man-made actions. Both technical aspects and assets are used to frame the\nlegal risk notions in many of the legislative acts. A threat-centric viewpoint\nis also present in one of the acts. Notable gaps are related to acceptable\nrisks, non-probabilistic risks, and residual risks. Conclusion: The EU's new\ncyber security legislation has significantly extended the risk-based approach\nto regulations. At the same time, complexity and compliance burden have\nincreased. With this point in mind, the paper concludes with a few practical\ntakeaways about means to deal with compliance and research it.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.21811", "categories": ["cs.SE", "68", "D.2.9"], "pdf": "https://arxiv.org/pdf/2508.21811", "abs": "https://arxiv.org/abs/2508.21811", "authors": ["Ashley Hourigan", "Ridewaan Hanslo"], "title": "The Integration of Agile Methodologies in DevOps Practices within the Information Technology Industry", "comment": "10 pages, 2 figures, conference", "summary": "The demand for rapid software delivery in the Information Technology (IT)\nindustry has significantly intensified, emphasising the need for faster\nsoftware products and service releases with enhanced features to meet customer\nexpectations. Agile methodologies are replacing traditional approaches such as\nWaterfall, where flexibility, iterative development and adaptation to change\nare favoured over rigid planning and execution. DevOps, a subsequent evolution\nfrom Agile, emphasises collaborative efforts in development and operations\nteams, focusing on continuous integration and deployment to deliver resilient\nand high-quality software products and services. This study aims to critically\nassess both Agile and DevOps practices in the IT industry to identify the\nfeasibility and applicability of Agile methods in DevOps practices. Eleven\nsemi-structured interviews were conducted with Agile and DevOps practitioners\nin varying capacities across several sectors within the IT industry. Through\nthematic analysis, 51 unique codes were extracted and synthesised into 19\nthemes that reported on each phase of the DevOps lifecycle, specifically\nregarding the integration and implementation of Agile methods into DevOps\npractices. Based on the findings, a new understanding detailing the\ninterrelationship of Agile methods in DevOps practices was discussed that met\nthe research objectives.", "AI": {"tldr": "This study explores Agile-DevOps integration in IT, using interviews to identify how Agile's flexibility enhances DevOps practices, achieving faster, resilient software delivery.", "motivation": "Driven by the IT industry's shift toward rapid software delivery, this study addresses the need to critically assess the applicability of Agile methodologies within DevOps practices to balance flexibility and operational resilience.", "method": "The research employed eleven semi-structured interviews with Agile and DevOps practitioners, utilizing thematic analysis to extract 51 codes and synthesize 19 themes across the DevOps lifecycle.", "result": "The analysis revealed 19 synthesized themes detailing the integration of Agile into DevOps, leading to a novel framework that clarifies their interrelationship and supports their combined use in software development.", "conclusion": "The study concludes that Agile methods are feasible and beneficial when integrated into DevOps practices, enhancing collaborative and iterative software delivery processes."}}
{"id": "2508.21393", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21393", "abs": "https://arxiv.org/abs/2508.21393", "authors": ["Guofu Liao", "Taotao Wang", "Shengli Zhang", "Jiqun Zhang", "Shi Long", "Dacheng Tao"], "title": "zkLoRA: Fine-Tuning Large Language Models with Verifiable Security via Zero-Knowledge Proofs", "comment": null, "summary": "Fine-tuning large language models (LLMs) is crucial for adapting them to\nspecific tasks, yet it remains computationally demanding and raises concerns\nabout correctness and privacy, particularly in untrusted environments. Although\nparameter-efficient methods like Low-Rank Adaptation (LoRA) significantly\nreduce resource requirements, ensuring the security and verifiability of\nfine-tuning under zero-knowledge constraints remains an unresolved challenge.\nTo address this, we introduce zkLoRA, the first framework to integrate LoRA\nfine-tuning with zero-knowledge proofs (ZKPs), achieving provable security and\ncorrectness. zkLoRA employs advanced cryptographic techniques -- such as lookup\narguments, sumcheck protocols, and polynomial commitments -- to verify both\narithmetic and non-arithmetic operations in Transformer-based architectures.\nThe framework provides end-to-end verifiability for forward propagation,\nbackward propagation, and parameter updates during LoRA fine-tuning, while\nsafeguarding the privacy of model parameters and training data. Leveraging\nGPU-based implementations, zkLoRA demonstrates practicality and efficiency\nthrough experimental validation on open-source LLMs like LLaMA, scaling up to\n13 billion parameters. By combining parameter-efficient fine-tuning with ZKPs,\nzkLoRA bridges a critical gap, enabling secure and trustworthy deployment of\nLLMs in sensitive or untrusted environments.", "AI": {"tldr": "zkLoRA: A secure, verifiable framework combining zero-knowledge proofs and parameter-efficient fine-tuning for LLMs in untrusted environments.", "motivation": "Fine-tuning LLMs is computationally intensive and raises security/privacy concerns in untrusted environments, while existing methods like LoRA lack verifiability under zero-knowledge constraints.", "method": "The framework integrates LoRA with zero-knowledge proofs (ZKPs), using cryptographic techniques like lookup arguments, sumcheck protocols, and polynomial commitments to verify arithmetic and non-arithmetic operations in Transformer architectures while preserving privacy.", "result": "zkLoRA achieves end-to-end verifiability for forward/backward propagation and parameter updates during LoRA fine-tuning, demonstrating practicality on LLaMA with up to 13B parameters through GPU-based implementations.", "conclusion": "zkLoRA bridges the gap between parameter-efficient fine-tuning and secure verification in untrusted environments, enabling trustworthy LLM deployment."}}
{"id": "2508.21417", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21417", "abs": "https://arxiv.org/abs/2508.21417", "authors": ["Shuhan Liu", "Xing Hu", "Xin Xia", "David Lo", "Xiaohu Yang"], "title": "An Empirical Study of Vulnerable Package Dependencies in LLM Repositories", "comment": null, "summary": "Large language models (LLMs) have developed rapidly in recent years,\nrevolutionizing various fields. Despite their widespread success, LLMs heavily\nrely on external code dependencies from package management systems, creating a\ncomplex and interconnected LLM dependency supply chain. Vulnerabilities in\ndependencies can expose LLMs to security risks. While existing research\npredominantly focuses on model-level security threats, vulnerabilities within\nthe LLM dependency supply chain have been overlooked. To fill this gap, we\nconducted an empirical analysis of 52 open-source LLMs, examining their\nthird-party dependencies and associated vulnerabilities. We then explored\nactivities within the LLM repositories to understand how maintainers manage\nthird-party vulnerabilities in practice. Finally, we compared third-party\ndependency vulnerabilities in the LLM ecosystem to those in the Python\necosystem. Our results show that half of the vulnerabilities in the LLM\necosystem remain undisclosed for more than 56.2 months, significantly longer\nthan those in the Python ecosystem. Additionally, 75.8% of LLMs include\nvulnerable dependencies in their configuration files. This study advances the\nunderstanding of LLM supply chain risks, provides insights for practitioners,\nand highlights potential directions for improving the security of the LLM\nsupply chain.", "AI": {"tldr": "LLMs have pervasive, long-disclosed dependency vulnerabilities (50% for +56 months) and 75% include vulnerable packages, revealing critical supply chain security risks.", "motivation": "Existing research overlooks LLM dependency supply chain vulnerabilities despite their critical security risks due to heavy reliance on external packages.", "method": "Empirical analysis of 52 open-source LLMs' dependencies and vulnerabilities, examination of repository vulnerability management practices, and comparison to the Python ecosystem.", "result": "50% of LLM vulnerabilities remain undisclosed for >56 months (vs. Python) and 75.8% of LLMs contain vulnerable dependencies in config files.", "conclusion": "This study advances understanding of LLM supply chain risks, provides practitioner insights, and highlights security improvement directions."}}
{"id": "2508.21432", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21432", "abs": "https://arxiv.org/abs/2508.21432", "authors": ["Wenjie Qu", "Yuguang Zhou", "Bo Wang", "Wengrui Zheng", "Yuexin Li", "Jinyuan Jia", "Jiaheng Zhang"], "title": "RepoMark: A Code Usage Auditing Framework for Code Large Language Models", "comment": null, "summary": "The rapid development of Large Language Models (LLMs) for code generation has\ntransformed software development by automating coding tasks with unprecedented\nefficiency.\n  However, the training of these models on open-source code repositories (e.g.,\nfrom GitHub) raises critical ethical and legal concerns, particularly regarding\ndata authorization and open-source license compliance. Developers are\nincreasingly questioning whether model trainers have obtained proper\nauthorization before using repositories for training, especially given the lack\nof transparency in data collection.\n  To address these concerns, we propose a novel data marking framework RepoMark\nto audit the data usage of code LLMs. Our method enables repository owners to\nverify whether their code has been used in training, while ensuring semantic\npreservation, imperceptibility, and theoretical false detection rate (FDR)\nguarantees. By generating multiple semantically equivalent code variants,\nRepoMark introduces data marks into the code files, and during detection,\nRepoMark leverages a novel ranking-based hypothesis test to detect memorization\nwithin the model. Compared to prior data auditing approaches, RepoMark\nsignificantly enhances sample efficiency, allowing effective auditing even when\nthe user's repository possesses only a small number of code files.\n  Experiments demonstrate that RepoMark achieves a detection success rate over\n90\\% on small code repositories under a strict FDR guarantee of 5\\%. This\nrepresents a significant advancement over existing data marking techniques, all\nof which only achieve accuracy below 55\\% under identical settings. This\nfurther validates RepoMark as a robust, theoretically sound, and promising\nsolution for enhancing transparency in code LLM training, which can safeguard\nthe rights of repository owners.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.21440", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.21440", "abs": "https://arxiv.org/abs/2508.21440", "authors": ["Shan Wang", "Ming Yang", "Yu Liu", "Yue Zhang", "Shuaiqing Zhang", "Zhen Ling", "Jiannong Cao", "Xinwen Fu"], "title": "Time Tells All: Deanonymization of Blockchain RPC Users with Zero Transaction Fee (Extended Version)", "comment": null, "summary": "Remote Procedure Call (RPC) services have become a primary gateway for users\nto access public blockchains. While they offer significant convenience, RPC\nservices also introduce critical privacy challenges that remain insufficiently\nexamined. Existing deanonymization attacks either do not apply to blockchain\nRPC users or incur costs like transaction fees assuming an active network\neavesdropper. In this paper, we propose a novel deanonymization attack that can\nlink an IP address of a RPC user to this user's blockchain pseudonym. Our\nanalysis reveals a temporal correlation between the timestamps of transaction\nconfirmations recorded on the public ledger and those of TCP packets sent by\nthe victim when querying transaction status. We assume a strong passive\nadversary with access to network infrastructure, capable of monitoring traffic\nat network border routers or Internet exchange points. By monitoring network\ntraffic and analyzing public ledgers, the attacker can link the IP address of\nthe TCP packet to the pseudonym of the transaction initiator by exploiting the\ntemporal correlation. This deanonymization attack incurs zero transaction fee.\nWe mathematically model and analyze the attack method, perform large-scale\nmeasurements of blockchain ledgers, and conduct real-world attacks to validate\nthe attack. Our attack achieves a high success rate of over 95% against normal\nRPC users on various blockchain networks, including Ethereum, Bitcoin and\nSolana.", "AI": {"tldr": "This paper introduces a zero-cost, real-world effective deanonymization attack on blockchain RPC users by exploiting timing correlations between network traffic and ledger data, achieving >95% accuracy across major blockchains.", "motivation": "RPC services in public blockchains introduce insufficiently studied privacy risks. Existing deanonymization approaches either are inapplicable or require transaction fees, but this work addresses a new attack vector that requires no active participation from the adversary.", "method": "The authors exploit temporal correlations between transaction confirmation timestamps on the blockchain ledger and the timestamps of TCP packets sent by the victim when querying transaction status. They mathematically model the attack, analyze ledger data, and conduct real-world experiments.", "result": "The attack achieves >95% success in linking victims' IP addresses to pseudonyms on Ethereum, Bitcoin, and Solana blockchains, with no transaction fees incurred by the attacker.", "conclusion": "The paper demonstrates a highly effective and cost-free deanonymization attack on blockchain RPC users, emphasizing the critical privacy vulnerabilities in such services and the threat posed by passive adversaries with network infrastructure access."}}
{"id": "2508.21457", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.21457", "abs": "https://arxiv.org/abs/2508.21457", "authors": ["Fengchao Chen", "Tingmin Wu", "Van Nguyen", "Carsten Rudolph"], "title": "SoK: Large Language Model-Generated Textual Phishing Campaigns End-to-End Analysis of Generation, Characteristics, and Detection", "comment": "13 pages, 3 tables, 4 figures", "summary": "Phishing is a pervasive form of social engineering in which attackers\nimpersonate trusted entities to steal information or induce harmful actions.\nText-based phishing dominates for its low cost, scalability, and\nconcealability, advantages recently amplified by large language models (LLMs)\nthat enable ``Phishing-as-a-Service'' attacks at scale within minutes. Despite\nthe growing research into LLM-facilitated phishing attacks, consolidated\nsystematic research on the phishing attack life cycle remains scarce. In this\nwork, we present the first systematization of knowledge (SoK) on LLM-generated\nphishing, offering an end-to-end analysis that spans generation techniques,\nattack features, and mitigation strategies. We introduce\nGeneration-Characterization-Defense (GenCharDef), which systematizes the ways\nin which LLM-generated phishing differs from traditional phishing across\nmethodologies, security perspectives, data dependencies, and evaluation\npractices. This framework highlights unique challenges of LLM-driven phishing,\nproviding a coherent foundation for understanding the evolving threat landscape\nand guiding the design of more resilient defenses.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.21636", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21636", "abs": "https://arxiv.org/abs/2508.21636", "authors": ["Cristina Improta"], "title": "Detecting Stealthy Data Poisoning Attacks in AI Code Generators", "comment": "Accepted to the 3rd IEEE International Workshop on Reliable and\n  Secure AI for Software Engineering (ReSAISE, 2025), co-located with ISSRE\n  2025", "summary": "Deep learning (DL) models for natural language-to-code generation have become\nintegral to modern software development pipelines. However, their heavy\nreliance on large amounts of data, often collected from unsanitized online\nsources, exposes them to data poisoning attacks, where adversaries inject\nmalicious samples to subtly bias model behavior. Recent targeted attacks\nsilently replace secure code with semantically equivalent but vulnerable\nimplementations without relying on explicit triggers to launch the attack,\nmaking it especially hard for detection methods to distinguish clean from\npoisoned samples. We present a systematic study on the effectiveness of\nexisting poisoning detection methods under this stealthy threat model.\nSpecifically, we perform targeted poisoning on three DL models (CodeBERT,\nCodeT5+, AST-T5), and evaluate spectral signatures analysis, activation\nclustering, and static analysis as defenses. Our results show that all methods\nstruggle to detect triggerless poisoning, with representation-based approaches\nfailing to isolate poisoned samples and static analysis suffering false\npositives and false negatives, highlighting the need for more robust,\ntrigger-independent defenses for AI-assisted code generation.", "AI": {"tldr": "This study shows existing defenses against stealthy, triggerless dataset poisoning attacks in code generation models are ineffective, requiring new trigger-independent detection methods to secure AI-assisted code generation.", "motivation": "DL models for natural language-to-code generation face stealthy data poisoning attacks that replace secure code with vulnerable implementations without detectable triggers, making existing detection methods inadequate to distinguish poisoned samples from clean ones.", "method": "The researchers conducted targeted poisoning on three DL models (CodeBERT, CodeT5+, AST-T5) and evaluated the effectiveness of spectral signatures analysis, activation clustering, and static analysis as defense mechanisms under stealthy threat conditions.", "result": "All tested defense methods failed to adequately detect triggerless poisoning: representation-based approaches couldn't isolate poisoned samples, while static analysis had high false positive/negative rates.", "conclusion": "The study highlights the need for more robust, trigger-independent defenses for AI-assisted code generation as existing methods struggle with detecting stealthy, triggerless poisoning attacks."}}
{"id": "2508.21480", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.21480", "abs": "https://arxiv.org/abs/2508.21480", "authors": ["Narges Dadkhah", "Khan Reaz", "Gerhard Wunder"], "title": "Towards a Decentralized IoT Onboarding for Smart Homes Using Consortium Blockchain", "comment": null, "summary": "The increasing adoption of smart home devices and IoT-based security systems\npresents significant opportunities to enhance convenience, safety, and risk\nmanagement for homeowners and service providers. However, secure\nonboarding-provisioning credentials and establishing trust with cloud\nplatforms-remains a considerable challenge. Traditional onboarding methods\noften rely on centralized Public Key Infrastructure (PKI) models and\nmanufacturer-controlled keys, which introduce security risks and limit the\nuser's digital sovereignty. These limitations hinder the widespread deployment\nof scalable IoT solutions. This paper presents a novel onboarding framework\nthat builds upon existing network-layer onboarding techniques and extends them\nto the application layer to address these challenges. By integrating consortium\nblockchain technology, we propose a decentralized onboarding mechanism that\nenhances transparency, security, and monitoring for smart home architectures.\nThe architecture supports device registration, key revocation, access control\nmanagement, and risk detection through event-driven alerts across dedicated\nblockchain channels and smart contracts. To evaluate the framework, we formally\nmodel the protocol using the Tamarin Prover under the Dolev-Yao adversary\nmodel. The analysis focuses on authentication, token integrity, key\nconfidentiality, and resilience over public channels. A prototype\nimplementation demonstrates the system's viability in smart home settings, with\nverification completing in 0.34 seconds, highlighting its scalability and\nsuitability for constrained devices and diverse stakeholders. Additionally,\nperformance evaluation shows that the blockchain-based approach effectively\nhandles varying workloads, maintains high throughput and low latency, and\nsupports near real-time IoT data processing.", "AI": {"tldr": "This paper introduces a blockchain-based decentralized onboarding framework for smart home IoT devices, addressing security and scalability challenges. It achieves rapid verification, strong authentication, and efficient real-time performance through consortium blockchain and smart contracts.", "motivation": "Traditional IoT onboarding depends on centralized PKI models and manufacturer-controlled keys, posing security risks and limiting user digital sovereignty. These limitations hinder scalable IoT adoption in smart homes.", "method": "A decentralized framework integrating consortium blockchain for secure onboarding, extending network-layer techniques to the application layer. Key features include smart contracts for device registration, key revocation, access control, and risk detection. Evaluated using Tamarin Prover and a prototype implementation.", "result": "Prototype verification completed in 0.34 seconds, confirming scalability for constrained devices. Blockchain-based approach achieved high throughput and low latency, enabling near real-time IoT data processing while maintaining security under adversarial analysis.", "conclusion": "The proposed blockchain-based onboarding framework effectively enhances security, transparency, and scalability for smart home IoT systems, demonstrating viability for real-world deployment through rapid verification and robust performance metrics."}}
{"id": "2508.21558", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.21558", "abs": "https://arxiv.org/abs/2508.21558", "authors": ["Federica Bianchi", "Edoardo Di Paolo", "Angelo Spognardi"], "title": "Generalized Encrypted Traffic Classification Using Inter-Flow Signals", "comment": "Accepted manuscript at Availability, Reliability and Security (ARES\n  2025), published in Lecture Notes in Computer Science, vol. 15992, Springer,\n  Cham. DOI: https://doi.org/10.1007/978-3-032-00624-0_11", "summary": "In this paper, we present a novel encrypted traffic classification model that\noperates directly on raw PCAP data without requiring prior assumptions about\ntraffic type. Unlike existing methods, it is generalizable across multiple\nclassification tasks and leverages inter-flow signals - an innovative\nrepresentation that captures temporal correlations and packet volume\ndistributions across flows. Experimental results show that our model\noutperforms well-established methods in nearly every classification task and\nacross most datasets, achieving up to 99% accuracy in some cases, demonstrating\nits robustness and adaptability.", "AI": {"tldr": "The paper introduces a new encrypted traffic classification model that works directly on raw PCAP data, captures temporal correlations and packet volume distributions using inter-flow signals, and outperforms existing methods with up to 99% accuracy across multiple tasks and datasets.", "motivation": "Existing encrypted traffic classification methods require prior assumptions about traffic types and lack generalizability across different tasks.", "method": "The proposed model operates on raw PCAP data and uses inter-flow signals as an innovative representation to capture temporal correlations and packet volume distributions across flows.", "result": "The model achieves up to 99% accuracy in various classification tasks and outperforms established methods across multiple datasets.", "conclusion": "The novel encrypted traffic classification model demonstrates robustness and adaptability by achieving high accuracy without traffic-type assumptions and generalizing across tasks."}}
{"id": "2508.21579", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.21579", "abs": "https://arxiv.org/abs/2508.21579", "authors": ["Ziyue Wang", "Liyi Zhou"], "title": "Agentic Discovery and Validation of Android App Vulnerabilities", "comment": null, "summary": "Existing Android vulnerability detection tools overwhelm teams with thousands\nof low-signal warnings yet uncover few true positives. Analysts spend days\ntriaging these results, creating a bottleneck in the security pipeline.\nMeanwhile, genuinely exploitable vulnerabilities often slip through, leaving\nopportunities open to malicious counterparts.\n  We introduce A2, a system that mirrors how security experts analyze and\nvalidate Android vulnerabilities through two complementary phases: (i) Agentic\nVulnerability Discovery, which reasons about application security by combining\nsemantic understanding with traditional security tools; and (ii) Agentic\nVulnerability Validation, which systematically validates vulnerabilities across\nAndroid's multi-modal attack surface-UI interactions, inter-component\ncommunication, file system operations, and cryptographic computations.\n  On the Ghera benchmark (n=60), A2 achieves 78.3% coverage, surpassing\nstate-of-the-art analyzers (e.g., APKHunt 30.0%). Rather than overwhelming\nanalysts with thousands of warnings, A2 distills results into 82 speculative\nvulnerability findings, including 47 Ghera cases and 28 additional true\npositives. Crucially, A2 then generates working Proof-of-Concepts (PoCs) for 51\nof these speculative findings, transforming them into validated vulnerability\nfindings that provide direct, self-confirming evidence of exploitability.\n  In real-world evaluation on 169 production APKs, A2 uncovers 104\ntrue-positive zero-day vulnerabilities. Among these, 57 (54.8%) are\nself-validated with automatically generated PoCs, including a medium-severity\nvulnerability in a widely used application with over 10 million installs.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.21602", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.21602", "abs": "https://arxiv.org/abs/2508.21602", "authors": ["Tomasz Kazana"], "title": "Condense to Conduct and Conduct to Condense", "comment": null, "summary": "In this paper we give the first examples of low-conductance permutations. The\nnotion of conductance of permutations was introduced in the paper\n\"Indifferentiability of Confusion-Diffusion Networks\" by Dodis et al., where\nthe search for low-conductance permutations was initiated and motivated. In\nthis paper we not only give the desired examples, but also make a general\ncharacterization of the problem -- i.e. we show that low-conductance\npermutations are equivalent to permutations that have the information-theoretic\nproperties of the so-called Multi-Source-Somewhere-Condensers.", "AI": {"tldr": "The paper provides the first examples of low-conductance permutations and links them to Multi-Source-Somewhere-Condensers' information-theoretic properties.", "motivation": "The search for low-conductance permutations was initiated and motivated by Dodis et al. in their previous work to enhance cryptographic network security.", "method": "The authors started by generating low-conductance permutations as per Dodis et al.'s requirements and then established a general equivalence between low-conductance permutations and the information-theoretical properties of Multi-Source-Somewhere-Condensers.", "result": "The paper achieved the creation of the first low-conductance permutations and formalized a characterization showing their equivalence to Multi-Source-Somewhere-Condensers' properties.", "conclusion": "This analysis connects low-conductance permutations to an important information-theoretical construct, suggesting implications for the design and analysis of cryptographic systems."}}
{"id": "2508.21606", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.21606", "abs": "https://arxiv.org/abs/2508.21606", "authors": ["Nishant Chinnasami", "Rasha Karakchi"], "title": "Hybrid Cryptographic Monitoring System for Side-Channel Attack Detection on PYNQ SoCs", "comment": "This paper is submitted at Supercomputing (SC'25)", "summary": "AES-128 encryption is theoretically secure but vulnerable in practical\ndeployments due to timing and fault injection attacks on embedded systems. This\nwork presents a lightweight dual-detection framework combining statistical\nthresholding and machine learning (ML) for real-time anomaly detection. By\nsimulating anomalies via delays and ciphertext corruption, we collect timing\nand data features to evaluate two strategies: (1) a statistical threshold\nmethod based on execution time and (2) a Random Forest classifier trained on\nblock-level anomalies. Implemented on CPU and FPGA (PYNQ-Z1), our results show\nthat the ML approach outperforms static thresholds in accuracy, while\nmaintaining real-time feasibility on embedded platforms. The framework operates\nwithout modifying AES internals or relying on hardware performance counters.\nThis makes it especially suitable for low-power, resource-constrained systems\nwhere detection accuracy and computational efficiency must be balanced.", "AI": {"tldr": "A machine learning-based dual-detection framework detects AES-128 anomalies in real-time on embedded systems with high accuracy and low computational overhead, outperforming traditional thresholding methods without requiring hardware changes.", "motivation": "AES-128 faces practical security threats via timing and fault injection attacks in embedded systems. Existing protections either require hardware modifications or lack real-time feasibility, necessitating a lightweight, non-invasive solution.", "method": "A lightweight framework combining statistical thresholding on execution time and a Random Forest classifier trained on block-level anomalies from simulated delays/ciphertext corruption. Implemented on CPU and FPGA (PYNQ-Z1) with real-time capabilities.", "result": "The ML approach achieved higher accuracy than static thresholds while maintaining real-time performance on embedded platforms. The framework operates without hardware performance counters or AES modification, validated on PYNQ-Z1 FPGA.", "conclusion": "The proposed dual-detection framework is suitable for low-power, resource-constrained systems, offering accurate and computationally efficient anomaly detection without modifying AES internals or hardware counters."}}
{"id": "2508.21654", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.21654", "abs": "https://arxiv.org/abs/2508.21654", "authors": ["Daryna Oliynyk", "Rudolf Mayer", "Kathrin Grosse", "Andreas Rauber"], "title": "I Stolenly Swear That I Am Up to (No) Good: Design and Evaluation of Model Stealing Attacks", "comment": "Under review", "summary": "Model stealing attacks endanger the confidentiality of machine learning\nmodels offered as a service. Although these models are kept secret, a malicious\nparty can query a model to label data samples and train their own substitute\nmodel, violating intellectual property. While novel attacks in the field are\ncontinually being published, their design and evaluations are not standardised,\nmaking it challenging to compare prior works and assess progress in the field.\nThis paper is the first to address this gap by providing recommendations for\ndesigning and evaluating model stealing attacks. To this end, we study the\nlargest group of attacks that rely on training a substitute model -- those\nattacking image classification models. We propose the first comprehensive\nthreat model and develop a framework for attack comparison. Further, we analyse\nattack setups from related works to understand which tasks and models have been\nstudied the most. Based on our findings, we present best practices for attack\ndevelopment before, during, and beyond experiments and derive an extensive list\nof open research questions regarding the evaluation of model stealing attacks.\nOur findings and recommendations also transfer to other problem domains, hence\nestablishing the first generic evaluation methodology for model stealing\nattacks.", "AI": {"tldr": "This paper addresses evaluation standardization gaps in model stealing attacks by providing the first comprehensive framework and methodology for comparing image classification attacks while identifying key research directions.", "motivation": "The lack of standardized design and evaluation of model stealing attacks hinders comparison of prior works and assessment of progress, prompting the need for a unified methodology.", "method": "The authors study substitution model attacks against image classification models, propose a comprehensive threat model, develop a framework for attack comparison, analyze existing attack setups, and derive best practices for attack development.", "result": "The paper introduces a framework for attack comparison, identifies most-studied tasks/models in the field, presents best practices for attack development stages, and highlights open research questions transferable to other domains.", "conclusion": "The paper establishes the first generic evaluation methodology for model stealing attacks, providing standardized frameworks and best practices to improve comparison and progress in the field."}}
{"id": "2508.21669", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.21669", "abs": "https://arxiv.org/abs/2508.21669", "authors": ["V\u00edctor Mayoral-Vilches", "Per Mannermaa Rynning"], "title": "Cybersecurity AI: Hacking the AI Hackers via Prompt Injection", "comment": null, "summary": "We demonstrate how AI-powered cybersecurity tools can be turned against\nthemselves through prompt injection attacks. Prompt injection is reminiscent of\ncross-site scripting (XSS): malicious text is hidden within seemingly trusted\ncontent, and when the system processes it, that text is transformed into\nunintended instructions. When AI agents designed to find and exploit\nvulnerabilities interact with malicious web servers, carefully crafted reponses\ncan hijack their execution flow, potentially granting attackers system access.\nWe present proof-of-concept exploits against the Cybersecurity AI (CAI)\nframework and its CLI tool, and detail our mitigations against such attacks in\na multi-layered defense implementation. Our findings indicate that prompt\ninjection is a recurring and systemic issue in LLM-based architectures, one\nthat will require dedicated work to address, much as the security community has\nhad to do with XSS in traditional web applications.", "AI": {"tldr": "AI cybersecurity tools vulnerable to 'prompt injection' attacks via malicious web servers; paper demonstrates exploits against CAI framework and proposes layered defenses against this XSS-like threat.", "motivation": "AI-powered vulnerability hunters face novel risks when interacting with hostile servers, as attackers can hijack their execution flow via adversarial prompts.", "method": "Demonstrated proof-of-concept exploits against the Cybersecurity AI (CAI) framework and CLI tool, combined with multi-layered defense mechanisms.", "result": "Successfully exploited CAI framework through crafted responses, validated systemic vulnerabilities, and proposed effective multi-layered mitigations.", "conclusion": "Prompt injection is a systemic issue in LLM-based cybersecurity AI, requiring proactive defenses similar to how XSS was addressed in web applications."}}
{"id": "2508.21727", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21727", "abs": "https://arxiv.org/abs/2508.21727", "authors": ["Jiazheng Xing", "Hai Ci", "Hongbin Xu", "Hangjie Yuan", "Yong Liu", "Mike Zheng Shou"], "title": "OptMark: Robust Multi-bit Diffusion Watermarking via Inference Time Optimization", "comment": null, "summary": "Watermarking diffusion-generated images is crucial for copyright protection\nand user tracking. However, current diffusion watermarking methods face\nsignificant limitations: zero-bit watermarking systems lack the capacity for\nlarge-scale user tracking, while multi-bit methods are highly sensitive to\ncertain image transformations or generative attacks, resulting in a lack of\ncomprehensive robustness. In this paper, we propose OptMark, an\noptimization-based approach that embeds a robust multi-bit watermark into the\nintermediate latents of the diffusion denoising process. OptMark strategically\ninserts a structural watermark early to resist generative attacks and a detail\nwatermark late to withstand image transformations, with tailored regularization\nterms to preserve image quality and ensure imperceptibility. To address the\nchallenge of memory consumption growing linearly with the number of denoising\nsteps during optimization, OptMark incorporates adjoint gradient methods,\nreducing memory usage from O(N) to O(1). Experimental results demonstrate that\nOptMark achieves invisible multi-bit watermarking while ensuring robust\nresilience against valuemetric transformations, geometric transformations,\nediting, and regeneration attacks.", "AI": {"tldr": "OptMark is an optimization-based watermarking method for diffusion-generated images that embeds a robust multi-bit watermark into the intermediate latents. It uses early structural and late detail watermark insertion with tailored regularization, and adjoint gradient methods to optimize memory usage, achieving imperceptible and resilient watermarking against various attacks.", "motivation": "Current diffusion watermarking approaches either have limited capacity for user tracking (zero-bit) or lack robustness against transformations and generative attacks (multi-bit). There is a need for a comprehensive solution with high capacity, visibility, and resilience without excessive memory consumption.", "method": "OptMark embeds watermarks into intermediate latents of the diffusion denoising process. It strategically inserts structural watermarks early (to resist generative attacks) and detail watermarks late (to withstand transformations) with regularization terms for image quality and imperceptibility. Adjoint gradient methods reduce memory usage from O(N) to O(1) during optimization.", "result": "Experiments show OptMark achieves imperceptible multi-bit watermarking while demonstrating robust resilience against valuemetric transformations, geometric transformations, editing, and regeneration attacks. The adjoint method successfully addresses memory consumption challenges.", "conclusion": "OptMark overcomes limitations of existing diffusion watermarking by combining optimization-based structural/detail watermark design with memory-efficient adjoint gradients, providing a robust and scalable solution for copyright protection and user tracking in diffusion-generated images."}}
